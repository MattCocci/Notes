\BOOKMARK [1][-]{section.1}{Statistical Inference: From Classical to Bayesian}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Statistical Inference}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Classical Statistics}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{From Classical to Bayesian Paradigm}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Summarizing the Bayesian Approach}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Prediction}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.6}{Likelihood and Odds Ratios}{section.1}% 7
\BOOKMARK [2][-]{subsection.1.7}{Advantages and Disadvantages of the Bayesian Approach}{section.1}% 8
\BOOKMARK [1][-]{section.2}{Prior Distributions}{}% 9
\BOOKMARK [2][-]{subsection.2.1}{Conjugate Priors}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.2}{Other Classes of Priors}{section.2}% 11
\BOOKMARK [1][-]{section.3}{Single Parameter Models}{}% 12
\BOOKMARK [2][-]{subsection.3.1}{Binomial Data with Conjugate Beta Prior}{section.3}% 13
\BOOKMARK [3][-]{subsubsection.3.1.1}{Likelihood}{subsection.3.1}% 14
\BOOKMARK [3][-]{subsubsection.3.1.2}{Conjugate Prior and Special Cases}{subsection.3.1}% 15
\BOOKMARK [3][-]{subsubsection.3.1.3}{Posterior Distribution}{subsection.3.1}% 16
\BOOKMARK [3][-]{subsubsection.3.1.4}{Posterior Inference}{subsection.3.1}% 17
\BOOKMARK [2][-]{subsection.3.2}{Poisson Data}{section.3}% 18
\BOOKMARK [3][-]{subsubsection.3.2.1}{Likelihood}{subsection.3.2}% 19
\BOOKMARK [3][-]{subsubsection.3.2.2}{Conjugate Prior and Special Cases}{subsection.3.2}% 20
\BOOKMARK [3][-]{subsubsection.3.2.3}{Posterior Distribution}{subsection.3.2}% 21
\BOOKMARK [3][-]{subsubsection.3.2.4}{Extension: Rate, Exposure Modification}{subsection.3.2}% 22
\BOOKMARK [2][-]{subsection.3.3}{Normal Data \(Known Variance\)}{section.3}% 23
\BOOKMARK [3][-]{subsubsection.3.3.1}{Likelihood}{subsection.3.3}% 24
\BOOKMARK [3][-]{subsubsection.3.3.2}{Conjugate Prior and Resulting Posterior}{subsection.3.3}% 25
\BOOKMARK [3][-]{subsubsection.3.3.3}{Jeffrey's Prior}{subsection.3.3}% 26
\BOOKMARK [3][-]{subsubsection.3.3.4}{Posterior Predictive Distribution}{subsection.3.3}% 27
\BOOKMARK [2][-]{subsection.3.4}{Normal Data \(Known Mean, Unknown Variance\)}{section.3}% 28
\BOOKMARK [1][-]{section.4}{Multiparameter Models}{}% 29
\BOOKMARK [2][-]{subsection.4.1}{Univariate Normal Model}{section.4}% 30
\BOOKMARK [3][-]{subsubsection.4.1.1}{Likelihood}{subsection.4.1}% 31
\BOOKMARK [3][-]{subsubsection.4.1.2}{Conjugate Prior and Resulting Posterior}{subsection.4.1}% 32
\BOOKMARK [3][-]{subsubsection.4.1.3}{Non-Informative Prior}{subsection.4.1}% 33
\BOOKMARK [3][-]{subsubsection.4.1.4}{Semi-Conjugate Prior}{subsection.4.1}% 34
\BOOKMARK [2][-]{subsection.4.2}{Multivariate Normal Model}{section.4}% 35
\BOOKMARK [1][-]{section.5}{Bayesian Regression}{}% 36
\BOOKMARK [2][-]{subsection.5.1}{Likelihood for Simple and Multiple Regression}{section.5}% 37
\BOOKMARK [2][-]{subsection.5.2}{Flat Prior}{section.5}% 38
\BOOKMARK [2][-]{subsection.5.3}{Informative Prior with iid j }{section.5}% 39
\BOOKMARK [2][-]{subsection.5.4}{Extensions}{section.5}% 40
\BOOKMARK [2][-]{subsection.5.5}{Optimization}{section.5}% 41
\BOOKMARK [2][-]{subsection.5.6}{Optimization for Non-Conjugate Priors}{section.5}% 42
\BOOKMARK [1][-]{section.6}{Mixture Models}{}% 43
\BOOKMARK [2][-]{subsection.6.1}{Likelihood as a Mixture of Two Models}{section.6}% 44
\BOOKMARK [2][-]{subsection.6.2}{Expectation Maximization \(EM\) Algorithm}{section.6}% 45
\BOOKMARK [1][-]{section.7}{Hierarchical Models}{}% 46
\BOOKMARK [2][-]{subsection.7.1}{General Form}{section.7}% 47
\BOOKMARK [2][-]{subsection.7.2}{Normal Model}{section.7}% 48
\BOOKMARK [2][-]{subsection.7.3}{Normal Model with the EM Algorithm}{section.7}% 49
\BOOKMARK [2][-]{subsection.7.4}{Binomial Hierarchical Model}{section.7}% 50
\BOOKMARK [1][-]{section.8}{Markov Chain Monte-Carlo \(MCMC\) Algorithms}{}% 51
\BOOKMARK [2][-]{subsection.8.1}{Gibbs Sampler}{section.8}% 52
\BOOKMARK [2][-]{subsection.8.2}{Metropolis Algorithm}{section.8}% 53
\BOOKMARK [2][-]{subsection.8.3}{Metropolis-Hastings Algorithm}{section.8}% 54
\BOOKMARK [2][-]{subsection.8.4}{Gibbs Sampler as a Special Case of the M-H Algorithm}{section.8}% 55
\BOOKMARK [1][-]{section.9}{EM Algorithm}{}% 56
\BOOKMARK [1][-]{appendix.A}{Finding Parameters for and Drawing from Non-Standard Distributions}{}% 57
\BOOKMARK [2][-]{subsection.A.1}{Grid Method in One Dimension}{appendix.A}% 58
\BOOKMARK [2][-]{subsection.A.2}{Grid Method in Two Dimensions}{appendix.A}% 59
\BOOKMARK [2][-]{subsection.A.3}{Newton's Method in One Dimension}{appendix.A}% 60
\BOOKMARK [2][-]{subsection.A.4}{Newton's Method in Multiple Dimensions}{appendix.A}% 61
\BOOKMARK [2][-]{subsection.A.5}{Gradient Descent}{appendix.A}% 62
\BOOKMARK [1][-]{appendix.B}{Parametric Bootstrap}{}% 63
\BOOKMARK [1][-]{appendix.C}{Jacobian Transformation}{}% 64
