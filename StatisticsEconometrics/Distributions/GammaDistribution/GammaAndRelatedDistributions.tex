\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Gamma Distribution and Special Cases}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim}

\begin{document}

\begin{center}
   \LARGE
   \textbf{Gamma Distribution and Special Cases}
\end{center}
\section{General Gamma Function}

The gamma function has many special cases, but let's consider it 
in its most general form, along with the mean and variance:
\begin{equation}
   f_X(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1}
      e^{-\beta x}, \qquad x > 0
\end{equation}
\[ \text{Mean} = \frac{\alpha}{\beta} \qquad \text{Variance} = 
   \frac{\alpha}{\beta^2} \]
Gamma random variables also have the nice property that
\[ X_i \sim \text{Gamma}(\alpha_i, \;\beta) \quad \Rightarrow \quad
   \sum^n_{i=1} X_i \sim \text{Gamma}\left(\sum^n_{i=1} \alpha_i,\;\beta
   \right) \]

\section{Special Case: Exponential Random Variable}

This family has $\alpha=1$ with $\beta$ unrestricted. Changing the way
we denote the variables (set $\beta =\theta$), this simplifies to
   \[ f_X(x) = \theta\; e^{-\theta} \]
This is particularly useful for waiting times, as it is a memoryless
distribution. In addition, as it is technically a Gamma random
variable, sums of exponential variables will have a Gamma distribution:
\[ X_i \sim \text{Exp}(\theta) \quad \Rightarrow \quad 
   \sum^n_{i=1} X_i \sim \text{Gamma}(n, \theta) \]
Another attraction is the intimate connection this distribution has
with the Poisson process.  Namely, suppose $N(t)$ is a Poisson process
with parameter $\theta$, in which case $N(t)$ is Poisson distributed
with parameter $\theta t$, then the interarrival times are 
exponentially distributed with parameter $\theta$. 


\section{Special Case: Chi-Squared Random Variable}

\subsection{Density Function and Descriptive Statistics}
Suppose that $Y$ has a $\chi^2$ distribution with $\nu$ degrees
of freedom. Then the distribution is a special case of the gamma 
distribution, where $\alpha = \nu/2$ and $\beta = 1/2$ with the pdf
and descriptive statistics
\begin{equation}
   f_Y(y) = \frac{1}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)}
      x^{\frac{\nu}{2} - 1} e^{-\frac{x}{2}}
\end{equation}
   \[ \text{Mean} = \nu, \qquad \text{Variance} = 2 \nu \]

\subsection{Properties}

The $\chi^2$ distribution has several nice properties that we'll
want to discuss:
\begin{itemize}
   \item[-] If $Y$ is chi-squared distributed with $\nu$ degrees of
      freedom ($Y\sim \chi^2(\nu)$), then we can generate $Y$ by
	 \[ Y = \sum^\nu_{i=1} Z^2_i \]
      where $Z_i$ is a standard, normal random variable.
   \item[-] By the way we just defined a $\chi^2$ RV, it's immediately
      clear that
	 \[ Y_i \sim \chi^2(\nu_i) \quad \Rightarrow \quad
	    \sum^n_{i=1} Y_i \sim \chi^2\left(\sum^n_{i=1} \nu_i \right)
	    \]
      So sums of $\chi^2$ RVs are also $\chi^2$ distributed.
   \item[-] Suppose that $Y_i \sim \text{NID}(\mu,\sigma^2)$. Then 
      \[ \sum^n_{i=1} \frac{(Y_i-\mu)^2}{\sigma^2}  \sim \chi^2(n) \]
   \item[-] Now let's consider a similar situation, but where
      only the sample mean is avalable.
      \begin{align*}
	 \sum^n_{i=1} \frac{(Y_i-\mu)^2}{\sigma^2} &= \frac{1}{
	    \sigma^2} \sum^n_{i=1} (Y_i - \bar{Y}+ \bar{Y}-\mu)^2 \\
	    &= \frac{1}{\sigma^2} \sum^n_{i=1}  (Y_i - \bar{Y})^2
	    + \frac{2}{\sigma^2} \sum^n_{i=1} (Y_i-\bar{Y})(\bar{Y}-\mu)
	    + \frac{1}{\sigma^2} \sum^n_{i=1} (\bar{Y}-\mu)^2 \\
	    &= \frac{1}{\sigma^2} \sum^n_{i=1}  (Y_i - \bar{Y})^2 
	    + \frac{2(\bar{Y}-\mu)}{\sigma^2} \sum^n_{i=1} (Y_i-\bar{Y})
	    + \frac{n(\bar{Y}-\mu)^2 }{\sigma^2}  \\
	 &= \sum^n_{i=1}\frac{(Y_i - \bar{Y})^2 }{\sigma^2} +
	    \left(\frac{\bar{Y}-\mu }{\sigma /\sqrt{n}}\right)^2
      \end{align*}
      Now it's clear that the left hand side is a sum of squared normal
      random variables, so it has a $\chi^2(n)$ distribution. On
      the right hand side, the second term has a $\chi^2(1)$
      distribution, as it is a squared normal random variable. We
      can rearrange to assert that
	 \[ \sum^n_{i=1}\frac{(Y_i - \bar{Y})^2 }{\sigma^2}  \sim
	    \chi^2(n-1) \]
\end{itemize}





%%%% APPPENDIX %%%%%%%%%%%

% \appendix


\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
