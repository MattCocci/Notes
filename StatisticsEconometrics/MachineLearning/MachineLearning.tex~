\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Machine Learning}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black,
}
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim}

\begin{document}
\maketitle

%\tableofcontents %adds it here

\section{Machine Learning Problems}

There are two main types of learning in the field of machine learning:
\begin{enumerate}

   \item Supervised Learning: In this case, we know the right answer
      to our problem.  For example, we might have data on house prices
      and a number of covariates, including bedrooms, age, location,
      etc.  From there, we try to fit a model that best predicts price
      using the covariates, and we can check the fit of our model 
      against the observed values.

   \item Unsupervised Learning: Here, you don't know the right answer.
      For example, you might want to group a set of customers
      into clusters; however, there's no known right answer that you can
      use to check the quality of your clustering model.

\end{enumerate}
These learning methods are applied to serve two main types of 
learning problems:
\begin{enumerate}
   \item Regression Problems: The predicted or response variable
      takes on continuous values.
   \item Classification Problems: Each observation or training example
      belongs to some category, and must be classified. This might
      be supervised (like a binary dead-alive type problem) or 
      unsupervised (like clustering).
\end{enumerate}



\section{Terminology}

In order to calibrate an algorithm, we use a set of 
\textbf{training examples} (the data), denoted 
$(\mathbf{x}^{(1)}, y^{(1)}), 
(\mathbf{x}^{(2)}, y^{(2)}), \ldots, (\mathbf{x}^{(m)}, y^{(m)})$, 
where $(\mathbf{x}^{(i)}, y^{(i)})$ is the $i$th training example. Each 
training example consists of a vector of $n$ \textbf{features} 
(covariates)
   \[ \mathbf{x}^{(i)} = \left(x_1^{(i)},x_2^{(i)},\ldots, 
      x_n^{(i)}\right)^T\]
We use these training examples to fit the parameters of
a \textbf{hypothesis} (model), denoted $h_\theta(\mathbf{x})$. If we
consider a simple linear model and that might look like 
   \[ h_\theta(x) = \theta_0 + \theta_1 x\]
If we have a multivariate linear model, that would look like
\begin{align*}
   h_\theta(\mathbf{x}) &= \theta^T \mathbf{x}\\
   \mathbf{x} &= (1, x_1, x_2, \ldots, x_n)^T \in \mathbb{R}^{n+1}, 
      \qquad x_0 = 1\\
   \theta &= (\theta_0, \theta_1, \theta_2, \ldots, \theta_n)^T
   \in \mathbb{R}^{n+1}
\end{align*}




\section{Gradient Descent}

Again, suppose we want to find point estimates of the components in
a vector $\theta = (\theta_1, \theta_2, \ldots, \theta_n)$ by
minimizing some cost function, $\mathcal{J}(\theta)$.\footnote{We
can easily make the necessary changes if we want to maximize. In
particular, we can minimize the negative of some target function. (The
name ``cost function'' isn't really appropriate if we're trying to 
maximize.)} Then we update our parameters all at once as follows:
\begin{align*}
   \theta_1^{(t+1)} &= \theta_1^{(t)} - \alpha \frac{\partial}{
      \partial \theta_1^{(t)}}\left[ \mathcal{J}(\theta)\right]\\
   \vdots \qquad & \qquad \vdots \\
   \theta_n^{(t+1)} &= \theta_n^{(t)} - \alpha \frac{\partial}{
      \partial \theta_n^{(t)}}\left[ \mathcal{J}(\theta)\right]
\end{align*}
where $\alpha$ is the learning rate. This parameter controls the
size of our steps and influences the aggressiveness of our 
descent. So use this process to keep updating and iterating 
until convergence.  

\paragraph{Note} At each iteration, we update all the components of 
$\theta$ \emph{simultaneously}. That is, the equations above are a 
package deal. We don't update one parameter at a time as we did
with Gibbs Sampling. Instead, we take all the partial derivatives
of the cost function, $\mathcal{J}(\theta)$, then plug into the RHS
of the above expressions.
\\
\\
{\sl Feature Scaling} If we have multiple features, the gradient
of descent may need some help to converge more quickly.  In particular,
if the features can take on ranges of values that are very 
different---i.e. $x_1^{(i)} \in (1,3)$ and $x_2^{(i)} 
\in (500,10000)$---then our contours will appear very elongated and the
gradient descent algorithm may converge slowly.  To fix this, we might
want to normalize the data and make each feature into a z-score or
apply some other reasonable normoalization.
\\
\\
{\sl Choice of $\alpha$:} First, notice how this process 
self-adjusts in the
magnitude of the update.  In particular, as you get close to
the minimum, the derivatives approach zero.  So even with 
$\alpha$ constant, the shrinking derivative means that the gradient
descent algorithm will take smaller steps, which is desireable.
To choose a particular value of $\alpha$, you face a tradeoff. Very
small values of $\alpha$ will always allow the gradient descent 
algorithm to converge, but it will do so very slowly.  Larger values
of $\alpha$ risk overshooting the local minimum and may cause the 
algorithm to diverge.

\section{Multiple Regression}

Recall the \emph{normal equation} for multiple regression, which
in matrix notation, says that parameter estimates should be
   \[ \theta = (X^T X)^{-1} X^T y\]
where $X$ will be an $m\times n+1$ matrix for $m$ training examples
and $n$ features.  The first feature of our $n+1$ features in $X$
is a 1 for each training example to include a constant in our 
regression.
\textbf{Note} 
that we must have $m\geq n$---that is the number of training
examples greater than the number of features---in order to invert
the matrix in parentheses. Otherwise, the matrix is
\emph{singular} (non-invertible).
\\
\\
The typical cost function that we want to minimize (over $m$
training examples) is:
\begin{equation}
   \label{cost}
   \mathcal{J}(\theta) = \frac{1}{2} \sum^m_{i=1} (h_\theta(x^{(i)}) - 
      y^{(i)})^2 
\end{equation}


\section{Logistic Regression}

\subsection{Single-Class Problems}

Recall that the response variable can take on one of two 
values---typically positive or negative, or $y \in \{0,1\}$.
This is a classification problem where we want to assign observations
(or training examples) to a group based on features. We set our 
hypothesis as follows:
\begin{align*}
   h_\theta(x) &= \frac{1}{1+e^{-\theta^T \; x}}
\end{align*}
where $\theta^T$ is a vector of $n+1$ parameters and $x$ is a vector
of $n+1$ features.
The term on the right is called the \emph{logistic} or \emph{sigmoid}
function, and it is guaranteed to lie between 0 and 1.
The resulting output from $h_\theta(x)$ is a conditional probability 
that an observation is in the 1 group given the observed features.
\\
\\
{\sl Cost Function:} It turns out that the cost function we used
for multiple regression in Equation \ref{cost} will be 
\emph{non-convex}, so the gradient descent algorithm will get stuck
at local modes.  This leads us to modify our cost function in 
line with maximum likelihood so that
the cost for a particular training example is
\begin{align}
    \text{Cost}(h_\theta(x),\;y) &= \begin{cases}
      -\ln h_\theta(x) & \text{if $y=1$} \\
      -\ln (1-h_\theta(x)) &\text{if $y=0$} \end{cases}\notag\\
    &=-y \ln h_\theta(x)- (1-y) \ln(1-h_\theta(x))\\
    \Rightarrow \quad
    \mathcal{J}(\theta) &= -\frac{1}{m}\sum^m_{i=1}\left[
    y^{(i)} \ln h_\theta(x^{(i)}) +(1-y^{(i)})\ln (1-h_\theta(x^{(i)})
    )\right]
\end{align}


\subsection{Multi-Class Problems}

Suppose that we want to generalize single-class logistic regression
to the case of multiple groups. Specifically, suppose that each
$y^{(i)}$ can belong to groups $\{1,2,\ldots,k\}$. Then the
\emph{one-vs-all} approach says
\begin{enumerate}
   \item Run $k$ different logistic regressions, where you take one 
      group, $i$, and treat it as the posive case. The remaining
      $k-1$ groups are treated as one big negative group.
   \item Using your training examples 
      to fit a hypothesis for each such logistic regression so that 
      there are $k$ hypotheses in total: $h_\theta^{(i)}(x), \ldots,
      h_\theta^{(k)}(x)$. 
   \item Then, given some covariates, $x$, you predict the group
      for this observation to be
      \[ \max_i \{ h_\theta^{(1)}(x), \ldots, h_\theta^{(k)}(x)\} \]
      i.e. group $i$, the group that has the maximum chance
      of containing the observation.
\end{enumerate}



\section{Regularization}

In order to avoid the problem of overfitting, we typically apply
\emph{regularization}, where we penalize large values of the
parameters we're trying to fit by adding a term to the cost
function. This means we want to find
\begin{equation}
   \label{regularized}
   \arg \min_\theta \mathcal{J}(\theta) = f\left(h_\theta(x), y\right)
   + g(\theta)
\end{equation}
where $\theta = (\theta_1, \ldots, \theta_n)$ and where $x$ and
$y$ are a vector of $m$ training examples. Note that 
$\theta_0$---the conventional constant parameter---is not included
in the regularization.
\\
\\
As an example of a standard form for Equation \ref{regularized}, 
we might choose
\begin{equation}
   \label{regstd}
   \arg \min_\theta \mathcal{J}(\theta) = \arg\min \;
   \frac{1}{2m}\left[ \sum^m_{i=1} \left(h_\theta(x^{(i)})-y^{(i)}
   \right)^2 + \lambda \sum^n_{j=1} \theta_j^2
   \right]
\end{equation}


\subsection{Implementation by Normal Equations, Multiple Regression}

Suppose we consider the problem of multiple regression, and we
need to choose the regression parameters with the regularization
and cost function in Equation \ref{regstd}. Then the normal equations
are modified to
\begin{equation}
   \label{normreg}
   \theta = \left(X^T X + \lambda R\right)^{-1}  X^T y
\end{equation}
where $R$ is nearly the identity matrix $I_n$, but with entry 
$R_{1,1}=0$ instead of 1.
\\
\\
Note that \emph{without} regularization
we must have $m\geq n$---that is the number of training
examples greater than the number of features. Otherwise the 
matrix in parentheses is \emph{singular} (non-invertible). 
\textbf{But} if we choose $\lambda>0$ and apply regularization,
then the matrix in parentheses is \emph{guaranteed} to 
be invertible, even if $m < n$.


\subsection{Implementation by Gradient Descent, Multiple 
Regression}

If this were used in an implementation of gradient descent, this 
would yield an updating rule of:
\begin{align*}
   \theta_0 &:= \theta_0 - \alpha \; \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_0^{(i)}\\
   \theta_j &:= \theta_j - \alpha \left[ \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_j^{(i)} +
   \frac{\lambda}{m} \theta_j \right]\\
   &:= \theta_j\left(1-\alpha \frac{\lambda}{m}\right) - \alpha \;
   \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_j^{(i)} 
\end{align*}
where $j=1, \ldots, n$ and $h_\theta(x)$ is the standard
multiple regression form. 
It's clear that the first term is $\theta_j$
times a constant less than 1, so we're \emph{shrinking} the 
parameter towards zero, then the second term updates as before the
shrunken parameter.
   

\subsection{Implementation by Gradient Descent, Logistic 
Regression}

If we want to implement a regularized logistic regression, we'll take
the cost function to be
\begin{equation}
   \label{logreg}
   \mathcal{J}(\theta) = -\left[\frac{1}{m} \sum_{i=1}^m
   y^{(i)} \ln h_\theta(x^{(i)}) + (1-y^{(i)}) \ln \left(1-
   h_\theta(x^{(i)})\right)\right] + \frac{\lambda}{2m} \sum^n_{j=1}
   \theta_j^2
\end{equation}
Given this cost function, the update rule for gradient descent
becomes
\begin{align*}
   \theta_0 &:= \theta_0 - \alpha \; \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_0^{(i)}\\
   \theta_j &:= 
      \theta_j - \alpha \left[ \frac{1}{m} \sum^m_{i=1}
      \left( h_\theta(x^{(i)})-y^{(i)}\right) x_j^{(i)} +
      \frac{\lambda}{m} \theta_j \right]
\end{align*}
where $j=1,\ldots,n$.  Now this looks very similar to the update
rule for multiple regression, except it doesn't make explicit
the different hypothesis functions.  In particular, for logistic
regression, we have
\[ h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} \]


\newpage
\section{Neural Networks}

This is useful for classification problems involving a high
number of features.  We saw with logistic regression that including
different powers and cross-products of the features can give a 
better fit for the classification problem, fitting circles and ellipses
to the data; \emph{however},
while this works well for, say, two features, the parameter 
optimization problem grows very quickly with more features.  
\\
\\
Neural networks are also very useful because they can represent
highly non-linear functions and classifications.  In particular,
they can even be used to construct logical decision-making and
classification.

\subsection{Model Representation}

Here's the basic idea:
between the \emph{input layer} (the features) and the 
\emph{output layer} (the hypothesis), we introduce at least one
\emph{hidden layer} transforming the inputs into new variables 
that will be used in the hypothesis. This forms the 
\emph{architecture} of the network.
\\
\\
We introduce the following notation to be used in computing/evaluating
layers,\footnote{ 
Don't take ``activation'' and ``weights'' too seriously.  ``Activation''
just indicates the \emph{value} computed and output by a specific
layer or unit in the layer. In addition, ``weights'' are just
the name for parameters in the neural networks literature.}
\begin{align*}
   a^{(j)}_i &= \text{the ``activation'' of unit $i$ in layer $j$} \\
   \Theta^{(j)} &= \text{matrix of weights controlling function}\\
   &\qquad \text{mapping from layer $j$ to $j+1$}
\end{align*}
Now suppose there are $n$ features, $L$ hidden layers, and $s_j$
units in layer $j$. Note that in each layer, we'll typically introduce 
a constant term, $x_0$ (for the features) or $a^{(j)}_0$ in
the intermediate layers. This is commonly called the ``bias unit.''
\begin{align*}
   a^{(2)}_i &= f^{(1)}_i\left(x_0, \ldots, x_n,
      \Theta_{i0}^{(1)},\ldots,\Theta_{in}^{(1)}\right), \qquad
      i=1,\ldots,n 
   \\	\\
   a^{(j+1)}_i &= f^{(j)}_i\left(a_0^{(j)}, \ldots, 
      a_{s_j}^{(j)}, \;
      \Theta_{i0}^{(j)},\ldots,\Theta_{is_j}^{(j)}\right), \qquad
      \begin{cases}
	 i=1,\ldots,s_{j+1} \\  j = 1, \ldots, L-1 \end{cases}
      \\
      \\
   h_\Theta(x) = 
      a^{(L)}_1 &= f^{(L-1)}_1\left(a_0^{(L-1)}, 
      \ldots, 
      a_{s_{L-1}}^{(L-1)}, \;
      \Theta_{i0}^{(L-1)},\ldots,\Theta_{is_{L-1}}^{(L-1)}
      \right)
\end{align*}
It follows, by our definition of $s_j$ and $\Theta^{(j)}$, that
$\Theta^{(j)}$ is of dimension $s_{j+1} \times (s_j + 1)$.
I know; there's a lot of notation. But basically, it says each unit
in the first layer is some function of the features and parameters.
Then the activiation of all units in subsequent layers are functions of 
the activations of units in the immediately previous layer. This
goes on until you get to your hypothesis, and this process is called 
\emph{forward propogation}.

\newpage
\subsection{Vectorized Implementation}

Here, we simplify notation a bit and take the special case where the
function of the attributes, $g(\cdot)$, is the sigmoid
function. So we define the stage 2 attributes as follows
\begin{align*}
    a_1^{(2)} &= g\left(\Theta_{10}^{(1)} x_0 
	+ \Theta_{11}^{(1)} x_1 + \cdots 
	+ \Theta_{1n}^{(1)} x_n\right) = g\left(z_1^{(2)}\right)
    \\ 
    \vdots \quad & \qquad \vdots\\
    a_{s_2}^{(2)} &= g\left(\Theta_{s_2 0}^{(1)} x_0 
	+ \Theta_{s_2 1}^{(1)} x_1 + \cdots 
	+ \Theta_{s_2 n}^{(1)} x_n\right) 
	= g\left(z_{s_2}^{(2)}\right)
\end{align*}
Now let's vectorize, redefining the vector $x$ of dimension 
$(n+1)\times1$ (with one more row than the number of features for the 
extra constant term) as follows to make notation
simpler: $x = a^{(1)}$. Next, we define $z^{(j)}$ as a vector of 
dimension $s_j \times 1$, where $s_j$ is the number of attributes in 
layer $j$. We do the following steps to compute the hypothesis:
\begin{enumerate}
    \item Compute $z^{(2)} = \Theta^{(1)} x = \Theta^{(1)} a^{(1)}$,
	then use $z^{(2)}$ to compute $a^{(2)} = g(z^{(2)})$.
    \item Prepend $a_0^{(2)}=1$ to $a^{(2)}$ to allow for a 
	constant/bias term, then compute $z^{(3)} = \Theta^{(2)}a^{(2)}$.
    \item Continue this process, prepending $a_0^{(j)}=1$ to 
	$a^{(j)}$, computing $z^{(j+1)} = \Theta^{(j)} a^{(j)}$ for
	all $j$ number of layers until you reach $h_\Theta(x)$, a 
	process called \emph{forward propagation}.
\end{enumerate}


\subsection{Cost Function for a Classfication Neural Network}

{\sl Binary Classification:} In this special case, $y \in \{ 0, 1\}$
and there is 1 output unit that computes $h_\Theta(x) \in \mathbb{R}$.
So $s_L = 1$, where $s_\ell$ is the number of units in layer $\ell$.
\\
\\
{\sl Multi-Class Classification:} In this more general case, 
$y\in \mathbb{R}^K$ where there are $K$ distinct classes. 
For ex., $y$ could be one of four classes (pedestrian,
car, motorcycle, truck), denoted
\[ y = \begin{bmatrix} 1 & 0 & 0 & 0 \end{bmatrix}, \qquad 
   \begin{bmatrix} 0 & 1 & 0 & 0 \end{bmatrix}, \qquad
   \begin{bmatrix} 0 & 0 & 1 & 0 \end{bmatrix}, \qquad\text{or}\qquad
   \begin{bmatrix} 0 & 0 & 0 & 1 \end{bmatrix} \]
So there are $K$ output units. Typically, we only use this setup
when $K\geq 3$.
\\
\\
{\sl Cost Function:}
This is a generalization of the cost function for
logistic regression:
\begin{align}
   h_\Theta(x) &\in \mathbb{R}^K \qquad \left(h_\Theta(x)\right)_i = 
   \text{$i$th output} \notag\\
   J(\Theta) &= -\frac{1}{m} \left[\sum^m_{i=1}\sum^K_{k=1} y_k^{(i)} 
   \ln \left(h_\Theta(x^{(i)})\right)_k + \left(1-y_k^{(i)}\right)
   \ln \left(1-\left(h_\Theta(x^{(i)})\right)_k\right)
   \right] \notag\\
   &\qquad + \frac{\lambda}{2m} \sum^{L-1}_{\ell=1} \sum^{s_\ell}_{i=1}
   \sum^{s_{\ell+1}}_{j=1} \left(\Theta^{(\ell)}_{ji}\right)^2
\end{align}
The first term is a simple generalization of the regular
logistic regression cost function---we just sum over the $K$ different 
classes. Note that the next term in $J(\Theta)$ is a summation over 
weights \emph{not} including the constant bias terms (so we don't
include $i=0$).

\newpage
\subsection{Backpropogation Algorithm}

In order to use gradient descent, we need the value of the
cost function, $J(\Theta)$, along with the partial derivatives,
$\frac{\partial J(\Theta)}{\partial \Theta^{(\ell)}_{ij}}$.
\\
\\
%We start by supposing that we have one training example, $(x,y)$.
%Now to compute a hypothesis, we apply \emph{forward propagation}:
%\begin{enumerate}
%   \item We set $a^{(1)} = x$.
%   \item Compute the activation value of the next layer as
%      some function of the first activation layer, adding
%      in the bias terms: $a^{(2)} = f^{(1)}\left( a^{(1)}, \Theta^{(1)}
%      \right)$.
%   \item We continue the propogation until we hit the final layer:
%      \[ a^{(L)} = h_\Theta(x) = f^{(3)}\left( a^{(3)}, \Theta^{(3)}
%      \right) \]
%\end{enumerate}
To compute the derivatives, we use the \emph{backpropogation}
algorithm. Here's the intuition and the impliementation for
multi-class logistic classification:
\begin{enumerate}
   \item At each node, we compute an ``error'' term, 
      \[ \delta^{(\ell)}_j = \text{``error'' of node $j$ in layer
	 $\ell$.} \]
   \item We start with the final node:
      \[ \delta_j^{(L)} = a_j^{(L)} - y_j  
	 = \left(h_\Theta(x)\right)_j - y_j \]
   \item We then move to earlier nodes:
      \[ \delta^{(\ell)}_j = \left(\Theta^{(\ell)}\right)^T
      \delta^{(\ell+1)} .* g'\left(z^{(\ell)}\right), \qquad 
      z^{(\ell+1)} = \Theta^{(\ell)} a^{(\ell)} \]
      where $.*$ is elementwise multiplication.
   \item It then follows from a lengthy proof that
      \[ \frac{\partial J(\Theta)}{\partial \Theta^{(\ell)}_{ij}}
	 = a_j^{(\ell)} \delta_i^{(\ell+1)}  \]


\end{enumerate}




%%%% APPPENDIX %%%%%%%%%%%

% \appendix




\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
