************************************************************************
** Header **************************************************************
************************************************************************

* NOTE: These are a useful bunch of commands to put at the top of any do
* file.  They help ensure a "clean start"

* Clear any loaded dataset from memory
clear

* Close open log files
cap log close

* Set more off, so you don't have to press a key to show more output
set more off

* Open log file to catch output generated by commands below.
* "quiet" (can be abbreviated qui) is useful for suppressing output of
* commands so that less clutter gets printed to a log file.
* I'll use it repeatedly below
quiet log using StataExamples.log, replace

* Turn log file off temporarily until we need it
qui log off

* Turn log file back on
qui log on


*************************************************************************
** Data Important and Prep **********************************************
*************************************************************************

* Import excel files
import excel data.xls, firstrow clear

* Load csv via insheet
insheet using data.csv, comma clear

* Load csv via import
* varnames(1) says row 1 has variable names
import delimited data.csv, delimiter(comma) clear varnames(1)

* Load stata dataset
use Main.dta, clear



* Define label for values of a categorical var and apply label
#delimit ;
capture label define lbl_categoricalvar 1 "lab1"
                                        2 "lab2"
                                        3 "lab3"
                                        4 "lab4"
;
#delimit cr
label val categoricalvar lbl_categoricalvar


*************************************************************************
** sort and bysort ******************************************************
*************************************************************************

* Sort is useful for inspecting your data
* Sorting by year, then d
sort year id
list id year lwage fem blk in 1/100

* Sorting by id, then year
sort id year
list id year lwage fem blk in 1/100

* Use bysort to compute averages "by" some variable, *after* sorting
* This is preferable to looping in Stata when possible because it's fast

  * Compute average wage by year, after sorting
  bysort year: egen avebyyear = mean(lwage)

  * Compute average by id, after sorting
  bysort id: egen avebyid = mean(lwage)

  * Inspect
  sort id
  list id year lwage avebyyear avebyid in 1/100

  sort year
  list id year lwage avebyyear avebyid in 1/100



*************************************************************************
** Tempfiles ************************************************************
*************************************************************************

* Saving Files Temporarily in Stata
* ---------------------------------
* - I don't actually want to save copies of the cps data in Stata
*   format permanently, since I already have the csvs.
* - Rather, I just want to save in Stata format for the purpose of
*   appending the data, doing some analysis, and printing that
*   analysis to a log file, then ditch the stata file
* - So use Stata "tempfile" feature, which allows me to creates/save
*   temporary datasets, which are then deleted automatically after
*   this .do file has finished running

* Declare names of temporary files
tempfile mytempfile

* Save current dataset in Stata format
save `mytempfile'

* Load tempfile
use `mytempfile'



*************************************************************************
** Appending and Merging ************************************************
*************************************************************************

* Appending
append using Appending.dta, generate(appended)


* Load main dataset that will have new data merged into it
use Master.dta, clear
des


* Because each has one observation per id (e.g. one gender per id),
* perform a one-to-one merge on id
merge 1:1 id using Using1.dta


* The _merge variable tells you what were matched (3), what was there in
* the "master"/loaded dataset (here Master.dta) but not the
* "using"/merged-in dataset (1), and vice versa (2)
tab _merge
drop if _merge ~= 3
drop _merge

* If the current dataset has multiple ("many") observations per id (the
* merge variable), and the using dataset has one observation per id,
* perform a "many-to-one merge" matching on id
merge m:1 id using Using2.dta




*************************************************************************
** t-test ***************************************************************
*************************************************************************

* T-Test: Assuming equal and unequal variances
ttest y, by(D)
ttest y, by(D) unequal



*************************************************************************
** Matrices *************************************************************
*************************************************************************

matrix A = J(`Nrow', `Ncol', .)
matrix A[1,1] = `y_mean'
matrix rownames A = rowname1 rowname2 rowname3
matrix list A



*************************************************************************
** Summary Statistics ***************************************************
*************************************************************************

* Percentiles
_pctile y, percentiles(2.5 97.5)

* Summary statistics
tabstat wages, by(year) stats(n mean sd min max)

* Reproduce figure presented in problem set
preserve
collapse (mean) y_mean=y (mean) x_mean=x, by(groupvar)

restore


*************************************************************************
** OLS ******************************************************************
*************************************************************************

* Can regress wage on year
* F-stat in upper righthand corner has arguments/label F(k-1,n-k) where
* k is total number of regressors including constant. Implicitly testing
* whether all coefficients except constant jointly equal to zero.
reg lwage year



*************************************************************************
** Stata Interaction Syntax *********************************************
*************************************************************************

* Treat x as categorical variable: regress y on dummies for each value
* of x, which are generated by i.
reg y i.x


* Include interactions with #
* - x#w will assume that you want to treat x and w as categorical
*   variables, generating dummies for each value and interacting those
*   dummies.
* - x#w is equivalent to i.x#i.w, to i.x#w, to x#i.w, see below
* - Under x#w in the regression table are two values that define the
*   interaction of dummies, indicating which x dummy was interacted with
*   which w dummy
* - Be sure to include baseline effects for x and w as well
reg y x w x#w
reg y x w i.x#i.w
reg y x w i.x#w
reg y x w x#i.w


* - If you'd like to treat a variable as continuous in the interactions
*   (rather than the default of categorical), use c.
* - Of course, if a variable is just a dummy, trying to use c. on it
*   doesn't make a difference
reg y x w x#c.w


* Include interactions *and* baseline effects with ##
* x##w      Assumes both x and w categorical, so includes x dummies, w
*           dummies, and their interaction terms
* x##c.w    Assumes x categorical and w continuous, and includes
*           x dummies, w, and interaction terms generated by x#c.w
reg y x##w
reg y x##c.w


* Can interact multiple terms at once, e.g.
reg lnwage year1985##(fe union hisp nonwh c.ed c.ex c.exsq)


*************************************************************************
** IV Regression ********************************************************
*************************************************************************

* GMM Estimation
* - Overidentified, so weighting matrix matters, can specify
* - Then do overid test
ivregress gmm y (x = z1 z2), robust
estat overid

* 2SLS Estimation
* - Overidentified, so "2sls" option matters and means using the 2sls
*   weighting matrix (which is efficient only under homoskedasticity)
* - Robust standard errors so we don't assume homoskedasticity
* - "first" option shows first stage
ivregress 2sls y (x = z1 z2), robust first

* Report statistics about first stage
estat firststage

* 2SLS Estimation
* - Just identified, so 2sls and GMM give the same answer
* - Robust standard errors so we don't assume homoskedasticity
ivregress gmm  y x1 x2 x3 x4 (x5=z), robust
ivregress 2sls y x1 x2 x3 x4 (x5=z), robust


*************************************************************************
** GMM Estimation *******************************************************
*************************************************************************

gmm (mpg - {b1}*weight - {b0}), instruments(weight) deriv(/b0 = -1) deriv(/b1 = -weight)

gmm (mpg - {b1}*weight - {b0}), instruments(weight)


*************************************************************************
** Time Series Data *****************************************************
*************************************************************************

* tsset gives you access to time series commands, e.g. tsline
tsset year

* Time series plot
tsline lwage




*************************************************************************
** Panel Data and Syntax ************************************************
*************************************************************************

* Tell Stata the variables that index entity and time
xtset id year

* Summarize your data
xtsum


* Running FE Regressions with xtreg.
* The ", fe" will include FE's for each entity (here, each id)
*
* Notice: ed ommitted bc it does not vary across entities. Effect soaked
* up by FE entirely. Said another way, FE and ed colinear.

* FE regression, with unit FE based on id
* - Differencing out FE
* - R2 based on the differenced regression
xtreg lwage ed union, fe

* FE regression, with unit FE based on id
* - Including dummies for each
* - R2 high, based on fulll regresion with dummies
* - Super inefficient
reg lwage ed union i.id

* Can also try to est the model by including dummies for each entity
* But in this dataset, that requires like 500 dummies for each id, which
* requires a huge X matrix, and Stata complains that it can't create a
* matrix that big.
set matsize 400
capture reg lwage ed union i.id


* Can sometimes fix that by increasing the size of matrices Stata can
* construct. But notice that computation is slower.
*
* Also notice
* - Coefficient on union the same as xtreg
* - All FE estimates printed out
* - Coeff on ed now estimated, but only bc one FE got ommitted instead.
* - R2 much higher than xtreg bc of FE, although you haven't really
*   "explained" anything
*
* I'm showing this for interpretation and completeness. But you
* generally never want to do a FE this way. Generally always
* use xtreg instead
set matsize 800
reg lwage ed union i.id
reg lwage union i.id


*************************************************************************
** Nonlinear Least Squares **********************************************
*************************************************************************

capture drop cons
gen cons = 1
nl (y = exp({xb: cons x1 x2})), vce(robust)


*************************************************************************
** Poisson Regression ***************************************************
*************************************************************************

poisson y x1 x2, vce(robust)


*************************************************************************
** Binary Choice Models *************************************************
*************************************************************************

* Baseline QMLE probit estimation
* - Standard errors "robust" to misspecification
* - Stata will treat x1 as continuous and x2 as discrete when using
*   margins command. Necessary to use i. for dummy variables, because
*   margins will not treat them as discrete/categorical unless told to
probit y x1 i.x2 x3 x4, robust

* Marginal effect of x1 at the mean
* - "post" means posting results for later use
* - Marginal effects and std errors stored in _b and _se now
margins , dydx(x1) atmeans post
local me_x1    = _b[x1]
local me_x1_se = _se[x1]

* Average partial effect of x1
* - "post" means posting results for later use
* - Average partial effects and std errors now stored in _b and _se
* - Store marginal effects for each unit in variable me_var
margins , dydx(x1) post gen(me_var)
local ape_x1    = _b[x1]
local ape_x1_se = _se[x1]


* Compute marginal effects with respect to x1 for each x2 group, with
* all other variables at the mean.
forval x2values = 1/`Nvalues' {
  margins, dydx(x1) atmeans at(x2=`x2values')
}



*************************************************************************
** Quantile Regression **************************************************
*************************************************************************


* Quantile regression
qreg wage educ exper expersq, quantile(0.5)


* Estimates and standard errors (obtained via bootstrapping) for
* multiple quantiles
sqreg wage educ, quantiles(`qs') reps(500)


* Test homoskedasticity. Under that, all quantiles should be equal
test [q25]x = [q75]x


*************************************************************************
** Nonparametrics *******************************************************
*************************************************************************


* Local polynomial regression
* Kernel regression is just this with degree(0)
lpoly y x, nograph degree(3) at(x) generate(cef)


* Using a command. Degree 0 specifies fitting a degree zero local
* polynomial reg estimator, which we know is simply kernel regression
semipar luwe educ, kernel(gaussian) nonpar(exper) degree(0) ci robust xtitle("experience") ytitle("log(wage) minus linear function of education")



* Robinson estimator
program RobinsonEst, rclass
  args Y X R kernel degree plot Ybwidth Xbwidth

  tempvar X_cef Y_cef X_tilde Y_tilde

  * Fit and plot
  foreach var in "X" "Y" {

    if ("``var'bwidth'" ~= "") {
      local bwidth_opt "bwidth(``var'bwidth')"
    }
    else local bwidth_opt ""

    * Degree 0 local polynomial reg (kernel reg) of  on R
    lpoly ``var'' `R', kernel(`kernel') `bwidth_opt' nograph degree(`degree') at(`R') generate(``var'_cef')
    local `var'bwidth = r(bwidth)

    * Generate residuals
    gen ``var'_tilde' = ``var'' - ``var'_cef'

    * Plot
    if `plot' {
      twoway (scatter ``var'' `R') || (line ``var'_cef' `R')
      graph export Q2_`var'fit.pdf, replace
    }
  }

  * Do OLS regresison using residuals
  reg `Y_tilde' `X_tilde', noconstant
  local b  = _b[`X_tilde']
  local se = _se[`X_tilde']


  return scalar b       = `b'
  return scalar se      = `se'
  return scalar Ybwidth = `Ybwidth'
  return scalar Xbwidth = `Xbwidth'
end


*************************************************************************
** Causal Inference *****************************************************
*************************************************************************

foreach method in "aipw" "ipw" "psmatch" {
  teffects `method' (y) (d x), vce(robust)
}

teffects psmatch (logsaleprice) (D `z'), vce(robust)

teffects psmatch (logsaleprice) (D `z'), vce(robust) atet


*************************************************************************
** Standard Errors ******************************************************
*************************************************************************

* vce( ): Standard Error Options
* ------------------------------
* - You've seen "vce(cluster id)" to get cluster-robust std errors.
* - You've seen ", robust" to get robust standard errors.
* - You can also do "vce(robust)" to get robust standard errors.
* - More generally "vce( )" is the regression option that lets you
*   modify the way that standard errors are computed in a regression.
*   For example, can get standard errors by bootstrap too
* - So ", robust" is valid and the same as "vce(robust)". But
*   vce(robust) is maybe preferred from the perspective of
*   common syntax for standard error computation.
reg y x
reg y x, vce(cluster id)
reg y x, robust
reg y x, vce(robust)
reg y x, vce(bootstrap, reps(1000) seed(314))



* Bootstrap in FE regressions using options on the xtreg command
xtreg y `xvars', fe vce(bootstrap, reps(1000) seed(314)) cluster(stateid)

* Bootstrap in FE regressions using bootstrap command
* cluster(stateid):  Specifies the cluster variable
* idcluster(myclid): Name of new variable to hold unique id for each
*                    resampled cluster. We need this so Stata treats a
*                    cluster that's resampled twice as two distinct
*                    clusters, not one big one.
* group(stateid):    Also necessary to include, see help file
*
* Note: Because we're not using the default standard errors, it
* normally doesn't matter whether we include vce(cluster stateid) or
* equivalently robust at the end of the xtreg command. Those
* analytical se's would be thrown out anyway. I only include it
* because we need those cluster-robust se's for constructing the
* percentile t interval.
*
* Note: se's match above command exactly
bootstrap _b _se, cluster(stateid) idcluster(myclid) group(stateid) reps(1000) seed(314) saving(bootstrapdraws.dta, replace): xtreg y `xvars', fe robust


* Compute the normal-based bootstrap interval, which
* should match the interval in the table just-computed
use bootstrapdraws.dta, clear
sum _b_myxvar
local se_myxvar_boot = r(sd)
local ci_l = `b_myxvar_xtreg' - 1.96*`se_myxvar_boot'
local ci_u = `b_myxvar_xtreg' + 1.96*`se_myxvar_boot'
di "Bootstrap Normal approx interval: [`ci_l', `ci_u']"

* Alternatively, report bstrap %ile CI rather than bootstrap normal CI
* Specify percentile so that it's not bias-corrected, which is default
estat bootstrap, percentile

* Load saved draws, verify that CI interval matches what's reported by
* previous command
_pctile _b_myxvar, percentiles(2.5 97.5)
di "Efron's Percentile Interval:      [`r(r1)', `r(r2)']"



* Percentile t interval

  * Generate t-stat, no need to multiply through sqrt(N) since already
  * incorporated in _se_shall, but wouldn't matter if you did, so long
  * as you divide through by it below
  gen t = (_b_myxvar-`b_myxvar_xtreg')/_se_myxvar

  * Compute percentiles of t-stat
  _pctile t, percentiles(2.5 97.5)
  local ci_l = `b_myxvar_xtreg' - `se_myxvar_xtreg'*`r(r2)'
  local ci_u = `b_myxvar_xtreg' - `se_myxvar_xtreg'*`r(r1)'
  di "Percentile t interval:            [`ci_l', `ci_u']"



* Apply bootstrap to estimation command
bootstrap b=r(b) se=r(se) Ybwidth=r(Ybwidth) Xbwidth=r(Xbwidth), reps(1000) seed(314): RobinsonEst luwe educ exper "gaussian" 0 0 `Ybwidth' `Xbwidth'
bootstrap me_educ=r(me_educ) ape_educ=r(ape_educ) ape_kidslt6=r(ape_kidslt6), reps(1000) seed(314): EstEffects



*************************************************************************
** Postestimation *******************************************************
*************************************************************************

* Getting residuals and storing in newvar
predict newvar, residuals

* Get fitted values. For binary choice model, this will be G(xb) after
* piping through the link function
predict fitted

* In binary choice models, can get the index x*b (before piped through
* the link function) via. For regression model, no difference
predict index, xb

* Testing whether linear combination is zero
lincom _cons + 12*educ
lincom _b[ed]*(12-16) + _b[ex]*(12-8) + _b[exsq]*(144-64)


* Test whether interaction terms are zero
testparm advise#agegroup
testparm year1985#(fe union hisp nonwh c.ed c.ex c.exsq)


* Confidence ellipse
ellip advise educ, coefs constant(chi2 2) level(95)


* F test and critical value
test advise educ
di "F-stat: `r(F)'"
di "Critical Value: " invchi2(2,0.95)/2
di "p-value, computed by Stata, using F_{q,n-k} distribution: `r(p)'"
di "p-value, using asymptotic Chi^2 distribution: " chi2tail(2, 2*`r(F)')

di "Wald-stat: " 2*`r(F)'
di "Critical Value: " invchi2(2,0.95)
di "p-value: " chi2tail(2, 2*`r(F)')




*************************************************************************
** Anderson Rubin Confidence Set ****************************************
*************************************************************************

* Part (iii): Anderson-Rubin Confidence Set

  * Crit val for F stat
  local cv = invchi2(2,0.95)/2

  capture erase "ar.dta"
  nois _dots 0, title(Loop running) reps(`=ceil((1+4)/0.01)')
  local i = 0

  * Loop over values of b
  forvalues b=-4(0.01)1 {
    preserve

    * Regress y-x*b on instruments
    qui: gen y = logq - `b'*logp
    qui: regress y stormy mixed, robust

    * If don't reject zeros, store coefficient in ar.dta
    if `e(F)' < `cv' {
      clear
      qui: set obs 1
      gen b = `b'
      cap append using ar
      qui: save ar, replace
      sleep 200 // Wait a bit so Stata doesn't throw file error
    }
    local i = `i' + 1
    nois _dots `i' 0

    restore
  }

  * Report Anderson-Rubin set
  use ar, clear
  su b
  gen temp = 0
  scatter temp b, ytitle("")



*************************************************************************
** Plotting *************************************************************
*************************************************************************

* Changing colors in figure
hist wages, graphregion(color(white))

* Exporting figures to pdf
graph export Figure.pdf, replace

* Add OLS fit to scatterplot
twoway (scatter price lot) (lfit price lot, lwidth(thick))

* Adding line to scatterplot
twoway (scatter ``var'' `R') || (line ``var'_cef' `R')


* Plot of line with error bands
twoway (line y1 x) (rcap lower upper x) (line y2 x), legend(off)




*************************************************************************
** Storing Estimates for Later ******************************************
*************************************************************************


* During estimation, can use to store estimation results
eststo estname: reg y x


* After estimation, can store active estimations results using either of
* the following, but prefer eststo
eststo estname, title("My Title for Tables")
estimates store estname


* By default, eststo and estimates store store e(sample), which takes
* lots of memory. Can get around this with
eststo estname, noesample


* Add marginal effects to set of stored estimates
estadd margins , dydx(x1) atmeans


* Display stored estimates
eststo dir


* Restore estimation result
estimates restore estname


* Clear any stored estimates/estimations. Will be storing/saving some
* now, and don't want old ones in the way.
eststo clear

* Clear a particular set of estimation results stored under estname
eststo drop estname




*************************************************************************
** Tables ***************************************************************
*************************************************************************

* Display stored estimations side-by-side using esttab
* - mtitles: Label the columns with titles stored from eststo
* - not: Do not show t-stats below coefficient estimates
esttab est_*, mtitles not

* Report stored marginal effects for each model
esttab est_*, main(margins_b) aux(margins_se) mtitles


*************************************************************************
** Programming Stata ch *************************************************
*************************************************************************


*************************************************************************
** Monte Carlo and Such *************************************************
*************************************************************************

gen y = rnormal()

* Do gommand separately by group, and gather stats
statsby _b _se, by(group) nodots clear: reg y x


* Repeat command a certain number of times, and gather stats
simulate col1=r(stat1) col2=r(stat2), reps(10000): cmd


*************************************************************************
** Formatted Printing ***************************************************
*************************************************************************

nois _dots 0, title(Loop running) reps(`=ceil((0.95-0.05)/0.01)')
forval i = 1/100 {
  nois _dots `i' 0
}


*************************************************************************
** Footer ***************************************************************
*************************************************************************

* Close log file
qui log close


















