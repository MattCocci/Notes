\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{ECO-513: Notes}
\date{\today}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}

%% Formatting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=1in]{geometry}
    %   Adjust the margins with geometry package
%\usepackage{pdflscape}
    %   Allows landscape pages
%\usepackage{layout}
    %   Allows plotting of picture of formatting



%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %   Make the header bigger to avoid overlap

%\fancyhf{}
    %   Erase header settings

%\renewcommand{\headrulewidth}{0.3pt}
    %   Width of the line

%\setlength{\headsep}{0.2in}
    %   Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\usepackage{bbm} % For bold numbers

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{courier}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{%
    colorlinks,
        %   This colors the links themselves, not boxes
    citecolor=black,
        %   Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\definecolor{codegreen}{RGB}{28,172,0}
\definecolor{codelilas}{RGB}{170,55,241}

% David4 color scheme
\definecolor{d4blue}{RGB}{100,191,255}
\definecolor{d4gray}{RGB}{175,175,175}
\definecolor{d4black}{RGB}{85,85,85}
\definecolor{d4orange}{RGB}{255,150,100}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %   For including verbatim code from files, no colors
\usepackage{listings}
    %   For including code snippets written directly in this doc

\lstdefinestyle{bash}{%
  language=bash,%
  basicstyle=\footnotesize\ttfamily,%
  showstringspaces=false,%
  commentstyle=\color{gray},%
  keywordstyle=\color{blue},%
  xleftmargin=0.25in,%
  xrightmargin=0.25in
}
\lstdefinestyle{log}{%
  basicstyle=\scriptsize\ttfamily,%
  showstringspaces=false,%
  xleftmargin=0.25in,%
  xrightmargin=0.25in
}


\lstdefinestyle{matlab}{%
  language=Matlab,%
  basicstyle=\footnotesize\ttfamily,%
  breaklines=true,%
  morekeywords={matlab2tikz},%
  keywordstyle=\color{blue},%
  morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
  identifierstyle=\color{black},%
  stringstyle=\color{codelilas},%
  commentstyle=\color{codegreen},%
  showstringspaces=false,%
    %   Without this there will be a symbol in
    %   the places where there is a space
  %numbers=left,%
  %numberstyle={\tiny \color{black}},%
    %   Size of the numbers
  numbersep=9pt,%
    %   Defines how far the numbers are from the text
  emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{blue},%
    %   Some words to emphasise
}

\newcommand{\matlabcode}[1]{%
    \lstset{style=matlab}%
    \lstinputlisting{#1}
}
    %   For including Matlab code from .m file with colors,
    %   line numbering, etc.

\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   %alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]

\lstdefinestyle{julia}{%
    language         = Julia,
    basicstyle       = \scriptsize\ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{codegreen},
    commentstyle     = \color{codegreen},
    showstringspaces = false,
    literate         = %
      {ρ}{{$\rho$}}1
      {ℓ}{{$\ell$}}1
      {∑}{{$\Sigma$}}1
      {Σ}{{$\Sigma$}}1
      {√}{{$\sqrt{}$}}1
      {θ}{{$\theta$}}1
      {ω}{{$\omega$}}1
      {ɛ}{{$\varepsilon$}}1
      {φ}{{$\varphi$}}1
      {σ²}{{$\sigma^2$}}1
      {Φ}{{$\Phi$}}1
      {ϕ}{{$\phi$}}1
      {Dₑ}{{$D_e$}}1
      {Σ}{{$\Sigma$}}1
      {γ}{{$\gamma$}}1
      {δ}{{$\delta$}}1
      {τ}{{$\tau$}}1
      {μ}{{$\mu$}}1
      {β}{{$\beta$}}1
      {Λ}{{$\Lambda$}}1
      {λ}{{$\lambda$}}1
      {r̃}{{$\tilde{\text{r}}$}}1
      {α}{{$\alpha$}}1
      {σ}{{$\sigma$}}1
      {π}{{$\pi$}}1
      {∈}{{$\in$}}1
      {∞}{{$\infty$}}1
}


%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %   Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %   For bibliographies
\usepackage{pdfpages}
    %   For including whole pdf pages as a page in doc
\usepackage{pgffor}
    %   For easier looping


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\hat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}


% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bshatOmega}{\boldsymbol{\hat{\Omega}}}
\newcommand{\bshatG}{\boldsymbol{\hat{G}}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}

% Special cases like \bshatb for \boldsymbol{\hat{b}}
\generate[\hat]{bshat}{\boldsymbol}{b,y,x,X,V,S,W}
\newcommand{\bshatbeta}{\boldsymbol{\hat{\beta}}}
\newcommand{\bshatmu}{\boldsymbol{\hat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\hat{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\hat{\Sigma}}}
\newcommand{\bstildebeta}{\boldsymbol{\tilde{\beta}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bsbarbeta}{\boldsymbol{\overline{\beta}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}

% Redefine \bso to be the zero vector
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\hat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\hat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\hat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}


% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}


% Various probability and statistics commands
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{P}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}


% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\nNz}{_{n=0}^N}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}

% Check and x symbols
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

\tableofcontents


\clearpage
\section{Temp}

\begin{align*}
  \max
  \;&\E_1[\varphi(y)]
  \\
  \qquad \text{s.t.}\quad
  \alpha
  &= \E_0[\varphi(y)]
\end{align*}
Or
\begin{align*}
  \max
  \;&
  \int \varphi(y)p_1(y)\;dy
  \\
  \qquad \text{s.t.}\quad
  \alpha
  &= \int\varphi(y)p_0(y)\;dy
\end{align*}
Lagrangian
\begin{align*}
  L
  &=
  \int \varphi(y)p_1(y)\;dy
  +
  \lambda
  \left(
  \alpha
  - \int\varphi(y)p_0(y)\;dy
  \right)
\end{align*}
FOC
\begin{align*}
  0
  &=
  p_1(y)
  -
  \lambda
  p_0(y)
  \\
  \implies\quad
  p_1(y)
  &=
  \lambda
  p_0(y)
\end{align*}
Therefore, we are maxing
\begin{align*}
  \int\varphi(y)p_1(y)\;dy
  =
  \lambda\int\varphi(y)p_0(y)\;dy
  = \lambda\alpha
\end{align*}



\clearpage
\section{Causality and Exogeneity}

Model
\begin{align*}
  y_n
  = \bsx_n'\bsbeta + \varepsilon_t
\end{align*}
Note that $\bsx_n$ could potentially contained lagged $y_n$. But that
limits whether or not a particular assumption is reasonable for time
series.
\begin{table}[htbp!]
\centering
\begin{tabular}{c|lcccc}
    &
    & Implies
    & Justifies
    & Consistency \&
    & Reasonable for
  \\
  $\bsx_t$ is\dots
    & Defining Feature
    & $\E[\varepsilon_t\varepsilon_{s}]=0$
    & GLS
    & Asymptotic $\calN$
    & Time Series
  \\\hline\hline
  Strictly Exogenous
    & $0=\E[\varepsilon_t|\{\bsx_s\}_{s=-\infty}^\infty]$
    & \xmark
    & \cmark
    & \cmark
    & \xmark
  \\
  Predetermined
    %& $0=\E[\varepsilon_t|\{\bsx_s,y_{s-1}\}_{s=-\infty}^t]$
    & $0=\E[\varepsilon_t|\{\bsx_s\}_{s=-\infty}^t]$
    & \cmark
    & \xmark
    & \cmark
    & \cmark
  \\
  Weaker Predetermined
    & $0=\E[\varepsilon_t\bsx_t]$
    & \xmark
    & \xmark
    & \cmark
    & \cmark
\end{tabular}
\end{table}

Weaker predetermined is used in Hayashi.

Strict exogeneity Generally impossible when $\bsx_n$ has lagged $y_n$ in
it.  Hence, generally not imposed in models with AR structure.
Mostly imposed when $y_n$ and $\bsx_n$ completely distinct

Predetermined and exogeneity are different. One does not imply the
other.
But if already assumping absence of serial correlation, then strict
exogeneity is stronger and does imply predetermined.
But no need to assume that for strict exogeneity.





\clearpage
\section{Asymptotics}

Asymptotic analysis is useful as a general \emph{framework} or
\emph{device} with two applications:
\begin{itemize}
  \item Finding/deriving approximate distributions, tests,
    test statistics, confidence regions
  \item \emph{Given} tests, test statistics, confidence regions,
    we can evalute their asymptotic optimality, efficiency, or quality,
    in an asymptotic variance or asymptotic power sense
\end{itemize}
This is carried by starting with likelihood/model $p(y_{1:N}|\theta)$ fo
the sample $y_{1:N}$, taking $N\ra\infty$, and invoking the LLN and CLT
to get to get the asymptotic distribution of some function $S(y_{1:N})$
of the data, whether that function is an estimator, a test, a confidence
region, etc.

The magic lies in the fact that, while the finite sample distribution of
$y_{1:N}$ is often complicated or intractable, the LLN and CLT
drastically simplify the asymptotic distribution of $S(y_{1:N})$ into
something normal. Asymptotic normality of estimators, test statistics,
and even models (in the limits of experiments literature) are a hallmark
of asymptotic statistics.

Note that all of htis assumes the asymptotic distribution is a good
approximation of the finite sample distribution.
There's a priori no reason this should be the case, and asymptotics does
not explicitly imply bounds on the error or mistake. But can do
simulation studies to test whether it's a good approximation.


\clearpage
\section{Contiguity and Limits of Experiments}

\subsection{Experiments and Sequence of Experiments}

\begin{defn}(Model/Experiment)
A \emph{model} or \emph{experiment} is the triple
$(\Omega,\sF,\{P_\theta\}_{\theta\in  \Theta})$ consisting of a measurable space
$(\Omega,\sF)$ together with a collection of probability measures for
that measurable space $\{P_\theta\}_{\theta\in  \Theta}$.
Often though, we don't even specify the measurable space $(\Omega,\sF)$
since it's either obvious or unimportant, and we just refer to
$\{P_\theta\}_{\theta\in  \Theta}$ as the model/experiment. That's where
all the action is anyway.

In particular, $\{P_\theta\}_{\theta\in  \Theta}$ represents the collection of all
possible distributions for some RV $X$.
Fixing $\theta$, we can make ex ante probability statements about the
distribution of $X$. Or, given an observation $X$, we can do inference
about which distribution $P_\theta$ most likely produced that particular
realization.
Most generally, $ \Theta$ represents an arbitrary index set.
However in practice, we often consider distributions $P_\theta$ in some
\emph{parameteric} family that are absolutely continuous with respect to
the Lebesgue or counting measure, hence they permit a density or mass
function of the form $p(y|\theta)$ (aka a likelihood) with $\theta$ representing a
parameter vector (rather than just an arbitrary index).
\end{defn}
\begin{rmk}
From now on, I will use the customary phrase ``experiment'' rather than
``model.''
\end{rmk}

\begin{defn}(Sequence of Experiments)
A \emph{sequence of experiments} is the sequence
\begin{align*}
  \big\{\Omega_n,\sF_n,\{P_{n,\theta}\}_{\theta\in  \Theta}
  \big\}_{n\in \N}
\end{align*}
Each element is a measurable space $(\Omega_n,\sF_n)$ and
collection of candidate measures $\{P_{n,\theta}\}_{\theta\in  \Theta}$.

Most often, such a sequence arises in thinking about an iid sample
$\{X_i\}_{i=1}^n$, where each observation $X_i$ has distribution $P_\theta$
on $(\Omega,\sF)$.
Then the $n$th element in the sequence of experiments is the
triplet $(\Omega_n,\sF_n,\{P_{n,\theta}\}_{\theta\in  \Theta})$ where
$(\Omega_n,\sF_n)$ is the product space and $P_{n,\theta}$ the product
measure for the sample $\{X_i\}_{i=1}^n$, built from $n$ copies of
underlying $P_\theta$.
\end{defn}


\clearpage
\subsection{Contiguity}

\begin{defn}(Contiguity)
Suppose we have sequence of experiments
\begin{align*}
  \big\{\Omega_n,\sF_n,\{P_n,Q_n\}\big\}_{n\in\N}
\end{align*}
Sequence of measures $Q_n$ is \emph{contiguous} to $P_n$, denoted $Q_n
\vartriangleleft P_n$, if and only if
\begin{align*}
  P_n[A_n]\ra 0
  \quad\implies\quad
  Q_n[A_n]\ra 0
\end{align*}
This generalizes absolute continuity of measures to \emph{sequences} of
measures.
Contiguity is useful because it allows us to transfer convergence in
probability or distribution under the baseline experiment with $P_n$ to
the contiguous experiment with $Q_n$. Often the baseline experiment with
$P_n$ is some well-behaved experiment we know a lot about, while $Q_n$
is a trickier experiment that's we'd \emph{like} to know more about.

Correct image: Sequence of measures ``on top of each other'' in the
limit, can't distinguish.
%The set of contiguous models are often idexed by the rate of approach,
%i.e. $\theta_0+gT^{-1/2}$ contiguous for \emph{all} $g\in\R$.
\end{defn}

\begin{thm}\emph{(LeCam's First Lemma: Establishing Contiguity)}
\label{thm:lecams1st}
The following are equivalent
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $Q_n \vartriangleleft P_n$, i.e. $Q_n$ contiguous to $P_n$
  %\item $\frac{dP_n}{dQ_n}\dto_{Q_n} U$ with $P[U>0]=1$
  \item $\frac{dQ_n}{dP_n}\underset{P_n}{\dto} L$ with
    $\E[L]=1$.\footnote{%
      We are implicitly assuming $Q_n<<P_n$ for all $n$ so
      $\frac{dQ_n}{dP_n}$ is well-defined
    }
  \item For any sequence of RVs $X_n:\Omega_n\ra\Rk$,
    $X_n = o_{P_n}(1)$
    implies
    $X_n = o_{Q_n}(1)$
\end{enumerate}
\end{thm}
\begin{rmk}
Often, we use (ii) to establish (i).
Intuition: If the limiting distribution of the sequence of likelihood
ratios behaves like a Radon-Nikodym/likelihood-ratio, no mass has
escaped to infinity and the experiments are close even in the limit.
\end{rmk}

\begin{thm}
\label{thm:lecams3rd}
\emph{(Generalized LeCam's Third Lemma)}
%: Deriving Asymptotic
%Distribution of the Contiguous Model from Baseline Model)}
Suppose we have a sequence of experiments with measures $P_n,Q_n$ and
RV $X_n$ (generally some test, statistic, etc.) such that $Q_n<<P_n$ for
all $n$ and
\begin{align*}
  \left(\frac{dQ_n}{dP_n},X_n\right)
  \quad\underset{P_n}{\dto}\quad
  (L,X)
  \qquad \text{where}\quad
  \E[L]=1
\end{align*}
Then by the previous theorem, $Q_n\vartriangleleft P_n$ and,
additionally
\begin{align}
  X_n\;\underset{Q_n}{\dto}\;\tilde{X}
  \qquad \text{where}\quad
  P_{\tilde{X}}\big[\tilde{X}\in B\big]
  = \E_{\tilde{X}}\big[\mathbf{1}\{\tilde{X}\in B\}\big]
  =\E_X\big[L\cdot \mathbf{1}\{X\in B\}\big]
  \label{lecams3rd}
\end{align}
\end{thm}
\begin{rmk}
Given $Q_n\vartriangleleft P_n$, Part (iii) of Lecam's First Lemma
allows us to transfer convergence in \emph{probability} under $P_n$ to
$Q_n$.
Lecam's Third Lemma similarly lets us transfer convergence in
\emph{distribution}, and also fully specifies the distribution under
$Q_n$ as in Expression~\ref{lecams3rd}.
\end{rmk}

%\begin{cor}\emph{(Specialized LeCam's Third Lemma)}
%Suppose that we have
%\begin{align*}
  %\left(X_n,\ln \frac{dQ_n}{dP_n}\right)
  %\dto_{P_n}
  %\calN\left(
  %\begin{pmatrix}
    %\mu \\ -\frac{1}{2}\sigma^2
  %\end{pmatrix},\;
  %\begin{pmatrix}
    %\Sigma & \tau  \\
    %\tau' & \sigma^2
  %\end{pmatrix}
  %\right)
%\end{align*}
%The all the conditions of teh above theorem are satisfied, hence
%$Q_n\vartriangleleft P_n$ and
%\begin{align*}
  %X_n
  %\dto_{Q_n}
  %\calN(\mu+\tau, \Sigma)
%\end{align*}
%Note, asymptotic covariance of $X_n$ is the same under $Q_n$ but mean
%shifted.
%\end{cor}

%\begin{ex}
%Suppose we have
%\begin{align}
  %\frac{dQ_n}{dP_n}\dto_{P_n}
  %e^{\calN(\mu,\sigma^2)}
  %\label{lanintro}
%\end{align}
%Then already, $P_n\vartriangleleft Q_n$ since (ii) is equivalent to (i).
%Moreover, $Q_n\vartriangleleft P_n$ iff $\mu=-\frac{1}{2}\sigma^2$ since
%(iii) is equivalent to (i).
%This is important because Assumption~\ref{lanintro} means that the model
%is locally asymptotically normal, which is a special concept we will
%define and study below.
%\end{ex}

%\begin{proof}
%(Theorem~\ref{thm:lecams1st})
%\end{proof}



\clearpage
\subsection{Limits of Experiments}


\begin{defn}(Convergence to Limit Experiment)
\label{defn:limitexp}
We say that the sequence of experiments
$\{\Omega_n,\sF_n,\{P_{n,g}\}_{g\in G}\}_{n\in\N}$
\emph{converges to limit experiment}
$(\Omega,\sF,\{P_{g}\}_{g\in G})$ if,
for every possible baseline parameter value $g_0\in G$ and
finite subset $S\subseteq  G$,
\begin{align}
  \left(
  \frac{dP_{n,g}}{dP_{n,g_0}}(X_n)
  \right)_{g\in S}
  \quad\underset{g_0}{\dto}\quad
  \left(
  \frac{dP_{g}}{dP_{g_0}}(X)
  \right)_{g\in S}
  \label{limexp}
\end{align}
In words, this is ``Joint convergence in distribution
(taking the \emph{baseline} law as that with parameter $g_0$)
of the \emph{random vector} where each element is a likelihood ratio
with a different $g\in S$.''
The likelihood ratios have arguments $X_n$ and $X$ to remind that the
LRs are themselves RVs whose distributions depend on the realization of
the observation $X_n$ (in the $n$th experiment in the sequence) and $X$
(in the limit experiment).
If Expression~\ref{limexp} holds, then observing $X_n$ in the $n$th
experiment in the sequence is just like (i.e. has the same asymptotic
properties as) observing $X$ in the limit experiment.

We most often use such convergence to \emph{approximate} the
\emph{sequence} of experiments with the \emph{limiting} experiment.
Intuitively, since an experiment with large $n$ has a LR whose
distribution behaves like the LR in the limit experiment, we might
reasonably think the limit experiment provides a good approximation for
experiments in the converging sequence.
This is especially common when $n$ represents the sample size, and each
experiment in the sequence is the joint distribution for an increasingly
large sample.
This also directly generalizes to \emph{entire models/experiments} the
common practice of approximating a sequence of estimators or tests with
their asymptotic limit.
\end{defn}

\begin{prop}\emph{(Limit Experiment Implies Contiguity)}
\label{prop:impcontiguity}
Suppose the sequence of experiments
$\{\Omega_n,\sF_n,\{P_{n,g}\}_{g\in G}\}_{n\in\N}$
converges to limit experiment $(\Omega,\sF,\{P_{g}\}_{g\in G})$
as in Definition~\ref{defn:limitexp}.
Then for any $g,g_0\in G$,
sequence $P_{n,g}$ is contiguous to $P_{n,g_0}$.
\end{prop}
\begin{proof}
By assumption, Expression~\ref{limexp} holds for arbitrary $g,g_0$.
Since $L:=\frac{dP_g}{dP_{g_0}}$ is a RN derivative between $P_{g}$ and
$P_{g_0}$ in the limit experiment, we know it has expectation $\E[L]=1$.
So then, since Part (ii) of Theorem~\ref{thm:lecams1st} is equivalent to
Part (i), $P_{n,g}\vartriangleleft P_{n,g_0}$.
\end{proof}

\begin{thm}
\label{thm:limitexp}
Suppose sequence of experiments
$\{\Omega_n,\sF_n,\{P_{n,g}\}_{g\in G}\}_{n\in\N}$
converges to limit experiment
$(\Omega,\sF,\{P_{g}\}_{g\in G})$.
Suppose also that sequence of statistics $T_n=T_n(X_n)$ converges in
distribution under any $g$ to \emph{some} (generally $g$-specific) RV on
some probability space, i.e.
\begin{align*}
  T_n \quad\underset{g}{\dto}\quad T_g
  \qquad\forall g
\end{align*}
Then we can represent $T_g$ as a (possibly randomized) function of the
observation $X$ in the limit experiment, i.e.
%Then there exists a (possibly randomized) statistic $T$
%\emph{in the limit experiment}
\begin{align*}
  T_n \underset{g}{\dto} T(X,U)
  \qquad
  \forall g
\end{align*}
\end{thm}
%\begin{rmk}
%The theorem statement might seem a little redundant regarding
%convergence in distribution. But note the subtlety: We only assume $T_n$
%converges in distribution to \emph{some} RV in \emph{some} probability
%space that we don't necessarily care about. Then this theorem guarantees
%that $T_n$ \emph{also} converges to an RV that is a \emph{function} of
%$X$ in the \emph{particular} probability space
%$(\Omega,\sF,\{P_g\}_{g\in G})$ that is the limit
%experiment.
%\end{rmk}

\begin{proof}
(\emph{Establish Joint Convergence under $g_0$})
By assumption, we have \emph{individually}
\begin{align}
  \frac{dP_{n,g}}{dP_{n,g_0}}
  \quad\underset{g_0}{\dto}\quad
  \frac{dP_{g}}{dP_{g_0}}
  \qquad\text{and}\qquad
  T_n
  \quad\underset{g_0}{\dto}\quad
  T_{g_0}
  \label{limitmarginals}
\end{align}
We want to establish \emph{joint} convergence of these objects under
$g_0$, in addition to this (weaker) convergence of the marginals.

To start, by Part (i) of Prohorov's Theorem, since
$\frac{dP_{n,g}}{dP_{n,g_0}}$ and $T_n$ converge in distribution, both
are uniformly tight under $g_0$ individually.
But by the definition of uniform tightness, it's clear that if a finite
number of RVs are uniformly tight individually, the \emph{joint} vector
$(\frac{dP_{n,g}}{dP_{n,g_0}},T_n)$ is \emph{also} uniformly tight under
$g_0$.
And then, by Part (ii) Prohorov's Theorem, uniform tighness of the
vector sequence implies that there exists a subsequence that converges
in distribution under $g_0$, i.e. there exists a set of indices
$\{n_i\}_{i=1}^\infty$ such that
\begin{align*}
  \left(\frac{dP_{n_i,g}}{dP_{n_i,g_0}},T_{n_i}\right)
  \quad\underset{g_0}{\dto}\quad
  \left(Y,Z\right)
\end{align*}
for some RVs $Y,Z$. But we can pin down those RVs. In particular, since
the marginals of limiting RV $(Y,Z)$ must match the marginals we already
know/assumed in Expressions~\ref{limitmarginals}, we can deduce that, in
fact, the subsequence converges to
\begin{align}
  \left(\frac{dP_{n_i,g}}{dP_{n_i,g_0}},T_{n_i}\right)
  \quad\underset{g_0}{\dto}\quad
  \left(\frac{dP_g}{dP_{g_0}},T_{g_0}\right)
  \label{lecams3rdstep}
\end{align}
Again, this is just along the subsequence, but we will move to the
general limit in a bit.

(\emph{Derive Distribution Under $g$})
Now that we have joint convergence under $g_0$, we want the distribution
of the statistic under $g$.
By Proposition~\ref{prop:impcontiguity},
$P_{n_i,g}\vartriangleleft P_{n_i,g_0}$, i.e. the subsequence of
measures $\{P_{n_i,g}\}_{i=1}^\infty$ is contiguous to
$\{P_{n_i,g_0}\}_{i=1}^\infty$.
Therefore, we can use LeCam's Third Lemma in Theorem~\ref{thm:lecams3rd}
to get the limiting distribution under $g$.
In particular, we have
\begin{align}
  T_{n_i}\;\underset{g}{\dto}\;\tilde{T}_g
  \qquad \text{where}\quad
  P_{g}[\tilde{T}_g\in B]
  = \E_{g}[\mathbf{1}\{\tilde{T}_g\in B\}]
  =\E_{g_0}\left[\frac{dP_g}{dP_{g_0}}\cdot \mathbf{1}\{T_{g_0}\in B\}\right]
  \label{tildeT}
\end{align}
This is the limiting distribution of the subsequence
$\{T_{n_i}\}_{i=1}^\infty$. But because the original sequence $T_n$
converges in distribution to $T_g$ (for any $g$) by assumption, this
limit of the subsequence must equal the limit of the sequence, i.e.
$\tilde{T}_g=T_g$. Hence Expression~\ref{tildeT} also characterizes the
distribution of the limit $T_g$ of the original sequence $T_n$.

(\emph{Construct Function $T(X,U)$})
Finally, we can always choose a function $T$ so that
\begin{align*}
  \begin{pmatrix}
    T(X,U) \\ X
  \end{pmatrix}
  \sim
  \begin{pmatrix}
    T_{g_0} \\ X
  \end{pmatrix}
\end{align*}
Check that this has the correct distribution under $g$ by importance
sampling.


\end{proof}



%\subsubsection{Local Asymptotic Normality}

%\begin{defn}(Local Asymptotic Normality)
%Model is locally asymptotical normal if
%\begin{align*}
  %\ln
  %\frac{p(y_{1:T}|\theta_0+gT^{-1/2})}{p(y_{1:T}|\theta_0)}
  %\dto
  %\calN\left(-\frac{1}{2}g'\calI g, g'\calI g\right)
%\end{align*}
%Just like small sample behavior of log like from model
%\begin{align*}
  %Y \sim \calN(\calI^{1/2}g,I_k)
%\end{align*}
%\end{defn}



\clearpage
\section{Limits of Experiments}

\begin{rmk}
As $N\ra\infty$, an unknown parameter $\theta$ will eventually be known
with arbitrary precision, and any sensible test of the parameter value
will reject an incorrect null with probability one.
So how do we compare tests that, asymptotically, all have the same power
(i.e. reject for sure)?
Well, we look at the likelihood ratio (which forms the basis for
sensible tests) and consider how it behaves when the alternative
approaches the null at an appropriate rate so that, asymptotically, the
null and alternative remain tough to distinguish.

We do this be reparameterizing the model/experiment.
If we do it correctly, then the LR will converge to the LR ratio of a
single observation Gaussian model/experiment.
Consider a known $\theta_0$, and define local parameter
$\sqrt{N}(\theta-\theta_0)$, which implies that we're considering
$\theta=\theta_0+\frac{g}{\sqrt{N}}$.
Note that $h$ is the wedge between the (properly scaled) true parameter
$\theta_0$ and $\theta$.
For large $N$, the following models/experiments have similar statistical
properties
\begin{align*}
  \{P_{\theta_0+g/\sqrt{N}}\}_{h\in H}
  \qquad
  \{\calN(h,\calI_{\theta_0}^{-1})\}_{h\in H}
\end{align*}
The log-likelihood ratio of this model has the same distribution as the
limit distribution of the above LAN model.
\end{rmk}


\begin{defn}(Log-Likelihood Approximation)
Suppose $p_\theta$ is a density for $P_\theta$ with respect to some
measure $\mu$. Suppose the density is differentiable,
and we take a first order expansion of the log likelihood ratio of the
model with $\theta+\frac{g}{\sqrt{N}}$ against the model with $\theta$
(about $\theta$):
\begin{align*}
  \ln\frac{p(y_{1:N}|\theta+\frac{g}{\sqrt{N}})}{p(y_{1:N}|\theta)}
  =
  \frac{g}{\sqrt{N}}
  \sumnN s_n(\theta)
  +
  \frac{1}{2}\frac{g^2}{N}
  \sumnN
  h_n(\theta)
\end{align*}
Need to suppose the log likelihood permits a forecast error
decomposition.
Under certain conditions
\begin{align*}
  \frac{g}{\sqrt{N}}
  \sumnN s_n(\theta)
  +
  \frac{1}{2}\frac{g^2}{N}
  \sumnN
  h_n(\theta)
  \quad\dto\quad
  \calN(0,\calI)
  -\frac{g^2}{2}
  \calI
\end{align*}
\end{defn}









%% APPPENDIX %%

% \appendix




\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% VIEW LAYOUT %%

        \layout

    %% LANDSCAPE PAGE %%

        \begin{landscape}
        \end{landscape}

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}  %Use in text; cites
        \citep{LabelInSourcesFile} %Use in text; cites in parens

        \nocite{LabelInSourceFile} % Includes in refs w/o specific citation
        \bibliographystyle{apalike}  % Or some other style

        % To ditch the ``References'' header
        \begingroup
        \renewcommand{\section}[2]{}
        \endgroup

        \bibliography{sources} % where sources.bib has all the citation info

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}

    %% URLS, EMAIL, AND LOCAL FILES %%

      \url{url}
      \href{url}{name}
      \href{mailto:mcocci@raidenlovessusie.com}{name}
      \href{run:/path/to/file.pdf}{name}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%

        %\verbatiminput{file.ext}
            %   Includes verbatim text from the file

        \texttt{text}
            %   Renders text in courier, or code-like, font

        \matlabcode{file.m}
            %   Includes Matlab code with colors and line numbers

        \lstset{style=bash}
        \begin{lstlisting}
        \end{lstlisting}
            % Inline code rendering


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}

        % Side by Side figures: Use the tabular environment


