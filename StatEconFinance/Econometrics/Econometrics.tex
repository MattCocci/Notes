\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Econometrics}
\date{\today}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}

%% Formatting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=1in]{geometry}
    %   Adjust the margins with geometry package
%\usepackage{pdflscape}
    %   Allows landscape pages
%\usepackage{layout}
    %   Allows plotting of picture of formatting



%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %   Make the header bigger to avoid overlap

%\fancyhf{}
    %   Erase header settings

%\renewcommand{\headrulewidth}{0.3pt}
    %   Width of the line

%\setlength{\headsep}{0.2in}
    %   Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{courier}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{%
    colorlinks,
        %   This colors the links themselves, not boxes
    citecolor=black,
        %   Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\definecolor{codegreen}{RGB}{28,172,0}
\definecolor{codelilas}{RGB}{170,55,241}

% David4 color scheme
\definecolor{d4blue}{RGB}{100,191,255}
\definecolor{d4gray}{RGB}{175,175,175}
\definecolor{d4black}{RGB}{85,85,85}
\definecolor{d4orange}{RGB}{255,150,100}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %   For including verbatim code from files, no colors
\usepackage{listings}
    %   For including code snippets written directly in this doc

\lstdefinestyle{bash}{%
  language=bash,%
  basicstyle=\footnotesize\ttfamily,%
  showstringspaces=false,%
  commentstyle=\color{gray},%
  keywordstyle=\color{blue},%
  xleftmargin=0.25in,%
  xrightmargin=0.25in
}

\lstdefinestyle{matlab}{%
  language=Matlab,%
  basicstyle=\footnotesize\ttfamily,%
  breaklines=true,%
  morekeywords={matlab2tikz},%
  keywordstyle=\color{blue},%
  morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
  identifierstyle=\color{black},%
  stringstyle=\color{codelilas},%
  commentstyle=\color{codegreen},%
  showstringspaces=false,%
    %   Without this there will be a symbol in
    %   the places where there is a space
  numbers=left,%
  numberstyle={\tiny \color{black}},%
    %   Size of the numbers
  numbersep=9pt,%
    %   Defines how far the numbers are from the text
  emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
    %   Some words to emphasise
}

\newcommand{\matlabcode}[1]{%
    \lstset{style=matlab}%
    \lstinputlisting{#1}
}
    %   For including Matlab code from .m file with colors,
    %   line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %   Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %   For bibliographies
\usepackage{pdfpages}
    %   For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Misc Math
\newcommand{\Prb}{\mathrm{P}}
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}{\boldsymbol{1}}

% Script
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sA}{\mathscr{A}}

% Mathcal
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}

% Blackboard
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rk}{\mathbb{R}^n}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}

\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

% Redefine real and imaginary from fraktur to plain text
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}

% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\tableofcontents %adds it here

\clearpage

\section{Introduction}

This document aims to bring together the elements of probability,
measure theory, and stochastic calculus useful for the anlysis of
stochastic process that have particular relevance in finance and
economics.

\section{Finite Probability Spaces}

\subsection{Finite Probability Space}

\begin{defn}{(Sample Space)}
The \emph{sample space} $\Omega=\{\omega_1,\ldots,\omega_N\}$ is the set
of all \emph{elementary} outcomes of some statistical experiment. For a
given experiment, only one $\omega \in \Omega$ can occur when we conduct
and observe that experiment.
\end{defn}
\begin{ex}
If our experiment consists of flipping a single coin, the sample space
consisting of elementary events is $\{H,T\}$. If we flip two coins, the
sample space is $\{HH,HT,TH,TT\}$.
\end{ex}
\begin{rmk}
The previous example suggests that there's some subtlety in how you
define ``elementary'' event. In the case where we flipped two coins, it
might seem like a single flip is more ``elementary.'' However, if we define
our experiment to be the flipping of two coins, the set of outcomes from
that experiment really is $\Omega=\{HH,HT,TH,TT\}$. It's an unfortunate
bit of nomenclatural confusion.

Of course, we'll fix this quite soon when we encounter \emph{product
spaces}, which resolve a lot of this subtlety. In particular, product
spaces will let us define a narrow product space like $\Omega=\{H,T\}$
and then construct a richer probability space like
$\Omega\times \Omega = \{HH,HT,TH,TT\}$ or even $\Omega^N$, building up
from relatively simples ones.
\end{rmk}

\begin{defn}{(Finite Probability Space)}
A \emph{finite probability space} is a pair $(\Omega,P)$ consisting of a
finite sample space $\Omega=\{\omega_1,\ldots,\omega_N\}$ of elementary
events together with probabilities $\{p_1,\ldots,p_N\}$ satisfying
\begin{align*}
  \sumnN p_n = 1
  \quad\text{and}\quad
  p_n \in [0,1] \quad \forall n=1,\ldots,N
\end{align*}
where $p_n$ is the probability of elementary event $\omega_n\in\Omega$
occurring.
\end{defn}

\begin{defn}{(Event)}
An \emph{event} is any subset $A\subseteq \Omega$ of the sample space.
\end{defn}
\begin{ex}
Given sample space $\Omega=\{HH,HT,TH,TT\}$ for an experiment where we
toss two coins, we might consider the event $A=\{HH,HT\}$ which is
``heads tossed first.''
\end{ex}


\begin{defn}{(Set Algebra)}
A collection $\calF$ of subsets of $\Omega$ (i.e.\ a collection of
events) is called a \emph{set algebra}, or simply \emph{algebra}, if it
satisfies the following three properties:
\begin{enumerate}
  \item $\Omega\in\calF$
  \item $A,B\in\calF \implies A\cup B\in\calF$
  \item $A\in\calF\implies A^c\in\calF$.
\end{enumerate}
\end{defn}
\begin{rmk}
This is not to be confused with a $\sigma$-algebra, which we'll see
later. A set algebra is closed only under finite unions, which is not
going to be a problem in a finite probability space. Once we move to
infinite probability spaces, we will need $\sigma$-algebras, which are
closed under \emph{infinite} unions. But more on that later.

At any rate, the reason we care about algebras is that they collect and
contain the sets---the events---to which we will assign probabilities.
Our notion of a probability will be defined for elements in this
collection, as we see in the next definition.
\end{rmk}


\begin{defn}{(Probability Measure)}
\label{defn:finitemeasure}
Given a finite probability space, define the \emph{probability measure}
on $\Omega$ as the mapping
\begin{align*}
  \Prb&: 2^\Omega \rightarrow [0,1] \\
  \Prb[A] &:= \sum_{\{n|\omega_n\in A\}} p_n
\end{align*}
In this way, we use the probabilities $\{p_1,\ldots,p_n\}$ over the
elementary events to construct the probabilites of any event
$A\subseteq \Omega$ or $A\in\calF$.

Probability measures display the following properties
\begin{enumerate}
  \item $\Prb[\omega_n]=p_n$ for $\omega_n\in\Omega$
  \item $\Prb[\emptyset] = 0$
  \item $\Prb[\Omega] = 1$
  \item $\Prb[A\cup B] = \Prb[A] + \Prb[B] - \Prb[A\cap B]$ for any
    events $A,B$.
\end{enumerate}
\end{defn}


\begin{defn}{(Random Variable)}
A \emph{random variable} on finite probability space $\Omega$ is a
function
\begin{align*}
  X: \Omega\ra \R
\end{align*}
Since it's inconvenient to deal with sets and $\omega$'s in $\Omega$
directly, we use random variables that map these elements into the real
line. But in general, we only know how to assign probabilities to
events in $\Omega$. How can assign probabiilities to random variables?
That leads us to the next definition.
\end{defn}

\begin{defn}{($\calF$-Measurable)}
A random variable $X$ is $\calF$-measurable if
\begin{align*}
  \{X=x\} :=
  X^{-1}(x) =
  \{\omega\in\Omega \; | \; X(\omega)=x\}
  \in \calF
\end{align*}
for all $x\in\R$. The idea is that Definition~\ref{defn:finitemeasure}
tells us how to assign probabilities to events or ``measure'' events in
$\calF$. To ``measure'' or assign probabilities to random variables
taking values on the real line, we need to be able to map those values
back into the events in $\calF$ that we do know how to measure.

Not surprisingly, we differentiate between algebras and say
$\calF$-measurable because random variables might be measurable with
respect to some algebras and not others.
\end{defn}

\subsection{Product Spaces}

\begin{defn}{(Product Space)}
Given two finite probability spaces $(\Omega,P)$ and $(\Omega',P')$,
define a new probability space called the \emph{product space}
$(\Omega\times\Omega', P \otimes P')$ as
\begin{align*}
  \Omega\times\Omega'
  &= \{(\omega,\omega') \; |\; \omega\in\Omega, \; \omega'\in\Omega'\}
  \\
  (P \otimes P')(\omega, \omega')
  &= \Prb[\omega]\cdot \Prb[\omega']
\end{align*}
\end{defn}

\begin{defn}{(Lifting Random Variables to a Product Space)}
Suppose that we have events $A\subseteq \Omega$ and
$B\subseteq \Omega'$. We want to make sure that the same events are
defined on product space $\Omega\times \Omega'$. Then we can define new
sets
\begin{align*}
  \hat{A} &:= A\times \Omega' \\
  \hat{B} &:= \Omega \times B
\end{align*}
The probabilities of these events will be exactly the same on $\Omega
\times \Omega'$ as they were on just $\Omega$ or just $\Omega'$ because
notice
\begin{align*}
  (\Prb\otimes \Prb')[\hat{A}] &= \Prb(A) \cdot \Prb'(\Omega') = \Prb(A) \cdot 1\\
  (\Prb\otimes \Prb')[\hat{B}] &= \Prb(\Omega) \cdot \Prb'(B) = 1\cdot \Prb'(B)
\end{align*}
Moreover, $\hat{A}$ and $\hat{B}$ will be independent events, which
makes sense given that they started by having nothing to do with each
other, hanging out completely on their own in $\Omega$ or $\Omega'$.
\end{defn}

\begin{defn}{(Lifting Random Variables to a Product Space)}
We can easily ``lift'' random variables on $\Omega$ or $\Omega'$ into
the new product space $\Omega\times \Omega'$. To do so, suppose that we
have random variables $X:\Omega\ra\R$ and $Y:\Omega'\ra\R$. Then define
\begin{align*}
  \forall (\omega,\omega') \in \Omega\times \Omega' \qquad
  \begin{cases}
  \hat{X}(\omega, \omega') &= X(\omega) \\
  \hat{Y}(\omega, \omega') &= Y(\omega')
  \end{cases}
\end{align*}
Then $\hat{X}$ and $\hat{Y}$ are RVs $X$ and $Y$ suitably adapted to
live on probability space $(\Omega,\Omega')$.
\end{defn}

\clearpage
\section{Inequalities}

\begin{prop}{\emph{(Jensen's Inequality)}}
Let $g$ be a function and let $X$ be some random variable. Then
\begin{align*}
  \text{$g$ convex} &\implies \E[g(X)] \geq g\left(\E[X]\right) \\
  \text{$g$ concave} &\implies \E[g(X)] \leq g\left(\E[X]\right) \\
\end{align*}
\end{prop}
\begin{proof}
Since $g$ is a convex function, given any $x$, there is a constant $a$
such that
\begin{align*}
  g(y) \geq g(x) + a (y-x)
\end{align*}
Choose $x=\E[X]$. Then we can say that there is an $a$ such that
\begin{align*}
  g(y) &\geq g(\E[X]) + a (y-\E[X])
\end{align*}
But this must also be true for $y=X$, so
\begin{align*}
  g(X) &\geq g(\E[X]) + a (X-\E[X])
\end{align*}
Now take expectation
\begin{align*}
  \E[g(X)]
  &\geq \E[g(\E[X]) + a (X-\E[X])] \\
  &= \E[g(\E[X])] + a \E[(X-\E[X])] \\
  &= g(\E[X]) + a (\E[X]-\E[X]) \\\\
  \implies \quad
  \E[g(X)] &\geq g(\E[X])
\end{align*}
Concave proof is analogous.
\end{proof}

\begin{thm}{\emph{(Markov's Inequality)}}
\label{thm:markov}
Given random variable $X$,
\begin{equation}
    \label{markov}
    \Prb\left[
      \left\lvert X\right\rvert
      \geq \varepsilon\right]
    \leq \frac{\E\left[|X|^p\right]}{|\varepsilon|^p}
\end{equation}
\end{thm}

\begin{proof}
Write out the expectation in the numerator, where $F$ is the cdf of $X$:
\begin{align*}
  \E\left[|X|^p\right]
  &= \int^\infty_{-\infty} |X|^p \; dF \\
  &= \int^{\varepsilon}_{-\infty} |X|^p \; dF
    + \int^\infty_{\varepsilon} |X|^p \; dF
\end{align*}
Next, let's just throw out the first term in the sum above to get an
inequality. Since $|X|^p$ and the cdf are always positive, we know that
we're throwing out a positive portion, which gives us inequality
\begin{align*}
  \E\left[|X|^p\right]
  &\geq \int^\infty_{\varepsilon} |X|^p \; dF
\end{align*}
Next, notice that a lower bound for $|X|^p$ over this support will be
$|\varepsilon|^p$, so we can modify the inequality again to get
\begin{align*}
  \E\left[|X|^p\right]
  &\geq  |\varepsilon|^p \int^\infty_{\varepsilon} \; dF
  = |\varepsilon|^p \; \Prb[X\geq \varepsilon] \\
  \implies \quad
  \frac{\E\left[|X|^p\right]}{|\varepsilon|^p}
  &\geq \Prb(X\geq \varepsilon)
\end{align*}
\end{proof}

\begin{cor}{\emph{(Chebyshev's Inequality)}}
Given random variable $X$, we have
\begin{equation}
  \label{chebyshev1}
  \Prb\left[
    \left\lvert X\right\rvert
    \geq \varepsilon\right]
  \leq \frac{\E\left[X^2\right]}{\varepsilon^2}
\end{equation}
or equivalently
\begin{equation}
  \label{chebyshev2}
  \Prb\left[
    \big\lvert X-\E[X]\big\rvert
    \geq \varepsilon\right]
  \leq \frac{\Var(X)}{\varepsilon^2}
\end{equation}
\end{cor}
\begin{proof}
The proof of Statement~\ref{chebyshev1} follows by taking $p=2$ in
Markov's Inequality (Theorem~\ref{markov}).
To prove Statement~\ref{chebyshev2}, define $Z=X-\E[X]$ and use that in
Markov's inequality with $p=2$ on $Z$.
\end{proof}

\clearpage
\section{Large Sample Theory}

\subsection{Convergence Results}

Recall the usual Analysis definition of convergence of a sequence: A
sequence $\{x_n\}$ converges to limit $x$ if for all $\varepsilon> 0$,
there exists a finite $N$ such that
\begin{align*}
  n > N \implies ||x_n - x|| < \varepsilon
\end{align*}
But this, of course, is for a sequence of numbers $\{x_n\}$ in some
metric space. There's no randomness there. So what about a sequence of
random variables---something non-deterministic?

There are a few options when talking about convergence of random
variables, but all involve resolving the randomness somehow and checking
convergence of functions, probabilities, or expectations---all of which
are possible given the usual analysis machinary.
\begin{enumerate}
  \item \emph{Almost Sure Convergence}: Check if the sequence of
    functions $\{X_n\}$ on sample space $\Omega$ converge to some
    function $X$ on $\Omega$, i.e. $X_n(\omega)=X(\omega)$ for all
    $\omega\in\Omega$ that have nonzero probability.
  \item \emph{Convergence in Probability}: The sequence of probabilities
    that $X_n(\omega)$ is far from $X(\omega)$ over all $\omega$ goes to
    zero.
  \item \emph{Converge in $p$-Norm}: The sequence of expected distances
    $\E[|X_n-X|^p]$ goes to zero.
\end{enumerate}
In all of these cases, we've framed convergence of random variables in
terms of convergence of functions or numerical sequences. Now onto the
complete definitions.

\begin{defn}{(Almost Sure or Strong Convergence)}
Given a sequence of random variables $\{X_n\}$ and a random variable
$X$, we say that ``$X_n$ \emph{converges almost surely} to $X$'' written
\begin{align*}
  X_n\asto X
  \quad \iff \quad
  \limn \Prb[X_n = X]
  = \limn \Prb[\,\omega \,|\,X_n(\omega) = X(\omega)] = 1
\end{align*}
This checks that $X_n(\omega)\ra X(\omega)$ for each elementary event
$\omega$ in the sample space $\Omega$ that has nonzero probability.
(Though the functions can differ arbitrarily on elementary events that
will never occur.) If $X_n$ and $X$ are vectors, we need almost surely
convergence element-by-element.

This notion is called ``strong convergence'' because it it computes the
probability of the limit of a sequence of random variables, rather than
the limit of a sequence of probabilities. We're effectively asking that
the random variable \emph{itself}, which is a function on sample space
$\Omega$, converges pointwise to another function on $\Omega$---the
random variable $X$.
\end{defn}


\begin{defn}{(Convergence in Probability)}
We say that a sequence of random variables $\{ X_n \}$ on measure space
$(\Omega,\calF,\Prb)$ \emph{converges in probability} to $X$ if
\begin{equation}
  \label{plim}
  X_n\pto X
  \quad\iff\quad
  \limn p_n(\varepsilon) :=
  \limn
  \Prb(\left\lvert X_n - X \right\rvert > \varepsilon) = 0
  \qquad \forall  \varepsilon> 0
\end{equation}
This is someetimes written as $\plim X_n = X$.
 If $X_n$ and $X$ are vectors, we need convergence in probability
 element-by-element.


Convergence in probability is also known as \emph{weak convergence}
because it doesn't ask the random variables (functions) $X_n$ to
converge to the random variable (function) $X$, i.e.\ that
$X_n(\omega)=X(\omega)$ identically for all $\omega\in\Omega$.  Instead,
we just stipulate that the realizations are very close to $X$ as
$n\rightarrow\infty$.
\end{defn}

\begin{defn}{(Mean Square and $p$-Norm Convergence)}
We say that a sequence of random variables $\{ X_n \}$
\emph{converges in $p$-Norm} to $X$, written
\begin{align*}
  X_n \Lpto X
  \quad\iff\quad
  \limn \E\left[|X_n-X|^p\right] = 0
\end{align*}
The special case where $p=2$ is called \emph{mean square convergence} or
\emph{convergence in quadratic mean}, and is often written $X_n\msto X$.
If $X_n$ and $X$ are vectors, we need convergence in $p$-norm
element-by-element.
\end{defn}


\begin{defn}{(Convergence in Distribution)}
\label{defn:convergeInDistribution}
We say that a sequence of random variables $\{X_n\}$
\emph{converges in distribution} to random variable $X$, written
\begin{align*}
  X_n\dto X
  \quad\iff\quad
  \limn F_{X_n}(x) = F_X(x)
\end{align*}
for all values of $x$ where $F_x$ is continuous,
where $F_{X_n}$ and $F_X$ are the cdfs of $X_n$ and $X$, respectively.

If $X_n$ and $X$ are vectors, then the joint cdf of $X_n$ must converge
to the joint CDF of $X$. In that case, $a'X_n \dto a'X$ for any
non-stochastic vector $a$ (which is called the Cramer-Wold device).
\end{defn}
\begin{ex}
To see why we need to restrict convergence to points of continuity on
$F_X(x)$, consider the following example:
\begin{align*}
  F_{X_n}(x) &= \one_{\left\{x\geq \frac{1}{n}\right\}}\\
  F_{X}(x) &= \one_{\left\{x\geq 0\right\}}
\end{align*} In other words, random variable $X_n$ takes on value
$\frac{1}{n}$ with probability one, while $X$ takes on value zero with
probability one.  So if we check convergence at $x=0$, we have a
sequence $\{F_{X_n}(0)\}=\{0\}$, which clearly does not converge to
$F_X(0)=1$.

However, it is the case that $F_{X_n}$ is well-approximated by $F_X$ at
all other points, so we'd like to be able to say $X_n\dto X$. And we
can, if we simply restrict ourselves to points where $F_X$ is
continuous.
\end{ex}

\begin{prop}
Sequence of random variables $\{X_n\} \dto X$ if and only if
\begin{align*}
  \E[g(X_n)] = \E[g(X)]
\end{align*}
\end{prop}
\begin{rmk}
This is another useful way to characterize to characterize convergence
in distribution aside from Definiton~\ref{defn:convergeInDistribution}.
It's extremely handy for a lot of results.
\end{rmk}

\begin{prop}
Given a sequence of random variables $\{X_n\}$ and random variable $X$,
the various convergence concepts are related in the following way:
\begin{enumerate}
  \item $X_n\asto X \implies X_n\pto X$. Converse not true in general.
  \item $X_n\Lpto X \implies X_n\pto X$
  \item Any one of
    \begin{align*}
      \begin{rcases}
        X_n&\asto &X \\
        X_n&\pto &X \\
        X_n&\Lpto &X
      \end{rcases}
      \implies X_n\dto X
    \end{align*}
    We see here even more explicitly that converge in distribution is
    \textsc{super weak}.
  \item Almost Surely and $p$-norm, undecidable.
  \item $X_n\pto X \implies$ There's a deterministic subsequence such
    that $X_{n_k}\asto X$ (but does not imply almost sure convergence
    for the original sequence).
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn.
\begin{enumerate}
  \item
    %Suppose that we are given arbitrary $\delta,\varepsilon>0$. We
    %need to show that we can find an $N$ such that
    %\begin{align*}
      %n>N
      %\implies
      %P(|X_n-X|>\varepsilon) < \delta
    %\end{align*}
    %Since $X_n$ converges to $X$ almost surely, for any $\omega$, there
    %is an $N_\omega$ such that
    %\begin{align*}
      %n > N_\omega
      %\implies
      %|X_n(\omega)-X(\omega)| < \varepsilon
    %\end{align*}

  \item Define random variable $Z=X_n-X$. Suppose that $\varepsilon$ is
    given. From Markov's inequality,
    \begin{align*}
      P(|Z| \geq \varepsilon) \leq \frac{\E[|Z|^p]}{|\varepsilon|^p}
      \implies
      P(|X_n-X| \geq \varepsilon) \leq \frac{\E[|X_n-X|^p]}{|\varepsilon|^p}
    \end{align*}
    As $n\ra\infty$, the righthand side of the inequality goes to zero
    since $X_n$ converges to $X$ in $p$-norm. Hence, the lefthand side
    holds.

\end{enumerate}
\end{proof}


%Now for some final notes. Convergence in probability
%is \textbf{not} convergence in expectation. The former
%concerns a sequence of probabilities, while the latter a
%sequence of expectations. Finally, the probability limit
%$X$ must be free of all dependence upon the sample size $n$.



\section{Related Concepts}

\paragraph{Consistency} A sequence of estimators $\{ \hat{\theta}_n \}$
where $n=1,2,\ldots$ is \emph{consistent} for parameter $\theta$ if
$\hat{\theta}_n$ converges \emph{In Probability} to $\theta$.

\paragraph{Strongly Consistent} If convergence of $\hat{\theta}_n$
to $\theta$ holds with probability 1.



\section{Probability with Measure Theory}


\begin{rmk}
Because a $\sigma$-algebra is closed under countable unions,
interesections, and complements, it provides a natural way to think
about collections of elementary outcomes that we might care about. Said
another way, the definition of a $\sigma$-algebra results gives us
pretty much all of the sets we'd like to assign probabilities to.
\end{rmk}

\begin{ex}
For an even simpler example than die rolling of something that is
a perfectly valid $\sigma$-algebra, we
sometimes take the power set of $\Omega$---the collection of all
possible subsets of $\Omega$---to be our $\sigma$-algebra.  It will
surely satisfy the necessary properties, and it happens to be very easy
to denote.
\end{ex}


\subsection{Measure}

Once we have a $\sigma$-algebra, we can pair the sample space with it's
$\sigma$-algebra to form what's called a
\textbf{measurable space}, which is simply the ordered pair
$(\Omega, \mathcal{F})$.  From there, we can start assigning
probabilities to those elements in the $\sigma$-algebra which all have
some chance of occurring.  To do so, we will use a special
type of function, called a measure.

A \textbf{measure} is a function that maps sets into real numbers.
In our case, we will define a specific type of measure, a
\textbf{probability measure}, as a
function that has as it's domain a $\sigma$-algebra and maps measurable
sets in the $\sigma$-algebra into the
real line, subject to a few conditions. More precisely, it has the
following characteristics:
\begin{itemize}
   \item[i.] If $P$ is our probability measure and $\mathcal{F}$ is
      our $\sigma$-algebra, then
	 \[ P: \mathcal{F} \rightarrow \mathbb{R} \]
   \item[ii.] $ P(F) \in [0,1]$ for all measurable sets
      $F \in \mathcal{F}$. In addition, $P(\Omega) =1$.
   \item[iii.] Our probability measure satisfies the probability axioms.
      And if $F_i$ are all disjoint and members of the $\sigma$-algebra,
      then
	 \[ p( \cup F_i) = \sum P(F_i) \]
\end{itemize}

\paragraph{Definition} Now that we have our definition of a probability,
I just want to define a common term more precisely.  Specifically,
if a property is true \emph{except} for an event of probability 0,
then we say that property holds ``almost surely.''

\subsection{Probability Space}

So now we have a way of assigning probabilities to any arbitrary event in our measurable space, $(\Omega, \mathcal{F})$. If we
join the probability measure, $P$, to our measurable space, we obtain a proper \textbf{Probability Space}.  It is defined as
the ordered triplet
\[(\Omega, \mathcal{F}, P) \]

\subsection{Measurable Function and Random Variables}

One of the fundamental notions of probability is that of a random
variable; therefore, making the definition more rigorous
deserves some attention.

So let's start with the notion of a \textbf{measurable} function.
If $(\Omega_1, \mathcal{F}_1)$ and $(\Omega_2, \mathcal{F}_2)$ are two
measurable spaces then, a function
   \[ f: (\Omega_1, \mathcal{F}_1) \rightarrow (\Omega_2, \mathcal{F}_2)
      \]
is a measurable function if
   \[ f^{-1}(B) \in \mathcal{F}_1, \;\; \forall B \in\mathcal{F}_2 \]
In words, a function is measurable if the pre-image of an element in
the $\sigma$-algebra of the co-domain is in the
$\sigma$-algebra of the domain.  Simply, it preserves the structure
between two measurable spaces, sending measurable sets
in one measurable space to measurable sets in the other.

Turning to probability, suppose  that we have our probability space,
$(\Omega_1, \mathcal{F}_1, P)$, and another measurable space,
$(\Omega_2,\mathcal{F}_2)$.  Then an
$\mathbf{(\Omega_2, \mathcal{F}_2)}$\textbf{-valued random variable}
is a \emph{measurable function}
\[ X: (\Omega_1, \mathcal{F}_1) \rightarrow (\mathbb{R},
   \mathcal{F}_2) \]

Since $X$ is a measurable function, we know that
   \[ X^{-1}(B) \in \mathcal{F}_1, \;\; \forall B \in \mathcal{F}_2\]
in which case we say that $X$ is $\mathcal{F}_1$-measurable.
To clarify further
   \[ X^{-1}(B) = \left\{ \omega \in \Omega_1 | X(\omega)
      \in B \right\} \]

\paragraph{Note} Oftentimes, we'll just take the set
$\mathcal{F}_2$ to be some properly
defined $\sigma$-algebra on the real line, like the Borel Set
(defined below).

\subsection{Sigma Algebras Generated by Random Variables}

Above, we assumed that we knew the $\sigma$-algebra for
$\Omega_1$ already---that it was given. But we
could just as well \emph{generate} one from some random variable $X$,
and this generated $\sigma$-algebra could very well differ from
$\mathcal{F}_1$.

So let's assume that $X$ is a random variable
   \[X: (\Omega_1,\mathcal{F}_1) \rightarrow
      (\mathbb{R},\mathcal{F}_2).\]
Then the $\sigma$-algebra generated by $X$, denoted by $\sigma(X)$, is
defined as \emph{all} sets of the form
   \[ \sigma(X) := \{ \omega \in \Omega_1 | X(\omega) \in A \}, \;\;
      \forall \; A \subset \mathbb{R} \]
which can be written more compactly as
   \[\sigma(X)  = \{ X^{-1}(A) | A\subset \mathbb{R}  \} \]

\paragraph{Definition} Finally, if $\mathcal{G}$ is a
sub-$\sigma$-algebra of the set $\mathcal{F}$ defined above, then the
random variable $X$ is $\mathcal{G}$-measurable if
   \[ \sigma(X) \subset \mathcal{G} \]

Let's give a concrete example.  Suppose $\Omega$ consists of all the
possible combination of up and down moves in a binomial tree.  $X$ is
a random variable denoting stock price.  Then for a set of real numbers
representing the possible prices the stock could take on (this is
our $A \subset \mathbb{R}$), the set $\sigma(X)$ will be the sigma
algebra resulting from the set of all possible paths that that the
stock could have taken to get to those prices in $A$.

\subsection{Random Variables and Their Distributions}

A random variable is actually quite distinct from its distribution.  Recall that a Random Variable is just a mapping from the
sample space $\Omega$ into something more tractable, like the real line.  Therefore, we could discuss this mapping--the random variable--
without ever considering probabilities.

However, oftentimes we will want to discuss probabilities, but the sufficiently general definition of the random variable just given
offers a lot of flexibility.  So much, in fact, that a random variable could have more than one distribution, as we know occurs in
stock process which have traditional probabilities and also \emph{risk-neutral} probabilities (or \emph{pseudo-probabilities}).

So let's define a \textbf{distribution} as a measure that assigns probabilities to the real numbers that a random variable generates
(after being passed an element in the sample space).  The most natural distribution is the induced measure, $\mathcal{L}_X$ defined
as follows
\[ \mathcal{L}_X(A) := P \{ X \in A \}, \;\; A \subset \mathbb{R} \]

Let's unpack that. If $A$ is a subset of the real line, it the random variable may or may not take on values in that subset, so we want
a notion of probability.  Well, we can take the pre-image, $X^{-1}(A)$, and look at all those elements of the sample space that map to
$A \subset \mathbb{R}$ under the random variable $X$.  Then, once we have elements of the sample space, we can use our traditional notion
of the $\sigma$-algebra, $\mathcal{F}$, and the probability measure $P$ that are already defined on the probability space to assign
the probabilities.

Thus, we can associate probabilities with our random variable so long as there exists a distribution measure, like $\mathcal{L}_X$.
But recall that $\mathcal{L}_X$ was not unique.  It's simply a function to assign probabilities to values that the random variable can
take on.  We could consider other measures that also accomplish this, and that insight legitimizes the use of such things as
\emph{risk-neutral probabilities}.

\subsection{Lebesgue Measure and the Lebesgue Integral}

\subsection{Introduction}

The \textbf{Lebesgue Measure} is the standard way of assigning a measure to subsets of $n$-dimensional Euclidean space.  If we restrict
to $\mathbb{R}$, then the Lebesgue Measure of intervals is simply the length. But rather than consider all of $\mathbb{R}$, we'll
restrict further to \emph{Borel Sets}.

This will allow us to construct the \emph{Lebesgue Integral}, a generalization of the Riemann Integral.

\subsection{Borel Sets}

The \emph{Borel $\sigma$-algebra}, denoted $\mathcal{B}(\mathbb{R})$, is the smallest $\sigma$-algebra containing (and, in a sense,
generated by) all open intervals
in $\mathbb{R}$.

\paragraph{Examples} The following are a few samples of Borel Sets in $\mathcal{B}(\mathbb{R})$:
\begin{itemize}
   \item{Every open interval of the form $(a,b)$.}
   \item{The open rays $(a,+\infty)$ and $(-\infty,b)$.}
   \item{All unions of the form
	 \[ B_1 \cup B_2, \;\; B_1,B_2 \in \mathcal{B}(\mathbb{R}) \]
      }
   \item{All complements of sets in $\mathcal{B}(\mathbb{R})$, since it's a sigma algebra. Note that this implies all \emph{closed}
      intervals in $\mathbb{R}$ are Borel Sets as well, in addition to all half-open and half-closed intervals.}
   \item{All one point sets, $a\in\mathbb{R}$, as we see that it is in the intersection of other Borel Sets, implying inclusion in
	 $\mathcal{B}(\mathbb{R})$ since it's a $\sigma$-algebra
	 \[ \{a \} = \bigcap_{n=1}^{\infty} \left( a - \frac{1}{n}, a + \frac{1}{n} \right). \]
      }
   \item{The last item implies that all finite countable collections of points in $\mathbb{R}$ are Borel Sets too. Therefore, the set
      of all rational numbers is Borel since coutable, and the set of all irrational numbers is Borel since it's the complement of a
      set in the sigma algebra.}
\end{itemize}

However, it should be noted that not all sets of real numbers are Borel Sets.  In particular, any non-Borel set must be uncountable
(though the opposite is not ture, as shown above).

\subsection{Lebesgue Measure}

Let's start more generally and define a \emph{measure} on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ to be a function
\[ \mu: \mathcal{B} \rightarrow [0,\infty) \]
with the following properties
\begin{itemize}
   \item[i.]{$\mu(\emptyset) = \emptyset$.}
   \item[ii.]{If $A_1, A_2, \ldots$ are disjoing sets in $\mathcal{B}(\mathbb{R})$, then
	 \[ \mu\left(\bigcup_{k=1}^{\infty} A_k\right) =  \bigcup_{k=1}^{\infty} \mu(A_k). \]
      }
\end{itemize}

We define the \textbf{Lebesgue Measure} on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ to the measure $\mu_0$ that assigns the measure of
each interval to be its length.

\subsection{Functions in This World}

Let $f$ be a function
\[ f: \mathbb{R} \rightarrow \mathbb{R}. \]
We say that $f$ is \textbf{Borel-Measurable} if
\[ A \in \mathcal{B}(\mathbb{R}) \Rightarrow f^{-1}(A) \in \mathcal{B}(\mathbb{R}). \]
Or equivalently, we could say that we want the $\sigma$-algebra generated by $f$ to be contained in $\mathcal{B}(\mathbb{R})$.

\subsection{The Lebesgue Integral}

Let $I$ be the \emph{indicator function}.  We define
\[ A := \{ x \in \mathbb{R}| I(x) =1 \} \]
to be the set \emph{indicated by I}.

The \textbf{Lebesgue Integral} of $I$ is defined
\[ \int_{\mathbb{R}} I d\mu_0 = \mu_0(A). \]


\section{Stochastic Processes}

Throughout this section, we will explore stochastic processes building
from the simple case of a discrete state space in discrete time to the
most general case of an infinite state in continuous time, relaxing constraints through the subsections.

However, before proceeding, I here lay down the most generaly
definitions for stochastic process, which will we restrict to special
cases later on.

\subsection{General Definitions}

\begin{defn}{\citep{pavliotis}}
Let $T$ be an ordered set, $(\Omega,\mathscr{F},\mathbb{P})$ be a
probability space where $\Omega$ is the sample space, and
$(E,\mathscr{G})$ be a measurable space.

A \emph{stochastic process} is a collection of random variables $X =
\{X_t | t\in T\}$ such that for each $t\in T$, $X_t:
(\Omega,\mathscr{F},\mathbb{P})\mapsto (E,\mathscr{G})$ is a random
variable taking on values in its state space $E$.
\end{defn}

\begin{rmk}
Though it might not be obvious at first, a stochastic process dependends
upon two arguments: some $\omega\in\Omega$ and some $t\in T$. Therefore
the following representations are equivalent: $X_t$, $X_t(\omega)$,
$X(t,\omega)$ where the notation used will depend upon context.

Therefore, we might first think about $X(t,\omega)$ fixing $\omega$ and
letting $t$ vary. This corresponds to choosing a realization $\omega$ of
some experiment and looking at the resulting \emph{trajectory} of the
process.

On the other hand, we might fix $t$ and think about the different values
that $X(t,\omega)$ might take on depending upon the trajectory that is
realized in some experiment.
\end{rmk}

\begin{defn}
A process $\{X_t\}$ satisfies the \emph{Markov Property} if the
\begin{align*}
\mathbb{P}\left(X_{s+t} | \{X_{s-\varepsilon}\}_{\forall\varepsilon>0}\right)
= \mathbb{P}\left(X_{s+t} | X_{s}\right)
\end{align*}
In other words, only the latest state $X_s$ matters for the future distribution of $X_t$

\end{defn}

\subsection{Discrete State Space, Discrete Time}

\begin{defn}
A \emph{discrete space, discrete time Markov chain} is a stochastic
process $\{X_{t_i}\}_{t_i\in T}$ for some finite or countable set $T$
that satisfies
\begin{equation}
  P(X_n = x_n | X_{n-1} = x_{n-1}, \ldots
  X_0 = x_0) = P(X_n = x_n | X_{n-1}=x_{n-1})
\end{equation}
In words, only the most recent state matters for the future.
\end{defn}

\subsection{Infinite State Space, Discrete Time}
\subsection{Infinite State Space, Continuous Time}

\section{Stochastic Differential Equations}

Ordinary differential equations allow us to naturally relate rates of
change for some function to time and the value of the function itself:
\begin{align*}
  \frac{dy(t)}{dt} = b(t,y(t))
\end{align*}
Now, we want to modify this to allow random noise to influence the
change in the value of some stochastic process:
\begin{align*}
  \frac{dX_t}{dt} = b(t,X_t) + \sigma(t,X_t) \xi_t
\end{align*}
where $\xi_t$ is some random variable representing ``noise.'' (Though
because of some mathematical technicalities, we can't really define
$\xi_t$ this way since it is like a derivative, but one that cannot
exist. So we'll need to move to a definition with integrals that can be
properly defined.)




\clearpage
\bibliographystyle{apalike}  % Or some other style
\bibliography{biblio} % where sources.bib has all the citation info






\section{Delta Method}

Suppose we have a sample of $\left\{\boldsymbol{y}_i\right\}_{i=1}^N$ of
some random variable $\boldsymbol{y}\sim
N(\boldsymbol{\mu},\boldsymbol{\Sigma})$. We construct an estimator
$\boldsymbol{\hat{\mu}}$ for the first moment vector $\boldsymbol{\mu}$
whose sampling distribution we can easily characterize. Then, we form a
more complicated function of the first moment vector,
$\boldsymbol{g}(\boldsymbol{\mu})$.  We want a sampling distribution for
that guy too (mean and standard errors). The \emph{delta method} allows
us to do that.

\subsection{Notation}

Suppose that we have random variable $\boldsymbol{y}\in \mathbb{R}^m$,
with mean and variance-covariance matrix as follows:
\begin{align*}
  \boldsymbol{\mu} &:= \mathbb{E}(\boldsymbol{y}) =
  \begin{pmatrix}
    \mathbb{E}(y_1)\\\mathbb{E}(y_2)\\\vdots\\\mathbb{E}(y_m)
  \end{pmatrix}
  \in \mathbb{R}^m
  \\
  \boldsymbol{\Sigma}
  &:=
  \mathbb{E}\left[
    (\boldsymbol{y}-\boldsymbol{\mu})
    (\boldsymbol{y}-\boldsymbol{\mu})'
  \right]
  \in \mathbb{R}^{m\times m}
\end{align*}
Given sample $\{\boldsymbol{y}_i\}_{i=1}^N$, we can form an estimator
for $\boldsymbol{\mu}$ as follows:
\begin{align*}
  \boldsymbol{\hat{\mu}}
  &=
  \frac{1}{N}
  \sum^N_{i=1}
  \boldsymbol{y}_i\\
  \sqrt{N}(\boldsymbol{\hat{\mu}} - \boldsymbol{\mu})
  &\sim N(0,\Sigma)
\end{align*}
That's the easy sampling distribution to characterize. The point of the
delta method is to characterize the sampling distribution of
\emph{transformations} of $\boldsymbol{\mu}$. In particular, we want a
sampling distribution for an estimator of arbitrary transformation
$\boldsymbol{g}(\boldsymbol{\mu})$.

\paragraph{Note}
The fact that $\boldsymbol{g}(\boldsymbol{\mu})$ is only a function of
the first moment of $\boldsymbol{y}$ isn't that restrictive if we're
willing to generalize our notion of $\boldsymbol{y}$. We can easily
accommodate more complicated moments with a minor tweak.

To see this, suppose that we have some base random variable
$\boldsymbol{x}\in\mathbb{R}^n$
\begin{align*}
  \boldsymbol{x} =
  \begin{pmatrix}
    x_1\\x_2\\\vdots\\x_n
  \end{pmatrix}
  \in \mathbb{R}^n
\end{align*}
along with sample $\{\boldsymbol{x}_i\}_{i=1}^N$. If we want second
moments or cross-correlations of elements in $\boldsymbol{x}$, simply
define a function $\boldsymbol{h}: \mathbb{R}^n \rightarrow
\mathbb{R}^m$ to do the transformations you want and set
\begin{align*}
  \boldsymbol{y}
  :=\boldsymbol{h}(\boldsymbol{x})
  =
  \begin{pmatrix}
    h_1(\boldsymbol{x})\\h_2(\boldsymbol{x})\\\vdots\\h_m(\boldsymbol{x})
  \end{pmatrix}
  \in\mathbb{R}^m
\end{align*}
Then, to generate the sample for $\boldsymbol{y}$, just take
\begin{align*}
  \{\boldsymbol{y}_i\}_{i=1}^N &:=
  \left\{\boldsymbol{h}\left(\boldsymbol{x}_i\right)\right\}_{i=1}^N
\end{align*}
From there, proceed exactly as before. In this way, we can always use
the ``first moment representation'' above, leaving open the possibility
that $\boldsymbol{y}$ is actually a transformation or function of some
base random variable.


\subsection{Delta Method Statement}

\begin{thm}
Let $\boldsymbol{\hat{\mu}}_N$ denote the mean of iid sample
$\{\boldsymbol{y}_i\}^N_{i=1}$ for random variable $\boldsymbol{y}$.
The central limit theorem tells us that as $N\rightarrow\infty$,
\begin{align*}
  \sqrt{N}\left(
  \boldsymbol{\hat{\mu}}_N
  -\boldsymbol{{\mu}}
  \right)
  \xrightarrow{d} N(0, \boldsymbol{\Sigma})
\end{align*}
For continous transformation
$\boldsymbol{g}:\mathbb{R}^m\rightarrow \mathbb{R}^p$, the
\emph{delta method} states that
\begin{align*}
  \sqrt{N}\left(
  \boldsymbol{g}(\boldsymbol{\hat{\mu}}_N)
  -
  \boldsymbol{g}(\boldsymbol{\mu})
  \right)
  &\xrightarrow{d} N\left(
    0, \boldsymbol{G} \boldsymbol{\Sigma} \boldsymbol{G'}
  \right)\\\\
  \text{where}\quad
  \boldsymbol{G}(\boldsymbol{u})
  &:=\frac{\partial}{\partial \boldsymbol{u'}}
  \left[\boldsymbol{g}(\boldsymbol{u})\right]\\
  \text{and} \quad
  \boldsymbol{G} &:= \boldsymbol{G}(\boldsymbol{\mu})
\end{align*}
%More explicitly, $\boldsymbol{G}$ is the Jacobian of $\boldsymbol{g}$
%evaluated at the true value $\boldsymbol{\mu}$.
\end{thm}
\begin{proof}
Start with a Taylor Series expansion of
$\boldsymbol{g}(\boldsymbol{\hat{\mu}}_N)$ about the true value
$\boldsymbol{\mu}$:
\begin{align*}
  \boldsymbol{g}(\boldsymbol{\hat{\mu}}_N)
  &\approx
  \boldsymbol{g}(\boldsymbol{\mu})
  + \boldsymbol{G}
  \left(
  \boldsymbol{\hat{\mu}}_N - \boldsymbol{\mu}
  \right)\\
  \Rightarrow\qquad
  \text{Var}\left(\boldsymbol{g}(\boldsymbol{\hat{\mu}}_N)\right)
  &\approx
  \text{Var}\left(\boldsymbol{g}(\boldsymbol{\mu})
  + \boldsymbol{G}
  \left(
  \boldsymbol{\hat{\mu}}_N - \boldsymbol{\mu}
  \right)
  \right)
\end{align*}
Since $\boldsymbol{\mu}$ and $\boldsymbol{G}$ are
constants,\footnote{Frequentist alert! Dogmatic Bayesians please avert
eyes and forgive my impurity.} we can drop the variance of all
$\boldsymbol{\mu}$ and $\boldsymbol{G}$ terms to get:
\begin{align*}
  \text{Var}\left(\boldsymbol{g}(\boldsymbol{\hat{\mu}}_N)\right)
  &\approx
  \text{Var}\left(
  \boldsymbol{G}
  \boldsymbol{\hat{\mu}}_N
  \right)
  \approx
  \boldsymbol{G}\;
  \text{Var}\left(
  \boldsymbol{\hat{\mu}}_N
  \right)
  \boldsymbol{G'}
  = \frac{1}{{N}}
  \boldsymbol{G}
  \boldsymbol{\Sigma}
  \boldsymbol{G'}
\end{align*}
where the last inequality followed by substituting in the distribution
of $\boldsymbol{\hat{\mu}}_N$. This gives us everything we need.
\end{proof}

\subsection{Delta Method Intuition}

\clearpage
\section{Generalized Method of Moments (GMM)}

GMM is often done in the context of time series models, and so I will
use $T$ to denote sample size rather than $N$.

\subsection{Notation}

Suppose that we have vector random variable $\boldsymbol{y}_t$ with
population mean $\mathbb{E}(\boldsymbol{y}_t)$. Given a sample
$\{\boldsymbol{y}_t\}^T_{t=1}$, we can form an estimator of the mean
\begin{align*}
  \mathbb{E}_T(\boldsymbol{y}_t)
  := \frac{1}{T} \sum^T_{t=1} \boldsymbol{y}_t
\end{align*}
Oten, however, we have a model and want to estimate a vector of
parameters, $\boldsymbol{b}$. When this is the goal, we define a function
that suppresses some of the notation above and makes the focus on
parameters more explicit:
\begin{align*}
  \boldsymbol{g}(\boldsymbol{b})
  &:= \mathbb{E}[\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})]\\
  \boldsymbol{g}_T(\boldsymbol{b})
  &:= \mathbb{E}_T[\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})]
  = \frac{1}{T}\sum^T_{t=1}
      \boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})
\end{align*}
where $\boldsymbol{f}$ is some function that mixes data and parameters
according to model-implied structure. We'll see more clearly how this is
used below.

But in general, note the convention: For a population concept like
$\mathbb{E}(\cdot)$ or $\boldsymbol{g}(\cdot)$, we form the sample
estimators (given $T$ observations) of $\mathbb{E}_T(\cdot)$ and
$\boldsymbol{g}_T(\cdot)$.

This convention might seem a little odd at first since
$\mathbb{E}_T(\cdot)$ is often used to denote the expectation
conditional on time $T$ information. But that's not going to be a
problem here, since GMM will deal pretty much exclusively with
\emph{unconditional} moments (or at least unconditional with respect to
time).\footnote{%
To take a quick example, consider the simple de-meaned, stationary AR(1)
model
\begin{align*}
  y_t = \rho y_{t-1} + \varepsilon_t
  \qquad \varepsilon_t \sim N(0,\sigma^2)
\end{align*}
You might reasonably care about \emph{either} the unconditional or (on
time $t-1$ information) mean and variance of $y_t$.
\begin{alignat*}{3}
  \mathbb{E}(y_t) &= 0 &\mathbb{E}_t(y_{t}) &= \rho y_{t-1}\\
  \text{Var}(y_t) &= \frac{\sigma^2}{1-\rho^2}
  \qquad
  &\text{Var}_t(y_t) &= \sigma^2
\end{alignat*}
In GMM, we'll be talking about the unconditional estimates on the left.
}
Plus, the convention has the added benefit of connecting estimators to
their sample estimates in about the most explicit way possible (i.e.\
just add ${}_T$ to go from estimator to estimate).


\subsection{Distribution of Sample Mean}
\label{sec:mean}

This is a rather straightfoward exercise, but it allows us to derive a
few important concepts in GMM notation. So let's go ahead.

Given a sample $\{\boldsymbol{y}_t\}^T_{t=1}$ of stationary (typically
autocorrelated) random variable $\boldsymbol{y}_t$, we defined an
estimator of the sample mean, $\mathbb{E}_T(\boldsymbol{y}_t)$. Now
let's characterize that estimator's distribution, since it too is a
random variable.

First, the mean:
\begin{align*}
  \mathbb{E}\left(\mathbb{E}_T(\boldsymbol{y}_t)\right)
  &= \mathbb{E}\left(\frac{1}{T}\sum^T_{t=1} \boldsymbol{y}_t\right)
  = \frac{1}{T}\sum^T_{t=1} \mathbb{E}\left(\boldsymbol{y}_t\right)
  = \frac{1}{T} \times T \; \mathbb{E}\left(\boldsymbol{y}_t\right)\\
  &= \mathbb{E}\left(\boldsymbol{y}_t\right)
\end{align*}
So $\mathbb{E}_T(\boldsymbol{y}_t)$ is an unbiased estimator of the
unconditional mean of $\boldsymbol{y}_t$.

Now we get the variance of the estimator, accounting for the fact that
the sample $\{\boldsymbol{y}_t\}_{t=1}^T$ is not necessarily iid (though
assumed stationary):
\begin{align}
  \text{Var}(\mathbb{E}_T(\boldsymbol{y}_t))
  &=
  \text{Var}\left(
  \frac{1}{T}\sum^T_{t=1} \boldsymbol{y}_t
  \right)
  =
  \frac{1}{T^2}\text{Var}\left(
  \sum^T_{t=1} \boldsymbol{y}_t
  \right)\notag\\
  &=
  \frac{1}{T^2}
  \sum^T_{j=-T} (T-|j|)\text{Cov}\left(
    \boldsymbol{y}_t, \boldsymbol{y}_{t-j}
  \right) \notag\\
  &=
  \frac{1}{T^2}
  \left[
    \sum^T_{t=1} \text{Var}(\boldsymbol{y_t})
    +
    2\sum^T_{j=1} (T-|j|)\text{Cov}(\boldsymbol{y}_t,\boldsymbol{y}_{t-j})
  \right]\notag\\
  \Rightarrow\quad
  \text{Var}(\mathbb{E}_T(\boldsymbol{y}_t))
  &=
  \frac{1}{T}
  \left[
    \text{Var}(\boldsymbol{y_t})
    +
    2\sum^T_{j=1} \frac{T-|j|}{T}
    \text{Cov}(\boldsymbol{y}_t,\boldsymbol{y}_{t-j})
  \right]
  \label{eq:estvar}
\end{align}
A few things to note about this last result:
\begin{enumerate}
  \item In the case of iid observations,
    $\text{Cov}(\boldsymbol{y}_t,\boldsymbol{y}_{t-j})$ will be zero for
    all $j\neq 0$, in which case we get the classic ``$\sqrt{T}$
    result'' for the standard deviation:
    \begin{align*}
      \text{Var}(\mathbb{E}_T(\boldsymbol{y}_t))
      &=
      \frac{\text{Var}(\boldsymbol{y_t})}{T}\\
      \Rightarrow\quad
      \sigma(\mathbb{E}_T(\boldsymbol{y}_t))
      &=
      \frac{\sigma(\boldsymbol{y_t})}{\sqrt{T}}
    \end{align*}
    In words, ``The average of any sample always gives a unbiased
    estimate of the mean. And when the data is uncorrelated, the
    precision of that estimate increases at a rate of $\sqrt{T}$.''

  \item When the observations are \emph{not} iid (when there is serial
    correlation in the sample), Equation~\ref{eq:estvar} says that you
    don't really have $T$ observations. The serial correlation decreases
    the number of \emph{effective} observations you have. So crank up
    the variance of your estimate, depending upon how strong that serial
    correlation is.

  \item
    You can see from Equation~\ref{eq:estvar} that when $T$ get's large,
    $\frac{T-|j|}{T}$ approaches unity, which gives
    \begin{align*}
      \lim_{T\rightarrow\infty}
      \text{Var}(\mathbb{E}_T(\boldsymbol{y}_t))
      &=
      \frac{1}{T}
      \left[
        \text{Var}(\boldsymbol{y_t})
        +
        2\sum^\infty_{j=1}
        \text{Cov}(\boldsymbol{y}_t,\boldsymbol{y}_{t-j})
      \right]
      =
      \frac{\boldsymbol{S}}{T}
    \end{align*}
    where $\boldsymbol{S}$ is the spectral density of $\boldsymbol{y}_t$
    at frequency at zero.
\end{enumerate}
From there, we can use the Central Limit Theorem to state the following
result.

\begin{thm}
Given a sample $\{\boldsymbol{y}_t\}_{t=1}^T$ for random variable
$\boldsymbol{y}_t$
\begin{align*}
  \sqrt{T}\left(
    \mathbb{E}_T(\boldsymbol{y}_t) -\mathbb{E}(\boldsymbol{y}_t)
  \right)
  \xrightarrow{d} N(0,\boldsymbol{S})
\end{align*}
where $\boldsymbol{S}$ is the spectral density at frequency zero defined
\begin{align*}
  \boldsymbol{S} :=
  \sum^\infty_{j=-\infty}
    \text{\normalfont Cov}(\boldsymbol{y}_t, \boldsymbol{y}_{t-j})
  =
  \text{\normalfont Var}(\boldsymbol{y}_t)
  + 2 \sum^\infty_{j=1}
  \text{\normalfont Cov}(\boldsymbol{y}_t,\boldsymbol{y}_{t-j})
\end{align*}
This holds regardless of whether or not the data are iid or serially
correlated (since you're using the correct covariance matrix,
$\boldsymbol{S}$).
\end{thm}

\clearpage
\subsection{Casting the Model in GMM Form}

The first step is to write your model in standard GMM form:
\begin{align}
  0 &= \boldsymbol{g}(\boldsymbol{b})\label{eq:stdform}\\
  \Leftrightarrow\qquad
  0 &= \mathbb{E}[\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})]
  \notag
\end{align}
where $\boldsymbol{b}\in\mathbb{R}^p$,
$\boldsymbol{y}_t\in\mathbb{R}^n$, and $\boldsymbol{f},
\boldsymbol{g}\in\mathbb{R}^m$.  Equation~\ref{eq:stdform} states the
\emph{moment conditions} of the model, since it encapsulates
restrictions the model imposes on the moments.
\\
\\
Examples
\begin{enumerate}
  \item An asset pricing model of asset ruturns for $N$ assets can be
    expressed
    \begin{align*}
      1 &= \mathbb{E}(m_{t+1}(b) \boldsymbol{R}_{t+1}) \\
      \Leftrightarrow \qquad
      0 &= \mathbb{E}(m_{t+1}(b) \boldsymbol{R}_{t+1}-1)\\
      \boldsymbol{R}_{t+1} &=
      \begin{pmatrix}
        R^1_{t+1}
        &\cdots&
        R^N_{t+1}
      \end{pmatrix}'
    \end{align*}
    where the discount factor $m_{t+1}(b)$ is a scalar that depends on
    data and model parameters.
  \item Let $\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})$ represent
    forecast or regression errors at time $t$ for a vector of different
    data series that we're trying to predict or describe with the model.
    Then Equation~\ref{eq:stdform} expresses the natural idea ``Set
    errors to zero.''
  \item Take a typical regression equation:
    \begin{align*}
    \end{align*}
\end{enumerate}
So it's usually very easy to write any model (linear or nonlinear) in
standard form. Your model's structure is going to consist of a bunch of
equations that place restrictions on data and parameters.  Simply figure
out what combination of data and parameters should be zero in
expectation, and the shift terms in those equations around as necessary
until you work your model's restrictions/implications into the standard
representation of Equation~\ref{eq:stdform}.

\clearpage
\subsection{Estimating the Model}

Suppose that your model is the correct model of the world. Then by
assumption, if we can find the \emph{true} parameter vector
$\boldsymbol{b}_0\in \mathbb{R}^p$, it must be the case that
\begin{align}
  \boldsymbol{g}(\boldsymbol{b}_0) = 0
  \label{eq:gmmtrue}
\end{align}
This suggests how to find an estimate $\boldsymbol{\hat{b}}$ of true
parameter vector $\boldsymbol{b}_0$: just find the vector that sets the
the function $\boldsymbol{g}(\boldsymbol{b})$ to zero---i.e.\ solve the
system.  But in practice, we need to overcome two very obvious issues
with that recommendation.

First, we don't have access to the population moment conditions
$\boldsymbol{g}(\boldsymbol{b})$. Remember that the function
$\boldsymbol{g}$ depends upon random variable $\boldsymbol{y}_t$ under
the hood, and we work with samples like $\{\boldsymbol{y}_t\}_{t=1}^T$
in real life.  But we have an easy fix for that: just set the sample
equivalent $\boldsymbol{g}_T(\boldsymbol{b})=0$ instead. Done, this
problem is solved.  \emph{However}, that leads us right to our next
issue.

We won't be able to simply set $\boldsymbol{g}_T(\boldsymbol{b})=0$
since the model The model probably isn't \emph{actually} the true model
of the world. And in general, we're not going to be able set all $m$
elements of vector $\boldsymbol{g}_T(\boldsymbol{b})$ to zero with a
misspecified model when $p<m$ (when the number of free parameters $p$ is
less than the number of moment conditions $m$).  Now of course, we want
$p<m$ because models should be parsimonious with less degress of freedom
than data points, but the consequence is that you can't cleanly solve
the system to set $\boldsymbol{g}_T(\boldsymbol{b})=0$.

So how exactly do you estimate the $p$ parameters given these issues,
sample $\{\boldsymbol{y}_t\}_{t=1}^T$, and corresponding vector-valued
function $\boldsymbol{g}_T(\boldsymbol{b})\in \mathbb{R}^m$? There are
two approaches:
\begin{enumerate}
  \item Choose a matrix $\boldsymbol{a}_T\in\mathbb{R}^{p\times m}$ and
    set only $p$ linear combinations of the
    moments to zero, i.e.\ solve the system of $p$ equations for the
    $\boldsymbol{\hat{b}}$ that satisfies
    \begin{align}
      \boldsymbol{a}_T
      \;
      \boldsymbol{g}_T(\boldsymbol{\hat{b}}) = 0
      \label{eq:bhatdef}
    \end{align}
    Most of the time, $\boldsymbol{a}_T$ will be a matrix of ones and
    zeros to pick out the moments in $\boldsymbol{g}_T(b)$ (or form
    linear combination of moments) we want to force to zero. But it
    really can be as complicated as you want or depend upon sample size
    (hence the ${}_T$ subscript). We'll see an example just below.

    The main idea: $p<m$ means we can't set \emph{all} of the $m$
    moments to zero, so focus on $p$ important ones.\footnote{Of course,
      this might all be pointless if $r > g$ and patrimonial capitalism
    kills us all. ``Piketty!!! [Shakes fist at sky]''}

  \item If we don't want to take a stance on the important moments, form
    a weighting matrix $\boldsymbol{W}\in \mathbb{R}^{m\times m}$ and
    choose the parameter vector that gets the squared (weighted)
    elements of $\boldsymbol{g}_T(\boldsymbol{b})$ as close to zero as
    possible:
    \begin{align}
      \boldsymbol{\hat{b}} = \arg\min_{\boldsymbol{b}}\;
      \boldsymbol{g}_T(\boldsymbol{b})' \boldsymbol{W}
      \boldsymbol{g}_T(\boldsymbol{b})
      \label{eq:bhatmin}
    \end{align}
    We're minimizing a weighted sum of squared ``errors'' (i.e.\
    differences of $\boldsymbol{g}_T(b)$ from its target of zero), which
    is both conceptually and computationally straightfoward.

    And, just to tie things together, this is actually equivalent to
    Approach 1, if you set
    \begin{align}
      \boldsymbol{a}_T =
      \frac{\partial \boldsymbol{g}'_T}{\partial \boldsymbol{b}}
      \boldsymbol{W}
      =
      \boldsymbol{d}'
      \boldsymbol{W}
      \label{eq:equivrep}
    \end{align}
    where $\boldsymbol{d}
    :=\frac{\partial \boldsymbol{g}'_T}{\partial \boldsymbol{b}}
    \in \mathbb{R}^{p\times m}$.
    This follows from the first order conditions to the minimization
    defined by Equation~\ref{eq:bhatmin}:
    \begin{align*}
      0
      = \frac{\partial \boldsymbol{g}'_T}{\partial \boldsymbol{b}}
      \boldsymbol{W} \boldsymbol{g}_T(\boldsymbol{b})
      = \boldsymbol{d}'
      \boldsymbol{W} \boldsymbol{g}_T(\boldsymbol{b})
    \end{align*}
    We'll talk about choices of the weighting matrix
    $\boldsymbol{W}$ later on. For now though, you might reasonably just
    choose $\boldsymbol{W}=\boldsymbol{I}$, treating all moments
    equally.
\end{enumerate}

\subsection{Standard Errors}

Next, suppose we have a sample $\{\boldsymbol{y}_t\}^T_{t=1}$. To get
standard errors for the various objects we will estimate, we proceed in
a sequence of steps
\begin{enumerate}
  \item Define $\boldsymbol{S}$, the spectral density matrix of
    population moment vector
    $\boldsymbol{g}(
    \boldsymbol{b})=\mathbb{E}[\boldsymbol{f}(\boldsymbol{y},
    \boldsymbol{b})]$ for fixed $\boldsymbol{b}$.

    Given fixed $\boldsymbol{b}$, the vector
    $\boldsymbol{g}(\boldsymbol{b})
    =\mathbb{E}[\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})]$
    is simply the mean of object
    $\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})$.
    We recall the definition from Section~\ref{sec:mean} for
    the spectral density at frequency zero:
    \begin{align*}
      \boldsymbol{S} = \sum^\infty_{j={-\infty}}
        \text{Cov}(\boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b}),
          \boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b}))
        =
        \mathbb{E}\left[
        \boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})
        \boldsymbol{f}(\boldsymbol{y}_t,\boldsymbol{b})'
        \right]
    \end{align*}
    Nothing deep there, just applying our definition from earlier. Of
    course, eventually, we'll have to talk about estimating this beast
    given a sample (since we usually won't know $\boldsymbol{S}$), but
    for now, let's just take it as given and work through the math for
    everything else. We'll return to it later.

  \item Use $\boldsymbol{S}$ to derive the standard error of sample mean
    $\boldsymbol{a}_T\boldsymbol{g}_T(\boldsymbol{b})$ for fixed
    $\boldsymbol{b}$.\footnote{We compute standard errors for
      $\boldsymbol{a}_T\boldsymbol{g}_T(\boldsymbol{b})$, not just
      $\boldsymbol{g}_T(\boldsymbol{b})$ because we impose that the
      former equals zero, but we don't know anything about the latter.
      And we will need to use the equality
      $\boldsymbol{a}_T\boldsymbol{g}_T(\boldsymbol{b})=0$,
      Taylor-expand the lefthand side, shuffle things around, and get
      results in the next step.}

    Just like in Section~\ref{sec:mean}, we have a sample estimate
    $\boldsymbol{g}_T(\boldsymbol{b})$ for a population mean vector
    $\boldsymbol{g}(\boldsymbol{b})$. We also have $\boldsymbol{S}$, the
    spectral density at frequency zero. Put this together to get
    \begin{align*}
      \sqrt{T}\left(\boldsymbol{g}_T(\boldsymbol{b})
      - \boldsymbol{g}(\boldsymbol{b})\right)
      &\xrightarrow{d} N(0,\boldsymbol{S})\\
      \Rightarrow \quad
      \text{Var}\left(
        \boldsymbol{a}
        \boldsymbol{g}_T(\boldsymbol{b})
      \right)
      &=
      \frac{\boldsymbol{a}\boldsymbol{S}\boldsymbol{a}'}{T}
    \end{align*}
    where $\boldsymbol{a}= \text{plim}\;\boldsymbol{a}_T$.

  \item Use the delta method to derive standard errors for
    $\boldsymbol{\hat{b}}$.

    We know the distribution of $\boldsymbol{g}_T(\boldsymbol{b})$
    for any fixed $\boldsymbol{b}$,
    so we can do the usual delta function math,\footnote{%
    Note, I could have used the delta method formula above, but the
    notation makes it
    tricky to apply here since this derivation treats
    $\boldsymbol{g}_T(\boldsymbol{b})$ as the sampling distribution we
    \emph{know}, and $\boldsymbol{b}$ as the transformation of
    $\boldsymbol{g}_T(\boldsymbol{b})$ whose sampling distribution we
    \emph{want to know}.  Now that's fine since $\boldsymbol{b}$ really
    is a function of $\boldsymbol{g}_T(\boldsymbol{b})$ (it's the
    inverse), but it's not conducive to the notation in the delta method
    section. So I just opt for the more direct approach this one time.%
    }
    writing out a Taylor Series expansion of $\boldsymbol{a}
    \boldsymbol{g}_T(\boldsymbol{\hat{b}})$ about the true parameter
    vector $\boldsymbol{b}_0$ to get
    \begin{align}
      \boldsymbol{a} \boldsymbol{g}_T(\boldsymbol{\hat{b}})
      &= 0\notag\\
      \boldsymbol{a} \boldsymbol{g}_T(\boldsymbol{b}_0)
      + \boldsymbol{a}
      \frac{\partial \boldsymbol{g}_T}{\partial \boldsymbol{b}'}
      (\boldsymbol{\hat{b}}-\boldsymbol{b}_0)
      &= 0\notag\\
      (\boldsymbol{\hat{b}}-\boldsymbol{b}_0)
      &=
      -(\boldsymbol{a} \boldsymbol{d})^{-1}
      \boldsymbol{a} \boldsymbol{g}_T(\boldsymbol{b}_0)
      \label{eq:bhat_b0}\\
      \text{Var}\left(\boldsymbol{\hat{b}}-\boldsymbol{b}_0\right)
      &=
      (\boldsymbol{a} \boldsymbol{d})^{-1}
      \; \left[\text{Var}\left(
        \boldsymbol{a} \boldsymbol{g}_T(\boldsymbol{b}_0)
      \right)
      \right]
      {(\boldsymbol{a} \boldsymbol{d})^{-1}}'\notag\\
      \text{Var}\left(\boldsymbol{\hat{b}}-\boldsymbol{b}_0\right)
      &=
      \frac{1}{T}
      (\boldsymbol{a} \boldsymbol{d})^{-1}
      \boldsymbol{a} \boldsymbol{S} \boldsymbol{a}'
      {(\boldsymbol{a} \boldsymbol{d})^{-1}}'\notag
    \end{align}
    where we defined
    \begin{align*}
      \boldsymbol{d} &:=
      \mathbb{E}\left[
        \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{b}'}
      \right]
      =
      \frac{\partial \boldsymbol{g}_T(\boldsymbol{b})}{%
        \partial \boldsymbol{b}'}
    \end{align*}

  \item Use the delta method to derive standard errors for
    $\boldsymbol{g}_T(\boldsymbol{\hat{b}})$.

    We just got the distribution of $\boldsymbol{\hat{b}}$, now let's
    get the distribution of $\boldsymbol{g}_T(\boldsymbol{\hat{b}}$,
    which incorporates both variation in our sample \emph{and} in our
    parameter estimates). Again, delta method
    \begin{align}
      \boldsymbol{g}_T(\boldsymbol{\hat{b}})
      &\approx
      \boldsymbol{g}_T(\boldsymbol{b}_0)
      + \boldsymbol{d}
      \left(\boldsymbol{\hat{b}}-\boldsymbol{b}_0\right)\notag\\
      \text{From (\ref{eq:bhat_b0})}
      \qquad\qquad
      &=
      \boldsymbol{g}_T(\boldsymbol{b}_0)
      -\boldsymbol{d}(\boldsymbol{a} \boldsymbol{d})^{-1}
      \boldsymbol{a} \boldsymbol{g}_T(\boldsymbol{b}_0)\notag \\
      \boldsymbol{g}_T(\boldsymbol{\hat{b}})
      &\approx
      \left(\boldsymbol{I}
      -\boldsymbol{d}(\boldsymbol{a} \boldsymbol{d})^{-1}
      \boldsymbol{a} \right)
      \boldsymbol{g}_T(\boldsymbol{b}_0)\notag\\
      \Rightarrow\quad
      \text{Var}\left(
      \boldsymbol{g}_T(\boldsymbol{\hat{b}})
      \right)
      &\approx
      \frac{1}{T}
      \left(\boldsymbol{I}
      -\boldsymbol{d}(\boldsymbol{a} \boldsymbol{d})^{-1}
      \boldsymbol{a} \right)
      \boldsymbol{S}
      \left(\boldsymbol{I}
      -\boldsymbol{d}(\boldsymbol{a} \boldsymbol{d})^{-1}
      \boldsymbol{a} \right)'
      \label{eq:vargtbhat}
    \end{align}
    And lastly, just a quick word about this variance.

    Recall that there are $m$ moments, and we set $p$ linear
    combinations of them to zero. Hence, $p$ elements of
    $\boldsymbol{g}_T(\boldsymbol{b})$ will have zero variance
    for \emph{all} choices of $b$, by construction. That means the
    variance-covariance matrix for
    $\boldsymbol{g}_T(\boldsymbol{\hat{b}})$ given by
    Equation~\ref{eq:vargtbhat} will be \emph{singular}.  Since some
    formulas require us to invert this object, we will need to use a
    pseudo-inverse.\footnote{Here's one quick way to do that given
      variance-covariance matrix $\Sigma$. Do an eigenvalue
      decomposition $\Sigma = Q\Lambda Q'$. Construct $\Lambda_*$, which
      simply inverts all non-zero eigenvalues on the diagonal of
      $\Lambda$, and set $\Sigma^{-1}=Q\Lambda_*Q'$.}

\end{enumerate}

\subsection{Test of Overidentifying Restrictions}

We're trying to set $\boldsymbol{g}_T(\boldsymbol{b})$ to zero, but
again, we have to make some tradeoffs and set only a few moments (or
linear combinations of moments) to zero. We might then run tests to see
if the remaining moments we didn't force to zero are statistically
large---i.e.\ far from zero, so bad news for the model.

To that end, form the $J_T$ statistic
\begin{align}
  J_T =
  T
  \boldsymbol{g}_T(\boldsymbol{\hat{b}})'
  \left[
  \left(
    \boldsymbol{I}-\boldsymbol{d}(\boldsymbol{a}\boldsymbol{d})^{-1}
    \boldsymbol{a}
  \right)
  \boldsymbol{S}
  \left(
    \boldsymbol{I}-\boldsymbol{d}(\boldsymbol{a}\boldsymbol{d})^{-1}
    \boldsymbol{a}
  \right)'
  \right]^{-1}
  \boldsymbol{g}_T(\boldsymbol{\hat{b}})
  \label{eq:jtest}
\end{align}
Ignore the details and focus on the intution of the line above: We're
just squaring and adding up each elements of
$\boldsymbol{g}_T(\boldsymbol{\hat{b}})$, but weighting those elements
by the object in brackets, which is precisely the inverse
variance-covaraince matrix of $\boldsymbol{g}_T(\boldsymbol{\hat{b}})$
from Equation~\ref{eq:vargtbhat} above.  In other words, we're dividing
the size of each squared ``deviation from zero'' by its variance (to
standardize), and then adding all of them up to evaluate the deviations
\emph{jointly}.

Finally, we just need to specify what distribution $J_T$ follows. By the
central limit theorem, $\boldsymbol{g}_T(\boldsymbol{\hat{b}})$ should
be multivariate normal. We standardized all the elements by their
variance, squared them, and added them up, so we have a sum of squared
standard normals: $\Chi^2$ distribution
\begin{align*}
  J_T \sim \Chi^2_{m-p}
\end{align*}
where $m$ is the number of moments and $p$ is the number of parameters
we estimated. We're used to this kind of degrees of freedom calculation,
but in the GMM context, it has additional meaning. In particular, we can
think of the $m-p$ degrees of freedom as a consequence of setting $p$
elements of $\boldsymbol{g}_T(\boldsymbol{\hat{b}})$ to zero
\emph{always}. Recall that this meant zero variance for those elements,
hence a singular variance-covariance matrix for
$\boldsymbol{g}_T(\boldsymbol{\hat{b}})$ of rank $m-p$. So there are
only $m-p$ squared standard normals to add up for our $\Chi^2$
distribution.

\clearpage
\subsection{Efficient Estimates}

I've so far been agnostic about the choice of $\boldsymbol{a}_T$ or
weighting matrix $\boldsymbol{W}$ in our two alternative definitions of
estimate $\boldsymbol{\hat{b}}$:
\begin{align}
  \boldsymbol{\hat{b}} &= \arg\min_{\boldsymbol{b}}\;
  \boldsymbol{g}_T(\boldsymbol{b})' \boldsymbol{W}
  \boldsymbol{g}_T(\boldsymbol{b})
  \label{eq:bhatmin2}\\
  0 &= \boldsymbol{a}_T
    \;
    \boldsymbol{g}_T(\boldsymbol{\hat{b}})
  \label{eq:bhatdef2}
\end{align}
There is a a statistically \emph{efficient} choice:
\begin{align}
  \boldsymbol{W} &= \boldsymbol{S}^{-1} \label{eq:efficient1}\\
  \Leftrightarrow \quad
  \boldsymbol{a} &= \boldsymbol{d}' \boldsymbol{S}^{-1}
  \label{eq:efficient2}
\end{align}
I'll start with the very intuitive choice of weighting matrix in
Equation~\ref{eq:efficient1}.

Remember that $\boldsymbol{S}$ is the variance-covariance matrix of
$\boldsymbol{g}(\boldsymbol{b})$. So if we weight each element of vector
$\boldsymbol{g}(\boldsymbol{b})$ by the \emph{inverse} variance, we're
mechanically forcing the minimization in Equation~\ref{eq:bhatmin2} to
pay more attention to the low-variance elements. That's because low
variance means large inverse variance, so we heavily weight those
elements of $\boldsymbol{g}(\boldsymbol{b})$ that are \emph{more
precisely measured}. So the weighting will make these elements of
$\boldsymbol{g}(\boldsymbol{b})$ more sensitive to choice of parameters.

To understand the choice of $\boldsymbol{a}$ matrix in
Equation~\ref{eq:efficient2}, recall Equation~\ref{eq:equivrep}, which
showed how to translate the $\boldsymbol{W}$ matrix in
Representation~\ref{eq:bhatmin2} into an equivalent $\boldsymbol{a}$
matrix in Representation~\ref{eq:bhatdef2}.

A bonus to all of this is that many of the formulas simplify
considerably. By plugging Equation~\ref{eq:efficient2} into the formulas
we had for the various estimates we had, they reduce to the following
with a little bit of algebra:
\begin{align*}
  \sqrt{T}\left(\boldsymbol{\hat{b}}-\boldsymbol{b}_0\right)
  &\xrightarrow{d}
  N\left(0,
  (\boldsymbol{d}' \boldsymbol{S}^{-1} \boldsymbol{d})^{-1}
  \right)\\
  \sqrt{T}\boldsymbol{g}_T(\boldsymbol{\hat{b}})
  &\xrightarrow{d}
  N\left(
  0,
  \boldsymbol{S}
  -\boldsymbol{d}(\boldsymbol{d}' \boldsymbol{S}^{-1} \boldsymbol{d})^{-1}
  \boldsymbol{d}'
  \right)\\
  T
  \boldsymbol{g}_T(\boldsymbol{\hat{b}})'
  \boldsymbol{S}^{-1}
  \boldsymbol{g}_T(\boldsymbol{\hat{b}})
  &\xrightarrow{d} \Chi^2_{m-p}
\end{align*}
where the last line follows from Hansen's paper (the result is not an
immediately clear simplification of the more general formula above, at
least to me).


%%%% APPPENDIX %%%%%%%%%%%

\clearpage
\appendix

\section{Matrix Calculus}

Here, I lay down a few conventions for with respect to vectors and of
vector-valued functions.

For scalar-valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$
which takes column vector $\boldsymbol{\theta}$ as an argument, we have
\begin{align*}
  \frac{\partial f}{\partial \boldsymbol{\theta}}
  &=
  \begin{pmatrix}
    \frac{\partial f}{\partial \theta_1} \\
    \vdots\\
    \frac{\partial f}{\partial \theta_n} \\
    \end{pmatrix} \in \mathbb{R}^n\\
  \frac{\partial f}{\partial \boldsymbol{\theta}'}
  &=
  \begin{pmatrix}
    \frac{\partial f}{\partial \theta_1} &
    \cdots&
    \frac{\partial f}{\partial \theta_n}
  \end{pmatrix}\in \mathbb{R}^n
\end{align*}
For vector-valued function $\boldsymbol{g}:
\mathbb{R}^n\rightarrow\mathbb{R}^m$,
\begin{align*}
  \boldsymbol{g}(\boldsymbol{\theta})
  &=
  \begin{pmatrix}
    g_1(\boldsymbol{\theta}) \\
    \vdots \\
    g_m(\boldsymbol{\theta}) \\
  \end{pmatrix}\in \mathbb{R}^m\\
  \frac{\partial \boldsymbol{g}}{\partial \boldsymbol{\theta'}}
  &=
  \begin{pmatrix}
    \frac{\partial g_1}{\partial \theta_1}
      & \cdots & \frac{\partial g_1}{\partial \theta_n}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial g_m}{\partial \theta_1}
      & \cdots & \frac{\partial g_m}{\partial \theta_n}\\
  \end{pmatrix}
  \in \mathbb{R}^{m\times n}
\end{align*}
We also have, in general,
\begin{align*}
  \frac{\partial\boldsymbol{g'}}{\partial \boldsymbol{\theta}}
  =
  \left(
  \frac{\partial\boldsymbol{g}}{\partial \boldsymbol{\theta'}}
  \right)'
\end{align*}

\section{Adjusted R-Squared}

Adjusted R-squared, $R^2_A$, is computed
\[ R^2_A = 1 - \frac{e'e }{(n-k) s_y^2} \]
where $e$ is the vector of error/residual terms resulting from a
model estimate,
$n$ is the number of observations, $k$ equals the number of
independent variables on the right hand side of the equation, and
$s_y^2$ is the unbiased variance estimate of the dependent variable.



\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf}
%	 }
%      }
%   \end{figure}


%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
