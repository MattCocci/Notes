\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Econometrics}
\date{\today}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}

%% Formatting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=1in]{geometry}
    %   Adjust the margins with geometry package
%\usepackage{pdflscape}
    %   Allows landscape pages
%\usepackage{layout}
    %   Allows plotting of picture of formatting



%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %   Make the header bigger to avoid overlap

%\fancyhf{}
    %   Erase header settings

%\renewcommand{\headrulewidth}{0.3pt}
    %   Width of the line

%\setlength{\headsep}{0.2in}
    %   Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{courier}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{patterns}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures

\pgfdeclarepatternformonly[\LineSpace]{my north east lines}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{\LineSpace}{\LineSpace}}{\pgfqpoint{\LineSpace}{\LineSpace}}%
{
    \pgfsetlinewidth{0.4pt}
    \pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
    \pgfpathlineto{\pgfqpoint{\LineSpace + 0.1pt}{\LineSpace + 0.1pt}}
    \pgfusepath{stroke}
}

\pgfdeclarepatternformonly[\LineSpace]{my north west lines}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{\LineSpace}{\LineSpace}}{\pgfqpoint{\LineSpace}{\LineSpace}}%
{
    \pgfsetlinewidth{0.4pt}
    \pgfpathmoveto{\pgfqpoint{0pt}{\LineSpace}}
    \pgfpathlineto{\pgfqpoint{\LineSpace + 0.1pt}{-0.1pt}}
    \pgfusepath{stroke}
}

\newdimen\LineSpace
\tikzset{
    line space/.code={\LineSpace=#1},
    line space=3pt
}

%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{%
    colorlinks,
        %   This colors the links themselves, not boxes
    citecolor=black,
        %   Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\definecolor{codegreen}{RGB}{28,172,0}
\definecolor{codelilas}{RGB}{170,55,241}

% David4 color scheme
\definecolor{d4blue}{RGB}{100,191,255}
\definecolor{d4gray}{RGB}{175,175,175}
\definecolor{d4black}{RGB}{85,85,85}
\definecolor{d4orange}{RGB}{255,150,100}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %   For including verbatim code from files, no colors
\usepackage{listings}
    %   For including code snippets written directly in this doc

\lstdefinestyle{bash}{%
  language=bash,%
  basicstyle=\footnotesize\ttfamily,%
  showstringspaces=false,%
  commentstyle=\color{gray},%
  keywordstyle=\color{blue},%
  xleftmargin=0.25in,%
  xrightmargin=0.25in
}

\lstdefinestyle{matlab}{%
  language=Matlab,%
  basicstyle=\footnotesize\ttfamily,%
  breaklines=true,%
  morekeywords={matlab2tikz},%
  keywordstyle=\color{blue},%
  morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
  identifierstyle=\color{black},%
  stringstyle=\color{codelilas},%
  commentstyle=\color{codegreen},%
  showstringspaces=false,%
    %   Without this there will be a symbol in
    %   the places where there is a space
  numbers=left,%
  numberstyle={\tiny \color{black}},%
    %   Size of the numbers
  numbersep=9pt,%
    %   Defines how far the numbers are from the text
  emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
    %   Some words to emphasise
}

\newcommand{\matlabcode}[1]{%
    \lstset{style=matlab}%
    \lstinputlisting{#1}
}
    %   For including Matlab code from .m file with colors,
    %   line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %   Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %   For bibliographies
\usepackage{pdfpages}
    %   For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\hat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}

% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsTheta}{\boldsymbol{\Theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bsGamma}{\boldsymbol{\Gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}
\newcommand{\bsLambda}{\boldsymbol{\Lambda}}
\newcommand{\bshatLambda}{\boldsymbol{\hat{\Lambda}}}
\newcommand{\bshatG}{\boldsymbol{\hat{G}}}
\newcommand{\bsdelta}{\boldsymbol{\delta}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bsPsi}{\boldsymbol{\Psi}}
\newcommand{\bspsi}{\boldsymbol{\psi}}
\newcommand{\bsPhi}{\boldsymbol{\Phi}}

% Special cases like \bshatb for \boldsymbol{\hat{b}}
\generate[\hat]{bshat}{\boldsymbol}{a,b,y,x,X,V,S,W,A,B,P}
\newcommand{\bshatbeta}{\boldsymbol{\hat{\beta}}}
\newcommand{\bshatmu}{\boldsymbol{\hat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\hat{\theta}}}
\newcommand{\bsbartheta}{\boldsymbol{\bar{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\hat{\Sigma}}}
\newcommand{\bstildebeta}{\boldsymbol{\tilde{\beta}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bstildePsi}{\boldsymbol{\tilde{\Psi}}}
\newcommand{\bsbarbeta}{\boldsymbol{\overline{\beta}}}
\newcommand{\bshatOmega}{\boldsymbol{\hat{\Omega}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}
\newcommand{\bstildey}{\boldsymbol{\tilde{y}}}
\newcommand{\bstildeX}{\boldsymbol{\tilde{X}}}
\newcommand{\bstildevarepsilon}{\boldsymbol{\tilde{\varepsilon}}}

% Zero vector as \bso
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\hat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\hat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\hat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}

% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

% Various probability and statistics commands
\newcommand{\wn}{\sim\operatorname{WN}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Prb}{\operatorname{P}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}

% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\tTz}{_{t=0}^T}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\tableofcontents %adds it here

\clearpage

\section{Introduction}

This document aims to bring together the elements of probability,
measure theory, and stochastic calculus useful for the anlysis of
stochastic process that have particular relevance in finance and
economics.


\section{Notation}


\begin{defn}(Extended Real Line)
We will use $\bar{\R}$ to denote $\R \cup \{\pm \infty\}$---i.e.\ the
real line with positive and negative infinity.
\end{defn}

\begin{defn}(Power Set)
We will use $\sP(\Omega)$ to denote the power set (the collection of all
subsets of $\Omega$).
\end{defn}

\begin{defn}(Indexing)
To denote elements in a generic indexing set $J$ (possibly uncountable),
we use $\alpha \in J$. When the index set is countable, I will
explicitly use $|_{n=1}^\infty$ or $n\in\N$ notation.
\end{defn}

\begin{defn}(Preimages)
For a function $f:\Omega\ra X$, I will often use abbreviations like
\begin{alignat*}{3}
  \{f=x\} &:= f^{-1}(x)
  &&= \left\{ \omega\in\Omega \;|\; f(\omega)= x\right\} \\
  \{f \leq x\} &:= f^{-1}\big((-\infty,x]\big)
  &&= \left\{ \omega\in\Omega \;|\; f(\omega)\leq x \right\}
\end{alignat*}
This is a slight abuse of notation, but it will considerably reduce
clutter and is consistent with standard random variable and probability
notation.
\end{defn}

\begin{defn}(Inequalities)
Similar to the last definition, given functions
$f,g:\Omega \ra \R$, I will often want to suppress the $\omega\in\Omega$
arguments to the functions. So when you see
\begin{align*}
  |f+g|\leq |f| + |g|
\end{align*}
that really means
\begin{align*}
  |f(\omega)+g(\omega)|\leq |f(\omega)| + |g(\omega)|
  \qquad \forall\omega\in\Omega
\end{align*}
Similarly, you might see something like
\begin{align*}
  |f|\leq ||f||
\end{align*}
for some norm $||\cdot ||$ applied to function $f$, returning a real
number $||f||\in\R$. The line above really means that
\begin{align*}
  |f(\omega)| \leq ||f|| \qquad\forall \omega\in\R
\end{align*}
so that $||f||$ is an upper bound for the range of $f$.
\end{defn}


\clearpage
\section{Finite Probability Spaces}

\subsection{Definition}

\begin{defn}{(Sample Space)}
The \emph{sample space} $\Omega$ is the set of all \emph{elementary}
outcomes of some statistical experiment. For a given experiment, only
one $\omega \in \Omega$ can occur when we conduct and observe that
experiment.
\end{defn}
\begin{ex}
If our experiment consists of flipping a single coin, the sample space
consisting of elementary events is $\{H,T\}$. If we flip two coins, the
sample space is $\{HH,HT,TH,TT\}$.
\end{ex}
\begin{rmk}
The previous example suggests that there's some subtlety in how you
define ``elementary'' event. In the case where we flipped two coins, it
might seem like a single flip is more ``elementary.'' However, if we define
our experiment to be the flipping of two coins, the set of outcomes from
that experiment really is $\Omega=\{HH,HT,TH,TT\}$. It's an unfortunate
bit of nomenclatural confusion.

Of course, we'll fix this quite soon when we encounter \emph{product
spaces}, which resolve a lot of this subtlety. In particular, product
spaces will let us define a narrow product space like $\Omega=\{H,T\}$
and then construct a richer probability space like
$\Omega\times \Omega = \{HH,HT,TH,TT\}$ or even $\Omega^N$, building up
from relatively simples ones.
\end{rmk}

\begin{defn}{(Finite Probability Space)}
A \emph{finite probability space} is a pair $(\Omega,P)$ consisting of a
finite sample space $\Omega=\{\omega_1,\ldots,\omega_N\}$ of elementary
events together with probabilities $\{p_1,\ldots,p_N\}$ satisfying
\begin{align*}
  \sumnN p_n = 1
  \quad\text{and}\quad
  p_n \in [0,1] \quad \forall n=1,\ldots,N
\end{align*}
where $p_n$ is the probability of elementary event $\omega_n\in\Omega$
occurring, and $P$ is the \emph{probability measure} implied by these
$p_n$, where we now define probability measure.
\end{defn}

\begin{defn}{(Probability Measure)}
\label{defn:finitemeasure}
Given a finite probability space, define the \emph{probability measure}
on $\Omega$ as the mapping
\begin{align*}
  \Prb&: 2^\Omega \rightarrow [0,1] \\
  \Prb[A] &:= \sum_{\{n|\omega_n\in A\}} p_n
  \qquad \forall A\subseteq\Omega
\end{align*}
In this way, we use the probabilities $\{p_1,\ldots,p_n\}$ over the
elementary events to construct the probabilites of any event
$A\subseteq \Omega$.

Probability measures display the following properties
\begin{enumerate}
  \item $\Prb[\omega_n]=p_n$ for $\omega_n\in\Omega$
  \item $\Prb[\emptyset] = 0$
  \item $\Prb[\Omega] = 1$
  \item $\Prb[A\cup B] = \Prb[A] + \Prb[B] - \Prb[A\cap B]$ for any
    events $A,B$.
\end{enumerate}
\end{defn}


\clearpage
\subsection{Events, Algebras, Decompositions, and Atoms}

\begin{defn}{(Event)}
An \emph{event} is any subset $A\subseteq \Omega$ of the sample space.
\end{defn}
\begin{ex}
Given sample space $\Omega=\{HH,HT,TH,TT\}$ for an experiment where we
toss two coins, we might consider the event $A=\{HH,HT\}$ which is
``heads tossed first.''
\end{ex}

\begin{defn}{(Set Algebra)}
A collection $\sF$ of subsets of $\Omega$ (i.e.\ a collection of
events) is called a \emph{set algebra}, or simply \emph{algebra}, if it
satisfies the following three properties:
\begin{enumerate}
  \item $\Omega\in\sF$
  \item $A,B\in\sF \implies A\cup B\in\sF$
  \item $A\in\sF\implies A^c\in\sF$.
\end{enumerate}
\end{defn}
\begin{rmk}
This is not to be confused with a $\sigma$-algebra, which we'll see
later. A set algebra is closed only under finite unions, which is not
going to be a problem in a finite probability space. Once we move to
infinite probability spaces, we will need $\sigma$-algebras, which are
closed under \emph{infinite} unions. But more on that later.

At any rate, the reason we care about algebras is that they collect and
contain the sets---the events---to which we will assign probabilities.
Our notion of a probability will be defined for elements in this
collection, as we see in the next definition.
\end{rmk}

\begin{defn}(Decomposition and its Atoms)
Suppose we have a finite probability space $(\Omega,\sA,P)$. Then a
\emph{decomposition} is any collection
\begin{align*}
  \sD = \{D_1,\ldots,D_N\}
\end{align*}
such that the collection is pairwise disjoint (i.e.\
$D_n\cap D_m=\emptyset$ for all $n\neq m$) and
\begin{align*}
  \Omega = \bigcup\nN D_n
\end{align*}
We call the $D_n\in\sD$ the \emph{atoms of decomposition $\sD$}.
\end{defn}

\begin{defn}(Atoms of Algebra $\sA$)
Suppose we have an algebra $\sA$. An \emph{atom of $\sA$} is any set
$A\in \sA$ such that it's only subsets are $\emptyset$ and $A$.  Atoms
of algebras are like its prime numbers---you can't break atoms up into
smaller pieces that are also in the algebra.
\end{defn}

\begin{prop}\emph{(Results about Atoms of $\sA$)}
Let $\sA$ be an algebra and let $\sD$ be the set of all atoms of $\sA$.
\begin{enumerate}
  \item Any arbitrary $B\in\sA$ can be expressed as a union of atoms.
  \item Any two atoms of an algebra are disjoint.
  \item $\sD$, the set of all atoms of $\sA$, is a decomposition of the
    finite sample space $\Omega$.
\end{enumerate}
\end{prop}
\begin{proof}
Suppose that $A,B\in\sA$ are two distinct atoms. Since $\sA$ is an
algebra, then $A\cap B\in\sA$. If this intersection were nonempty, it
would violate the definition of $A$ and $B$ as atoms.
\end{proof}

\begin{ex}
Let $\{x_1,\ldots,x_N\}$ denote the range of some function
$X:\Omega\ra\R$ (which we'll later call a \emph{random variable}). Then
the sets
\begin{align}
  \{X=x_1\}\quad\cdots\quad\{X=x_N\}
  \label{atoms}
\end{align}
form a decomposition of $\Omega$. Moreover, Collection~\ref{atoms}
is also the collection of all atoms of $\alpha(X)$, the algebra
generated by $X$ (i.e.\ the algebra generated by Collection~\ref{atoms}).
\end{ex}

\clearpage
\subsection{Random Variable}

\begin{defn}{(Random Variable)}
A \emph{random variable} on finite probability space $\Omega$ is a
function
\begin{align*}
  X: \Omega\ra \R
\end{align*}
Since it's inconvenient to deal with sets and $\omega$'s in $\Omega$
directly, we use random variables that map these elements into the real
line. But in general, we only know how to assign probabilities to
events in $\Omega$. How can assign probabiilities to random variables?
That leads us to the next definition.
\end{defn}

\begin{defn}{(Measurability)}
A random variable $X$ is $\sF$-measurable if
\begin{align*}
  \{X=x\} :=
  X^{-1}(x) =
  \{\omega\in\Omega \; | \; X(\omega)=x\}
  \in \sF
\end{align*}
for all $x\in\R$. The idea is that Definition~\ref{defn:finitemeasure}
tells us how to assign probabilities to events or ``measure'' events in
$\sF$. To ``measure'' or assign probabilities to random variables
taking values on the real line, we need to be able to map those values
back into the events in $\sF$ that we do know how to measure.

Not surprisingly, we differentiate between algebras and say
$\sF$-measurable because random variables might be measurable with
respect to some algebras and not others.
\end{defn}

\begin{defn}(Distribution of a Random Variable)
Let $X:\Omega\ra\R$ be a random variable on finite probability space
$(\Omega,\sA,P)$. While $P$ gives probabilities of the different
$\omega$'s in $\Omega$ ocurring, we can instead define
the \emph{probability distribution of $X$}, denoted $P_X$,
\begin{align*}
  P_X(A) :=&\; P[X \in A] \\
  \iff \quad
  =&\; P\big[
    \{\omega \in \Omega\;|\; X(\omega) \in A\}
  \big] \\
  \iff \quad
  =&\; P\big[ X^{-1}(A) \big]
\end{align*}
which defines the probabilities of different \emph{values of $X$}
occurring, rather than the $\omega$'s. In practice, this is what we care
about.
\end{defn}

\begin{rmk}
Instead of thinking about the finite probability space
$(\Omega,\sA,P)$ and a random variable $X$ on that space, we could
alternatively and equivalent work with the induced finite probability
space $(\Omega_X,\sA_X,P_X)$, where
$\Omega_X:=\{X(\omega_1),\ldots,X(\omega_n)\}$, algebra
$\sA_X:=2^{\Omega_X}$, and $P_X$ is as above. This is a valid finite
probability space induced by $X$.
\end{rmk}

\clearpage
\subsection{Product Spaces}

\begin{defn}{(Product Space)}
Given two finite probability spaces $(\Omega,P)$ and $(\Omega',P')$,
define a new probability space called the \emph{product space}
$(\Omega\times\Omega', P \otimes P')$ as
\begin{align*}
  \Omega\times\Omega'
  &= \{(\omega,\omega') \; |\; \omega\in\Omega, \; \omega'\in\Omega'\}
  \\
  (P \otimes P')(\omega, \omega')
  &= \Prb[\omega]\cdot \Prb[\omega']
\end{align*}
\end{defn}

\begin{defn}{(Lifting Random Variables to a Product Space)}
Suppose that we have events $A\subseteq \Omega$ and
$B\subseteq \Omega'$. We want to make sure that the same events are
defined on product space $\Omega\times \Omega'$. Then we can define new
sets
\begin{align*}
  \hat{A} &:= A\times \Omega' \\
  \hat{B} &:= \Omega \times B
\end{align*}
The probabilities of these events will be exactly the same on $\Omega
\times \Omega'$ as they were on just $\Omega$ or just $\Omega'$ because
notice
\begin{align*}
  (\Prb\otimes \Prb')[\hat{A}] &= \Prb(A) \cdot \Prb'(\Omega') = \Prb(A) \cdot 1\\
  (\Prb\otimes \Prb')[\hat{B}] &= \Prb(\Omega) \cdot \Prb'(B) = 1\cdot \Prb'(B)
\end{align*}
Moreover, $\hat{A}$ and $\hat{B}$ will be independent events, which
makes sense given that they started by having nothing to do with each
other, hanging out completely on their own in $\Omega$ or $\Omega'$.
\end{defn}

\begin{defn}{(Lifting Random Variables to a Product Space)}
We can easily ``lift'' random variables on $\Omega$ or $\Omega'$ into
the new product space $\Omega\times \Omega'$. To do so, suppose that we
have random variables $X:\Omega\ra\R$ and $Y:\Omega'\ra\R$. Then define
\begin{align*}
  \forall (\omega,\omega') \in \Omega\times \Omega' \qquad
  \begin{cases}
  \hat{X}(\omega, \omega') &= X(\omega) \\
  \hat{Y}(\omega, \omega') &= Y(\omega')
  \end{cases}
\end{align*}
Then $\hat{X}$ and $\hat{Y}$ are RVs $X$ and $Y$ suitably adapted to
live on probability space $(\Omega,\Omega')$.
\end{defn}

\clearpage
\subsection{Conditional Probability}

\begin{defn}(Conditional Probability Given an Event)
\label{defn:condprob}
The conditional probability of event $A$ ocurring given $B$ occurring
(assuming $P(B)>0$) is
\begin{align*}
  P[A|B] = \frac{P[A\cap B]}{P[B]}
\end{align*}
The numerator adds up the probabilities of all the $\omega \in A\cap B$,
while the denominator adds up the probabilities of all the
$\omega\in B$.
\end{defn}

\begin{defn}(Conditional Probability Given a Decomposition)
Let $\sD$ be a decomposition of sample space $\Omega$. Then
the \emph{conditional probability with respect to decomposition $\sD$}
denoted $P[A|\sD]$ is a random variable defined
\begin{align*}
  P[A|\sD](\omega) = \sumnN P[A|D_n] I_{D_n}(\omega)
  \qquad A \in \sA
\end{align*}
Since a decomposition consists of pairwise disjoint sets, $\omega$ is in
\emph{some} $D_i\in\sD$. Random Variable $P[A|\sD](\omega)$ takes
$\omega$, finds that $D_i$, and returns $P[A|D_i]$ as defined in
Definition~\ref{defn:condprob}.
\end{defn}

We see here how a decomposition can represent our ``information.''
Suppose we have an event $A$. We have some baseline probability of it
occurring, $P[A]$. The conditional probability $P[A|\sD](\omega)$ is a
function that tells us a new ``updated'' probability for $A$ occuring
given that $\omega$ occurred. Let's consider a few examples for $\sD$.

Say that $\sD_1=\{\Omega\}$. This is a very coarse decomposition.
And by the definition, it's clear
\begin{align*}
  P[A|\sD_1](\omega)=P[A|\Omega]=P[A]
  \qquad\forall\omega\in\Omega
\end{align*}
Here we see that $\sD_1$ represents a decomposition without much
information.  Knowing which $\omega$ turns up does not cause us to
revise our baseline probability $P[A]$. That's in sharp contrast to the
next example.

Suppose that we have a decomposition $\sD_2=\{A,A^c\}$. Then by the
definition
\begin{align*}
  P[A|\sD_2](\omega)
  =
  \begin{cases}
    1 & \omega \in A \\
    0 & \omega \in A^c \\
  \end{cases}
\end{align*}
The decomposition $\sD$ conveys a lot of information about event $A$.

More generally, conditional probabilities relative to events like
$P[A|B]$ are like ``I tell you $B$ happened. You tell me
the probability $A$ happened too.'' On the other hand, conditional
probabilities relative to decompositions are like ``I tell you $\omega$
happened and give you a collection of probabilities
$\{P[A|D_n]\}_{D_n\in\sD}$. You then look up the $D_i$ such that
$\omega\in D_i$ and tell me $P[A|D_i]$.'' It's more general because it's
a function mapping from the sample space to numbers, and that mapping
depends crucially on $\sD$.

\begin{defn}(Law of Total Probability)
Given a decomposition $\sD=\{D_1,\ldots,D_N\}$ of $\Omega$,
it's clear for $B\subseteq\Omega$ that
\begin{align*}
  B = \bigcup\nN (B\cap D_n)
\end{align*}
with each $B\cap D_n$ pairwise disjoint. In combination with
Definition~\ref{defn:condprob}, we get the
\emph{law of total probability}
\begin{align*}
  P[B] = \sumnN P[B|D_n]P[D_n]
\end{align*}
Using the definition of a conditional probability with respect to a
decomposition (and recalling that its a random variable), we could also
write this as
\begin{align*}
  P[B] = \E\big[P[B|\sD]\big]
\end{align*}
\end{defn}


\subsection{Conditional Expectation}

\begin{defn}(Conditional Expectation with Respect to a Decomposition)
Let $\sD = \{D_1,\ldots,D_N\}$  be a decomposition. The conditional
expectation of $X$ with respect to $\sD$ is the random variable
\begin{align*}
  \E[X|\sD](\omega)
  &=
  \sum
\end{align*}
\end{defn}


\subsection{Independence}

As mentioned in Shiryaev, \emph{independence} is the concept that
``distinguishes probability theory from the general theory of measure
spaces.''

\begin{defn}(Independence of Events)
We say that $A_1,\ldots,A_N\subseteq \Omega$ are
\emph{independent events} if and only if
\begin{align*}
  P[A_{n_1}\cap \cdots \cap A_{n_K}] =
  P[A_{n_1}]\cdots P[A_{n_K}]
\end{align*}
for any set of $K\leq N$ indices $n_1,n_2,\ldots,n_K\in\{1,\ldots,N\}$
\end{defn}

\begin{defn}(Independence of Algebras)
We say that $\sA_1,\ldots,\sA_N$ are \emph{independent algebras} if and
only if $A_1,\ldots,A_N$ are independent events for any set of events
such that $A_n\in\sA_n$ for $n=1,\ldots,N$.
\end{defn}

\begin{rmk}
Note that \emph{pairwise} independence among elements of collection of
events or algebras does not imply independence of the entire collection.
\end{rmk}

\begin{defn}(Independence of Random Variables)
We say that random variables $X_1,\ldots,X_N$ on finite probability
$(\Omega,\sA,P)$ are \emph{independent random variables} if
\begin{align*}
  P\big[
    X_1\in A_1,\ldots,X_N\in A_N
  \big]
  =
  P[X_1\in A_1] \cdots P[X_N\in A_N]
\end{align*}
for all $A_1,\ldots,A_N\in 2^{\Omega_X}$ where
$\Omega_X=\{X(\omega_1),\ldots,X(\omega_N)\}$.
\end{defn}



%\section{Probability with Measure Theory}


%\begin{rmk}
%Because a $\sigma$-algebra is closed under countable unions,
%interesections, and complements, it provides a natural way to think
%about collections of elementary outcomes that we might care about. Said
%another way, the definition of a $\sigma$-algebra results gives us
%pretty much all of the sets we'd like to assign probabilities to.
%\end{rmk}

%\begin{ex}
%For an even simpler example than die rolling of something that is
%a perfectly valid $\sigma$-algebra, we
%sometimes take the power set of $\Omega$---the collection of all
%possible subsets of $\Omega$---to be our $\sigma$-algebra.  It will
%surely satisfy the necessary properties, and it happens to be very easy
%to denote.
%\end{ex}


%\section{Measure}

%Once we have a $\sigma$-algebra, we can pair the sample space with it's
%$\sigma$-algebra to form what's called a
%\textbf{measurable space}, which is simply the ordered pair
%$(\Omega, \mathcal{F})$.  From there, we can start assigning
%probabilities to those elements in the $\sigma$-algebra which all have
%some chance of occurring.  To do so, we will use a special
%type of function, called a measure.

%A \textbf{measure} is a function that maps sets into real numbers.
%In our case, we will define a specific type of measure, a
%\textbf{probability measure}, as a
%function that has as it's domain a $\sigma$-algebra and maps measurable
%sets in the $\sigma$-algebra into the
%real line, subject to a few conditions. More precisely, it has the
%following characteristics:
%\begin{itemize}
   %\item[i.] If $P$ is our probability measure and $\mathcal{F}$ is
      %our $\sigma$-algebra, then
	 %\[ P: \mathcal{F} \rightarrow \mathbb{R} \]
   %\item[ii.] $ P(F) \in [0,1]$ for all measurable sets
      %$F \in \mathcal{F}$. In addition, $P(\Omega) =1$.
   %\item[iii.] Our probability measure satisfies the probability axioms.
      %And if $F_i$ are all disjoint and members of the $\sigma$-algebra,
      %then
	 %\[ p( \cup F_i) = \sum P(F_i) \]
%\end{itemize}

%\paragraph{Definition} Now that we have our definition of a probability,
%I just want to define a common term more precisely.  Specifically,
%if a property is true \emph{except} for an event of probability 0,
%then we say that property holds ``almost surely.''

%\section{Probability Space}

%So now we have a way of assigning probabilities to any arbitrary event in our measurable space, $(\Omega, \mathcal{F})$. If we
%join the probability measure, $P$, to our measurable space, we obtain a proper \textbf{Probability Space}.  It is defined as
%the ordered triplet
%\[(\Omega, \mathcal{F}, P) \]

%\section{Measurable Function and Random Variables}

%One of the fundamental notions of probability is that of a random
%variable; therefore, making the definition more rigorous
%deserves some attention.

%So let's start with the notion of a \textbf{measurable} function.
%If $(\Omega_1, \mathcal{F}_1)$ and $(\Omega_2, \mathcal{F}_2)$ are two
%measurable spaces then, a function
   %\[ f: (\Omega_1, \mathcal{F}_1) \rightarrow (\Omega_2, \mathcal{F}_2)
      %\]
%is a measurable function if
   %\[ f^{-1}(B) \in \mathcal{F}_1, \;\; \forall B \in\mathcal{F}_2 \]
%In words, a function is measurable if the pre-image of an element in
%the $\sigma$-algebra of the co-domain is in the
%$\sigma$-algebra of the domain.  Simply, it preserves the structure
%between two measurable spaces, sending measurable sets
%in one measurable space to measurable sets in the other.

%Turning to probability, suppose  that we have our probability space,
%$(\Omega_1, \mathcal{F}_1, P)$, and another measurable space,
%$(\Omega_2,\mathcal{F}_2)$.  Then an
%$\mathbf{(\Omega_2, \mathcal{F}_2)}$\textbf{-valued random variable}
%is a \emph{measurable function}
%\[ X: (\Omega_1, \mathcal{F}_1) \rightarrow (\mathbb{R},
   %\mathcal{F}_2) \]

%Since $X$ is a measurable function, we know that
   %\[ X^{-1}(B) \in \mathcal{F}_1, \;\; \forall B \in \mathcal{F}_2\]
%in which case we say that $X$ is $\mathcal{F}_1$-measurable.
%To clarify further
   %\[ X^{-1}(B) = \left\{ \omega \in \Omega_1 | X(\omega)
      %\in B \right\} \]

%\paragraph{Note} Oftentimes, we'll just take the set
%$\mathcal{F}_2$ to be some properly
%defined $\sigma$-algebra on the real line, like the Borel Set
%(defined below).

%\section{Sigma Algebras Generated by Random Variables}

%Above, we assumed that we knew the $\sigma$-algebra for
%$\Omega_1$ already---that it was given. But we
%could just as well \emph{generate} one from some random variable $X$,
%and this generated $\sigma$-algebra could very well differ from
%$\mathcal{F}_1$.

%So let's assume that $X$ is a random variable
   %\[X: (\Omega_1,\mathcal{F}_1) \rightarrow
      %(\mathbb{R},\mathcal{F}_2).\]
%Then the $\sigma$-algebra generated by $X$, denoted by $\sigma(X)$, is
%defined as \emph{all} sets of the form
   %\[ \sigma(X) := \{ \omega \in \Omega_1 | X(\omega) \in A \}, \;\;
      %\forall \; A \subset \mathbb{R} \]
%which can be written more compactly as
   %\[\sigma(X)  = \{ X^{-1}(A) | A\subset \mathbb{R}  \} \]

%\paragraph{Definition} Finally, if $\mathcal{G}$ is a
%sub-$\sigma$-algebra of the set $\mathcal{F}$ defined above, then the
%random variable $X$ is $\mathcal{G}$-measurable if
   %\[ \sigma(X) \subset \mathcal{G} \]

%Let's give a concrete example.  Suppose $\Omega$ consists of all the
%possible combination of up and down moves in a binomial tree.  $X$ is
%a random variable denoting stock price.  Then for a set of real numbers
%representing the possible prices the stock could take on (this is
%our $A \subset \mathbb{R}$), the set $\sigma(X)$ will be the sigma
%algebra resulting from the set of all possible paths that that the
%stock could have taken to get to those prices in $A$.

%\section{Random Variables and Their Distributions}

%However, oftentimes we will want to discuss probabilities, but the
%sufficiently general definition of the random variable just given offers
%a lot of flexibility.  So much, in fact, that a random variable could
%have more than one distribution, as we know occurs in stock process
%which have traditional probabilities and also \emph{risk-neutral}
%probabilities (or \emph{pseudo-probabilities}).

%So let's define a \textbf{distribution} as a measure that assigns
%probabilities to the real numbers that a random variable generates
%(after being passed an element in the sample space).  The most natural
%distribution is the induced measure, $\mathcal{L}_X$ defined as follows
%\[ \mathcal{L}_X(A) := P \{ X \in A \}, \;\; A \subset \mathbb{R} \]

%Let's unpack that. If $A$ is a subset of the real line, it the random
%variable may or may not take on values in that subset, so we want a
%notion of probability.  Well, we can take the pre-image, $X^{-1}(A)$,
%and look at all those elements of the sample space that map to $A
%\subset \mathbb{R}$ under the random variable $X$.  Then, once we have
%elements of the sample space, we can use our traditional notion of the
%$\sigma$-algebra, $\mathcal{F}$, and the probability measure $P$ that
%are already defined on the probability space to assign the
%probabilities.

%Thus, we can associate probabilities with our random variable so long as there exists a distribution measure, like $\mathcal{L}_X$.
%But recall that $\mathcal{L}_X$ was not unique.  It's simply a function to assign probabilities to values that the random variable can
%take on.  We could consider other measures that also accomplish this, and that insight legitimizes the use of such things as
%\emph{risk-neutral probabilities}.


%\section{Lebesgue Measure and the Lebesgue Integral}

%\section{Introduction}

%The \textbf{Lebesgue Measure} is the standard way of assigning a measure to subsets of $n$-dimensional Euclidean space.  If we restrict
%to $\mathbb{R}$, then the Lebesgue Measure of intervals is simply the length. But rather than consider all of $\mathbb{R}$, we'll
%restrict further to \emph{Borel Sets}.

%This will allow us to construct the \emph{Lebesgue Integral}, a generalization of the Riemann Integral.

%\section{Borel Sets}

%The \emph{Borel $\sigma$-algebra}, denoted $\mathcal{B}(\mathbb{R})$, is the smallest $\sigma$-algebra containing (and, in a sense,
%generated by) all open intervals
%in $\mathbb{R}$.

%\paragraph{Examples} The following are a few samples of Borel Sets in $\mathcal{B}(\mathbb{R})$:
%\begin{itemize}
   %\item{Every open interval of the form $(a,b)$.}
   %\item{The open rays $(a,+\infty)$ and $(-\infty,b)$.}
   %\item{All unions of the form
	 %\[ B_1 \cup B_2, \;\; B_1,B_2 \in \mathcal{B}(\mathbb{R}) \]
      %}
   %\item{All complements of sets in $\mathcal{B}(\mathbb{R})$, since it's a sigma algebra. Note that this implies all \emph{closed}
      %intervals in $\mathbb{R}$ are Borel Sets as well, in addition to all half-open and half-closed intervals.}
   %\item{All one point sets, $a\in\mathbb{R}$, as we see that it is in the intersection of other Borel Sets, implying inclusion in
	 %$\mathcal{B}(\mathbb{R})$ since it's a $\sigma$-algebra
	 %\[ \{a \} = \bigcap_{n=1}^{\infty} \left( a - \frac{1}{n}, a + \frac{1}{n} \right). \]
      %}
   %\item{The last item implies that all finite countable collections of points in $\mathbb{R}$ are Borel Sets too. Therefore, the set
      %of all rational numbers is Borel since coutable, and the set of all irrational numbers is Borel since it's the complement of a
      %set in the sigma algebra.}
%\end{itemize}

%However, it should be noted that not all sets of real numbers are Borel Sets.  In particular, any non-Borel set must be uncountable
%(though the opposite is not ture, as shown above).

%\section{Lebesgue Measure}

%Let's start more generally and define a \emph{measure} on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ to be a function
%\[ \mu: \mathcal{B} \rightarrow [0,\infty) \]
%with the following properties
%\begin{itemize}
   %\item[i.]{$\mu(\emptyset) = \emptyset$.}
   %\item[ii.]{If $A_1, A_2, \ldots$ are disjoing sets in $\mathcal{B}(\mathbb{R})$, then
	 %\[ \mu\left(\bigcup_{k=1}^{\infty} A_k\right) =  \bigcup_{k=1}^{\infty} \mu(A_k). \]
      %}
%\end{itemize}

%We define the \textbf{Lebesgue Measure} on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ to the measure $\mu_0$ that assigns the measure of
%each interval to be its length.

%\section{Functions in This World}

%Let $f$ be a function
%\[ f: \mathbb{R} \rightarrow \mathbb{R}. \]
%We say that $f$ is \textbf{Borel-Measurable} if
%\[ A \in \mathcal{B}(\mathbb{R}) \Rightarrow f^{-1}(A) \in \mathcal{B}(\mathbb{R}). \]
%Or equivalently, we could say that we want the $\sigma$-algebra generated by $f$ to be contained in $\mathcal{B}(\mathbb{R})$.

%\section{The Lebesgue Integral}

%Let $I$ be the \emph{indicator function}.  We define
%\[ A := \{ x \in \mathbb{R}| I(x) =1 \} \]
%to be the set \emph{indicated by I}.

%The \textbf{Lebesgue Integral} of $I$ is defined
%\[ \int_{\mathbb{R}} I d\mu_0 = \mu_0(A). \]


%\section{Stochastic Processes}

%Throughout this section, we will explore stochastic processes building
%from the simple case of a discrete state space in discrete time to the
%most general case of an infinite state in continuous time, relaxing constraints through the sections.

%However, before proceeding, I here lay down the most generaly
%definitions for stochastic process, which will we restrict to special
%cases later on.

%\section{General Definitions}

%\begin{defn}{\citep{pavliotis}}
%Let $T$ be an ordered set, $(\Omega,\mathscr{F},\mathbb{P})$ be a
%probability space where $\Omega$ is the sample space, and
%$(E,\mathscr{G})$ be a measurable space.

%A \emph{stochastic process} is a collection of random variables $X =
%\{X_t | t\in T\}$ such that for each $t\in T$, $X_t:
%(\Omega,\mathscr{F},\mathbb{P})\mapsto (E,\mathscr{G})$ is a random
%variable taking on values in its state space $E$.
%\end{defn}

%\begin{rmk}
%Though it might not be obvious at first, a stochastic process dependends
%upon two arguments: some $\omega\in\Omega$ and some $t\in T$. Therefore
%the following representations are equivalent: $X_t$, $X_t(\omega)$,
%$X(t,\omega)$ where the notation used will depend upon context.

%Therefore, we might first think about $X(t,\omega)$ fixing $\omega$ and
%letting $t$ vary. This corresponds to choosing a realization $\omega$ of
%some experiment and looking at the resulting \emph{trajectory} of the
%process.

%On the other hand, we might fix $t$ and think about the different values
%that $X(t,\omega)$ might take on depending upon the trajectory that is
%realized in some experiment.
%\end{rmk}

%\begin{defn}
%A process $\{X_t\}$ satisfies the \emph{Markov Property} if the
%\begin{align*}
%\mathbb{P}\left(X_{s+t} | \{X_{s-\varepsilon}\}_{\forall\varepsilon>0}\right)
%= \mathbb{P}\left(X_{s+t} | X_{s}\right)
%\end{align*}
%In other words, only the latest state $X_s$ matters for the future distribution of $X_t$

%\end{defn}

%\section{Discrete State Space, Discrete Time}

%\begin{defn}
%A \emph{discrete space, discrete time Markov chain} is a stochastic
%process $\{X_{t_i}\}_{t_i\in T}$ for some finite or countable set $T$
%that satisfies
%\begin{equation}
  %P(X_n = x_n | X_{n-1} = x_{n-1}, \ldots
  %X_0 = x_0) = P(X_n = x_n | X_{n-1}=x_{n-1})
%\end{equation}
%In words, only the most recent state matters for the future.
%\end{defn}

%\section{Infinite State Space, Discrete Time}
%\section{Infinite State Space, Continuous Time}

%\section{Stochastic Differential Equations}

%Ordinary differential equations allow us to naturally relate rates of
%change for some function to time and the value of the function itself:
%\begin{align*}
  %\frac{dy(t)}{dt} = b(t,y(t))
%\end{align*}
%Now, we want to modify this to allow random noise to influence the
%change in the value of some stochastic process:
%\begin{align*}
  %\frac{dX_t}{dt} = b(t,X_t) + \sigma(t,X_t) \xi_t
%\end{align*}
%where $\xi_t$ is some random variable representing ``noise.'' (Though
%because of some mathematical technicalities, we can't really define
%$\xi_t$ this way since it is like a derivative, but one that cannot
%exist. So we'll need to move to a definition with integrals that can be
%properly defined.)


\clearpage
\section{Foundations of Measure Theoretic Probability}


\subsection{Collections of Subsets of $\Omega$}

In this section, we take some set $\Omega$ and then consider collections
of subsets of $\Omega$. Those collections may have a special structure
so that we call them an algebra, a $\sigma$-algebra, a Dynkin System,
etc.

This section will define these collections and describe relationships
between these different types of collections.

\subsubsection{Algebra}

\begin{defn}
\label{algebra}
(Algebra)
Let $\Omega$ be any set, and let $\mathscr{A}$ be a collection of subsets of
$\Omega$ (so $\mathscr{A}\subseteq2^\Omega$). We say that
$\mathscr{A}$ is an \emph{algebra} if
\begin{enumerate}
  \item $\Omega\in\sA$
  \item $A\in\mathscr{A}\Rightarrow A^c\in\mathscr{A}$
  \item $A,B\in\mathscr{A} \Rightarrow A\cup B\in\mathscr{A}$
\end{enumerate}
Algebras are collections of subsets of $\Omega$ that are closed
under complements and finite unions. (We will see below that
$\sigma$-algebras impose the addition requirement of being closed under
\emph{countable} unions).
\end{defn}

\begin{prop}
\label{prop:algebra}
Suppose that $\sA$ is an algebra. Then by the definition, we also have
\begin{enumerate}
  \item $A,B\in\mathscr{A}$ implies $A\cap B\in \mathscr{A}$.
  \item $\emptyset \in \mathscr{A}$.
  \item $A,B\in \sF$ implies $A\setminus B \in \sF$
\end{enumerate}
\end{prop}
\begin{proof}
We take each in turn:
\begin{enumerate}
  \item Since $A\cap B = (A^c \cup B^c)^c$. Then
    \begin{align*}
      A,B\in\sA
      &\quad\implies\quad
      A^c,B^c\in\sA \\
      &\quad\implies\quad
      (A^c\cup B^c)\in\sA \\
      &\quad\implies\quad
      (A^c\cup B^c)^c\in\sA
    \end{align*}
    Hence $A\cap B = (A^c \cup B^c)^c \in \sA$

  \item Since $A, A^c \in \mathscr{A}$ and so are intersections (as we
    just showed), then $\emptyset = A\cap A^c \in \mathscr{A}$.

  \item Write $A\setminus B = A \cap B^c$
\end{enumerate}
\end{proof}


\subsubsection{$\sigma$-algebra}

$\sigma$-algebras are algebras that are closed under \emph{countable}
unions (not just finite unions). So it's a stronger requirement on the
collection of subsets of $\Omega$. But note also that $\sigma$-algebras
retain the properties we saw in Proposition~\ref{prop:algebra}---along
with possibly some new properties because of the extra structure.

\begin{defn}($\sigma$-algebra)
A collection $\sF$ of subsets of $\Omega$ is a \emph{$\sigma$-algebra}
if
\begin{enumerate}
  \item $\Omega\in\sF$
  \item $\bigcup_{n=1}^\infty A_n \in \sF$ for all countable
    collections $\{A_n\}_{n=1}^\infty$ of sets in $\sF$.
  \item $A^c \in \sF$ for all $A\in\sF$.
\end{enumerate}
Again, the key difference from a plain algebra is that we require
countable unions also be in the $\sigma$-algebra as well as the whole
space $\Omega$.
\end{defn}

\begin{ex}
The simplest algebra is the trivial $\sigma$-algebra $\{\emptyset, \Omega\}$.
The easiest non-trivial $\sigma$-algebra is the power set
$\mathscr{P}(\Omega)$.
\end{ex}

\begin{ex}
There exist algebras which are not $\sigma$-algebras. Take
$\Omega=\mathbb{Z}$. Then take
\begin{align*}
  \mathscr{A}
  = \{A\subset \mathbb{Z} \; |\; \text{either $A$ or $A^c$ finite}\}
\end{align*}
Hence the distinction between algebras and $\sigma$-algebras is
non-trivial.
\end{ex}

\begin{prop}
\label{prop:sigma-intersection}
If $\{\sF_\alpha\}_{\alpha\in J}$ is a collection of
$\sigma$-algebras on $\Omega$, then $\bigcap_{\alpha\in J} \sF_\alpha$
is one too.
\end{prop}


\begin{thm}\emph{(Relative $\sigma$-Algebra)}
\label{thm:relsigal}
Suppose we have a $\sigma$-algebra $\sF$ on $\Omega$. Then for any set
$E\subset \Omega$, we call define the
\emph{relative $\sigma$-algebra}
\begin{align*}
  \sF_E = \{ A \cap E \; | \; A \in \sF\}
\end{align*}
\end{thm}

\subsubsection{Monotone Class, Dynkin System, Semiring}

In this section, we define collections of subsets of $\Omega$ that are
all \emph{weaker} notions than a $\sigma$-algebra.  We care about them
because they are often easier to write down or work with than
$\sigma$-algebras. Later on, we will have ``Extension Theorems'' that
let us uniquely extend a measure defined on these (weaker,
easier-to-work-with) collections to the (stronger) $\sigma$-algebras
that are generated by or contain these weaker collections of subsets of
$\Omega$. But in due course. Let's first define these collections.

\begin{defn}(Monotone Class)
We call collection $\sM$ of subsets of $\Omega$ a
\emph{monotone class} if
\begin{enumerate}
  \item $\bigcup\ninf A_n \in \sM$ for any increasing sequence of sets
    $A_1\subseteq A_2\subseteq\cdots$
  \item $\bigcap\ninf A_n \in \sM$ for any decreasing sequence of sets
    $A_1\supseteq A_2\supseteq\cdots$
\end{enumerate}
\end{defn}

\begin{defn}(Dynkin System)
We call collection $\sD$ of subsets of $\Omega$ a
\emph{Dynkin System} if
\begin{enumerate}
  \item $\Omega\in\sD$
  \item $\bigcup\ninf A_n \in \sD$ for every sequence of pairwise
    disjoint sets $\{A_n\}\subseteq \sD$.
  \item $A\in\sD$ implies $A^c\in\sD$
\end{enumerate}
This is \emph{almost} the definition of a $\sigma$-algebra, except $\sD$
is closed only under finite and countable unions of \emph{pairwise
disjoint} sets in $\sD$, not arbitrary sets in $\sD$.
\end{defn}


\begin{defn}(Semiring)
A collection $S$ of subsets of $\Omega$ is called a \emph{semiring} if
\begin{enumerate}
  \item $\emptyset \in S$
  \item $A\cap B\in S$ for all $A,B\in S$
  \item For all $A,B\in S$, there exist finitely many pairwise disjoint
    sets $\{C_n\}_1^N\subseteq S$ such that
    \begin{align*}
      A \setminus B = \bigcup\nN C_n
    \end{align*}
    This last definition is a bit abstract, so think about $S$ as the
    set of all intervals in $\R$ or hypercubes in $\Rn$, and it kind of
    makes sense.
\end{enumerate}
\end{defn}

\subsubsection{Generated Collections}

We now establish a result like Proposition~\ref{prop:sigma-intersection}
for \emph{all} of the different types of collections we've seen. That
result then lets us formulate the notion of a ``smallest'' collection.

\begin{prop}
Suppose that $\{\sC_\alpha\}_{\alpha \in J}$ is a collection of either
algebras, $\sigma$-algebras, monotone classes,
or Dynkin systems on $\Omega$---note that each $\sC_\alpha$ for all
$\alpha\in J$ must be the same type of collection. Then
\begin{align*}
  \sC = \bigcap_{\alpha \in J} \sC_\alpha
\end{align*}
will also be a collection of that type.
\end{prop}

\begin{defn}
Let $\sE$ be a collection of nonempty subsets of $\Omega$. Then
define
\begin{enumerate}
  \item $\alpha(\sE) := \bigcap_{\sA \supseteq \sE} \sA$ where each
    $\sA$ in the intersection is an algebra containing $\sE$.
  \item $\sigma(\sE) := \bigcap_{\sF \supseteq \sE} \sF$
    where each $\sF$ in the intersection is a $\sigma$-algebra
    containing $\sE$.
  \item $\mu(\sE) := \bigcap_{\sM \supseteq \sE} \sM$
    where each $\sM$ in the intersection is a monotone class containing
    $\sE$.
  \item $\delta(\sE) := \bigcap_{\sD \supseteq \sE} \sD$
    where each $\sD$ in the intersection is a Dynkin system containing
    $\sE$.
\end{enumerate}
We say that $\alpha(\sE)$, $\sigma(\sE)$, $\mu(\sE)$, and $\delta(\sE)$ are the
algebra, $\sigma$-algebra, monotone class, and Dynkin system
\emph{generated by $\sE$}, respectively. As the intersection over all
collections containing $\sE$, they are each the \emph{smallest}
of their collection type containing $\sE$.
\end{defn}

\begin{prop}
\label{prop:gensigal}
Given function $f:\Omega\ra E$ and collection $\sE$ of subsets of $E$,
\begin{align*}
  \sigma\left( \left\{ f^{-1}(A) \;|\; A\in\sE \right\} \right)
  =
  \left\{ f^{-1}(A) \;|\; A\in\sigma\left( \sE \right)\right\}
\end{align*}
So generating the $\sigma$-algebra containing $\sE$ and taking
preimages \emph{commutes} with taking preimages and generating the
$\sigma$-algebra.
\end{prop}


\subsubsection{Borel $\sigma$-Algebra, Product Borel, and Lebesgue
$\sigma$-Algebra}

\begin{defn}(General Borel $\sigma$-algebra, $\sB(\Omega,\sT)$)
\label{defn:borel-general}
Let $\sT$ be a topology on space $\Omega$---i.e.\ $\sT$ is a collection
of open sets in $\Omega$. Then define the \emph{Borel $\sigma$-algebra}
as
\begin{align*}
  \sB(\Omega,\sT):=\sigma(\sT)
\end{align*}
which is the $\sigma$-algebra generated by topological space
$(\Omega,\sT)$. It is the smallest $\sigma$-algebra that contains all
open sets (and hence, all closed sets too) in $(\Omega,\sT)$. It is the
intersection of all $\sigma$-algebras containing $\sT$.
\end{defn}

\begin{defn}(Borel $\sigma$-algebra on $\R$, $\sB(\R)$)
Consider the collection of all half-open intervals in $\R$:
\begin{align}
  \sI := \left\{(a,b] \cap \R \; | \; -\infty\leq a < b \leq \infty\right\}
  \label{intervals}
\end{align}
Then, in an abuse of notation, define the Borel $\sigma$-algebra on $\R$
as
\begin{align*}
  \sB(\R) := \sigma(\sI)
\end{align*}
Now a few things to note. First, I've dropped $\sI$ as a second argument
to write simply $\sB(\R)$, rather than $\sB(\R,\sI)$. It should be
understood that we're \emph{always} using the collection $\sI$ of
half-open intervals to generate the $\sigma$-algebra on $\R$. There's no
ambiguity there.

Second, $\sI$ isn't even a topology for $\R$. In particular, it's not
the topology generated by all open intervals of $\R$. So it seems to
violate the more general Definition~\ref{defn:borel-general}.
Yeah, fine, okay. But it actually doesn't matter. Because if we
took the topology generated by the open intervals of $\R$, built the
$\sigma$-algebra from that (taking complements, countable unions,
etc.), and called the result $\sB(\R)$, we would get exactly the same
thing as $\sB(\R):=\sigma(\sI)$. That's because we can write $(a,b)$ as
the countable union $\bigcup\ninf(a,b-1/n]$ (which must be in $\sB(\R)$
for that to be a $\sigma$-algebra), so really we could work with either.
More generally, our intervals in $\sI$ could alternatively have been of
the form
\begin{align}
  (a,b)
  \qquad
  [a,b)
  \qquad
  [a,b]
  \qquad
  (-\infty,b)
  \qquad
  (-\infty,b]
  \qquad
  (b,\infty)
  \qquad
  [b,\infty)
  \label{borelsets}
\end{align}
It just so happens that $\sI$ in Statement~\ref{intervals} will be the
most convenient thing to work with.
Therefore, $\sB(\R)$ will contain all of the sets in (\ref{borelsets})
along with singlton sets $\{a\}$ and many other weird types of unions,
intersections, constructions etc.\ built from these sets.
\end{defn}

\begin{defn}(Borel $\sigma$-algebra on $\bar{\R}$, $\sB(\bar{\R})$)
We can define the Borel $\sigma$-algebra for the extended real line
$\sB(\bar{\R})$ as
\begin{align*}
  \sB(\bar{\R}) :=& \; \sigma(\bar{\sI}) \\
  \bar{\sI} =& \; \left\{(a,b] \; | \; -\infty\leq a < b \leq \infty\right\}
\end{align*}
where basically the main difference is that we don't intersect
$(-a,b]$ with $\R$ to form $\bar{\sI}$, throwing out sets of the form
$(a,\infty]$ as we did above for $\sI$ defined in
Statement~\ref{intervals}.
\end{defn}

\begin{defn}(Lebesgue $\sigma$-Algebra)
The completion of the Borel sets is called the
\emph{Lebesgue $\sigma$-algebra}, which includes uncountable measure
zero sets. (The Borel $\sigma$-algebra has no way to generate those
bitches, so the completion is non-trivial.)
\end{defn}

\begin{defn}(Product Borel $\sigma$-Algebra,
$\sB(\R^J)=\sB(\R)^{\otimes J}$)
Suppose we wanted to generalize from $\R$ to the Borel $\sigma$-algebra
on $\Rn$, denoted $\sB(\R^n)$. One way to do that is to define the set
of rectangles in $\Rn$
\begin{align*}
  \sI^n &:=
  \left\{
    \times_{k=1}^n (a_{k},b_{k}] \cap \R
    \;|\; -\infty\leq a < b \leq \infty
  \right\} \\\\
  \text{where} \quad
  \times_{k=1}^n (a_{k},b_{k}]
  &:=
  (a_{1},b_{1}]
  \times \cdots\times
  (a_{n},b_{n}]
\end{align*}
and generate the $\sigma$-algebra from that
\begin{align*}
  \sB(\Rn) &:= \sigma(\sI^n)
\end{align*}
This is a very natural way to generate the $\sigma$-algebra. However,
there's a more general way. Suppose that instead of using intervals
$(a_k,b_k]$ in $\R$ as the sides of the rectangles, we used Borel sets
of $\R$ as the sides of the ``rectangle'' (in quotes because we might
not actually have rectangles now because Borel sets are not necessarily
intervals).
That is, we take all sets in $\Rn$ of the form
\begin{align*}
  A = B_1 \times \cdots \times B_n
  \qquad B_i \in \sB(\R) \quad\forall i
\end{align*}
and define $\sB(\R)^{\otimes n}$ as the $\sigma$-algebra generated by
these sets in $\Rn$, loosely referred to as ``rectangles with Borel
sides''
\begin{align*}
  \sB(\R)^{\otimes n}
  :=&\; \underbrace{\sB(\R) \otimes \cdots \otimes \sB(\R)}_{\text{$n$ times}} \\
  :=&\;
  \sigma
  \big(
  \left\{
  A \in \Rn \;|\;
  A = B_1 \times \cdots \times B_n
  \quad B_i \in \sB(\R) \quad\forall i
  \right\}
  \big)
\end{align*}
It happens that these two sensible definitions are equivalent:
$\sB(\Rn)=\sB(\R)^{\otimes n}$. See the next Lemma. However, when it's
time generalize to $\R^J$ for possibly infinite and possibly uncountable
indexing set $J$, it turns out that $\sB(\R)^{\otimes J}$ will
sometimes be more convenient than $\sB(\R^J)$ to think about and
conceptualize (though, again, they're equivalent even in $\R^J$).
%especially because we then don't have to
%conceptualize or work with rectangles in $\R^J$, whatever that means
%when $J$ is infinite. Instead, we just work with infinite products of
%sets in $\sB(\R)$. That's mildly nicer.

The reason we care about $\sigma$-algebras for $\R^J$ where $J$ is
arbitrary is that we want to be able to model infinite time stochastic
processes, whether in discrete or continuous time. For the former, we
choose $\R^\N$ while the latter could be $\R^{[0,\infty)}$. Basically,
it's useful when we want $\R^J$ to be an ordered set.

So let's define $\sB(\R)^{\otimes J}$, where $J$ is a nonempty, possibly
uncountable indexing set like $\{1,\ldots,N\}$, $\N$, or $\R$. Then
define
\begin{align*}
  \R^J &=
  \left\{
    \left\{ \omega_\alpha \right\}_{\alpha\in J}
    \; | \;
    \omega_\alpha \in \R
  \right\} \\
  \Leftrightarrow\quad
  &=
  \left\{
    f: J \ra \R
  \right\}
\end{align*}
That is, $\R^J$ is the set of all vectors/functions with $J$ components,
each taking a value in $\R$. Here are a few examples:
\begin{enumerate}
  \item $J=\{1,\ldots,N\}$: $\R^J$ is the set of all vectors in $\R^N$.
  \item $J=\N$: $\R^J$ is the set of all real-valued sequences. This is
    a model for discrete-time stochastic processes.
  \item $J=\R$: $\R^J$ is the set of all functions from the reals to the
    reals.\footnote{For reals.}
  \item $J=[0,T]$: $\R^J$ is the set of all functions
    $\{f:[0,T]\ra\R\}$, which is a model for stochastic processes in
    continuous time.
\end{enumerate}
This definition deeply exploits the notion that real-valued vectors and
sequences are just functions from the natural numbers to the real line,
which leads us the following definition of the
\emph{$\alpha$th projection function}:
\begin{align*}
  q_\alpha: \R^J &\ra \R \\
  f &\mapsto f(\alpha)
\end{align*}
This takes a vector/function in $\R^J$ and returns the $\alpha$th
element.

Now, define the \emph{product Borel $\sigma$-algebra} on $\R^J$ denoted
$\sB(\R)^{\otimes J}$ as the smallest $\sigma$-algebra on $\R^J$ such
that each projection $q_\alpha$ is measurable for all $\alpha\in J$.
This $\sigma$-algebra is generated by all sets of the form
\begin{align*}
  \{q_{\alpha}\leq a\}
  =
  \{\omega \in \R^J \;|\; \omega_\alpha \leq a\}
  \qquad \alpha \in J\quad a \in\R
\end{align*}
In two dimensions $J=\{1,2\}$, that means generate the $\sigma$-algebra
from all sets of the form $(-\infty,a]\times \R$ and $\R \times
(-\infty,a]$ for all $a\in \R$.
\end{defn}
\begin{rmk}
There are several different ways we could have defined
$\sB(\R)^{\otimes J}$, though the one above is probably the most no-fuss
way.
\end{rmk}


\begin{lem}
Let $\sC$ be a collection of subsets of $\Omega$. For $B\subseteq
\Omega$, define
\begin{align*}
  \sC \cap B = \{A \cap B \;|\; A \in \sC\}
\end{align*}
Then
\begin{align*}
  \sigma(\sC\cap B) = \sigma(\sE) \cap B
\end{align*}
\end{lem}
\begin{cor}
For any arbitrary indexing set $J$, we have
$\sB(\R^J)=\sB(\R)^{\otimes J}$.
\end{cor}

\clearpage
\subsubsection{Relating Types of Collections and the Monotone Class Theorems}

\begin{prop}\emph{(Dynkin $\implies$ Monotone Class)}
\label{prop:sal-mc}
If collection $\sD$ of subsets of $\Omega$ is a Dynkin System, then
it is also a monotone class---though the converse is not
necessarily true.
\end{prop}

\begin{prop}\emph{($\sigma$-algebra $\implies$ Dynkin, Monotone Class)}
\label{prop:sal-d}
If collection $\sF$ of subsets of $\Omega$ is a $\sigma$-algebra, then
it is a Dynkin System and, hence, a monotone class by
Theorem~\ref{prop:sal-mc}---though the converse is not necessarily true.
\end{prop}

\begin{prop}
Given collection $\sE$ of subsets of $\Omega$, the set $\sigma(\sE)$ is
smaller than $\delta(\sE)$, since $\sigma$-algebras are more
restrictive than Dynkin Systems.
\end{prop}


\begin{lem}\emph{(Algebra is a $\sigma$-algebra $\iff$ Monotone Class)}
\label{lem:alg-sigal-mc}
An algebra $\sA$ is a $\sigma$-algebra if and only if it is a monotone
class.
\end{lem}
\begin{proof}
For the $\Rightarrow$ direction, use Proposition~\ref{prop:sal-d}.
For the $\Leftarrow$ direction, suppose that algebra $\sA$ is a
monotone class. We want to show it is also a $\sigma$-algebra, which
requires showing only that $\sA$ is closed under countable unions. The
rest of the properties of a $\sigma$-algebra are ensured since $\sA$ is
already an algebra.

So suppose that we have a squence of sets
$\{A_n\}\ninf\subseteq\sA$. Define sequence
\begin{align*}
  B_m := \bigcup_{n=1}^m A_n
\end{align*}
Clearly, $B_m \in \sA$ since closed under finite unions and
\begin{align}
  \bigcup\minf B_m = \bigcup\ninf A_n
  \label{mcBm}
\end{align}
Also, the sequence $\{B_m\}$ is clearly increasing. Since $\sA$ is a
monotone class, the LHS of Line~\ref{mcBm} will also be in $\sA$,
implying the RHS equivalent is in $\sA$. Hence $\sA$ is closed under
countable unions so that it is a $\sigma$-algebra.
\end{proof}

\begin{thm}\emph{(Monotone Class Theorem 1)}
\label{thm:mct-1}
If $\sA$ is an algebra, then
\begin{align*}
  \mu(\sA) = \sigma(\sA)
\end{align*}
In words, the monotone class generated by algebra $\sA$ matches the
$\sigma$-algebra generated by $\sA$.
\end{thm}
\begin{proof}
($\mu(\sA) \subseteq \sigma(\sA)$)
First, since $\sigma(\sA)$ is a $\sigma$-algebra,
Lemma~\ref{lem:alg-sigal-mc} implies that $\sigma(\sA)$ is a monotone
class. It also contains $\sA$, therefore by the definition of
$\mu(\sA)$ as the \emph{smallest} monotone class containing $\sA$, we
have $\mu(\sA) \subseteq \sigma(\sA)$.
\\
\\
($\mu(\sA) \supseteq \sigma(\sA)$)
This follows if we can show that $\mu(\sA)$ is an algebra. Then
$\mu(\sA)$ would be a monotone class \emph{and} an algebra containing
$\sA$, so we could again apply Lemma~\ref{lem:alg-sigal-mc} to conclude
that it is a $\sigma$-algebra. Then $\sigma(\sA)$ as the smallest
$\sigma$-algebra containing $\sA$ would clearly imply
$\mu(\sA) \supseteq \sigma(\sA)$.

So we want to show $\mu(\sA)$ is an algebra; therefore, we must show
it satisfies the properties
\begin{itemize}
  \item \emph{Closed under Complements}:
    Start by defining
    \begin{align*}
      \sM :=& \;\mu(\sA) \\
      \tilde{\sM} :=& \; \{A\;|\; A,A^c\in\sM\}
    \end{align*}
    In words, $\sM$ is the collection $\mu(\sA)$ that we want to show is
    closed under complements, and $\tilde{\sM}$ is the subset of $\sM$
    for which it \emph{actually is} closed under complements---it's all
    the sets that satisfy the property we want.\footnote{%
      This is called the \emph{principle of appropriate sets}.
    }
    It's clear that $\sA\subseteq \tilde{\sM} \subseteq\sM$, by
    construction.

    Now, suppose we change the game. We started out wanting to show that
    $\sM$ is closed under complements. But how about we instead show
    that $\tilde{\sM}$ (that contains $\sA$) is a monotone class. Then,
    since $\sM$ is the smallest monotone class containing $\sA$, we have
    $\sM\subseteq\tilde{\sM}$ which implies $\sM=\tilde{\sM}$. In words,
    the subset of collection $\sM$ that satisfies the desired property
    is the whole collection.

    Okay, take any sequence of increasing sets
    $\{A_n\}\in\tilde{\sM}$, i.e.\ $A_1\subseteq A_2\subseteq \cdots$.
    We need to show that the union is $\tilde{\sM}$, which means showing
    \begin{align*}
      A:= \bigcup\ninf A_n &\in \tilde{\sM}
      \qquad \qquad
      A^c:=\left(\bigcup\ninf A_n\right)^c
      =
      \bigcap\ninf A_n^c
      \in \tilde{\sM}
    \end{align*}
    First, since $\tilde{\sM}\subseteq \sM$, with $\sM$ being a monotone
    class, we can say that
    \begin{align}
      A:=\bigcup\ninf A_n &\in \sM
      \label{mc:increasing}
    \end{align}
    Second, since $A_n\in\tilde{\sM}$, we have $A_n^c\in\tilde{\sM}$ by
    construction. Moreover, $\{A^c_n\}$ is a sequence of decreasing sets
    in $\tilde{\sM}\subseteq\sM$. Since $\sM$ is a monotone class, we
    can say that
    \begin{align}
      A^c:=\bigcap\ninf A_n^c &\in \sM
      \label{mc:decreasing}
    \end{align}
    Therefore, since Expressions~\ref{mc:increasing} and
    \ref{mc:decreasing} imply that $A,A^c\in\sM$, we have
    $A,A^c\in\tilde{\sM}$ by the definition of $\tilde{\sM}$.
    We can do the same for any decreasing sequence of sets $\{A_n\}$,
    hence $\tilde{\sM}$ is a monotone class containing $\sA$, hence
    $\sM\subseteq\tilde{\sM}$, hence $\sM=\tilde{\sM}$, hence $\sM$ is
    a monotone class \emph{and} an algebra, hence $\sM=\mu(\sA)$ is a
    $\sigma$-algebra.

  \item \emph{Closed under Finite Intersections}:
\end{itemize}
\end{proof}

\clearpage
\begin{lem}
\label{lem:dynkin-closed}
\emph{(Dynkin System closed under Finite $\cap$ $\implies$ $\sigma$-algebra)}
If $\sD$ is a Dynkin System that is closed under finite intersections,
then $\sD$ is also a $\sigma$-algebra.
\end{lem}
\begin{proof}
Recall that Dynkin Systems are basically $\sigma$-algebras, but they're
closed under countable unions of \emph{disjoint} sets only. We need to
extend that to countable unions of \emph{arbitrary} sets. So suppose
that $\{A_n\}\ninf\subseteq \sD$. Define
\begin{align*}
  B_1 &= A_1 \\
  B_m &= A_m \setminus \left( \bigcup_{n=1}^{m-1} A_n\right)
  \qquad m>1
\end{align*}
Then $\{B_m\}$ is a sequence of pairwise disjoint sets. Note also that
each $B_m$ can be written alternatively as
\begin{align*}
  B_m &= A_m \setminus \left( \bigcup_{n=1}^{m-1} A_n\right)
  = A_m \cap \left( \bigcup^{m=1}_{n=1}A_n\right)^c
  = A_m \cap \left( \bigcap^{m=1}_{n=1}A_n^c\right)
\end{align*}
Recall that $\sD$ is a Dynkin system (so $A^c_n\in \sD$ for all $n$) and
that $\sD$ is assumed closed under finite intersections. Then it's clear
that $B_m$ is in $\sD$ for all $m$.

So note that we can write the union of the sequence $\{A_n\}\ninf$ as
the union of pairwise disjoint $\{B_m\}_{m=1}^\infty$ in $\sD$:
\begin{align*}
  \bigcup\minf B_m = \bigcup\ninf A_n
\end{align*}
Since $\sD$ is a Dynkin system, the LHS is in $\sD$. Hence by the RHS,
$\sD$ is closed under countable unions of arbitrary sequences in $\sD$.
\end{proof}

\begin{thm}\emph{(Monotone Class Theorem 2)}
\label{thm:mct-2}
If $\sE$ is a nonempty collection of subsets of $\Omega$ that is closed
under finite intersections, then
\begin{align*}
  \delta(\sE)=\sigma(\sE)
\end{align*}
In words, the Dynkin System generated by $\sE$ matches the
$\sigma$-algebra generated by $\sE$.
\end{thm}

\clearpage
\subsection{Measurable Sets and Spaces, Measure Spaces, and Measures}

This subsection is all about measuring the sizes of subsets of $\Omega$.
A function like $\mu$ that takes a subset $A\subseteq\Omega$ and returns
a number $\mu(A)$ representing the ``size'' (or sometimes
``probability'') of $A$ is called a \emph{measure}.
We will use $\sigma$-algebras to define the collection
of subsets of $\Omega$ that we are \emph{able to measure}---i.e.\ to
define ``measurable sets''---because, in general, we might not be able
to measure every subset of $\Omega$ with a given measure $\mu$.


\begin{defn}(Measurable Space and Measurable Sets)
Let $\Omega$ be some space and $\sF$ be a $\sigma$-algebra on
$\Omega$. Then the couplet $(\Omega, \sF)$ is called a
\emph{measurable space} while the sets in $\sF$ are called
\emph{measurable sets}.

The space is ``measur\emph{able}'' in the sense that we can assign a
\emph{measure} (some special function) to it. The elements of $\sF$ are
``measurable sets'' in the sense that the measure has domain $\sF$ and
``measures'' them (i.e.\ tells us their size). We see all of this in the
next definition.
\end{defn}

\begin{defn}(Measure)
\label{defn:measure}
A \emph{measure} on a measurable space $(\Omega,\sF)$ is a set function
$\mu: \sF \rightarrow [0,\infty]$ that satisfies
\begin{enumerate}
  \item $\mu(\emptyset)=0$.
  \item \emph{Countable} or \emph{$\sigma$-additivity} for all pairwise
    disjoint sequences $\{A_n\}_{n=1}^\infty$ in $\sF$. That means
    \begin{align*}
      \mu\left(\bigcup^\infty_{n=1}A_n\right)
      = \sum^n_{n=1} \mu(A_n)
    \end{align*}
\end{enumerate}
If we're being pedantic, then $\mu$ as defined above is a
\emph{non-negative measure}.
\end{defn}

\begin{defn}(Measure Space)
Once we attach a measure to a measur\emph{able} space, we call the
resulting triplet $(\Omega,\sF,\mu)$ a \emph{measure space}.
\end{defn}

\begin{ex}(Counting Measure)
Take any finite or countable space $\Omega$ and set
$\sF=\mathscr{P}(\Omega)$. Then we can always define the
\emph{counting measure} $\mu(A) = \text{card}(A)$.  So $\mu$ just counts
the number of elements in any subset of $\Omega$.
\end{ex}

\begin{defn}(Finite Measure)
A measure $\mu$ is a \emph{finite measure} if $\mu(\Omega)<\infty$.
Note that as a consequence of the definition,
$\mu(A^c) = \mu(\Omega) - \mu(A)$.
\end{defn}

\begin{defn}(Probability Measure)
A \emph{probability measure} $P$ is a finite measure such that
$P(\Omega)=1$.
\end{defn}
\begin{rmk}
The key property is really finiteness $\mu(\Omega)<\infty$ of the
measure. We can always normalize \emph{any} finite measure to be a
probability measure.
\end{rmk}

\begin{defn}(Probability Space)
A \emph{probability space} is a measure space $(\Omega,\sF,P)$ where $P$
is a probability measure so that $P(\Omega)=1$.
\end{defn}

\clearpage
\begin{defn}(Finitely Additive Measure)
Given measurable space $(\Omega,\sF)$, the function
$\mu:\sF\ra [0,\infty]$ is a \emph{finitely additive measure} if
\begin{enumerate}
  \item $\mu(\emptyset)=0$
  \item $\mu(A\cup B) = \mu(A) + \mu(B)$ for all pairwise disjoint
    $A,B\in\sF$.
\end{enumerate}
\end{defn}
\begin{rmk}
This is weaker than a ``measure'' as introduced in
Definition~\ref{defn:measure}. Here, we don't \emph{necessarly} have
countable additivity; we might just have finite additivity.  (Of course,
Definition~\ref{defn:measure}  countably-additive measures are obviously
finitely additive measures as well.) Moreover, that's nontrivially
weaker, since many of the convergence results that we'll see for general
integration are definitions involving limits, they turn on
\emph{countable} additivity and fail to work with
only finite additivity.

The reason we also care about finitely additive measures is that they're
often easier to write down or define or verify than $\sigma$-additive
measures, \emph{plus} we'll be able to put conditions on finitely
additive measures that ensure they are \emph{also} $\sigma$-additive
measures as well.
\end{rmk}

\begin{note}
In general, ``measure'' refers to the countably-additive kind of
measure. We will explicitly use ``finitely additive measure'' to refer
to those measures that are not \emph{necessarily} countably-additive.
\end{note}

\begin{defn}($\sigma$-Finite Measure)
Given some collection $\sC$ of subsets of $\Omega$,
we say that a measure $\mu:\sC\ra [0,\infty]$
is \emph{$\sigma$-finite} if there exists a sequence of sets
$\{A_n\}\subseteq \sC$ such that
\begin{align*}
  \Omega = \bigcup\ninf A_n
  \quad \text{and}\quad
  \mu(A_n)<\infty
  \quad\forall n
\end{align*}
\end{defn}

\begin{rmk}(Important)
Any, I repeat, any \emph{finite}/\emph{probability} measure is going to
be \emph{$\sigma$-finite}, though this is not necessarily true for
general measures.
\end{rmk}

\begin{ex}
The Lebesgue measure $\mu([a,b])=b-a$ on the Borel $\sigma$-algebra
$\sB(\R)$ is a $\sigma$-finite, but non-finite measure.
\end{ex}

%\subsection{Properties of Measures}

Now, we state some properties of measures, taking care to specify
whether the result assumes that the measure $\mu$ is $\sigma$-additive
or only finitely additive. All results assume $\mu$ is defined on
measurable space $(\Omega,\sF)$.

\begin{prop}\emph{(Monotonicity of Measures)}
If $A\subseteq B$ for $A,B\in\sF$, then
\begin{align*}
  \mu(A) \leq \mu(B)
\end{align*}
whether $\mu$ is finitely- or $\sigma$-additive.
\end{prop}
\begin{proof}
We can write $B = A \cup (B\setminus A)$, which is a union of disjoint
sets. Then use finite additivity to say $\mu(B) = \mu(A) +
\mu(B\setminus A) \geq \mu(A)$.
\end{proof}

\clearpage
\begin{cor}
If $A\subseteq B$ for $A,B\in \sF$ and $\mu$ is finite measure, then
\begin{align*}
  \mu(B\setminus A) = \mu(B) - \mu(A)
\end{align*}
\end{cor}
\begin{proof}
Write
$\mu(B) = \mu((B\setminus A) \cup A) = \mu(B\setminus A) + \mu(A)$.
Rearrange and the result follows.
\end{proof}

\begin{prop}\emph{(Countable Sub-Additivity of Measures)}
Given $\sigma$-additive measure $\mu$ and \emph{any}---not necessarily
pairwise disjoint or nested---sequence of sets $\{A_n\}\ninf$, we have
\begin{align*}
  \mu\left(\bigcup^\infty_{n=1} A_n\right)
  \leq \sum\ninf \mu(A_n)
\end{align*}
If $\mu$ is a finitely-additive measure, then this holds only for finite
sequences of sets $\{A_n\}\nN$.
\end{prop}
\begin{proof}
Construct the sets
\begin{align*}
  B_1 = A_1 \qquad B_2 = A_2 \setminus A_1
  \qquad B_3 = A_3 \setminus (B_1 \cup B_2)
\end{align*}
Clearly, we have $B_i \cap B_j$ for all $i\neq j$. On top of that $B_n
\subset A_n$ for all $n$. Thus
\begin{align*}
  \mu\left(\bigcup^\infty_{n=1} A_n\right)
  = \mu\left(\bigcup^\infty_{n=1} B_n\right)
  = \sum\ninf \mu(B_n)
  \leq \sum\ninf \mu(A_n)
\end{align*}
\end{proof}

\begin{prop}\emph{(Continuity from Above and Below)}
\label{prop:prop-meas}
Given $\sigma$-additive measure $\mu$,
\begin{enumerate}
  \item \emph{(Continuous From Below)}: For any increasing sequence of
    measurable sets $A_1\subseteq A_2\subseteq A_3 \subseteq \cdots$ in
    $\sF$,
    \begin{align*}
      \mu\left(\bigcup^\infty_{n=1} A_n\right) =
      \lim_{n\rightarrow\infty} \mu(A_n)
    \end{align*}

  \item \emph{(Continuous From Above)}:
    For any decreasing sequence of measurable sets
    $A_1 \supseteq A_2 \supseteq A_3 \supseteq \cdots$ in $\sF$
    with $\mu(A_k)<\infty$ for some $k$,
    \begin{align*}
      \mu\left(\bigcap^\infty_{n=1} A_n\right)
      = \lim_{n\rightarrow\infty} \mu(A_n)
    \end{align*}
\end{enumerate}
\end{prop}
\begin{proof}
We prove each property in turn:
\begin{enumerate}
  \item
    Construct the following pairwise disjoint sequence of sets
    \begin{align*}
        B_1 = A_1 \qquad B_2 = A_2 \setminus A_1
        \qquad B_3 = A_3 \setminus A_2 \qquad \cdots
    \end{align*}
    Then note that we can equivalently work with the sequence
    $\{B_n\}\ninf$ instead of $\{A_n\}\ninf$:
    \begin{align*}
        A_N = \bigcup^N_{n=1} A_n =
        \bigcup^N_{n=1} B_n
        \qquad \Rightarrow \qquad
        \bigcup^\infty_{n=1} A_n =
        \bigcup^\infty_{n=1} B_n
    \end{align*}
    So then we can exploit countable and finite additivity for disjoint
    sets:
    \begin{align*}
        \mu\left(\bigcup^\infty_{n=1} A_n \right)
        = \mu\left(\bigcup^\infty_{n=1} B_n \right)
        &= \sum^\infty_{n=1} \mu(B_n)
            = \lim_{N\rightarrow\infty}\sum^N_{n=1} \mu(B_n)
        = \lim_{N\rightarrow\infty}
            \mu\left(\bigcup^N_{n=1} B_n\right) \\
        &= \lim_{N\rightarrow\infty}
            \mu\left(\bigcup^N_{n=1} A_n\right)
        = \lim_{N\rightarrow\infty}
            \mu\left(A_n\right)
    \end{align*}
  \item
    Without loss of generality, assume that $k=1$. Then construct the
    following sets
    \begin{align*}
      B_1 = A_1 \setminus A_2 \qquad
      B_2 = A_2 \setminus A_3 \qquad
      \cdots\qquad
      B_n = A_n \setminus A_{n+1}
    \end{align*}
    Then, for all $n$, we can express $A_1$ as a disjoint union
    \begin{align}
        A_1 &= B_1 \cup B_2 \cup \cdots \cup B_n
        \cup A_{n+1} \notag\\
        \Rightarrow \qquad
        \mu(A_1) &= \sum^n_{\ell=1} \mu(B_\ell) +
        \mu(A_{n+1}) \label{prop.6.1}
    \end{align}
    Next, we can also say that if $x\not\in \bigcup^\infty_{n=1} B_n$,
    then we must have $x\in A_n$ for all $n$. This implies that we can
    express $A_1$ as the following disjoint union:
    \begin{align}
        A_1 &= \left(\bigcup^\infty_{n=1} B_n \right)\cup
            \left(\bigcap^\infty_{n=1} A_n\right) \notag \\
        \Rightarrow \qquad
            \mu(A_1) &=
            \sum^\infty_{n=1} \mu(B_n) +
            \mu\left(\bigcap^\infty_{n=1} A_n\right)
            \label{prop.6.2}
    \end{align}
    And since we know that $\lim_{n\rightarrow \infty} \sum^n_{\ell=1}
    \mu(B_\ell) = \sum^\infty_{\ell=1} \mu(B_\ell)$, we can deduce from
    Equations \ref{prop.6.1} and \ref{prop.6.2} (and from the fact that
    $\mu(A_{n+1}) \leq \mu(A_n) <\infty$) that
    \[ \mu(A_{n+1})\rightarrow
        \mu\left(\bigcap^\infty_{n=1} A_n\right)
    \]
\end{enumerate}
\end{proof}

\begin{ex}
We stipulated in Property 2 of Proposition~\ref{prop:prop-meas} for our
measure space that there must be some $k$ such that $\mu(A_k)<\infty$.
Suppose that didn't hold, and we can construct a counterexample.

Consider $\Omega = \mathbb{N}$ and $\sF =\mathscr{P}(\mathbb{N})$.
Next let $A_n = \{n, n+1, n+2, \ldots\}$. In this case, $\mu(A_n) =
\infty$ for all $n$, but
\begin{align*}
  \mu\left(\bigcap^\infty_{n=1} A_n\right) =
  \mu(\emptyset) = 0
\end{align*}
Though of course, this never happens for probability measures. It's only
something to worry about for non-finite measures.
\end{ex}

\begin{thm}
\emph{(Necessary and Sufficient Conditions for a Finitely-Additive
Measure on an Algebra to be $\sigma$-additive)}
Suppose that $\mu$ is a finitely-additive measure over an algebra $\sA$
that is also finite, i.e. $\mu(\Omega)<\infty$ (so probability measures
qualify). Then the following are equivalent:
\begin{enumerate}
  \item $\mu$ is $\sigma$-additive, i.e.
    \begin{align*}
      \mu\left(\bigcup_{n=1}^\infty A_n\right)
      =
      \sumninf \mu\left( A_n\right)
    \end{align*}
    for all sequences of pairwise disjoint sets $\{A_n\}\ninf\in\sA$
    whose union $\bigcup_{n=1}^\infty A_n\in\sA$ as well.\footnote{%
    Since $\sA$ is just an algebra, not a $\sigma$-algebra necessarily,
    the inclusion of the union in $\sA$ is not guaranteed. You'll see
    this kind of qualification in the other cases as well.}

  \item $\mu$ is continuous from below, i.e.
    \begin{align*}
      \mu\left(\bigcup^\infty_{n=1} A_n\right) =
      \lim_{n\rightarrow\infty} \mu(A_n)
    \end{align*}
    for any increasing sequence $A_1\subseteq A_2 \subseteq \cdots$ in
    $\sA$ whose union $\bigcup_{n=1}^\infty A_n\in\sA$ as well.

  \item $\mu$ is continuous from above, i.e.
    \begin{align*}
      \mu\left(\bigcap^\infty_{n=1} A_n\right)
      = \lim_{n\rightarrow\infty} \mu(A_n)
    \end{align*}
    for any decreasing sequence
    $A_1 \supseteq A_2 \supseteq A_3 \supseteq \cdots$ in $\sA$ whose
    intersection $\bigcap_{n=1}^\infty A_n \in \sA$ as well.

  \item $\mu$ is continuous from above at $\emptyset$, i.e.
    \begin{align*}
      \lim_{n\rightarrow\infty} \mu(A_n)
      =0
    \end{align*}
    for any decreasing sequence
    $A_1 \supseteq A_2 \supseteq A_3 \supseteq \cdots$ in $\sA$ whose
    intersection $\bigcap_{n=1}^\infty A_n = \emptyset$.
\end{enumerate}
\end{thm}


\clearpage
\subsection{Complete and Relative Measure Spaces}

\begin{defn}(Complete Measure Space)
The measure space $(\Omega,\sF,\mu)$ is \emph{complete} if
\begin{align*}
  \begin{rcases}
      B\in\mathscr{F} \\
      \mu(B) = 0\\
      A\subseteq B
  \end{rcases}
  \quad \implies\quad
  A\in\mathscr{F}
\end{align*}
Said another way, in a complete measure spaces, \emph{all} subsets of
measure zero sets are themselves measurable (i.e.\ in $\sF$). This rules
out a certain type of weirdness for $\sF$ and $\mu$.

Moreover, the next theorem tells us that we can always extend or
``complete'' an incomplete measure space (without taking away the
original sets in $\sF$ or changing the value $\mu$ assigns to them). So
we don't ever have to work with weird, incomplete measure spaces or make
any behavior-changing modifications to $\mu$ to cope. It really is just
an extension.
\end{defn}

\begin{thm}\emph{(Completing a Measure Space)}
Given abitrary measure space $(\Omega,\sF,\mu)$, we can extend it to be
a \emph{complete} measure space denoted
$(\Omega,\hat{\sF},\hat{\mu})$ by defining
\begin{align*}
  \hat{\sF} = \{S\cup A \}
  \quad \text{where} \quad
  \left\{
      \begin{array}{l}
      A \subseteq B\\
      S, B\in\sF\\
      \mu(B) = 0 \\
  \end{array}
  \right.
  \qquad \text{and} \qquad
  \hat{\mu}(S\cup A) = \mu(S)
\end{align*}
\end{thm}

\begin{proof}
(Check that $\hat{\mu}$ is Well-Defined) Let $\hat{S}\in\sF$ and
suppose we have
\begin{align*}
    \hat{S} = S_1 \cup A_1 = S_2 \cup A_2
    \quad\text{with}\quad
    A_i \subset B_i, \quad \mu(B_i) = 0,
    \quad S_i, B_i \in \sF
\end{align*}
We need to show that $\mu(S_1) = \mu(S_2)$ to show $\hat{\mu}(S_1\cup
A_1) = \hat{\mu}(S_2\cup A_2$. To do so, use monotonicity of $\mu$ on
$\sF$, the equivalent representations of $\hat{S}$, and
countable additivity on $\sF$:
\begin{align*}
  \mu(S_2) \leq \mu(S_2 \cup B_2) \leq
      \mu(S_1 \cup B_1) \leq \mu(S_1) + \mu(B_1)
      \leq \mu(S_1) + 0 = \mu(S_1)  \\
  \mu(S_1) \leq \mu(S_1 \cup B_1) \leq
      \mu(S_2 \cup B_2) \leq \mu(S_2) + \mu(B_2)
      \leq \mu(S_2) + 0 = \mu(S_2)
\end{align*}
Therefore, $\mu(S_1) = \mu(S_2)$.
\\
\\
(Show $\hat{\sF}$ is a $\sigma$-algebra) We need to show a few properties.
\begin{enumerate}
  \item
    First, we show closed under countable unions. So take any sequence
    of sets $\{C_n\}\ninf$ in $\hat{\sF}$. By definition of
    $\hat{\sF}$ each $C_n$ can be written as $C_n=S_n\cup A_n$ for some
    $S_n \in \sF$ and $A_n\subseteq B_n$ with $B_n\in\sF$ (possibly
    equal to the emptyset). So consider the union
    \begin{align*}
        \bigcup^\infty_{n=1} C_n=
        \bigcup^\infty_{n=1} (S_n\cup A_n) =
        \left(\bigcup^\infty_{n=1} S_n\right)
        \cup \left(\bigcup^\infty_{n=1} A_n\right)
    \end{align*}
    Next, considering all of the $B_n\in\sF$ such that $B_n\supseteq
    A_n$, it's clear that
    \begin{align}
        B:=\bigcup^\infty_{n=1} B_n &\supseteq
        \bigcup^\infty_{n=1} A_n
        \label{BsupA}
    \end{align}
    But by countable sub-additivity of $\mu$ and $\{B_n\}\subseteq\sF$
    (so that the union is in $\sF$), we have
    \begin{align}
        \mu(B)
        =\mu\left(\bigcup^\infty_{n=1} B_n \right)
        \leq\sum^\infty_{n=1} \mu\left(B_n \right)
        =\sum^\infty_{n=1} 0 =0
    \end{align}
    This implies that
    \begin{align*}
        \left(\bigcup^\infty_{n=1} S_n\right) \cup B \in
        {\sF}
    \end{align*}
    which allows us to conclude that $\bigcup\ninf A_n$---a subset of
    the measure zero set $B\in\sF$ by Statement~\ref{BsupA}---can be
    joined to the $S_n$ sets so that
    $\bigcup\ninf (S_n \cap A_n)\in\hat{\sF}$.

  \item
    Next, we show that complements are in $\sF$. So take
    \begin{align*}
      (S\cup A)^c = S^c \cap A^c \supseteq S^c \cap B^c
    \end{align*}
    Now it's clear that $S^c \cap B^c\in \sF$. In addition, we can
    rewrite $S^c\cap A^c$ as
    \begin{align*}
      S^c\cap A^c = (S^c \cap B^c) \cup (B\cap A^c)
    \end{align*}
    Then $B \cap A^c = B\setminus A$ will be measure zero, implying that
    $S^c \cap A^c\in \hat{\sF}$.

  \item Countable additivity is straightforward, given how $\hat{\mu}$
    is defined.
\end{enumerate}
\end{proof}

\begin{thm}
Given any measure space $(\Omega,\sF,\mu)$, if $E\subset \Omega$ and
$E\in\sF$, then the triplet $(E,\sF_E,\mu_E)$ is called
a \emph{relative measure space} with
\[
    \sF_E = \{ A \cap E \; | \; A \in \sF\}
    \qquad
    \mu_E(A) = \mu(A\cap E)
\]
Moreover, $\sF_E$ is a proper $\sigma$-algebra and $\mu_E$ will,
indeed, be a non-negative measure.
\end{thm}
\begin{proof}
For the proof that $\sF_E$ is a $\sigma$-algebra, see Theorem
\ref{thm:relsigal}. To show $\mu_E$ is a non-negative measure, we must
show countable additivity. To do so, use the fact that any set
$B\in\sF_E$ can be respresented as $A\cap E$ for some $A\in\sF$:
\begin{align}
    \label{relsigal-chain}
    \mu_E\left(\bigcup^\infty_{n=1} A_n \right)
    &= \mu\left(\left(\bigcup^\infty_{n=1} A_n
    \right) \cap E \right)
    = \mu\left(\bigcup^\infty_{n=1} (A_n
    \cap E)\right)
\end{align}
And since $A_n$ is in $\sF_E$ for all $n$, we know that it can be
represented as $A_n=B_n\cap E$ for some $B_n\in \sF$ by construction. So
$B_n \cap E \cap E$ will be in $\sF$ for all $n$, allowing us to apply
countable additivity of $\mu$ to the final representation in
Expression~\ref{relsigal-chain}. Thus, countable additivity for $\mu_E$
will follow.
\end{proof}

\clearpage
\subsection{Dirac, Discrete, and (Absolutely) Continuous Measure}

\begin{defn}(Dirac Measure)
Given set $\Omega$, pick some element $\omega\in\Omega$. Then the
\emph{Dirac measure} $\delta_\omega$ for that element is defined as
\begin{align*}
  \delta_\omega: 2^\Omega &\ra \{0,1\} \\
  \delta_\omega[A]
  &\mapsto
  \begin{cases}
    1 & \omega\in A\\
    0 & \omega\not\in A\\
  \end{cases}
\end{align*}
In words, the Dirac measure $\delta_\omega$ will assign a value of one
to any $A\in2^\Omega$ that contains $\omega$, zero otherwise.
\end{defn}

\begin{defn}(Discrete, Continuous, Absolutely Continuous Meausures)
Let $\mu$ be a measure on $\Omega=\R^d$. Then we call the measure $\mu$
\begin{itemize}
  \item \emph{Discrete}: If we can express $\mu$ as a sum of Dirac
    measures
    \begin{align*}
      \mu[A] = \sum\ninf p_n \delta_{c_n}[A]
    \end{align*}
    for $c_1,c_2,\ldots\in\Omega=\R^d$ and $p_1,p_2,\ldots\geq 0$

  \item \emph{Continuous}: If the following
    \begin{align*}
      \mu\big[
        (a_1,b_1] \times\cdots\times (a_d,b_d]
      \big]
    \end{align*}
    is a continuous function of $b\in \R^d$

  \item \emph{Absolutely Continuous}: If there exists a Borel-function
    $f:\R^d\ra\R$ called a \emph{density} such that
    \begin{align*}
      \mu\big[
        (a_1,b_1] \times\cdots\times (a_d,b_d]
      \big]
      =
      \int_{a_1}^{b_1}
      \cdots
      \int_{a_d}^{b_d}
      f(x_1,\ldots,x_d)\;
      dx_1\;\cdots \;dx_d
    \end{align*}
    Recall that $f$ is a Borel function if
    $f^{-1}(A) \in \sB(\R)^{\otimes d}$ for any open $A\subseteq\R$.
    This is stronger than a continuous measure; it rules out weird,
    intractable cases.
\end{itemize}
\end{defn}

\begin{prop}
If $f_1,\ldots,f_d$ are densities on $\R$ then
\begin{align*}
  f(x) = f_1(x_1) \cdots f_d(x_d)
\end{align*}
is a density in $\R^d$ that implies a corresponding measure
\begin{align*}
  \mu = \mu_1\otimes \cdots \otimes \mu_d
\end{align*}
\end{prop}



\clearpage
\subsection{Extension Theorems}

Classic example for why extension theorems are important: CDFs give you
the probability that a random variable falls in some half-open interval
like $(-\infty,x]$. Though we naturally define a CDF in terms of these
intervals, we would like the corresponding measure implied by that
CDF to hold on $\sB(\R)$ as well, which contains these half-open
intervals along with many more sets. The Extension Theorems let us
``extend'' a measure defined on half-open intervals to all of the Borel
sets.

\begin{thm}
\label{thm:PQunq}
Let $P$ and $Q$ be probability measures on $(\Omega,\sigma(\sE))$ where
$\sE$ is a collection of subsets of $\Omega$ that is closed under finite
intersections. Then
\begin{align*}
  P[A] &= Q[A]
  \quad\forall A\in\sE
  \quad\implies\quad
  P[B] = Q[B]
  \qquad \forall B\in\sigma(\sE)
\end{align*}
%That is, if the probability measures match on $\sE$, they will match on
%the $\sigma$-algebra generated by $\sE$, the collection $\sigma(\sE)$.
In words a measure on $\sigma(\sE)$ is \emph{uniquely} specified simply
by defining the measure on $\sE$.
\end{thm}
\begin{proof}
Define the set
\begin{align*}
  \sD &= \left\{ A\in\sigma(\sE) \;|\; P[A]=Q[A]  \right\}
\end{align*}
Right off the bat, it's clear that $\sD\subseteq \sigma(\sE)$. And
by assumption that $P[A]=Q[A]$ for all $A\in \sE$ and construction of
$\sD$, it's clear that $\sD\supseteq \sE$. Putting these two results
together, we've then established that
\begin{align}
  \sE \subseteq \sD \subseteq \sigma(\sE)
  \label{Dsqueeze}
\end{align}
Moreover, suggestively named $\sD$ is also a Dynkin system, which
follows from checking the three properties:
\begin{enumerate}
  \item $\Omega\in\sD$: Since $P$ and $Q$ are probability measures,
    $P[\Omega]=Q[\Omega]=1$ and $\Omega\in\sigma(\sE)$ by definition, so
    $\Omega\in\sD$.
  \item Closed under countable pairwise disjoint unions:
    Consider any sequence of pairwise disjoint $\{A_n\}\ninf\subseteq
    \sD$.  By definition of $\sD$, it must be that $P[A_n]=Q[A_n]$ for
    each $A_n$. Moreover, since $P$ and $Q$ are probability measures,
    \begin{align*}
      P\left[\bigcup\ninf A_n\right]
      =
      \sumninf P\left[A_n\right]
      =
      \sumninf Q\left[A_n\right]
      =
      Q\left[\bigcup\ninf A_n\right]
    \end{align*}
    Hence, the union $\bigcup\ninf A_n\in \sD$.

  \item Closed under complements: Suppose that $A\in \sD$, then
    $P[A]=Q[A]$ so
    \begin{align*}
      P[A^c] = 1 - P[A]
      = 1-Q[A] = Q[A^c]
    \end{align*}
    Hence, $A^c\in\sD$.
\end{enumerate}
So $\sD$ is a Dynkin system that contains $\sE$. Hence
$\delta(\sE)\subseteq\sD$. So we can modify Statement~\ref{Dsqueeze} to
read instead that
\begin{align}
  \delta(\sE) \subseteq \sD \subseteq \sigma(\sE)
  \label{Dsqueeze2}
\end{align}
Finally, $\sE$ is closed under finite intersections, so
$\delta(\sE)=\sigma(\sE)$ by Monotone Class Theorem~\ref{thm:mct-2}.
Therefore, we modify Statement~\ref{Dsqueeze2} by replacing
$\delta(\sE)$ with $\sigma(\sE)$ to read
\begin{align}
  \sigma(\sE) \subseteq \sD \subseteq \sigma(\sE)
  \quad\implies\quad
  \sD=\sigma(\sE)
  \label{Dsqueeze3}
\end{align}
In other words, by the definition of $\sD$, the sets $A\in \sigma(\sE)$
such that $P[A]=Q[A]$ is \emph{all of the sets} in $\sigma(\sE)$.
\end{proof}

\begin{prop}
\emph{(Extending a Finite, Finitely-Additive Measure on a Semiring)}
Let $\mu$ be a finite, finitely additive measure over semiring $S$ on
$\Omega$ that also contains $\Omega$. Then
\begin{align*}
  \text{$\mu$ is $\sigma$-additive}
  \quad\iff\quad
  \text{$\mu$ continuous from above at $\emptyset$}
\end{align*}
i.e.  for every decreasing sequence of sets
$A_1\supseteq A_2\supseteq A_3\supseteq \cdots$ in $S$ such that
$\bigcap\ninf A_n =\emptyset$, we have $\limn \mu(A_n)=0$.
\end{prop}
\begin{rmk}
Here, $\mu$ is a measure defined on semiring $S$, though we normally
define measures on a $\sigma$-algebra. But just take the usual
properties and apply them to the semiring. Specifically, we should have
$\mu(\emptyset)=0$ and countable additivity for pairwise disjoint sets
in $S$.
\end{rmk}

\begin{thm}\emph{(Caratheodory's Extension Theorem)}
\label{thm:cara}
A measure $\mu$ on a semiring $S$ can be extended to a measure on
$\sigma(S)$. If $\mu$ is $\sigma$-finite, then the extension is unique.
\end{thm}

\begin{ex}
Let $S$ denote the set of all intervals
\begin{align*}
  S = \left\{ (a,b] \cap \R \;|\; a,b\in\bar{\R}\right\}
\end{align*}
This is precisely the set that generates the Borel $\sigma$-algebra.
It's clearly a semiring. For all elements of $S$, define the measure
\begin{align*}
  \mu\big[(a,b]\big] = b-a
\end{align*}
which is the Lebesgue measure. By Caratheodory, this has a unique
extension to $\sigma(S)=\sB(\R)$.
\end{ex}

\begin{defn}(Consistent Family of Probability Measures)
Let $J$ be a nonempty index set and assume that for every finite
sequence of unique elements $\alpha_1,\ldots,\alpha_n,\in J$, there
exists a probability measure $P^{\alpha_1,\ldots,\alpha_n}$ on
measurable space $(\Rn, \sB(\R)^{\otimes n})$. Then this family of
probability mesaures is called \emph{consistent} if for any
$\alpha_1,\ldots,\alpha_n\in J$ and $A_1,\ldots,A_n\in\sB(\R)$ we have
\begin{enumerate}[label=(\roman*)]
  \item For every permutation $\pi: \{1,\ldots,n\}\ra \{1,\ldots,n\}$,
    \begin{align*}
      P^{\alpha_1,\ldots,\alpha_n}[A_1\times \cdots \times A_n]
      =
      P^{\alpha_{\pi(1)},\ldots,\alpha_{\pi(n)}}\big[
          A_{\pi(1)}\times \cdots \times A_{\pi(n)}
      \big]
    \end{align*}
  \item
      $P^{\alpha_1,\ldots,\alpha_{n-1}}[A_1\times \cdots \times A_{n-1}]
      =
      P^{\alpha_1,\ldots,\alpha_{n-1},\alpha_n}[
        A_1\times \cdots \times A_{n-1} \times \R]
      $
\end{enumerate}
\end{defn}

\clearpage
\begin{thm}\emph{(Kolmogorov's Extension Theorem)}
Let $J$ be a nonempty index set and suppose we have a consistent family
of probability measures. Then there exists a \emph{unique} probability
measure $P$ on $(\R^J,\sB(\R)^{\otimes J})$ such that
\begin{align*}
  P\big[
    \omega\in \R^J \; |\; (\omega_{\alpha_1},\ldots,\omega_{\alpha_n})
    \in B
  \big]
  = P^{\alpha_1,\ldots,\alpha_n}[B]
\end{align*}
for every finite sequence $(\alpha_1,\ldots,\alpha_n)$ of elements in
$J$ and $B\in\sB(\R)^{\otimes n}$.
\end{thm}

\begin{cor}
Taking $J=\N$, let $P_1,P_2,\ldots$ be a sequence of probability
measures on
$(\R,\sB(\R)$,
$(\R^2,\sB(\R)^{\otimes 2})$,\ldots
such that
\begin{align*}
  P_{n+1}[B\times \R] = P_n[B]
\end{align*}
for all $n$ and $B\in\sB(\R)^{\otimes n}$. Then there exists a unique
probability measure on $(\R^{\N},\sB(\R)^{\otimes \N})$ such that
\begin{align*}
  P\big[
    \omega\in \R^{\N} \; |\; (\omega_{1},\ldots,\omega_{n})
    \in B
  \big]
  = P_n[B]
\end{align*}
for all $n$ and for all measurable sets $B\in\sB(\R)^{\otimes n}$.
\end{cor}

\begin{ex}
There exists a unique probability measure on
$(\R^{\N},\sB(\R)^{\otimes \N})$ such that
\begin{align*}
  P\big[
    \omega\in \R^{\N} \; |\;
    \omega_1 = j_1, \omega_2=j_2,\ldots,\omega_n=j_n
  \big]
  = \frac{1}{2^n}
\end{align*}
for all $n\geq 1$ and for every sequence $(j_1,\ldots,j_n)\in\{0,1\}^n$.
\end{ex}


\clearpage
\section{Measurable Functions}

\subsection{General Measurable Functions}

We now define the important notion of measurable functions, the class of
functions for which we will define the Lebesgue integral. Note that they
\emph{only} require a measurable space to be defined. We don't need a
measure unless we want to do integration, so I'll often leave it out in
the definitions below to work with measurable spaces (rather than
measure spaces).


\begin{defn}(Measurable Function)
A function $f:(\Omega,\sF)\ra (E,\sE)$ between two measurable space
is called a \emph{measurable function} if and only if
\begin{align*}
  f^{-1}(A) \in \sF \qquad \forall A\in \sE
\end{align*}
In words, the preimage of a measurable set of $(E,\sE)$ is always a
measurable set of $(\Omega,\sF)$.\footnote{%
  It shouldn't be surprising that the definition of ``nice'' functions
  in measure theory stems from preimages (as is the case for
  continuity), since preimages are generally nice to work with. For
  example, $f^{-1}\left( \cup A_n \right) = \cup f^{-1}(A_n)$ and
  $f^{-1}(A\cap B) = f^{-1}(A)\cap f^{-1}(B)$ and $f^{-1}(A^c)=
  [f^{-1}(A)]^c$
}
Measurable functions are the structure-preserving entities in Measury
Theory, similar to isomorphisms in linear algebra or homeomorphisms in
Topology.

Mesurable functions are important for probability, since random
variables are defined as (extended) real-valued measurable functions on
a probability space $(\Omega,\sF,P)$, as we'll see below.
\end{defn}

\begin{ex}
Measurability depends crucially on the $\sigma$-algebras involved. For
example, suppose we take for the domain the measure space
$(\Omega,2^\Omega)$, where the $\sigma$-algebra is the power set. Then
\emph{any} function $f:(\Omega,2^\Omega)\ra (E,\sE)$ with any range
$\sigma$-algebra $\sE$ will be measurable. However, if we restirct the
domain $\sigma$-algebra from $2^\Omega$ to a proper subset
$\sF \subset 2^\Omega$, this will clearly not be true in general.
\end{ex}

%Suppose we have a measurable space $(\Omega,\calF)$ and a function
%$f: \Omega\rightarrow X$. The most natural way to characterize the
%function is to take all $\omega$ in $\Omega$ and specify a rule for
%assigning $\omega$ to $f(\omega)\in X$.

%But you can flip that logic around to construct another equally valid
%characterization of the function $f$ via its range.  Namely, step over
%the elements $x\in X$ and define the disjoint collection $\{E_x\}_{x\in
%X}$ where $E_x = f^{-1}\left(x\right)$. In this way, we characterize the
%function $f$ \emph{and} partition the space $\Omega$ based on what $f$
%maps different subsets of $\Omega$ to.  Some of the sets $E_x$ will be
%measurable (i.e.\ they will be in $\calF$) while others will not.

%Moreover, this will be how we characterize the Lebesgue integral later
%on where we think about partitioning based on the range, not the domain
%(as in the Riemann approach).

\begin{prop}
\label{prop:meascomp}
Let $f:(\Omega,\sF)\ra(E,\sE)$ and $g:(E,\sE)\ra (G,\sG)$ be two
measurable functions. Then
\begin{align*}
  g\circ f: (\Omega,\sF) \ra (G,\sG)
\end{align*}
is a measurable function.
\end{prop}

\begin{defn}($\sigma$-algebra Generated by $f$)
Given $f:\Omega\ra (E,\sE)$ for $\sigma$-algebra $\sE$, define
\begin{align*}
  \sigma(f):=\left\{ f^{-1}(A) \; |\; A \in\sE \right\}
\end{align*}
which is called the \emph{$\sigma$-algebra generated by $f$}. The next
proposition verifies this is a valid $\sigma$-algebra.
\end{defn}

\begin{prop}
Given function $f:\Omega\ra (E,\sE)$, the collection $\sigma(f)$ is a
valid $\sigma$-algebra, and it is the smallest $\sigma$-algebra such
that $f:(\Omega,\sigma(f))\ra (E,\sE)$ is measurable.
\end{prop}
\begin{proof}
Since $\sE$ is a $\sigma$-algebra, we have $\sE=\sigma(\sE)$ so that
\begin{align*}
  \left\{ f^{-1}(A) \;|\; A\in\sigma\left( \sE \right)\right\}
  =
  \left\{ f^{-1}(A) \;|\; A\in \sE \right\}
\end{align*}
Then by Proposition~\ref{prop:gensigal}, we get that
\begin{align*}
  \left\{ f^{-1}(A) \;|\; A\in \sE \right\}
  =
  \sigma\left( \left\{ f^{-1}(A) \;|\; A\in\sE \right\} \right)
\end{align*}
Since the RHS is clearly a $\sigma$-algebra, the LHS must be one as
well. Moreover, by the definition of the generated $\sigma$-algebra
$\sigma(\cdot)$, it is indeed the smallest one such that $f$ is
measurable.
\end{proof}

\begin{prop}
Given function $f:(\Omega,\sF)\ra E$, the set
\begin{align*}
  \hat{\sigma}(f)
  :=
  \left\{ A\subseteq E \; |\; f^{-1}(A)\in\sF \right\}
\end{align*}
is the \emph{finest} $\sigma$-algebra such that
$f:(\Omega,\sF)\ra (E,\hat{\sigma}(f))$ is measurable.
\end{prop}

\clearpage
\subsection{%
  (Extended) Real-Valued Measurable Functions and Random Variables
}

The previous subsection defined measurable functions and properties of
measurable functions for functions $f:(\Omega,\sF)\ra(E,\sE)$ between
two arbitrary measurable spaces $(\Omega,\sF)$ and $(E,\sE)$. In this
section, we restrict our attention to real-valued
$f:(\Omega,\sF)\ra(\R,\sR(\R))$ or extended real-valued functions
$f:(\Omega,\sF)\ra(\bar{\R},\sB(\bar{\R}))$.

All results of the previous subsection hold, but we can say even more if
we restrict our attention to this class of functions. Moreover, this
class of functions is especially important for probability theory,
as we see in the next two definitions, which give a special name to
(extended) real-valued measurable functions.

\begin{defn}(Random Variable)
A measurable function $X:(\Omega,\sF)\ra (\R,\sB(\R))$ is called a
\emph{random variable}.
\end{defn}

\begin{defn}(Extended Random Variable)
A measurable function $X:(\Omega,\sF)\ra (\bar{\R},\sB(\bar{\R}))$ is
called an \emph{extended random variable}.
\end{defn}

\begin{defn}(Random Vector)
A \emph{random vector} on a probability space $(\Omega,\sF,P)$ is a
vector $(X_1 \; \cdots \; X_d)$ where $X_1,\ldots,X_d$ are all random
variables. In other words, it is a measurable mapping
$X:(\Omega,\sF)\ra(\R^d,\sB(\R)^{\otimes d})$.
\end{defn}

We'll have more to say about random variables and their distributions
later. But now, we just want to prove some general results about
(extended) random variables, a.k.a. (extended) real-valued measurable
functions. And before doing so, we just recall the standard shorthand
notation that will be used extensively in this subsection and in later
subsections dealing with random variables:
\begin{align*}
  \{f\leq x\} = \{\omega\in\Omega\;|\;f(\omega)\leq x\}
  = f^{-1}\big((-\infty,x]\big)
\end{align*}
and similar constructs $\{f<x\}$, $\{f=x\}$, $\{f\geq x\}$, etc.

\begin{prop}
\label{prop:measurableiff}
\emph{(Necessary and Sufficient Condition for Measurability)}
The real-valued function $f:(\Omega,\sF)\ra\R$ is measurable if and only
if
\begin{align}
  \{f\leq x\}
  \in \sF
  \qquad \forall x\in\R
  \label{measurableiff}
\end{align}
The same is true is for \emph{extended} real-valued functions, except
Statement~\ref{measurableiff} must hold for any
$x\in\bar{\R}=\R\cup\{\pm\infty\}$.
\end{prop}
\begin{rmk}
This provides a really easy and convenient way to check measurability of
(extended) real-valued functions. Note also that we could have used
$\{f<x\}$, $\{f\geq x\}$, or $\{f>x\}$ in the proposition instead.
\end{rmk}

\begin{cor}\emph{(Continuous Functions are Measurable)}
Suppose that $f:(\R,\sB(\R))\ra\R$ is a continuous function. Then $f$ is
also measurable.
\end{cor}
\begin{proof}
Since $(-\infty,x)$ is an open set, by the definition of continuity, we
know that $\{f<x\}$ will be open hence $\{f<x\}\in\sB(\R)$. Then by
Proposition~\ref{prop:measurableiff} and the fact that we could have
used $\{f<x\}$ instead of $\{f\leq x\}$, the function $f$ is measurable.
\end{proof}

\begin{prop}
Given two real-valued measurable functions $f,g:(\Omega,\sF)\ra\R$,
the following functions are also measurable:
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $f+g$
  \item $f-g$
  \item $f\cdot g$
  \item $f/g$, provided $g(\omega)\neq 0$ for all $\omega\in\Omega$
\end{enumerate}
\end{prop}
\begin{rmk}
Together, (i) and (ii) imply that the class of measurable real-valued
functions forms a vector space.
\end{rmk}
\begin{proof}
We prove each in turn
\begin{enumerate}[label=(\roman*)]
  \item
    By Proposition~\ref{prop:measurableiff}, we can prove the result by
    showing that
    \begin{align*}
      \{f+g> x\} \in\sF
    \end{align*}
    To start, suppose you give me $\omega\in \{f+g>x\}$. Then obviously
    \begin{align*}
      f(\omega) + g(\omega) > x
      \quad\implies\quad
      f(\omega) > x  - g(\omega)
    \end{align*}
    By the density of $\Q$ in $\R$, there then exists a $q\in \Q$ such
    that
    \begin{align}
      f(\omega) > q > x  - g(\omega)
      \quad\iff\quad
      \begin{cases}
        f(\omega) > q \\
        g(\omega) > x  - q
      \end{cases}
      \label{qexists}
    \end{align}
    This suggests that we can write
    \begin{align}
      \{f+g> x\}
      &=
      \bigcup_{q\in \Q}
      \{
        f > q,\;
        g > x-q
      \}
      \label{asunion}
    \end{align}
    To prove this claim rigorously, we will show each side of
    Equation~\ref{asunion} is a subset of the other:
    \begin{itemize}
      \item LHS $\subseteq$ RHS:
        Take any $\omega\in \{f+g>x\}$.  We argued above that there's
        going to be a $q$ such that Statement~\ref{qexists} holds for
        that $\omega$. So the RHS of Equation~\ref{asunion} (as the
        union over \emph{all} $q\in\Q$) will definitely pick out that
        $\omega$, implying LHS $\subseteq$ RHS.

      \item LHS $\supseteq$ RHS:
        Choose $\omega$ in the RHS of Equation~\ref{asunion}. Then
        there's some associated $q$ such that
        \begin{align*}
          \begin{rcases}
          f(\omega) &> q \\
          g(\omega) &> x-q
          \end{rcases}
          \quad\implies\quad&
          f(\omega) > q > x-g(\omega) \\
          \quad\iff\quad&
          f(\omega) + g(\omega) > q + g(\omega)> x
        \end{align*}
        Hence $f(\omega)+g(\omega) > x$ implying LHS $\supseteq$ RHS.
    \end{itemize}
    Okay, now we rewrite Equation~\ref{asunion}:
    \begin{align*}
      \{f+g> x\}
      &=
      \bigcup_{q\in \Q}
      \{
        f > q,\;
        g > x-q
      \} \\
      &=
      \bigcup_{q\in \Q}
      \{ f > q\}
      \cap
      \{ g > x-q \}
    \end{align*}
    But since both $f$ and $g$ are $\sF$-measurable, the sets $\{f>q\}$
    and $\{g>x-q\}$ will be in $\sF$ for any $x,q$. Therefore, each
    intersection in the above union will be in $\sF$ for fixed $x,q$,
    and so too will the countable union of those intersections be in
    $\sF$ over all $q\in\Q$.

  \item Trivial
  \item Write $f\cdot g$ as
    \begin{align*}
      f\cdot g
      =
      \frac{1}{2}
      \left[
      (f+g)^2 - f^2 - g^2
      \right]
    \end{align*}
    Proposition~\ref{prop:meascomp}, (i), and (ii) give that
    $\varphi\circ h$ is measurable for continuous (hence measurable)
    $\varphi(x)=x^2$ and measurable $f+g$. Invoking (i) and (ii) again,
    we get that $f\cdot g$ is measurable.
\end{enumerate}
\end{proof}

\begin{prop}\emph{(Pointwise Sup and Inf)}
Given a countable sequence $\{f_n\}\ninf$ of measurable, extended
real-valued functions $f_n:(\Omega,\sF)\ra\bar{\R}$, both the pointwise
sup and inf
\begin{align*}
  g(\omega) = \sup_n f_n(\omega)
  \qquad
  h(\omega) = \inf_n f_n(\omega)
\end{align*}
are measurable extended real-valued functions.
\end{prop}
\begin{proof}
Note that we can write
\begin{align}
  \{g \leq x \}
  &= \bigcap\ninf \{f_n\leq x\}
  \label{gintersection} \\
  \{h \geq x \}
  &= \bigcap\ninf \{f_n\geq x\}
  \notag
\end{align}
To show that these statements are valid, we show that each side is a
subset of the other for $g$. (The proof for $h$ is analogous.)
\begin{itemize}
  \item LHS $\subseteq$ RHS: Take any $\omega\in\{g\leq x\}$. Then by
    the definition of $g$ as the sup, we know that
    $f_n(\omega)\leq g(\omega)\leq x$ for all $n$.
    Hence $\omega\in \cap\ninf \{f_n \leq x\}$.
  \item LHS $\supseteq$ RHS:
    Take any $\omega\in \cap\ninf \{f_n\leq x\}$. Suppose by
    contradiction that
    \begin{align*}
      \omega\not\in \{g\leq x\}
      \quad&\iff\quad
      \omega\in \{g\leq x\}^c
      \quad\iff\quad
      \omega\in \{g > x\}
    \end{align*}
    But then by the definition of the sup, there would exist an integer
    $m$ such that
    \begin{align*}
      f_m(\omega)\in(x,g(\omega)]
      \quad\implies\quad
      f_m(\omega)>x
      \quad\implies\quad
      \omega\not\in \bigcap\ninf \{f_n\leq x\}
    \end{align*}
    which is a contradiction.
\end{itemize}
That shows that Statement~\ref{gintersection} and its counterpart for
$h$ are both true. Now we just have to show that $\{g\leq x\}$ is indeed
measurable. But that's easy.  Since all of the $f_n$ are measurable,
each term in the RHS intersection of Statement~\ref{gintersection} will
be in $\sF$, so that the countable intersection will in $\sF$ as well.
Hence $\{g\leq x\}$ is in in $\sF$, and same for $h$, implying $g$ and
$h$ are measurable by Proposition~\ref{prop:measurableiff}.
\end{proof}

\begin{cor}\emph{(Pointwise limsup and liminf)}
Let $\{f_n\}\ninf$ be a sequence of measurable, extended real-valued
functions on $(\Omega,\sF)$. Then the pointwise limsup $g$ and liminf
$h$
\begin{alignat*}{3}
  g(\omega) =
  \limsup_n f_n(\omega)
  &= \limn \bigg(\sup_{m\geq n} f_m(\omega) \bigg)
  &&=
    \inf_n\bigg(\sup_{m\geq n} f_m(\omega)\bigg) \\
  h(\omega) =
  \liminf_n f_n(\omega)
  &= \limn \bigg(\inf_{m\geq n} f_m(\omega) \bigg)
  &&= \sup_n\bigg(\inf_{m\geq n} f_m(\omega)\bigg)
\end{alignat*}
are both measurable functions.
\end{cor}

\begin{cor}\emph{(Pointwise Limit)}
Let $\{f_n\}\ninf$ be a sequence of measurable, extended real-valued
functions on $(\Omega,\sF)$. Then if the pointwise limit
\begin{align*}
  f(\omega) = \limn f_n(\omega)
\end{align*}
exists as an extended real-valued function (so possibly infinite at
points), then $f$ is measurable.
Moreover, for those $\omega\in\Omega$ such that the limit exists, we
have
\begin{align*}
  \liminf_n f_n(\omega)
  &=
  \limsup_n f_n(\omega)
\end{align*}
\end{cor}
\begin{rmk}
This last corollary, stating that measurability is preserved under
pointwise limits, is realy nice. Recall that continuity wasn't preserved
under pointwise limits (only uniform convergence). This will turn out to
be really useful in integration of measurable functions, where we work
with pointwise limits all of the time.
\end{rmk}


\clearpage
\subsection{Simple Functions}

Next, we come to the topic of simple functions. Simple functions are
(get ready for it) the simplest class of measurable functions. They are
extremely easy to characterize, and they will be the building block of
the Lebesgue Integral in a later section. We first define simple
functions, and then show you can approximate \emph{any}
extended-real-valued function $f$ with a sequence of simple functions
that converges pointwise to $f$.

\begin{defn}(Simple Function)
A function $f$ on measurable space $(\Omega,\sF)$ is called a
\emph{simple function} if it can be written
\begin{equation}
  \label{simplefn}
  f(\omega) = \sum^N_{n=1} c_n \cdot \one{A_n}(\omega)
  %\quad
  %\text{ where } A_i = f^{-1}(c_i)
\end{equation}
for $A_1, \ldots, A_N$ measurable sets in $\sF$. Simple functions
obviously have a finite range. Without loss of generality, it can be
assumed that the $A_n$ are pairwise disjoint.\footnote{%
  Otherwise, since $\sF$ is a $\sigma$-algebra, you can always redefine
  the simple function on pairwise disjoint sets.
}
Thus, $A_1,\ldots,A_N$ forms a \emph{measurable partition} of $\Omega$
based on the range of $f$, which takes on only a finite set of values,
$\{c_1, \ldots, c_N\}$. The case where the $A_n$ are disjoint is called
the \emph{standard representation}, and can also be expressed as:
\begin{align*}
  f
  %= \sum^N_{n=1} c_n \cdot \Chi_{A_n}
  = \sum^N_{n=1} c_n \cdot \one{\left\{f=c_n \right\}}
\end{align*}
\end{defn}
\begin{rmk} Simple functions form a vector space.
\end{rmk}

\begin{thm}\emph{(Simple Function Approximation)}
\label{lebapprox}
Suppose that $f: \Omega\rightarrow \bar{\R}_+$ is a nonnegative
measurable function on measurable space $(\Omega,\sF)$. Then there
exists a sequence $\{s_n\}\ninf$ of simple functions
\begin{align*}
    0 \leq s_1 \leq s_2 \leq \cdots \leq f
    \qquad \forall \omega\in \Omega
\end{align*}
such that $s_n\rightarrow f$ pointwise from below.
\end{thm}
\begin{proof}
We prove by successive approximation, partitioning the range.
%In particular, we will break up the range $\bar{\R}_+=[0,\infty]$ as
%follows:
%\begin{figure}[htpb!]
%\centering
%\begin{tikzpicture}[]
  %% n=1
  %\draw[color=d4black, -] (0,0) -- (1,0);
  %\draw[color=d4black, ->] (1,0) -- (5,0);
%\end{tikzpicture}
%\end{figure}
We partition the range $[0,\infty]$ and then construct our approximating
functions as follows:
\begin{align*}
      s_1 := &\; 0\cdot \one{\{f\in [0,1)\}}
        + 1 \cdot \one{\{f\in [1,\infty]\}} \\
      s_2 :=&\; 0\cdot \one{\{f\in [0,1/2)\}}
        + \frac{1}{2} \cdot \one{\{f\in [1/2,1)\}}
        + 1 \cdot \one{\{f\in [1,3/2)\}}
          + \frac{3}{2} \cdot \one{\{f\in [3/2,2)\}}
        + 2 \cdot \one{\{f\in [2,\infty]\}} \\
      \vdots \quad & \qquad \vdots
\end{align*}
So what are we doing here? Well, in the case of $s_1$, we approximate
$f$ at each $\omega\in\Omega$ such that $f(\omega)\in [0,1)$ by
zero---the minimum value that $f$ takes on for those $\omega$.
Similarly, we approximate $f$ at all $\omega\in \Omega$ such that
$f(\omega)\in [1,\infty]$ by the value one---again, the minimum value
$f$ takes on for those $\omega$.

And so we see the basic logic here. Each $s_n$ splits the range of $f$,
$[0,\infty]$, into two parts.
\begin{itemize}
  \item $[0,n]$: This part is very finely divided, where each interval
    $[k,k+1)$ for $k=0,\ldots,n-1$ is broken up into $2^{n-1}$ parts.
    This implies a total of $n\cdot 2^{n-1}$ subintervals for
    $[0,n]$.
  \item $[n,\infty]$: This part is obviously coarsly divided. We just
    take $[n,\infty]$ as the single subinterval.
\end{itemize}
Therefore, $s_n$ divides $[0,\infty]$ into $n\cdot 2^{n-1}+1$ total
subintervals. Since $f$ is measurable, all of the preimages of these
subintervals are measurable, implying $s_n$ is a well defined simple
function for whatever values we assign to it. What are those values?
Again, the value of $s_n$ over any one of those subintervals is set to
be the value of $f$ at the left endpoint of that interval.

And so we work out the process we saw above, first extending the finely
approximated part of $f$'s range by one integer span with each $n$, but
then halving the size of those fine subdivisions. We also continue to
approximate $f$ with the minimum value, giving us
\begin{align}
  s_n = \left(
  \sum^{n\cdot 2^{n-1}}_{k=0}\frac{k}{2^{n}}
  \one{%
    \left\{
      f\in \left[\frac{k}{2^{n}}, \; \frac{k+1}{2^{n}}\right)
    \right\}
  }
  \right)
  + n \cdot \one{\{f\geq n\}}
  \qquad n=1,2,\ldots
  \label{simpleapprox}
\end{align}
Now it's clear that $s_n$ approaches $f$ from below, since with each
$s_n$, we're taking the \emph{smallest possible} approximation of $f$.
So we can never have $s_n>f$ for any $f$. It's also clear that $s_n\leq
s_m$ for $m>n$ for the same reasons that a finer partition can only
increase the value of of the approximation at an $\omega\in\Omega$.

Finally, we just wish to show that we, in fact, do have
$s_n(\omega)\rightarrow f(\omega)$ for all $\omega\in \Omega$. Let's
take it case by case.
\begin{enumerate}[label=(\roman*)]
  \item Suppose that $f(\omega)=\infty$. Since $\omega\in \{f\geq n\}$
    for all $n$, we know that
    \begin{align*}
      \lim_{n\rightarrow\infty} s_n(\omega)
      = \lim_{n\rightarrow\infty}
      n\cdot\one{\{f\geq n\}}
      = \lim_{n\rightarrow\infty} n = \infty = f(\omega)
    \end{align*}
  \item Next, suppose that $f(\omega) = 0$. Then
    $\omega\in \big\{f \in [0,1/2^{n-1})\big\}$ for all $n$, so
    \begin{align*}
      \lim_{n\rightarrow\infty} s_n(\omega)
      = \lim_{n\rightarrow\infty}
      \frac{0}{2^{n-1}}\cdot\one{\{f\in[0,1/2^{n-1}]\}} =
      \lim_{n\rightarrow\infty} 0 = 0 = f(\omega)
    \end{align*}
  \item Finally, suppose $f(\omega)\in(0,\infty)$. Then, $N>f(\omega)$
    for some $N$. That implies
    \begin{align*}
      f(\omega) \in \left[\frac{k}{2^{N-1}}, \frac{k+1}{2^{N-1}}\right)
          \qquad
      \text{for some $k\in\{0,1,\ldots,N\cdot 2^{N-1}\}$}
    \end{align*}
    This, in turn, implies that
    \begin{align*}
      f(\omega) - s_N \leq \frac{1}{2^{N-1}}
      \quad \Rightarrow\quad
      \lim_{n\rightarrow\infty}  s_n  =
      f(\omega)
    \end{align*}
\end{enumerate}
Hence, for sequence $\{s_n\}\ninf$ defined by
Equation~\ref{simpleapprox}, we have $s_n\ra f$ pointwise.
\end{proof}

\clearpage
\section{General Theory of Integration}

\subsection{Intuition}

What we did in the proof of Theorem \ref{lebapprox} is the \emph{key} to
our general theory of integration.  And it all has to do with how we
partitioned the domain $\Omega$.

Namely, we partitioned $\Omega$ in almost
\emph{the complete opposite way} compared to what we did for Riemann
Integrals. Instead of imposing that our partitions of $\Omega$ will be
intervals, we partitioned $\Omega$ by what the points $\omega\in \Omega$
\emph{get mapped to} by $f$. The sets of this partition could be
crazy---things way more complicated that an interval
$[\omega_{i-1}, \omega_i]$, because the $\omega\in f^{-1}([a,b))$ don't even
have to be near each other.  And that's key.

So instead of starting with a partition of $\Omega$ into intervals and
studying how a function $f$ behaves on those intervals, we look at the
range of values $f$ can take, then pull it back to $\Omega$.

\paragraph{Metaphor}
A wise man, Sam Kapon, in quoting Lebesgue himself, described the
situation in the following way. Your coffee is rung up for \$1.74.
Riemann starts taking successive coins out of his pocket one by one
until he has enough to pay. He does so blindly, without any reference to
their value (their $f(\omega)$) and to the fact that one coin could represent
different amounts.

On the other hand, Mr. Lebesgue first takes all of the coins out of his
pocket and surveys their value (their $f(\omega)$) before putting down a
series of coins that makes sense \emph{given} their value. He pays in a
way that makes a distinction between quarters, dimes, and nickels rather
than lumping them all together simply as coins.

\paragraph{Notation and Terminology}
In this section, we develop a general integral for extended real-valued
functions on measure space $(\Omega,\sF,\mu)$. If the measure $\mu$ is
chosen to be the Lebesgue measure, then the integral is a particular
case, generally referred to as the \emph{Lebesgue integral}. However, we
can always use a different measure on measurable space $(\Omega,\sF)$,
in which case the resulting integral will be referred to simply as a
``general integral'' or just ``integral.'' Common notation that you
might see for this general integral includes
\begin{align*}
  \int_\Omega f \; d\mu
  \qquad
  \int_\Omega f(\omega) \; \mu(d\omega)
\end{align*}
The first is the most compact and preferred. The second makes the
variable of integration explicit and highlights that we weight
increments $dx$ by the measure $\mu$. Both are different notation for
the same object. The first is most preferred, though the second might
sometimes be used to highlight the argument of the function being
integrated.

\clearpage
\subsection{Definition of the Integral}

\begin{defn}(Integral of Simple Function)
\label{simpleint}
Let $s:\Omega\ra\R$ be a simple measurable function on the measure space
$(\Omega,\sF,\mu)$. Then define the integral of $s$ over $\Omega$ with
respect to $\mu$ as
\begin{equation}
    \int_\Omega s \; d\mu
    =
    \int_\Omega \sumnN c_n \Chi_{A_n} \; d\mu
    := \sumnN c_n \; \mu(A_n)
\end{equation}
where, as in our definition of a simple function,  $A_n = s^{-1}(c_n)$.
\end{defn}
\begin{rmk}
I also want to mention that this definition of the integral does,
indeed, make a lot sense. We see that this definition prescribes that we
average $s$ over $\Omega$.  It says simply to weight the values of the
function by, effectively, how many points of the domain map there (i.e.
``the measure of their preimage'').This is similar to what we did for
the Riemann integral, only now, our $A_n$ partitioning the domain can be
more arbitrary measurable sets, rather than more restrictive intervals.
\end{rmk}

\begin{defn}(Integral of Nonnegative Measurable Function)
Consider the measure space $(\Omega,\sF,\mu)$, and let $f\geq0$ be a
nonnegative, measurable, extended-real-valued function
$f:\Omega\ra\bar{\R}$. Then define
\begin{align*}
    \int_\Omega f \; d\mu := \sup_{0\leq s\leq f} \int_\Omega s \; d\mu
\end{align*}
where $s$ is any such simple function between zero and $f$, and the
rightand side integral is defined as in Definition~\ref{simpleint}. Note
that the integral is \emph{only} defined for measurable functions.
\end{defn}

\begin{defn}(Integral of Arbitrary Measurable Function)
Consider the measure space $(\Omega,\sF,\mu)$, and let $f$ be an
arbitrary measurable, extended real valued function. Define
\begin{align*}
    f_+(\omega) := \max\{f(\omega), 0\}
    \qquad
    f_-(\omega) := -\min\{f(\omega), 0\}
\end{align*}
where the max and min are both pointwise.
Because both $f$ and $0$ are measurable, these functions that equal the
pointwise max and min will \emph{also} be measurable. Thus $f_+$ and
$f_-$ are measurable. Also note, by our definition,
\begin{itemize}
  \item $f = (f_+) - (f_-)$ and $|f| = (f_+) + (f_-)$
  \item $f_+$ and $f_-$ will be non-negative \emph{everywhere}.
\end{itemize}
This allows us to define the integral for arbitrary measurable function
$f$ as
\begin{align*}
  \int_\Omega f\; d\mu := \int_\Omega f_+ \; d\mu - \int_\Omega f_- \; d\mu
\end{align*}
provided that at least one of the integrals on the RHS is
finite---otherwise the integral is undefined.
\end{defn}

\begin{defn}(The Space of Integrable Functions)
We say that measurable function $f$ is \emph{integrable}
on $\Omega$ with respect to $\mu$, denoted
$f\in\mathscr{L}(\Omega,\sF,\mu)$, if
\begin{align*}
  \int_\Omega |f| \; d\mu < \infty
  \quad\iff\quad
  \text{both} \;
  \begin{cases}
    \int_\Omega f_+ \; d\mu < \infty \\
    \int_\Omega f_- \; d\mu < \infty
  \end{cases}
\end{align*}
\end{defn}

\begin{thm}
$f\in\mathscr{L}(X,\mathscr{M},\mu)$ if and only if $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$.
\end{thm}
\begin{note}
This is an important theorem, since it means we that we can prove a function $f$ is in $\mathscr{L}(X,\mathscr{M},\mu)$ just by proving that $|f|$ is, which is often \emph{much}, much easier to work with since it is non-negative, allowing us to work with our integration primitive: simple functions. So we can approximate arbitrary integrals of $|f|$ with integrals of simple functions, then assert that integrability (or non-integrability) of $|f|$ holds for $f$ too.
\end{note}
\begin{proof}
($\Rightarrow$ Direction) Suppose that $f\in\mathscr{L}(X,\mathscr{M},\mu)$. Then $f_-, f_+\in\mathscr{L}(X,\mathscr{M},\mu)$. Since $|f|=f_+ + f_-$, we can conlcude that $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$ as well.
\\
\\
($\Leftarrow$ Direction) Suppose that $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$. Then we know from the fact $|f|=f_+ + f_-$ and from monotonicity that
\[
    f_-, f_+ \leq |f| \qquad \Rightarrow
    \qquad
    \int_X f_- \; d\mu \leq \int_X f \; d\mu <\infty\qquad
    \int_X f_+ \; d\mu \leq \int_X f \; d\mu <\infty
\]
which implies that $f_-, f_+\in\mathscr{L}(X,\mathscr{M},\mu)$, so that $f\in\mathscr{L}(X,\mathscr{M},\mu)$.
\end{proof}

\begin{defn}(Integral over Part of the Domain)
For measurable set $A\subseteq \Omega$, define
\begin{align*}
  \int_{A} f\; d\mu
  :=
  \int_{\Omega} f\cdot\Chi_A\; d\mu
\end{align*}
\end{defn}

\begin{ex}
Next, suppose that we have the measure space
$(\mathbb{N},\mathscr{P}(\mathbb{N}), \mu)$, where $\mu$ is the counting
measure. Then any function $f: \mathbb{N}\rightarrow[0,\infty]$  will be
measurable, and functions are simply sequences, where we use
$f=\{f(n)\}_1^\infty$ to denote any arbitrary function on
$[0,\infty]$.\footnote{Recall that a sequence is a function from the
natural numbers to some set ($[0,\infty]\subset\bar{\mathbb{R}}$, in
this case).} I claim that the value of the integral can be written
\begin{equation}
  \int_\Omega f \; d\mu = \sum^\infty_{n=1}f(n)
  \label{ex4.8.3.1}
\end{equation}
\begin{proof}
To prove this, we now consider a few cases for the integral of $f$:
\begin{enumerate}
  \item {\sl $f$ simple}: Start by unpacking the definition of the
    integral,
    \begin{equation}
      \label{ex4.8.3.2}
      \int_\Omega f \; d\mu := \sum^n_{i=1} c_i \; \mu(A_i)
    \end{equation}
    Now we consider whether Equation \ref{ex4.8.3.2} does indeed equal
    Equation \ref{ex4.8.3.1} given all the possible values
    \ref{ex4.8.3.2} could take on:
    \begin{enumerate}
      \item Suppose that Equation \ref{ex4.8.3.2} equals $\infty$. Then
        either there's a $c_i=\infty$ (in which case Equation
        \ref{ex4.8.3.1} would clearly be infinite as well) or there
        exists a $0< c_i<\infty$ such that $f = c_i$ at infinitely many
        points. Then $f(n)=c_i$ for infinitely many $n$, which implies
        that Equation \ref{ex4.8.3.1} equals $\infty$ as well.

      \item Suppose that Equation \ref{ex4.8.3.2} is less than $\infty$.
        Then by the same reasoning as we had before, $c_i<\infty$ for
        all $i$, and there must be only finitely many non-zero terms. In
        that case, we must also have an $N$ such that $f(n)=0$ for all
        $n>N$, in which case Equation \ref{ex4.8.3.1} equals Equation
        \ref{ex4.8.3.2}.
    \end{enumerate}

  \item {\sl $f$ is any non-negative function on $\mathbb{N}$}: First,
    assume that $f$ is finite valued.\footnote{In the case where it is
      \emph{not} finite valued, the statement is trivially true.}  Then
      let $s_N = f\cdot\chi_{[1,N]}$. Because we're restricting the
      non-zero values of the function $s_N$ to a finite domain, it will
      have finite range too: $\{f(n)\}$ for $n=\{1,\ldots,N\}$, and 0
      for $n>N$.

      So clearly, $s_N$ is a simple function such that $0\leq s_N \leq
      f$ for all $N$, since we're just considering non-negative
      functions.  That means
      \begin{equation}
          \label{4.8.3tosub}
          \int_\mathbb{N} s_N \;d\mu \leq
          \int_\mathbb{N} f \;d\mu
      \end{equation}
      since the latter is the sup over all simple functions with $0\leq
      s\leq f$. But we can simplify the integral of $s_N$, using the
      fact that Equation \ref{ex4.8.3.1} is true for simple functions
      like $s_N$ (which we proved in Case 1 of this exercise):
      \begin{align*}
        \int_\mathbb{N} s_N \;d\mu = \sum^\infty_{n=1}s_N(n)
        = \sum^N_{n=1} f(n)
      \end{align*}
      Substituting this expression into Equation \ref{4.8.3tosub}, and
      taking the limit, we get that
      \begin{equation}
          \label{toreverse}
          \sum^\infty_{n=1} f(n) \leq \int_\mathbb{N} f\; d\mu
      \end{equation}
      Note that the righthand side is unaffected by the limit; it is
      independent of $N$.

      Next, we need to show that we can reverse the inequality in
      Equation \ref{toreverse} to show equality as in Equation
      \ref{ex4.8.3.1}. To do so, suppose that $\sum^\infty_{n=1}
      f(n)<\infty$, otherwise there is nothing to prove. Then consider a
      simple function $s$ such that $0\leq s \leq f$. Then it's clear
      that
      \begin{align*}
        \sum^\infty_{n=1} s(n)<\infty
      \end{align*}
      It's also clear that, since $s$ is simple, we can write it as
      \begin{align*}
        \int_\mathbb{N} s \; d\mu = \sum^\infty_{n=1} s(n)
        \leq \sum^\infty_{n=1} f(n)
      \end{align*}
      given the work we did above for the case where $f$ is simple.
      Finally, note that since $s$ was an arbitrary simple function,
      since the integral of $f$ involves taking the sup over all simple
      functions, and since $\sum^\infty_{n=1}f(n)$ does not depend upon
      $s$, we can assert that
      \begin{align*}
        \int_\mathbb{N} f \; d\mu \leq \sum^\infty_{n=1} f(n)
      \end{align*}
      which, together with Equation \ref{toreverse} establishes Equation
      \ref{ex4.8.3.1} for arbitrary non-negative functions on
      $(\mathbb{N}, \mathscr{P}(\mathbb{N}), \mu)$.
\end{enumerate}
\end{proof}
\end{ex}

\begin{ex}
If we take $(\mathbb{N}, \mathscr{P}(\mathbb{N}), \mu)$ where $\mu$ is
the counting measure, then $f$ is integrable if and only if
$\sum^\infty_{n=1}|f(n)|<\infty$, as $|f| = f_+ + f_-$.
\end{ex}

\clearpage
\subsection{Properties of the Integral}

\begin{thm}\emph{(Homogoneity)}
Given measurable function $f$,
\begin{align*}
  f\in\mathscr{L}(\Omega,\sF,\mu)
  \quad\iff\quad
  cf\in\mathscr{L}(\Omega,\sF,\mu)
  \qquad\forall c\in\mathbb{R}
\end{align*}
Moreover,
\begin{align*}
  \int_\Omega cf\;d\mu = c\int_\Omega f\;d\mu
\end{align*}
\end{thm}
\begin{proof}
If $s$ is a simple function, then $cs$ is also a simple function. And so
we can write that $\int_\Omega cf \; d\mu = c\int_\Omega f \; d\mu$.
\end{proof}

\begin{prop}
Given measurable function $f\in\mathscr{L}(\Omega,\sF,\mu)$
\begin{align*}
  \int_\Omega |f| \; d\mu = 0
  \quad\iff\quad
  \mu[f\neq 0] = 0
\end{align*}
\end{prop}
%\begin{proof}
%If $f=0$ $\mu$-a.e., then it must be that $f_-$ and $f_+$ equal 0
%$\mu$-a.e. In that case, by monotonicity and non-negativity of $f_-$ and
%$f_+$,
%\begin{align*}
    %0 \leq f_- \leq 0
    %\quad \Rightarrow \quad
    %0 \leq \int f_- \leq 0  \\
    %0 \leq f_+ \leq 0
    %\quad \Rightarrow \quad
    %0 \leq \int f_+ \leq 0
%\end{align*}
%\end{proof}
%\begin{thm}
%Suppose $f$ is a measurable, nonnegative function. Then
%\begin{align*}
  %\int_\Omega f \; d\mu = 0 \quad \implies\quad
  %f = 0 \text{ $\mu$-a.e.}
%\end{align*}
%\end{thm}
\begin{proof}
Choose a sequence of simple functions $\{s_n\}$ that converge to $f$
from below such that
\begin{align*}
  0 \leq s_n \leq f
  \quad\Rightarrow\quad
  0 \leq \int_\Omega s_n \leq \int_\Omega f = 0
\end{align*}
That implies
\begin{align*}
  \mu\left(\{s_n > 0 \}\right) = 0
  \qquad \forall n
\end{align*}
otherwise an approximation of the integral by a simple function would be
positive, implying $\int f$ is positive. So then we can write
\begin{align*}
  \{f>0\} =
  \bigcup^\infty_{n=1} \{s_n>0\}
\end{align*}
Then, by countable additivity
\begin{align*}
    \mu\left(\{f>0 \} \right)=
    \mu\left(\bigcup^\infty_{n=1} \{ s_n>0 \}
    \right) = 0
\end{align*}
\end{proof}


\begin{prop}
Given $f,g\in\mathscr{L}(\Omega,\mathscr{F},\mu)$
\begin{enumerate}
  \item $\mu[f\neq g] = 0$
    implies
    $\int_\Omega f \; d\mu = \int_\Omega g \; d\mu$
  \item $\mu[f > g] = 0$ implies
    $\int_\Omega f \; d\mu \leq \int_\Omega g \; d\mu$
\end{enumerate}
\end{prop}

\begin{prop}\emph{(Monoticity)}
Given $f,g\in\mathscr{L}(\Omega,\mathscr{F},\mu)$
\begin{align*}
  f \leq g
  \quad\implies\quad
  \int_\Omega f \; d\mu \leq \int_\Omega f \; d\mu
\end{align*}
\end{prop}
\begin{proof}
Suppose that $f,g$ nonnegative.
Since $f\leq g$, given any simple function $s$, we have
\begin{align*}
  0\leq s\leq f
  \quad\implies\quad
  0\leq s\leq g
\end{align*}
Therefore, it's obvious that
\begin{align*}
  \int_\Omega g \; d\mu \leq
  \int_\Omega f \; d\mu
\end{align*}
For general $f\leq g$, we have $f_+ \leq g_+$ and $f_- \geq g_-$ which
implies that
\begin{align*}
  \int_\Omega f_+ \; d\mu \leq
  \int_\Omega g_+ \; d\mu
  \qquad\qquad
  -\int_\Omega f_- \; d\mu \leq
  -\int_\Omega g_- \; d\mu
\end{align*}
Taken together, the desired result follows.
\end{proof}


\begin{prop}
If $f$ is measurable and bounded, and if $\mu(\Omega)<\infty$, then
$f\in\mathscr{L}(\Omega,\mathscr{M},\mu)$.
\end{prop}

\begin{prop}
If $\mu(\Omega)=0$, then \emph{any} measurable function $f:
\Omega\rightarrow\bar{\mathbb{R}}$ will be integrable with $\int_\Omega
f \; d\mu=0$, even if $f=\infty$ on $\Omega$.
\end{prop}
\begin{proof}
Note that all simple functions will integrate to zero, so that the sup
over simple functions will be zero as well.
\end{proof}


\begin{cor} If we have $f\in\mathscr{L}(\Omega,\sF,\mu)$, then
\begin{align*}
  \left\lvert \int_\Omega f \; d\mu\right\rvert \leq
  \int_\Omega \left\lvert f \right\rvert \; d\mu
\end{align*}
\end{cor}
\begin{proof}
Since $f\in\mathscr{L}(\Omega,\sF,\mu)$, we know from the previous
theorem that $|f|\in\mathscr{L}(\Omega,\sF,\mu)$. Then it's clear that
\begin{align*}
  \left\lvert \int_\Omega f \; d\mu\right\rvert
  = c \int_\Omega f \; d\mu\qquad\text{where $c=1$ or $c=-1$}
\end{align*}
Then, since $cf\leq |f|$, we can say that
\begin{align*}
  \left\lvert \int_\Omega f \; d\mu\right\rvert
  = c \int_\Omega f \; d\mu
  = \int_\Omega c f \; d\mu \leq
    \int_\Omega |f| \; d\mu
\end{align*}
\end{proof}

\begin{thm}
Suppose $f\in\mathscr{L}(\Omega,\sF,\mu)$. Then for any $t>0$,
\begin{align*}
  \mu\big[\{|f|\geq t\}\big] \leq \frac{1}{t}\int_{\Omega} |f|\; d\mu
\end{align*}
\end{thm}

\begin{rmk}
You may have noticed that we didn't include the usual linearity
property. We will have this as a consequence of pillar convergence
theorems, but it turns out to be tougher for the more general integral
we've developed. We need to do the hard work in the next subsections to
establish this property.
\end{rmk}

\subsection{Pillar Convergence Theorems}

We defined the integral of nonnegative measurable function $f$ as the
supremum over integrals of simple functions bounded above by $f$. But
we know that we can always approximate any measurable $f$ by a sequence
of simple functions $s_n \uparrow f$ converging from below. One might
then reasonably think of approximating the integral of $f$ by
$\limn \int_\Omega s_n \;d\mu$. In other words, we might reasonably
think you could exchange the limit so that the integral of a limit
$\int_\Omega \limn s_n=\int_\Omega f \;d\mu$ is a limit of integrals
$\limn \int_\Omega s_n \;d\mu$. This subsection establishes that result
and, more generally, the conditions under which we can exchange limits
and the integral.


\begin{lem}
Suppose that $f$ is a nonnegative measurable function and $s$ is a
simple function such that $0\leq s\leq f$. If $\{f_n\}\ninf$ is a
sequence of simple functions converging pointwise to $f$ from below,
then
\begin{align*}
  \limn \int_\Omega f_n\;d\mu \geq \int_\Omega s \; d\mu
\end{align*}
\end{lem}
\begin{proof}
Since $s$ and the $f_n$ are simple functions, we will write them as
\begin{align*}
  s = \sumim x_{s,i} \cdot \Chi_{\left\{ s=x_{s,i}\right\}}
  \qquad
  f_n = \sum_{i=1}^{k_n} x_{n,i} \cdot
  \Chi_{\left\{ f_n = x_{n,i} \right\}}
\end{align*}
We will split up the evaluation of the integral of $s$ into two cases.
\\
\\
(\emph{Case 1})
First, assume that
$\mu[\left\{ s=x_{s,i}\right\}]=\infty$
%$\mu[s=x_i]=\infty$
for some $x_{s,i}$ in the range of $s$. Then define
\begin{align*}
  A_n := \left\{ s=x_{s,i} \right\}
  \cap \left\{ f_n\geq \frac{x_{s,i}}{2} \right\}
\end{align*}
First, since $A_n\subseteq \Omega$ and $f_n$ nonnegative, it's obvious
that
\begin{align*}
  \int_\Omega f_n \; d\mu
  \geq
  \int_{A_n} f_n \; d\mu
\end{align*}
But even more, since $f_n \geq \frac{x_{s,i}}{2}$ on $A_n$ by definition
of $A_n$, we can replace $f_n$ with the lower bound on $A_n$ to say that
\begin{align}
  \int_\Omega f_n \; d\mu
  \geq
  \int_{A_n} \frac{x_{s,i}}{2} \; d\mu
  =\frac{x_{s,i}}{2} \cdot \mu[A_n]
  \label{Anbound}
\end{align}
But what's the behavior of $A_n$? Well, since $f_n\ra f\geq s$ from
below, $\{A_n\}$ is an increasing sequence of sets such that
\begin{align*}
  \{s=x_{s,i}\} = \bigcup\ninf A_n
  \quad\implies&\quad
  \mu[\{s=x_{s,i}\}] = \mu\left[\bigcup\ninf A_n\right] \\
  \text{By $\sigma$-additivity of $\mu$}
  \quad \iff&\quad
  \mu[\{s=x_{s,i}\}] = \limn \mu\left[A_n\right]
\end{align*}
But since we assumed $\mu[\{s=x_{s,i}\}]=\infty$, we
have $\limn \mu[A_n]=\infty$ as well. Therefore, taking the limit of
Expression~\ref{Anbound} we get
\begin{align*}
  \limn \int_\Omega f_n \; d\mu
  &\geq
  \limn
  \frac{x_{s,i}}{2} \cdot \mu[A_n] = \infty \\
  \Rightarrow\quad
  \limn \int_\Omega f_n \; d\mu
  &\geq
  \infty
\end{align*}
So since the LHS is $\infty$ by implication, we clearly have
\begin{align*}
  \limn \int_\Omega f\;d\mu \geq \int_\Omega s\;d\mu
\end{align*}
(\emph{Case 2}) Now suppose that
$\mu[\left\{ s=x_{s,i} \right\}] <\infty$ for all $i$.
Then fix $\varepsilon \in (0,\min_i x_{s,i})$ and define for each $i$
\begin{align*}
  A_{i,n}
  =
  \{s = x_{s,i}\}
  \cap
  \left\{ f_n \geq x_{s,i} -\varepsilon\right\}
\end{align*}
Since $f_n\ra f \geq s$ from below, it's the case for all $i$ that
\begin{align*}
  \{s=x_{s,i}\} = \bigcup\ninf A_{i,n}
  \quad\implies&\quad
  \mu[\{s=x_{s,i}\}] = \mu\left[\bigcup\ninf A_{i,n}\right] \\
  \text{By $\sigma$-additivity of $\mu$}
  \quad \iff&\quad
  \mu[\{s=x_{s,i}\}] = \limn \mu\left[A_{i,n}\right]
\end{align*}
\end{proof}


\begin{thm}\emph{(Poor Man's Monotone Convergence or Beppo-Levi Theorem)}
Given nonnegative measurable $f$ on measure space $(\Omega,\sF,\mu)$,
let $\{s_n\}\ninf$ be a sequence of nonnegative simple functions
converging pointwise to $f$ from below.
Then we can exchange limits with the integral:
\begin{align*}
    \lim_{n\rightarrow\infty} \int_\Omega s_n \; d\mu &=
    \int_\Omega \left(\lim_{n\rightarrow\infty} s_n\right) \; d\mu \\
    \iff\quad
    \lim_{n\rightarrow\infty} \int_\Omega s_n \; d\mu &=
    \int_\Omega f \; d\mu
\end{align*}
This is valid whether $f$ is infinite or not.
\end{thm}
\begin{rmk}
This is ``Poor Man's'' because the convergence sequence consists of
simple functions rather than arbitrary measurable functions.
We see the strong version as the next theorem.
\end{rmk}
\begin{proof}
(LHS $\leq$ RHS) Since $s_n\leq f$ for all $n$, we have by monotonicity
that
\begin{align*}
  \int_\Omega s_n \;d\mu
  &\leq
  \int_\Omega f \;d\mu \\
  \implies\quad
  \limn \int_\Omega s_n \;d\mu
  &\leq
  \int_\Omega f \;d\mu
\end{align*}
(RHS $\leq$ LHS) This direction is tougher.
\end{proof}


\begin{thm}\emph{(Monotone Convergence or Beppo-Levi Theorem)}
Given arbitrary measurable $f,g$ on measure space $(\Omega,\sF,\mu)$,
let $\{f_n\}\ninf$ be a sequence of measurable functions converging
pointwise to $f$ a.e. from below. If $\int_\Omega|g|\;d\mu<\infty$ and
$g \leq f_1$ a.e., then we can exchange limits with the integral:
\begin{align*}
    \lim_{n\rightarrow\infty} \int_\Omega f_n \; d\mu =
    \int_\Omega \left(\lim_{n\rightarrow\infty} f_n\right)
    \; d\mu
\end{align*}
This is valid whether $f$ is infinite or not.
\end{thm}

\begin{lem}\emph{(Fatou's Lemma)}
Let $g$ and $\{f_n\}\ninf$ be measurable with $g\in\sL(\Omega,\sF,\mu)$
and $g \leq f_n$ a.e.\ for all $n$. Then
\begin{align*}
  \int_\Omega \left(\liminf_n f_n\right) \; d\mu
  \leq \liminf_n \int_\Omega f_n \; d\mu
\end{align*}
Or equivalently, if $g \geq f_n$ a.e.\ for all $n$
\begin{align*}
  \int_\Omega \left(\limsup_n f_n\right) \; d\mu
  \geq \limsup_n \int_\Omega f_n \; d\mu
\end{align*}
\end{lem}
\begin{rmk}
Inequality might be strict.
\end{rmk}

\begin{thm}\emph{(Lebesgue's Dominated Convergence Theorem)}
Let $\{f_n\}\ninf$ be a sequence of measurable functions converging to $f$
a.e. Suppose also that $g\in\sL(\Omega,\sF,\mu)$ is a measurable
function with $|f_n|\leq g$ a.e. Then
\begin{align*}
  \int_\Omega |f| \;d\mu
  &\leq
  \int_\Omega |g| \;d\mu \\
  \limn \int_\Omega f_n \; d\mu
  &= \int_\Omega \limn f_n \; d\mu \\
  \limn \int_\Omega |f_n-f|\;d\mu &= 0
\end{align*}
\end{thm}
\begin{rmk}
For bounded measurable functions on measure space with a finite measure,
we'll be able to use the result all of the time.
\end{rmk}

\clearpage
\subsection{Push Forward Measure and Change of Variables}

\begin{defn}(Push Forward Measure)
Suppose we have $f:\Omega\ra E$, a measurable function between measure
space $(\Omega,\sF,\mu)$ and measurable space $(E,\sE)$. Then the push
forward measure denoted $\mu \circ f^{-1}$ or $\mu_f$ is defined
\begin{align*}
  \mu_f[A] := \mu[f^{-1}(A)]
  \qquad\forall A \in\sE
\end{align*}
\end{defn}
\begin{rmk}
The push-forward measure is so named because we don't have a measure
defined for $(E,\sE)$, but we \emph{induce} one by pushing forward the
measure defined for measurable $(\Omega,\sF)$ to measurable space
$(E,\sE)$.
\end{rmk}

\begin{prop}\emph{(Change of Variables)}
Suppose we have $f:\Omega\ra E$, a measurable function between measure
space $(\Omega,\sF,\mu)$ and measurable space $(E,\sE)$, which together
imply push forward measure $\mu \circ f^{-1}$ or $\mu_f$. Then for any
$\sE$-measurable function $g:(E,\sE)\ra(G,\sG)$, we have
\begin{align*}
  \int_{f^{-1}(A)} g\circ f \; d\mu
  =
  \int_A g\;d\mu_f
  \qquad A \in \sE
\end{align*}
Expressing these integrals in more verbose, explicit notation
\begin{align*}
  \int_{f^{-1}(A)} g(f(\omega)) \; \mu(d\omega)
  =
  \int_A g(x)\;\mu_f(dx)
  \qquad A \in \sE
\end{align*}
\end{prop}

\clearpage
\subsection{Radon-Nikodym}

\begin{defn}(Absolutely Continuous and Equivalent Measures)
Given two measures $\mu_1$ and $\mu_2$ on measure space $(\Omega,\sF)$,
we say that $\mu_2$ is
\emph{absolutely continuous with respect to $\mu_1$}, denoted
$\mu_2<<\mu_1$ if
\begin{align*}
  \mu_1(A) = 0
  \quad\implies\quad
  \mu_2(A) = 0
\end{align*}
In words, $\mu_2$ does not assign possitive weight or probability to
events $\mu_1$ does not.

We say that two measures are equivalent, denoted $\mu_1\sim \mu_2$, if
both $\mu_1<<\mu_2$ and $\mu_2<<\mu_1$.
\end{defn}

\begin{lem}
Let $s$ be a nonnegative simple function on measure space
$(\Omega,\sF,\mu)$. Then
\begin{align*}
  \lambda(A) := \int_A s \; d\mu
  \qquad A \in \sF
\end{align*}
defines a new measure on $(\Omega,\sF)$.
\end{lem}
\begin{proof}
It's easy to see that $\lambda(\emptyset) = 0<\infty$. Moreover, since
$s$ is non-negative, we know that $\lambda(A) \in [0,\infty]$. So all
that's left to prove is countable additivity.

So let $E_1, E_2,\ldots$ be disjoint, and notice we can write
\begin{align*}
    \lambda\left(\bigcup^\infty_{n=1} E_n\right)
    = \int_{\cup E_n} s \;d\mu =
    \int_{\Omega} s \cdot \Chi_{\cup E_n}\;d\mu =
    \int_{\Omega} \left(s \cdot
    \sum^\infty_{n=1}\Chi_{E_n}\right)\;d\mu
\end{align*}
Now let's rewrite the final expression, using the fact that $s$ is
simple to swap the sum and the integral (which we couldn't necessarily
do for arbitrary functions):
\begin{align*}
    \int_{\Omega} \left(s \cdot  \sum^\infty_{n=1}
    \Chi_{E_n}\right)\;d\mu
    &= \sum^m_{i=1} \left(c_i \cdot
    \sum^\infty_{n=1}\Chi_{E_n}
    \right)\mu(A_i) \\
    &= \sum^m_{i=1} \sum^\infty_{n=1}c_i \cdot \Chi_{E_n}
    \mu(A_i)
    =  \sum^\infty_{n=1}\sum^m_{i=1} c_i \cdot \Chi_{E_n}
    \mu(A_i) \\
    &=   \sum^\infty_{n=1}\int_\Omega s \cdot \Chi_{E_n}
        \; d\mu =  \bigcup^\infty_{n=1}\lambda(E_n)
\end{align*}
So we see that because $s$ is simple, we can exchange the sum and the
integral, which allows us to gain the desired result of countable
additivity.
\end{proof}

\begin{prop}
Given measure space $(\Omega,\sF,\mu_1)$ and nonnegative measurable
function $f:\R\ra \R_+\cup\{+\infty\}$, define
\begin{align*}
  \mu_2(A) := \int_A f \; d\mu_1
\end{align*}
Then $\mu_2$ is a measure that is absolutely continuous with respect to
$\mu_1$, i.e. $\mu_2 << \mu_1$. If $f$ is $\mu_1$-integrable, then
$\mu_2$ is finite.
\end{prop}

\begin{thm}(Radon-Nikodym)
Let $\mu_1$ and $\mu_2$ be two measures on measurable space
$(\Omega,\sF)$ such that $\mu_2<<\mu_1$ and $\mu_1$ is $\sigma$-finite.
Then there exists a measurable function $f$ called the
\emph{Radon-Nikodym derivative}
$f:\Omega\ra \R_+\cup \{+\infty\}$ such that
such that
\begin{align*}
  \mu_2(A) = \int_A f \; d\mu_1
\end{align*}
Moreover, $f$ is unique up to $\mu_1$-a.e. equality. We also sometimes
denote this function by $\frac{d\mu_2}{d\mu_1}$.
\end{thm}


\subsection{Integral Inequalities}

\begin{prop}{\emph{(Jensen's Inequality)}}
Let $g$ be a function and let $X$ be some random variable. Then
\begin{align*}
  \text{$g$ convex} &\implies \E[g(X)] \geq g\left(\E[X]\right) \\
  \text{$g$ concave} &\implies \E[g(X)] \leq g\left(\E[X]\right) \\
\end{align*}
\end{prop}
\begin{proof}
Since $g$ is a convex function, given any $x$, there is a constant $a$
such that
\begin{align*}
  g(y) \geq g(x) + a (y-x)
\end{align*}
If $g$ is differentiable, then $a=g'(x)$. But if $g$ is not
differentiable at $x$, that's still okay because the function will be
differentiable from the left or right (a subgradient will exist) so that
we could just set
$a=\lim_{\delta \ra 0} \frac{g(x+\delta)-g(x)}{\delta}$ where
$\delta\ra 0$ from above. Anyway, the point is that $a$ exists.

Next, choose $x=\E[X]$. Again, there is an $a$ such that
\begin{align*}
  g(y) &\geq g(\E[X]) + a (y-\E[X])
\end{align*}
But this must also be true for $y=X$, so
\begin{align*}
  g(X) &\geq g(\E[X]) + a (X-\E[X])
\end{align*}
Now take expectation
\begin{align*}
  \E[g(X)]
  &\geq \E[g(\E[X]) + a (X-\E[X])] \\
  &= \E[g(\E[X])] + a \E[(X-\E[X])] \\
  &= g(\E[X]) + a (\E[X]-\E[X]) \\\\
  \implies \quad
  \E[g(X)] &\geq g(\E[X])
\end{align*}
Concave proof is analogous.
\end{proof}

\begin{cor}
Let $X$ be a random variable. Then $||X||_p\leq ||X||_q$ for
$1\leq p\leq q\leq \infty$.
\end{cor}

\begin{rmk}
The above proposition and corollary only work because the expectation is
computed with a probability measure. They don't work for for general
measures.
\end{rmk}

\begin{thm}{\emph{(Markov's Inequality)}}
\label{thm:markov}
Given random variable $X$,
\begin{equation}
    \label{markov}
    \Prb\left[
      \left\lvert X\right\rvert
      \geq \varepsilon\right]
    \leq \frac{\E\left[|X|^p\right]}{|\varepsilon|^p}
\end{equation}
\end{thm}

\begin{proof}
Write out the expectation in the numerator, where $F$ is the cdf of $X$:
\begin{align*}
  \E\left[|X|^p\right]
  &= \int^\infty_{-\infty} |X|^p \; dF \\
  &= \int^{\varepsilon}_{-\infty} |X|^p \; dF
    + \int^\infty_{\varepsilon} |X|^p \; dF
\end{align*}
Next, let's just throw out the first term in the sum above to get an
inequality. Since $|X|^p$ and the cdf are always positive, we know that
we're throwing out a positive portion, which gives us inequality
\begin{align*}
  \E\left[|X|^p\right]
  &\geq \int^\infty_{\varepsilon} |X|^p \; dF
\end{align*}
Next, notice that a lower bound for $|X|^p$ over this support will be
$|\varepsilon|^p$, so we can modify the inequality again to get
\begin{align*}
  \E\left[|X|^p\right]
  &\geq  |\varepsilon|^p \int^\infty_{\varepsilon} \; dF
  = |\varepsilon|^p \; \Prb[X\geq \varepsilon] \\
  \implies \quad
  \frac{\E\left[|X|^p\right]}{|\varepsilon|^p}
  &\geq \Prb(X\geq \varepsilon)
\end{align*}
\end{proof}

\begin{cor}{\emph{(Chebyshev's Inequality)}}
Given random variable $X$, we have
\begin{equation}
  \label{chebyshev1}
  \Prb\left[
    \left\lvert X\right\rvert
    \geq \varepsilon\right]
  \leq \frac{\E\left[X^2\right]}{\varepsilon^2}
\end{equation}
or equivalently
\begin{equation}
  \label{chebyshev2}
  \Prb\left[
    \big\lvert X-\E[X]\big\rvert
    \geq \varepsilon\right]
  \leq \frac{\Var(X)}{\varepsilon^2}
\end{equation}
\end{cor}
\begin{proof}
The proof of Statement~\ref{chebyshev1} follows by taking $p=2$ in
Markov's Inequality (Theorem~\ref{markov}).
To prove Statement~\ref{chebyshev2}, define $Z=X-\E[X]$ and use that in
Markov's inequality with $p=2$ on $Z$.
\end{proof}

\clearpage
\subsection{Uniform Integrability}

\begin{defn}(Uniform Integrability)
A family of random variables $\{X_\alpha\}_{\alpha \in J}$ is
\emph{uniformly integrable} if
\begin{align*}
  \lim_{c\ra\infty} \sup_{\alpha\in J}\quad
  \int_{\{|X_\alpha|>c\}}
  |X_i|\; dP = 0
\end{align*}
That is, you're integrating the absolute value of $X_\alpha$ over the
tails where it has large values. Moreover, you require that the value
of the integral goes to zero uniformly for \emph{all} $X_\alpha$ (since
you're taking the sup over $\alpha \in J$).
\end{defn}

\begin{rmk}
Given a family of random variables $\{X_\alpha\}_{\alpha \in J}$, we
might consider two notions of boundedness for the family:
\begin{enumerate}[label=(\roman*)]
  \item Random Variable in $L^1$ Bounded: For some random variable
    $X\in L^1(\Omega,\sF,P)$:
    \begin{align*}
      |X_\alpha|\leq |X|
      \qquad \forall \alpha \in J
    \end{align*}

  \item $L^p$ Bounded for $p\in[1,\infty)$:
    \begin{align*}
      \sup_{\alpha\in J} \lVert X_\alpha\rVert_p<\infty
      \qquad\text{or equivalently}\qquad
      \sup_{\alpha\in J} \E[|X_\alpha|^p]<\infty
    \end{align*}
\end{enumerate}
In the following lemmas and propositions, we will have the following
results:
\begin{itemize}
  \item RV in $L^1$ bounded $\implies$ UI
  \item $L^1$ bounded + tail condition $\implies$ UI
  \item UI $\implies$ $L^1$ bounded
  \item UI $\implies$ $L^p$ bounded for \emph{some} (but not all)
    $p \in (1,\infty]$
  \item $L^p$ bounded for any $p\in(1,\infty]$ $\implies$ UI
\end{itemize}
In this sense, uniformly integrable families are somewhere in between
``bounded in $L^1$ families'' (since UI implies this) and ``bounded in
$L^{p>1}$ families'' (since this implies UI).
\end{rmk}


\begin{lem}
If $\{X_\alpha\}_{\alpha\in J}$ is a family of random variables such
that $|X_\alpha|\leq |X|$ a.s. for some $X\in L^1(\Omega,\sF,P)$, then
$\{X_\alpha\}_{\alpha\in J}$ is a uniformly integrable family.
\end{lem}
%\begin{proof}
%First, note that for any $c\in\R$ and $\alpha\in J$, we can write
%\begin{align*}
  %\int_{\{|X_\alpha|>c\}}
  %|X_\alpha|\;dP
  %&=
  %\int_\Omega
  %|X_\alpha|\one{\{|X_\alpha|>c\}}\;dP \\
  %&=
  %\E\left[|X_\alpha|\one{\{|X_\alpha|>c\}}\right] \\
  %&\leq
  %\E\left[|X|\one{\{|X|>c\}}\right]
%\end{align*}
%since $|X_\alpha|\leq |X|$ for all $\alpha\in J$ by assumption.
%This inequality holds for all $\alpha$, so it will also hold if we take
%the sup over $\alpha$ on the LHS:
%\begin{align*}
  %\sup_\alpha \int_{\{|X_\alpha|>c\}}
  %|X_\alpha|\;dP
  %&\leq
  %\E\left[|X|\one{\{|X|>c\}}\right]
%\end{align*}
%Now take the limit as $c\ra\infty$ on both sides:
%\begin{align*}
  %\lim_{c\ra\infty}
  %\sup_\alpha \int_{\{|X_\alpha|>c\}}
  %|X_\alpha|\;dP
  %&\leq
  %\lim_{c\ra\infty}
  %\E\left[|X|\one{\{|X|>c\}}\right]
%\end{align*}
%Finally, by the Lebesgue dominated convergence theorem since
%$|X|\one{|X_\alpha|>c}<|X|$ a.s. for all $\alpha$, we can take the limit
%inside on the RHS to get
%\begin{align*}
  %\lim_{c\ra\infty}
  %\sup_\alpha \int_{\{|X_\alpha|>c\}}
  %|X_\alpha|\;dP
  %&\leq
  %\E\left[
  %\lim_{c\ra\infty}
  %|X|\one{\{|X|>c\}}
  %\right]
  %= 0
%\end{align*}
%which is exactly what we needed to show.
%\end{proof}

\begin{prop}
A family of random variables $\{X_\alpha\}_{\alpha\in J}$
on probability space $(\Omega,\sF,P)$ is uniformly integrable if and
only if
\begin{align*}
  \text{both}\quad
  \begin{cases}
    \sup_{\alpha \in J} \E[|X_\alpha|]<\infty \\
    \forall\varepsilon>0
    \quad\exists \delta >0
    \quad\text{s.t.}\;
    \E[|X_\alpha|\one{A}]\leq\varepsilon
  \end{cases}
\end{align*}
In words, uniform integrability implies that the family is $L^1$-bounded
(as mentioned above in the definition of uniform integrability),
\emph{and} we can control the tails.
\end{prop}

\begin{thm}\emph{(Lebesgue DCT Generalization)}
Let $\{X_n\}\ninf$ and $X$ be random variables on $(\Omega,\sF,P)$ such
that
\begin{enumerate}[label=(\roman*)]
  \item $X_n\pto X$
  \item $\{X_n\}\ninf$ is a UI family
\end{enumerate}
Then
\begin{enumerate}[label=(\roman*)]
  \item $X\in L^1(\Omega,\sF,P)$
  \item $\limn \E[X_n] = \E\left[\limn X_n\right] = \E[X]$
  \item $X_n \Lqto{1} X$, i.e. $\lim \E[|X_n-X|] = 0$.
\end{enumerate}
\end{thm}
\begin{rmk}
This is strictly weaker than Lebesgue's DCT for a few reasons. First, we
don't need $|X_n|\leq |X|$ for all $n$ as in Lebesgue---we just need the
family to be UI, which we saw is weaker. Moreover, we don't need
$X_n\asto X$---we just need $X_n\pto X$, which (again) is weaker.
This only works for a probability space where $P[\Omega]<1$, so this is
really a probability theory result, not a measure theory result.
\end{rmk}


\begin{cor}
A family of random variables $\{X_\alpha\}_{\alpha\in J}$ is UI if and
only if the family is $L^p$-bounded for some $p\in(1,\infty]$.
\end{cor}

\clearpage
\subsection{Fubini-Tonelli}

\begin{thm}\emph{(Fubini-Tonelli)}
Let
$(\Omega_1,\sF_1,\mu_1)$ and
$(\Omega_2,\sF_2,\mu_2)$
be measure spaces and let
\begin{align*}
  f:
  (\Omega_1\times \Omega_2, \sF_1\otimes\sF_2)
  \ra
  (\R,\sB(\R))
\end{align*}
be a measurable function. Then
\begin{enumerate}
  \item \emph{Fubini}:
    If $f$ is integrable on the product space
    \begin{align*}
      \int_{\Omega_1\times \Omega_2}
      |f(\omega_1,\omega_2)| \; d(\mu_1\otimes\mu_2)
      <\infty
    \end{align*}
    then you can swap the order of integration:
    \begin{align*}
      \int_{\Omega_1\times \Omega_2}
      f(\omega_1,\omega_2) \; d(\mu_1\otimes\mu_2)
      &=
      \int_{\Omega_1} \left[\int_{\Omega_2}
      f(\omega_1,\omega_2)
      \;\mu_2(d\omega_2) \right]\;\mu_1(d\omega_1) \\
      &=
      \int_{\Omega_2}\left[
        \int_{\Omega_1}
      f(\omega_1,\omega_2)
      \;\mu_1(d\omega_1) \right]\;\mu_2(d\omega_2)
    \end{align*}

  \item \emph{Tonelli}:
    Suppose that $f$ is nonnegative and $\mu_1,\mu_2$ are
    $\sigma$-finite. Then you can swap the order of integration:
    \begin{align*}
      \begin{rcases}
      f\geq 0 \\
      \text{$\mu_1,\mu_2$ are $\sigma$-finite}
      \end{rcases}
      \quad\implies \quad
      \int_{\Omega_1\times \Omega_2}
      f \; d(\mu_1\otimes\mu_2)
      =
      \int_{\Omega_1}\int_{\Omega_2}
      f(\omega_1,\omega_2)
      \;d\mu_2(\omega_2) \;d\mu_1(\omega_1)
    \end{align*}
\end{enumerate}
\end{thm}
\begin{rmk}
Fubini is often easy to check, since a bounded function with a finite
measure will be integrable, so then Fubini tells you that you can swap
the order of integration.
\end{rmk}



\clearpage
\section{$L^p$ Spaces}

We start with a discussion of \emph{normed vector spaces}, which take a
vector space and add the notion of a \emph{norm}, a function that can be
used to measure the length of vectors or distance between vectors.

After that, we move to \emph{inner product spaces}. which are vector
spaces that have defined a special function: the inner product. But,
from this inner product, we can always build to a norm, tying these two
concepts together.

Finally, we move onto $L^p$ spaces.

\subsection{Normed Vector Spaces and Banach Spaces}


\begin{defn}(Norm)
\label{defn.norm}
Let $X$ be a vector space over a field $\mathbb{R}$ or $\mathbb{C}$.
Then a \emph{norm} is a function
$\lvert\cdot\rvert:X\rightarrow[0,\infty]$
\begin{enumerate}
\item $\lvert x\rvert=0$ if and only if $x=0$
\item $\lvert x + y\rvert\leq \lvert x\rvert+\lvert y\rvert$ for all $x,y\in X$
\item $\lvert cx\rvert= |c|\cdot\lvert x\rvert$ for all scalar $c$ in the field and $x\in X$.
\end{enumerate}
It will be possible to use the norm to define a metric on a space, as we
will see in Definition \ref{defn.banach}.
\end{defn}

\begin{defn}(Banach Space)
\label{defn.banach}
A \emph{Banach Space} is a \emph{complete} normed vector space. Thus,
Banach spaces are vector spaces that have a metric, $d(x,y)=\lvert
x-y\rvert$, which can be used to compute vector lengths and distance
bewteen vectors. On top of that, it is also complete so that Cauchy
sequences always converge to a limit.
\end{defn}

\begin{thm}
A norm, $\lvert\cdot\rvert: x\mapsto \rvert x \lvert$ is a Lipschitz
continuous function.
\end{thm}
\begin{proof}
\begin{align*}
    \bigl\lvert \lvert x \rvert - \lvert y\rvert \bigr\rvert
    \leq
    \bigl\lvert \lvert x - y + y\rvert - \lvert y\rvert \bigr\rvert
    \leq
    \bigl\lvert \lvert x - y \rvert + \lvert y\rvert - \lvert y\rvert \bigr\rvert
    \leq
    \lvert x - y \rvert
\end{align*}
\end{proof}

\begin{ex}
Here are a few classic norms where vectors $x\in X$ have $n$ elements:
\begin{enumerate}
\item $\lvert x\rvert_\infty = \max_{1\leq i\leq n} |x_i|$
\item $\lvert x\rvert_2 = \left(\sum_{i=1}^n |x_i|^2\right)^{\frac{1}{2}}$
\item $\lvert x\rvert_p = \left(\sum_{i=1}^n
  |x_i|^p\right)^{\frac{1}{p}}$ for $p\in[1,\infty]$, not necessarily an
  integer.
\end{enumerate}
\end{ex}

\begin{ex}
Now for some norms in \emph{function} space, as opposed to norms for
$\mathbb{R}^n$ or $\mathbb{C}^n$:
\begin{enumerate}
\item $X=C([a,b])$, the set of continuous functions on $[a,b]$, take
  \begin{align*}
    \lvert f\rvert_\infty :=
    \sup_{x\in[a,b]} |f(x)|
  \end{align*}
\item $X=C'([a,b])$, the set of once-differentiable continuous functions
  on $[a,b]$, use the previous definition to define
  \begin{align*}
    \lvert f\rvert := \lvert f\rvert_\infty + \lvert f'\rvert_\infty
  \end{align*}
\end{enumerate}
\end{ex}

\begin{defn}(Semi-Norm)
A \emph{semi-norm} is defined as above in Definition \ref{defn.norm},
except condition (1) is not there. That is, there can be non-zero
vectors that the norm maps to zeros.
\end{defn}
\begin{ex}
For an example of a semi-norm, suppose that all vectors $x\in X$ have
$n$ elements, $x=(x_1,\ldots,x_n)$. Then define the a semi-norm for
$m<n$ as $\lvert x\rvert = \sum_{i=1}^m |x_i|$.
\end{ex}

\subsection{Inner Product and Hilbert Spaces}

\begin{defn}(Inner Products and Inner Product Space)
An \emph{inner product space} is a vector space $X$ with an
\emph{inner product}, a function $\langle\cdot,\cdot\rangle:X\times
X\rightarrow\mathbb{R}$ (or $\mathbb{C}$). It has the following
properities
\begin{enumerate}
  \item $\langle x,x\rangle \geq 0$ for all $x\in X$. Moreover,
    $\langle x,x\rangle=0$ if and only if $x=0$, the zero vector.
  \item $\langle x,y\rangle= \overline{\langle y,x\rangle}$.
  \item $\langle \cdot,y\rangle$ is linear. That is
    \begin{align*}
      \langle \alpha x + y,z\rangle =
      \alpha \langle x ,z\rangle + \langle y,z\rangle
    \end{align*}
    Conjugate linearity by the previous statement.
\end{enumerate}
\end{defn}

\begin{defn}(Hilbert Space)
A \emph{Hilbert Space} is a \emph{complete} inner product space. We
shall see that all Hilbert spaces are Banach spaces since an inner
product can be used to define a norm.
\end{defn}

\begin{defn}(Orthogonalility, Orthogonal Family, Orthonormal Family)
We say that two vectors, $x$ and $y$, are \emph{orthogonal} if
$\langle x,y\rangle =0$. We denote orthogonality by $x\perp y$.

We call a collection of vectors $\{x_i\}$ an \emph{orthogonal family} if
$x_i\perp x_j$ for all $i\neq j$. We call the family \emph{orthonormal}
if, in addition to being an orthogonal family, $\lVert x_i\rVert=1$ for
all $i$.
\end{defn}

\begin{thm}
\label{thm.cauchyscwarz}
\emph{(Cauchy-Schwartz Inequality)}
Given any inner product space, define $\lVert x\rVert := \sqrt{\langle
x,x\rangle}$.\footnote{Later, we will show that this definition does,
indeed, make $\lVert x\rVert$ a norm.  However, the proof requires the
Cauchy-Scwarz inequality. So now, just treat $\lVert x\rVert$ as a
function that has some special property we want to prove.} Then
\begin{align*}
    \lvert \langle x,y\rangle\rvert \leq \lVert x\rVert\cdot\lVert y\rVert
\end{align*}
\end{thm}
\begin{proof}
Start by assuming that $|\alpha|=1$ and that $\alpha\in\mathbb{C}$. Then
define
\begin{align*}
    Q_\alpha(t) = \lVert x - t\alpha y\rVert^2
\end{align*}
By the definition of $\lVert \cdot\rVert$, it's clear that
$Q_\alpha(t)\geq 0$ for all $t\in\mathbb{R}$. Then let's use the
properties of the inner product to write
\begin{align}
  Q_\alpha(t) &= \langle x -t\alpha y,\; x -t\alpha y\rangle \notag \\
  &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2
  - t\alpha \overline{\langle x,y\rangle}
  - t\bar{\alpha} {\langle x,y\rangle} \label{cauchyhold}
\end{align}
Without loss of generality, assume $\langle x,y\rangle\neq 0$, otherwise
trivial. Now since $\alpha$ was only restricted to be such that
$|\alpha|=1$, we can choose it to be whatever we want, and use the fact
that the Equality \ref{cauchyhold} \emph{must} be true for all
$|\alpha|=1$ and $t\in\mathbb{R}$, given the definition fo the inner
product. So let
\begin{align*}
  \alpha = \frac{\langle x,y\rangle}{|\langle x,y\rangle|}
  \quad\Rightarrow\quad
  Q_\alpha(t) &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2
  - t\frac{\langle x,y\rangle\overline{\langle x,y\rangle}}{\langle x,y\rangle}
  - t\frac{\overline{\langle x,y\rangle}{\langle x,y\rangle}}{\langle x,y\rangle}  \\
  &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2
  -2t |\langle x,y\rangle|
\end{align*}
\end{proof}

\begin{thm}\emph{(Induced Norm)}
\label{norminduce}
Given an inner product space, we can define a norm by $\lVert x\rVert :=
\sqrt{\langle x,x\rangle}$.
\end{thm}
\begin{proof}
Given the definition, this is obvious for all but the triangle
inequality, which relies on the Cauchy-Schwarz inequality, which was
proved above.
\end{proof}

\begin{thm}
\label{pythag}
\emph{(Pythagorean Relation)} Given $x\perp y$,
\begin{align*}
    \lVert x + y\rVert^2 = \lVert x\rVert^2 + \lVert y \rVert^2
\end{align*}
More generally, given an orthonormal set $\{u_1,\ldots,u_N\}$:
\begin{align*}
    \left\lVert \sum_{i=1}^N c_i u_i \right\rVert^2
    = \sum_{i=1}^N |c_i|^2
\end{align*}
\end{thm}

\begin{prop}
Let $X$ be an inner product space. Then for all $x,y\in X$, we have the
\emph{polarization identity}
\begin{align*}
    \lVert x+y\rVert^2 + \lVert x-y\rVert^2
    = 4\langle x,y\rangle
\end{align*}
along with the \emph{parallelogram law}:
\begin{align*}
  \lVert x+y\rVert^2 + \lVert x-y\rVert^2
  =2\left(\lVert x\rVert^2 +\lVert y\rVert^2\right)
\end{align*}
\end{prop}
\begin{rmk}
This only works with inner product spaces. Altough the parallelogram law
is written in terms of norms, it will not worked for \emph{all} normed
vector spaces. It only holds for norms induced from an inner product as
in Theorem~\ref{norminduce}.
\end{rmk}

\begin{thm}
\label{projorth}
Suppose $\{u_1,\ldots,u_N\}$ is an orthonormal set. Then given any
$x\in X$,
\begin{align*}
    \sum_{n=1}^N \langle x,u_n\rangle u_n
    \perp
    x -\sum_{n=1}^N \langle x,u_n\rangle u_n
\end{align*}
This (I think) is like projecting $X$ onto the axes defined by the
orthonormal set. So the residual will be perpendicular.
\end{thm}
\begin{proof}
Evaluate the inner product.
\end{proof}

\begin{thm}
Given an orthonormal set $\{u_1,\ldots,u_N\}$,
\begin{align*}
  \lVert x\rVert^2 =
  \sum_{i=1}^N \bigl\lvert\langle x,u_n\rangle\bigr\rvert^2
  + \left\lVert x -  \sum_{i=1}^N\langle x,u_n\rangle u_n \right\rVert^2
\end{align*}
\end{thm}
\begin{proof}
In $\lVert x\rVert^2$,substitute in
\begin{align*}
  \lVert x\rVert^2 =
  \left\lVert
  \sum_{n=1}^N \langle x,u_n\rangle u_n
  + \left( x
  -\sum_{n=1}^N \langle x,u_n\rangle u_n\right)
  \right\rVert^2
\end{align*}
Then use the Pythagorean Relation in Theorem \ref{pythag}, since these
two objects in the sum are orthogonal by Theorem \ref{projorth}.
\end{proof}

\begin{cor}
\emph{(Bessel's Inequality)}
Given an orthonormal set $\{u_1,\ldots,u_N\}$,
\begin{align*}
    \sum_{i=1}^N
    \bigl\lvert\langle x,u_n\rangle\bigr\rvert^2
    \leq \lVert x\rVert^2
\end{align*}
\end{cor}

\begin{rmk}
Cauchy Schwarz can also be proved via Bessel's Inequality if we take
$N=1$ and $u_1=\frac{y}{\lVert y\rVert}$. Then
\begin{align*}
    \frac{\langle x,y\rangle^2}{\lVert y\rVert^2} \leq \lVert x\rVert^2
    \quad\Rightarrow\quad
    \langle x,y\rangle \leq \lVert x\rVert\cdot \lVert y\rVert
\end{align*}
\end{rmk}


\clearpage
\subsection{$L^p$ Spaces}

\begin{defn}(Set of Measurable Functions $\sL^0$, $L^0$)
Given measure space $(\Omega,\sF,\mu)$, define
\begin{align*}
  \sL^0(\Omega,\sF,\mu) :=
  \left\{ f:\Omega\ra\R \; |\; \text{$f$ measurable wrt $\sF$, $\sB(\R)$} \right\}
\end{align*}
But note that some functions in $\sL^0$ will be equal $\mu$-almost
everywhere. So define
\begin{align*}
  [f] &:= \left\{ g\in \sL^0 \;|\; g=f \; a.e. \right\} \\
  L^0 &:= \left\{ [f] \;|\; f\in\sL^0 \right\}
  = \sL^0 / a.e.
\end{align*}
That is, $\mu$-almost everywhere is an equivalence relation on
$\sL^0$, so $L^0$ is just the quotient space of equivalence classes in
$\sL^0$.
\end{defn}

\begin{defn}($L^p$ Norm)
Given measurable functions $\sL^0$ on measure space
$(\Omega,\sF,\mu)$, define the $L^p$ norm of $f$ as
\begin{align*}
  ||f||_p := \left(\int_\Omega |f|^p\;d\mu\right)^{\frac{1}{p}}
  \qquad p \in [1,\infty)
\end{align*}
For $p=\infty$, define
\begin{align*}
  ||f||_\infty := \inf\{\lambda \in\R_+ \;|\; \mu(|f|>\lambda)=0\}
\end{align*}
i.e. the a.e. bound of $f$.
\\
\\
The $L^p$ norm generalizs the Euclidean norm.
We require $p\geq 1$ because otherwise we lose convexity and this
wouldn't be a norm.
\end{defn}
\begin{rmk}
We see from this definition why we distinguish $\sL^0$ from $L^0$.
There are functions in $\sL^0$ such that $||f||_p = 0$, though they
are not everywhere zero. This would violate the definition of the norm,
which returns zero iff passed the zero element (though it would be a
semi-norm). However, if we identify all zero a.e.\ functions as one
element---as in $L^0$---we avoid this issue.
\end{rmk}

\begin{defn}($L^p$ Space)
Next, we then define $L^p$ space, for $p\in[1,\infty]$ as
\begin{align*}
    \sL^p(\Omega,\sF,\mu)
    &:=\left\{f: \Omega \ra \R \;\big|\; || f||_p < \infty\right\} \\
    L^p(\Omega,\sF,\mu)
    &:= \sL^p(\Omega,\sF,\mu)/a.e.
\end{align*}
Note that this does no conflict with our earlier definition, where we
used $\sL(\Omega,\sF,\mu)$ or $\sL^1(\Omega,\sF,\mu)$ to denote the set
of all integrable functions on measure space $(\Omega,\sF,\mu)$ since
$f\in \sL^1(\Omega,\sF,\mu)$ implies that
$\int_\Omega |f| \; d\mu <\infty$, i.e. that $f$ is integrable.
\end{defn}

\begin{lem}
\label{lem:holder}
For any $a,b\geq 0$ and $p,q$ such that $\frac{1}{p}+\frac{1}{q}=1$, we
have
\begin{align*}
  ab
  \leq
  \frac{1}{p} a^p + \frac{1}{q} b^q
\end{align*}
\end{lem}
\begin{proof}
Suppose $a,b>0$.  Since the log is a concave function, for
$\alpha\in(0,1)$, we have
\begin{align*}
  \ln(\alpha x + (1-\alpha)y)
  &\geq
  \alpha \ln(x) + (1-\alpha)ln(y) \\
  \text{Exponentiating} \qquad
  \alpha x + (1-\alpha)y
  &\geq
  e^{\alpha \ln(x) + (1-\alpha)ln(y)} \\
  \Rightarrow\qquad
  \alpha x + (1-\alpha)y
  &\geq
  x^\alpha y^{1-\alpha}
\end{align*}
Then take $\alpha =\frac{1}{p}$. By assumption $1-\alpha=\frac{1}{q}$.
Also take $x=a^p$ and $y=b^q$ to get
\begin{align*}
  \frac{1}{p} a^p + \frac{1}{q} b^q
  &\geq
  ab
\end{align*}
That takes care of $a,b>0$, and for $a,b$ possibly zero, this inequality
obviously still holds, so we've established the result.
\end{proof}


\begin{thm}
\label{thm:holder}
\emph{(Holder's Inequality)}%\footnote{Hodor.}
Suppose $p,q\in[1,\infty]$ such that
\begin{align*}
  \frac{1}{p}
  +
  \frac{1}{q}
  =1
\end{align*}
For any measurable functions $f,g$, we have
\begin{align*}
  \int_\Omega |fg| \; d\mu \leq || f||_p \cdot || g||_{q}
\end{align*}
\end{thm}
\begin{rmk}
This a generalization of Cauchy-Schwarz, which is a special case where
$p=q=2$.
\end{rmk}
\begin{proof}
Throughout this proof, we assume that both $||f||_p$ and $||g||_q$ are
finite, otherwise, the statement holds trivially.

First, suppose $p=1$. Then $q=\infty$, and to have $g\in L^{q}$, it must
be that $g$ is bounded, i.e. there is an $M$ such that $|g|<M$ for all
$x\in\Omega$. So
\begin{align*}
    \int_\Omega |fg| \; d\mu \leq
    M\int_\Omega |f| \; d\mu \leq
    || g||_\infty\int_\Omega |f| \; d\mu
    =
    || g||_\infty|| f||_1
\end{align*}
The case where $p=\infty$ is the same.
\\
\\
Now suppose $p\in(1,\infty)$.
Start by normalizing
\begin{align*}
    f_0 = \frac{|f|}{|| f||_p}
    \quad
    g_0 = \frac{|g|}{|| g||_{q}}
    \quad \Rightarrow\quad
    || f_0||_p = || g||_{q}=1
\end{align*}
By Lemma~\ref{lem:holder},
\begin{align*}
  |f_0 \cdot g_0|=|f_0|\cdot|g_0|\leq
  \frac{1}{p} |f_0|^p
  +\frac{1}{q} |g_0|^{q}
\end{align*}
We can now integrate the last expression to get
\begin{align*}
    \int_\Omega
    |f_0\cdot g_0|\;d\mu&\leq
    \frac{1}{p}
    \int_\Omega |f_0|^p \;d\mu
    +\frac{1}{q}\int_\Omega |g_0|^{q}\;d\mu \\
    \Leftrightarrow\quad
    \frac{1}{|| f||_p \cdot || g||_{q}}
    \int_\Omega
    |f g|\;d\mu
    &\leq
    \frac{1}{p}\left(|| f_0||_p\right)^p
    +\frac{1}{q}\left(|| g_0||_{q}\right)^{q}
\end{align*}
But note that we can simplify the RHS of the inequality since we
normalized $f_0$ and $g_0$ to
\begin{align*}
    \frac{1}{|| f||_p \cdot || g||_{q}}
    \int_\Omega
    |f g|\;d\mu
    &\leq
    \frac{1}{p} + \frac{1}{q}=1\\
    \Rightarrow\quad
    \int_\Omega
    |f g|\;d\mu
    &\leq
    {|| f||_p \cdot || g||_{q}}
\end{align*}
\end{proof}

\begin{cor}
If $f\in L^p$ and $g\in L^{q}$, then $fg\in L^1$
\end{cor}

\begin{cor}
Suppose that $\mu(\Omega)<\infty$ and $q\leq p<\infty$. Then
\begin{align*}
  L^p(\Omega,\sF,\mu)
  \subseteq
  L^q(\Omega,\sF,\mu)
\end{align*}
\end{cor}
\begin{rmk}
There are no inclusion relations between $L^p$ spaces when
$\Omega=\mathbb{R}$ and $\mu$ is the Lebesgue measure. We obviously
can't apply the last theorem since $\mu(\mathbb{R})$ is infinite, but to
get some better inuition as to why there isn't inclusion, consider
functions like $1/x^2$ and $1/\sqrt{x}$ which behave very differently in
their tails and at $0$.
\end{rmk}

\begin{cor}
Theorem~\ref{thm:holder} gives equality when
$|f|=\lambda |g|^{\frac{q}{p}}$, hence
\begin{align*}
    \sup_{\lvert g\rvert_{q}=1}
    \int_\Omega fg \; d\mu = \lvert f\rvert_p
\end{align*}
\end{cor}

\begin{prop}
For all $(\Omega,\sF,\mu)$, we have for all $p\in[1,\infty]$
\begin{align*}
  L^1 \cap L^\infty \subseteq L^p \subseteq L^1+L^\infty
\end{align*}
where the largest set in the relation above is the set of all functions
that are the sum $f+g$ where $f\in L^1$ and $g\in L^\infty$.
\end{prop}

\begin{thm}\emph{(Minkowski Inequality)}
\label{thm:minkowski}
Given $f,g \in L^p(\Omega,\sF,\mu)$, we have
\begin{align*}
  ||f+g||_p \leq ||f||_p + ||g||_p
\end{align*}
In other words, the triangle inequality holds on $L^p$ space.
\end{thm}
\begin{proof}
($p=\infty$)
Since $|| \cdot||_\infty$ is just the essential supremum, by definition
\begin{align*}
    |f|&\leq || f||_\infty
    \quad \text{$\mu$-a.e.}\\
    |g|&\leq || g||_\infty
    \quad \text{$\mu$-a.e.}
\end{align*}
So then for almost all $\omega$, since
$|f(\omega)+g(\omega)|\leq |f(\omega)|+|g(\omega)|$, we have
\begin{align*}
    |f(\omega)+g(\omega)|\leq || f||_\infty
    + || g||_\infty
\end{align*}
Since this is true for all $\omega\in\Omega$, we can take the essential
sup over the lefthand side to get
\begin{align*}
    || f+g||_\infty
    \leq || f||_\infty
    + || g||_\infty
    \quad \text{$\mu$-a.e.}
\end{align*}
($p\in[1,\infty)$)
Next, suppose $p\in[1,\infty)$. Also, assume without loss of
generality that $f\neq 0$ and $g\neq 0$ (where, again, equality is
almost-everywhere equality).
Then start by writing
\begin{align}
  \left(||f+g||_p\right)^p
  = \int_\Omega |f+g|^p \; d\mu
  &= \int_\Omega |f+g| \cdot |f+g|^{p-1}\; d\mu \notag \\
  &\leq
  \int_\Omega \left(|f|+|g| \right)\cdot |f+g|^{p-1}\; d\mu \notag\\
  \implies\quad
  \left(||f+g||_p\right)^p
  &\leq
  \int_\Omega |f|\cdot |f+g|^{p-1}\; d\mu
  +
  \int_\Omega |g| \cdot |f+g|^{p-1}\; d\mu
  \label{fplusg}
\end{align}
Next, notice that $\frac{1}{p} +\frac{1}{p/(p-1)}=1$. So we can use
Holder's Inequality, Theorem~\ref{thm:holder}, to say
\begin{align*}
  \int_\Omega |f|\cdot |f+g|^{p-1}\; d\mu
  &\leq
  ||f||_p \cdot
  \big|\big|\;|f+g|^{p-1}\;\big|\big|_{\frac{p}{p-1}} \\
  &=
  ||f||_p \cdot
  \left[
    \int_\Omega \left(|f+g|^{p-1}\right)^\frac{p}{p-1}
  \right]^\frac{p-1}{p} \\
  &=
  ||f||_p \cdot
  \left[
    \int_\Omega |f+g|^p
  \right]^\frac{p-1}{p} \\
  &=
  ||f||_p \cdot
  \left(||f+g||_p\right)^{p-1}
\end{align*}
Similarly for the other integral
\begin{align*}
  \int_\Omega |g|\cdot |f+g|^{p-1}\; d\mu
  &\leq
  ||g||_p \cdot
  \left(||f+g||_p\right)^{p-1}
\end{align*}
Therefore, we can rewrite Inequality~\ref{fplusg} as
\begin{align*}
  \left(||f+g||_p\right)^p
  &\leq
  ||f||_p \cdot
  \left(||f+g||_p\right)^{p-1}
  +
  ||g||_p \cdot
  \left(||f+g||_p\right)^{p-1} \\
  &\leq
  \left(||f||_p + ||g||_p \right)
  \left(||f+g||_p\right)^{p-1} \\
  \Rightarrow\quad
  ||f+g||_p
  &\leq
  ||f||_p + ||g||_p
\end{align*}
\end{proof}
%Then define
%\begin{align*}
    %f_0 := \frac{|f|}{|| f||_p}
    %\qquad
    %g_0 := \frac{|g|}{|| g||_p}
    %\qquad
    %\quad\Rightarrow\quad
    %|| f_0||_p
    %=|| g_0||_p
    %=1
%\end{align*}
%Then, if we look pointwise and let $C=|| f||_p+||
%g||_p$ we have that
%\begin{align}
    %|f+g|^p&\leq \left(|f|+|g|\right)^p\notag\\
    %&\leq C^p\left(f_0\frac{|| f||_p}{C}
    %+ g_0\frac{|| g||_p}{C}\right)^p
    %\label{convstep}
%\end{align}
%Now let's consider the beast inside the parentheses, raised to the
%$p$th power. The term inside is mapped $t\mapsto t^p$, which is
%convex function for $p\in[1,\infty)$. Now we can use this convexity
%towards our purpose if we rewrite the whole term, a little more
%suggestively, using the definition of $C$:
%\begin{align*}
    %\left(f_0\frac{|| f||_p}{C}
    %+ g_0\frac{|| g||_p}{C}\right)^p
    %=
    %\left(\alpha f_0
    %+ (1-\alpha)g_0\right)^p
    %\qquad \text{where}\quad
    %\alpha=\frac{|| f||_p}{|| f||_p
    %+|| g||_p }
%\end{align*}
%Finally, given a convex function written in this way, we know that
%pointwise
%\begin{align*}
    %\left(\alpha f_0
    %+ (1-\alpha)g_0\right)^p
    %\leq
    %\left(\alpha f_0^p
    %+ (1-\alpha)g_0^p\right)
%\end{align*}
%Alright, almost there. Using this back in Inequality \ref{convstep},
%we get an expression, which we'll then integrate:
%\begin{align*}
    %|f+g|^p  &\leq
    %C^p\left(f_0^p\cdot\frac{|| f||_p}{C}
    %+ g_0^p\cdot\frac{|| g||_p}{C}\right)\\
    %\Rightarrow\quad
    %\int_\Omega|f+g|^p \;d\mu &\leq \int_\Omega
    %C^p\left(f_0^p\cdot\frac{|| f||_p}{C}
    %+ g_0^p\cdot\frac{|| g||_p}{C}\right)\;d\mu \\
    %&\leq C^p
    %\left[
    %\int_\Omega f_0^p\cdot\frac{|| f||_p}{C}
    %\;d\mu+ \int_\Omega
    %g_0^p\cdot\frac{|| g||_p}{C}\;d\mu
    %\right]
%\end{align*}
%But notice that $C = ||f||_p + ||g||_p$ implies
%\begin{align*}
  %\frac{||g||_p}{C}
  %=
  %\frac{||g||_p}{||f||_p + ||g||_p}
  %= 1 -
  %\frac{||f||_p}{||f||_p + ||g||_p}
  %= 1 -
  %\frac{||f||_p}{C}
%\end{align*}
%So set $\alpha = \frac{||f||_p}{||f||_p + ||g||_p}$ and simplify the
%above inequality
%\begin{align*}
    %\int_\Omega|f+g|^p \;d\mu
    %&\leq
    %C^p \int_\Omega \alpha f_0^p \;d\mu
    %+ C^p\int_\Omega (1-\alpha) g_0^p \; d\mu
%\end{align*}


\begin{thm}
Provided $p\in[1,\infty)$, $|| f+g||_p=|| f||_p+|| g||_p$ if and only if
$f=\lambda g$ for some $\lambda\in \mathbb{R}$.
\end{thm}

\begin{prop}
$L^p(\Omega,\sF,\mu)$ is a Banach space---a complete normed vector
space.
\end{prop}
\begin{proof}
We check the properties that a normed vector space must have
\begin{enumerate}
  \item First, it's clear that $|| f||_p=0$ if and only if
    $f=0$. (Recall that $f=0$ means that $f(x)=0$ for almost all
    $x\in\Omega$. It doesn't need to be the constant zero function.)
  \item Next, it's obvious that $|| cf||_p = c|| f||_p$.
  \item Finally, for the triangle inequality, the Minkowski Inequality,
    Theorem~\ref{thm:minkowski}, does the trick.
\end{enumerate}
\end{proof}

\begin{thm}
$L^2$ is a Hilbert space under inner product
\begin{align*}
  \langle f,g \rangle
  := \int_\Omega f\cdot g \; d\mu
\end{align*}
\end{thm}

\clearpage
\section{Probability Theory with Random Variables}

\subsection{Cumulative Distribution Functions}

Next we take up the following problem: You have the measurable space
$(\R,\sB(\R))$, how do you come up with a sensible probability measure
for it? We'll first define a cumulative distribution function (CDF) and
show that given a probability measure on $(\R,\sB(\R))$, you can
construct a CDF. Then, we'll show that the \emph{converse} is also true:
Any function satisfying the necessary properties to be called a CDF
\emph{implies} a probability measure on $(\R,\sB(\R))$. Moreover,
that implied probability measure will be \emph{uniquely} specified.

Therefore, cumulative distributions are a really handy way to introduce
measures on $(\R,\sB(\R))$. In later sections, we'll also see the close
relationship between CDFs and random variables.

\begin{defn}(Cumulative Distribution Function)
\label{defn:cdf}
A function $F:\R\ra\R$ is called a
\emph{cumulative distribution function} $F$ on $\R$ if it satisfies the
following properties:
\begin{enumerate}
  \item Non-decreasing
  \item Right continuous
  \item $\lim_{x\ra-\infty} F(x)=0$ and $\lim_{x\ra+\infty}=1$.
\end{enumerate}
Right rather than left continuity because only the former makes sense
for distributions with mass points---we can interpret the jump size as
the probability of point $x$ occuring.
\end{defn}

\begin{prop}\emph{(Prob. Measure on $(\R,\sB(\R))$ to CDF)}
Given prob. measure $P$ on $(\R,\sB(\R))$, define
\begin{align*}
  F(x) := P\big[(-\infty,x]\big]
  \qquad x\in\R
\end{align*}
which is well defined since $(-\infty,x]\in\sB(\R)$.
This is a cumulative distribution function.\footnote{%
  $(-\infty,x)$, $(x,\infty)$, $[x,\infty)$ also work, but $(-\infty,x]$
  is standard, see remark above.
}
\end{prop}
\begin{proof}
We check that it satisfies the properties in Definition~\ref{defn:cdf}
(except 3, which is obvious):
\begin{enumerate}
  \item Suppose $y \geq x$. Then by monoticity of the underlying
    probability measure $P$:
    \begin{alignat*}{3}
      (-\infty,x]\subseteq (-\infty,y]
      &\quad\implies\quad
      &P\big[(-\infty,x]\big]
      &\leq
      P\big[(-\infty,y]\big] \\
      &\quad\iff\quad
      &F(x) &\leq F(y)
    \end{alignat*}

  \item We can write $(-\infty,x]$ as the intersection of a decreasing
    sequence of sets
    \begin{align}
      (-\infty,x] = \bigcap\ninf \left(-\infty, \; x+1/n\right]
      \label{ascap}
    \end{align}
    Since $P$ is a probability measure, it is continuous from above,
    so for that decreasing sequence of sets, we have
    \begin{align*}
      P\left[ \bigcap\ninf \left(-\infty, \; x+1/n\right]
      \right]
      &=\limn P\big[ \left(-\infty, \; x+1/n\big]
      \right]
    \end{align*}
    So by Statement~\ref{ascap}, we have
    \begin{align*}
      P\big[(-\infty,x]\big]
      &=\limn P\big[ \left(-\infty, \; x+1/n\right]\big] \\
      \iff \quad
      F(x)
      &=\limn F(x+1/n)
    \end{align*}
    Hence, $F$ is right continuous.
\end{enumerate}
\end{proof}

%\begin{cor}
%A cumulative distribution function uniquely determines the underlying
%probability measure $P$.
%\end{cor}
%\begin{proof}
%The CDF $F(x)$ can be directly mapped back into
%$P\big[(-\infty,x]\big]$. From there, apply Theorem~\ref{thm:PQunq}.
%\end{proof}

The next statement is very important. In terms of the discussion above,
this next statement says that writing down $F$ will uniquely determine a
probability measure on measurable space $(\R,\sB(\R))$.

\begin{prop}\emph{(CDF to Prob. Measure on $(\R,\sB(\R))$)}
Let $F:\R\ra\R$ be a cumulative distribution function. There exists a
\emph{unique} probability measure $P_F$ on measurable space
$(\R,\sB(\R))$ such that
\begin{align*}
  P_F\big[(a,b]\big] = F(b)-F(a)
\end{align*}
\end{prop}
\begin{proof}
Let $S$ denote the set of all intervals
\begin{align*}
  S = \left\{ (a,b] \cap \R \;|\; a,b\in\bar{\R}\right\}
\end{align*}
This is precisely the set that generates the Borel $\sigma$-algebra.
It's clearly a semiring, and the measure $P_F$ is defined for all
elements of $S$.
Moreover, the measure $P_F$ is clearly $\sigma$-finite. Therefore,
by Caratheodory's Theorem~\ref{thm:cara}, it has a unique extension to
$\sigma(S)=\sB(\R)$.
\end{proof}


\begin{defn}
We define an operator that will be useful in the next definition:
\begin{align*}
  \Delta_{a_i,b_i} F(x_1,\ldots,x_n)
  &:= F(x_1,\ldots,b_i,\ldots,x_n)
  - F(x_1,\ldots,a_i,\ldots,x_n)
\end{align*}
\end{defn}

\begin{defn}(Cumulative Distribution Function on $\R^n$)
\label{defn:F}
Let $F:\Rn\ra[0,1]$ be a function satisfying the following properties
\begin{enumerate}
  \item
    We have
    \begin{align}
      \Delta_{a_1,b_1} \cdots
      \Delta_{a_n,b_n}
      F(x_1,\ldots,x_n) \geq 0
      \label{deltas}
    \end{align}
    where the lefthand side denotes the successive application of the
    $\Delta_{a,b}$ operator to each coordinate, from the $n$th to the
    first.

  \item Right continuity: $F(x_k) \ra F(x)$ from above for any
    $x_k \ra x$ from above.

  \item $\lim_{x^k \ra \infty} F(x^k)=1$ where $x^k\ra \infty$ denotes
    that every coordinate $x^k_i \ra \infty$ for all $i$.

  \item $\lim_{x^k_i\ra-\infty} F(x^k) = 0$ holds for each $i$. Note
    that the other coordinates need not be converging to $-\infty$.
\end{enumerate}
\end{defn}
\begin{rmk}
To understand Inequality~\ref{deltas} in the first requirement, if the
domain of $F$ is simply $\R$, the requirement reduces to
\begin{align*}
  F(b) - F(a) \geq 0
\end{align*}
which is the usual requirement that $F$ is nondecreasing. For domain
$\R^2$, it reduces to
\begin{align*}
  \Delta_{a_1,b_1}
  \Delta_{a_2,b_2}
  F(x_1,x_2)
  &=
  \Delta_{a_1,b_1}
  \left[F(x_1,b_2)-F(x_1,a_2)\right] \\
  &=
  \left[F(b_1,b_2)-F(b_1,a_2)\right]
  -
  \left[F(a_1,b_2)-F(a_1,a_2)\right] \\
  &=
  F(b_1,b_2)-F(b_1,a_2)
  - F(a_1,b_2)+F(a_1,a_2)
\end{align*}
Therefore, the requirement states
\begin{align*}
  F(b_1,b_2)-F(b_1,a_2)
  - F(a_1,b_2)+F(a_1,a_2)
  \geq 0
\end{align*}
This is the $\R^2$ analog of the ``$F$ nondecreasing'' assumption.

To visualize, consider the following figure. We want calculate the
area of the box---the probability that something lands in the box. We
want that guy to be positive. Rather than compute the area by taking the
produce of the sides of the rectangle (which would build in
independence), we compute the area by starting with the area of the
whole box (the gray shaded box plus all cross-hatched boxes),
subtracting off all northwest hatched areas, subtracting off all
northeast hatched areas, then adding back in the double-hatched area
(since we subtracted it twice).
\begin{figure}[htpb!]
\centering
\begin{tikzpicture}[yscale=1.0, xscale=1.5]
  \coordinate (zzzz) at (-1,-1);
  \coordinate (a1zz) at ( 2,-1);
  \coordinate (zza2) at (-1, 2);
  \coordinate (b1zz) at ( 4,-1);
  \coordinate (zzb2) at (-1, 4);
  \coordinate (b1b2) at ( 4, 4);
  \coordinate (a1b2) at ( 2, 4);
  \coordinate (b1a2) at ( 4, 2);
  \coordinate (a1a2) at ( 2, 2);

  % Boxes
  \draw[] %
    (zzb2) -- (b1b2) -- (b1zz) -- (zzzz) -- (zzb2);
  \draw[pattern=my north west lines, line space=10pt] %
    (zza2) -- (b1a2) -- (b1zz) -- (zzzz) -- (zza2);
  \draw[pattern=my north east lines, line space=10pt] %
    (zzb2) -- (a1b2) -- (a1zz) -- (zzzz) -- (zzb2);
  \draw[color=d4gray, fill=d4gray, opacity=0.5] %
    (a1a2) -- (a1b2) -- (b1b2) -- (b1a2);

  % Axis
  \draw[d4black, ultra thick, <->] (-1,0) -- (5,0);
  \draw[d4black, ultra thick, <->] (0,-1) -- (0,5);
\end{tikzpicture}
\end{figure}
\end{rmk}

\begin{prop}
\emph{(From CDF to Probability Measure on $(\R^n,\sB(\R)^{\otimes n})$)}
Let $S$ denote the set of all hypercubes
\begin{align*}
  S = \left\{ (a_1,b_1]\times\cdots\times(a_n,b_n]
  \;|\; a_i\in\bar{\R}, \; b_i\in\R\right\}
\end{align*}
Given a function $F$ satisfying Definition~\ref{defn:F}, there is a
unique probability measure on $(\R^n,\sB(\R)^{\otimes n})$ such
that
\begin{align*}
  P\big[\, (a_1,b_1]\times\cdots\times(a_n,b_n] \,\big] =
  \Delta_{a_1,b_2}
  \cdots
  \Delta_{a_n,b_n}
  F(x_1,\ldots,x_n)
\end{align*}
\end{prop}
\begin{proof}
$S$ forms a semiring, and it is precisely the set that generates the
Borel $\sigma$-algebra $\sB(\R)^{\otimes n}$, Moreover, $P$ is
$\sigma$-finite. So by Caratheodory's Extension Theorem, $P$ defined
above on $S$ has a unique extension to $\sB(\R)^{\otimes n}$.
\end{proof}


\clearpage
\subsection{Distributions of Random Variables}


\begin{defn}(Prob. Distribution and Distribution Function of Random Variable $X$)
Consider measurable function $X:(\Omega,\sF,P)\ra (\R,\sB(\R))$.
If $P$ is a probability measure, then the push forward
measure $P \circ X^{-1}$ or $P_X$ is \emph{also} a probability measure
called the \emph{probability distribution of $X$}, where
\begin{align*}
  \int_\Omega (g\circ X)(\omega)\; dP(d\omega)
  = \int_\R g(x) \; dP_X(dx)
  \qquad \forall g:(\R,\sB(\R))\ra (\R,\sB(\R))
\end{align*}
Moreover, we call
\begin{align*}
  F_X(x)
  = P_X\big[(-\infty,x]\big]
  = P\big[X^{-1}\big((-\infty,x]\big)\big]
\end{align*}
the \emph{distribution function of $X$}.
\end{defn}

\begin{defn}
We call the random vector $X:(\Omega,\sF)\ra(\R^d,\sB(\R)^{\otimes d})$
on probability space $(\Omega,\sF,P)$
\begin{itemize}
  \item \emph{Discrete}: If $P\circ X^{-1}$ is a discrete measure
  \item \emph{Continuous}: If $P\circ X^{-1}$ is a continuous measure
  \item \emph{Absolutely Continuous}: If $P\circ X^{-1}$ is an
    absolutely continuous measure
\end{itemize}
\end{defn}

The above definitions are motivated as follows. I could write down
$\Omega$, the set of elementary events that could happen. I could then
specify the measurable sets $\sF$, which are events or subsets of
$\Omega$ whose probabilities I want to compute. I could then also
specify a measure $P$ that assigns probabilities to the measurable sets
in $\sF$. Finally, if I define a random variable $X$ mapping $\Omega$
into real numbers, I could (in theory) compute integrals corresponding
to functions of my random variable like
\begin{align*}
  \int_\Omega g\circ X \; dP
\end{align*}
But the problem is that I don't really know how to integrate over
$\Omega$. On the other hand, I might know how to integrate over $\R$,
the range of $X$. This, in fact, is part of the reason I defined $X$ in
the first place---so that I wouldn't have to work exclusively with the
more abstract measure space $(\Omega,\sF,P)$. So rather than integrate
over subsets of $\Omega$ (the domain of $X$ and $g\circ X$), I change
variables to integrate over subsets of $\R$ (the range of $X$), using
the preimage of $X$ to invoke implicitly the underlying probability
measure $P$:
\begin{align}
  \int_{X^{-1}(A)} g\circ X \; dP
  =
  \int_A g \; dP_X
  \qquad A \in \sB(\R)
  \label{omega-Px}
\end{align}
Yeah, right okay. But the RHS is still a general (non-Riemman or
Riemann-Stieltjes) integral. How do I compute it? And what does $P_X$
actually look like? There's no reason that it should be nice.
Plus, it was dependent upon writing down $(\Omega,\sF)$, some measure
$P$ on that measurable space, and a random variable
$X:(\Omega,\sF)\ra\R$---all to get to $P_X$ and a general integral over
$\R$.  That's all super abstract and not often how we work with
probability spaces.

As a result, we actually work in a different way, starting with $P_X$
and showing that it implies all of that other formal structure that
makes probability work---the measurable space, the probablity measure,
the random variable, etc. That's what the rest of this section will
concern itself with.

Specifically, we usually start by defining a cumulative distribution
function $F:\R\ra\R$, which is just a special type of function
satisfying certain properties. By satisfying those properties, the CDF
then implicitly represents some distribution $P_X$ that implies an
underlying measure space $(\Omega,\sF)$ and probability measure $P$.
This then allows us to construct the chain
\begin{align*}
  \int_\R g(x) \; F(dx)
  =
  \int_\R g(x) \; P_X(dx)
  =
  \int_\Omega g(X(\omega)) \; P(d\omega)
\end{align*}
While the second two integrals operate with measure $P_X$ and $P$---set
functions---the first integral is a Lebesgue integral (that might
specialize to a Riemann-Stieltjes integral) using function
$F$ from $\R$ to $\R$.  That's huge. Moreover, that will be
equivalent to computing the integral $\int_\Omega g \circ X \; dP$ on
some underlying probability space by Equation~\ref{omega-Px}.

Finally, if we write down a density function for a random variable, then
we can also get
\begin{align*}
  \int_\R g(x)f(x) \; dx
  =
  \int_\R g(x) \; F(dx)
  \qquad \text{where}\quad
  F(x) &= \int_{(-\infty,x]} f(x)\; dx \\
  P_X(B) &= \int_{B} f(x)\; dx \quad B\in\sB(\R)
\end{align*}
where the first and last integral use the Lebesgue measure (which might
specialize to a Riemann integral).

\begin{defn}
Let $X:(\Omega,\sF,P)\ra (\R,\sB(\R))$ and $P$ a probability measure,
Then define
\begin{alignat*}{3}
  \E[X] &= \int_\Omega X dP &&\forall X\in L^1\\
  \Var[X] &= \E[(X-\E[X])^2]  &&\forall X\in L^2\\
  \Cov[X,Y] &= \E[(X-\E[X])(Y-\E[Y])] \qquad&&\forall X\in L^2
\end{alignat*}
\end{defn}

\begin{prop}
Given measurable $X\in L^1(\Omega,\sF,P)$ and $P$ a probability measure,
\end{prop}

\clearpage
\subsection{Complex Random Variables}

\begin{prop}
Let $X\in\C^d$ be a $d$-dimensional complex random variable. Then
\begin{align*}
  \E[X] = \E[\Re(X)] + i\;\E[\Im(X)]
\end{align*}
\end{prop}

\begin{defn}
Let $Z = V+iW$, where $V$ and $W$ are real-valued integrable random
variables. Then we say that $Z$ is \emph{square integrable} if
$\E[|Z|^2]=\E[V^2 + W^2]<\infty$.
\end{defn}

\begin{prop}
Let $Z = V+iW$ and $Z' = V'+iW'$, where $V$, $W$, $V'$, and $W'$ are
real-valued integrable random variables. Then
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $\E[Z] = \E[V] + i\E[W]$
  \item $\E[\,|Z|\,] = \E\left[\sqrt{V^2 + W^2}\right] \leq
    \E\big[|V|+|W|\big]<\infty$
  \item $\big|\E[Z]\big|\leq \E[\,|Z|\,]$
  \item $Z$ and $Z'$ are independent if $\sigma(V,W)$ is independent of
    $\sigma(V',W')$
  \item If $Z$ and $Z'$ are independent and square integrable, then
    \begin{align*}
      \E[Z\cdot Z'] = \E[Z]\cdot\E[Z']
    \end{align*}
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn:
\begin{enumerate}
  \item[(iii)]
    First, we'll want a clever way to write $\big|\E[Z]\big|$. To do so,
    we make use of the following result:
    \begin{align}
      |z| = \sup_{q\in\Q} \Re(e^{iq}z)
      \qquad\forall z = v+iw\in\C
      \label{absvalz}
    \end{align}
    To prove this claim, we expand out the object on the RHS:
    \begin{align*}
      \Re(e^{iq}z)
      &= \Re\big(
      \left(\cos(q) + i\sin(q)\right)
      \cdot
      \left(v + iw\right)
      \big) \\
      &=\cos(q)v - \sin(q)w
    \end{align*}
    Now we want the $q$ that maximizes this object. Since everything is
    differentiable, the first order necessary condition for an optimum
    is
    \begin{align*}
      0 &=-\sin(q)v - \cos(q)w \\
      -\frac{w}{v} &=
      \frac{\sin(q)}{\cos(q)} = \tan(q) \\
      \implies\quad
      q &= \arctan\left(-\frac{w}{v}\right)
    \end{align*}
    Using this in the expression for $\Re(e^{iq}z)$, we get
    \begin{align*}
      \sup_q
      \Re(e^{iq}z)
      &=
      \cos\left(\arctan\left(-\frac{w}{v}\right)\right)v -
      \sin\left(\arctan\left(-\frac{w}{v}\right)\right)w \\
      &=
      \frac{v}{\sqrt{1+\left(-\frac{w}{v}\right)^2}}
      -
      \frac{\left(-\frac{w}{v}\right)w}{\sqrt{1+\left(-\frac{w}{v}\right)^2}}
      \\
      &= \sqrt{v^2+w^2} \\
      \implies\quad
      \sup_q
      \Re(e^{iq}z)
      &= |z|
    \end{align*}
    We will be able to apply this to $\big|\E[Z]\big|$, since that's
    just a complex number in $\C$ like $z$ above. But first start by
    fixing $q\in\Q$ and use linearity of the expectation operator along
    with (i) to get
    \begin{align*}
      \Re\left(e^{iq}\E[Z]\right)
      &= \Re\left(\E[e^{iq}Z]\right)
      = \E\left[\Re(e^{iq}Z)\right]
    \end{align*}
    Now take the supremum over $q\in\Q$ on both sides to get
    \begin{align*}
      \sup_{q\in\Q}\Re\left(e^{iq}\E[Z]\right)
      &= \sup_{q\in\Q}\E\left[\Re(e^{iq}Z)\right] \\
      \text{By (\ref{absvalz})}\quad\implies\quad
      \big|\E[Z]\big|
      &= \sup_{q\in\Q}\E\left[\Re(e^{iq}Z)\right]
    \end{align*}
    Next, by Jensen since the supremum is a convex function, we get
    \begin{align*}
      \big|\E[Z]\big|
      &\leq \E\left[\sup_{q\in\Q}\Re(e^{iq}Z)\right] \\
      \text{By (\ref{absvalz}) again}\quad\implies\quad
      \big|\E[Z]\big|
      &\leq \E\left[\,|Z|\,\right]
    \end{align*}

\end{enumerate}
\end{proof}


\clearpage
\subsection{Characteristic Functions}



\begin{defn}(Characteristic Function)
Let $X$ be a $d$-dimensional random vector on probability space
$(\Omega,\sF,P)$ with CDF
\begin{align*}
  F_{X}(x_1,\ldots,x_d)
  =
  P[X_1\leq x_1,\ldots, X_d\leq x_d]
\end{align*}
Then the \emph{characteristic function} is the function
$\varphi_X:\R^d\ra \C$ defined
\begin{align*}
  \varphi_X(u)
  &= \E\left[e^{iu^TX}\right]
  = \E\left[\cos(u^TX)\right]+i\,\E\left[\sin(u^TX)\right]\\
  &= \int_{\R^d} e^{iu^TX} \; dF_{X}
  \qquad\text{where}\quad u^T X = \sum_{i=1}^d u_iX_i
\end{align*}
If $X$ is absolutely continuouse with density $f_{X}(x)$, we
can write
\begin{align*}
  \varphi_X(u)
  &=
  \int_{\R^d} e^{iu^TX} f_{X}(x)\; dx
\end{align*}
A few facts about characteristic functions:
\begin{enumerate}[label=(\roman*)]
  \item The characteristic function is the Fourier transform of a
    measure.
  \item They always exist, even for fat-tailed distributions. That's why
    we prefer them to moment generating functions
  \item Characteristic functions \emph{uniquely} determine distributions
  \item Convergence in distribution is equivalent to pointwise
    convergence of characteristic functions
\end{enumerate}
\end{defn}

\begin{thm}
\label{thm:charfcnunq}
\emph{(Characteristic Functions Uniquely Characterize the Distribution)}
\end{thm}

\begin{prop}\emph{(Properties of Characteristic Functions)}
Let $X$ be a random vector and $\varphi_X(u)$ the associated
characteristic function. Then
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $|\varphi_X(u)| \leq \varphi_X(0) = 1$
  \item $\varphi_X(u) = \overline{\varphi_X(-u)}$
  \item $\varphi_X(u)$ is real-valued if and only if $X$ is symmetric,
    i.e. $X \overset{d}{=} -X$
  \item $\varphi_X(u)$ is uniformly continuous in $u$
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn
\begin{enumerate}[label=(\roman*)]
  \item Recall that for complex $Z$, we have
    $\big|\E[Z]\big|\leq \E[\,|Z|\,]$. Well
    \begin{align*}
      |\varphi_X(u)|
      =
      \left\lvert\E\left[e^{iu^TX}\right]\right\rvert
      &\leq
      \E\left[\left\lvert e^{iu^TX}\right\rvert\right]
      = 1
    \end{align*}
    And also, rather trivially, $\varphi_X(0)=\E[e^{0}] = 1$

  \item Write out the complex conjugate:
    \begin{align*}
      \overline{\varphi_X(-u)}
      &=
      \overline{\E\left[e^{-iu^TX}\right]}
      =
      \E\left[\,\overline{e^{-iu^TX}}\,\right]
      =
      \E\left[e^{iu^TX}\right]
      =
      \varphi_X(u)
    \end{align*}

  \item
    ($\Rightarrow$)
    Suppose that $\varphi_X(u)$ is real-valued. To show $X$ is
    symmetric, we need to show $-X$ has the same distribution, i.e. the
    same characteristic function by Theorem~\ref{thm:charfcnunq}. So
    let's examine the characteristic function of $-X$:
    \begin{align*}
      \varphi_{-X}(u)
      =
      \E\left[e^{-iu^TX}\right]
      &=
      \overline{\E\left[e^{iu^TX}\right]} \\
      &=
      \overline{\varphi_X(u)} \\
      \text{Since $\varphi_X(u)$ real-valued}
      \quad\implies\quad
      \varphi_{-X}(u)
      &= \varphi_X(u)
    \end{align*}
    And by Theorem~\ref{thm:charfcnunq}, identical characteristic
    functions means identical distributions.

    ($\Leftarrow$)
    Suppose that $X$ is symmetric so that $X\overset{d}{=}-X$. Then
    again, by Theorem~\ref{thm:charfcnunq}, we have
    have
    \begin{align}
      \varphi_X(u)&=\varphi_{-X}(u) \notag \\
      \iff\quad
      \E\left[e^{iu^TX}\right]
      &= \E\left[e^{-iu^TX}\right]
      \label{charfcnsymmetric}
    \end{align}
    Well then since $\Im(z) = (z - \bar{z})/2i$, we get
    \begin{align*}
      \Im(\varphi_X(u))
      &=
      \frac{\varphi_X(u)-\overline{\varphi_X(u)}}{2i} \\
      &=
      \frac{\E\left[e^{iu^TX}\right]-\E\left[e^{-iu^TX}\right]}{2i} \\
      \text{By (\ref{charfcnsymmetric})}
      \quad\implies\quad
      \Im(\varphi_X(u))
      &=
      0
    \end{align*}
    Hence $\varphi_X(u)$ is real-valued.

  \item To prove uniform continuity of $u$, we need to be able to pick a
    $\delta$ given $\varepsilon$ regardless of $u$. So consider
    \begin{align*}
      \left\lvert
      \varphi_X(u+\delta) - \varphi_X(u)
      \right\rvert
      &=
      \left\lvert
      \E\left[e^{i(u^T+\delta)X}\right] - \E\left[e^{iu^TX}\right]
      \right\rvert \\
      &=
      \left\lvert
      \E\left[(e^{i\delta X}-1)e^{iu^TX}\right]
      \right\rvert \\
      &\leq
      \E\bigg[
      \left\lvert
      e^{i\delta X}-1
      \right\rvert
      \cdot
      \underbrace{\left\lvert e^{iu^TX} \right\rvert}_{\leq 1}
      \bigg]
      \\
      &\leq
      \E\big[
      \left\lvert
      e^{i\delta X}-1
      \right\rvert
      \big]
    \end{align*}
    Then by Lebesgue's DCT, that goes to zero as $\delta\ra 0$.
\end{enumerate}
\end{proof}

\begin{thm}
Let $X$ be a random vector. Then
\begin{enumerate}[label=\emph{(\roman*)}]
  \item If $\E[|X|^n]<\infty$ for some $n\in\N$, then
\end{enumerate}
\end{thm}


\clearpage
\subsection{Conditional Expectation and Probability}

\begin{defn}(Conditional Expectation)
Let $(\Omega,\sF,P)$ be a probability space and $X\in L^1(\Omega,\sF,P)$
with $\sG\subseteq\sF$ a sub-$\sigma$-algebra. Define the
\emph{conditional expectation} $Y=\E[X|\sG]$ as a random variable such
that
\begin{align*}
  \E[1_AY] = \E[\one{A}X]
  \qquad \forall A\in \sG
\end{align*}
If $Z$ is another random variable on $(\Omega,\sF,P)$, then we use the
shorthand
\begin{align*}
  \E[X|Z]
  \quad\iff\quad
  \E[X|\sigma(Z)]
\end{align*}
with the former being the shorthand notation and the latter expression
representing the most precise meaning (since the conditional
expectation is only defined for a $\sigma$-algebra).
\end{defn}

\begin{prop}
The conditional expectation \emph{exists} and is \emph{unique} up to
$P$-a.e.\ equality for any $X\in L^1(\Omega,\sF,P)$.
\end{prop}

\begin{defn}(Conditional Probability)
Given probability space $(\Omega,\sF,P)$, event $A\in\sF$, and
sub-$\sigma$-algebra $\sG\subseteq\sF$, define
\begin{align*}
  P[A|\sG] := \E[\one{A}|\sG]
\end{align*}
\end{defn}

\begin{prop}\emph{(Properties of the Conditional Expectation)}
Let $(\Omega,\sF,P)$ be a probability space and
$X,Y\in L^1(\Omega,\sF,P)$ with $\sG\subseteq\sF$ a
sub-$\sigma$-algebra. Then
\begin{enumerate}[label=(\roman*)]
  \item If $X$ is $\sG$ measurable, then $\E[X|\sG]=X$.
  \item Tower Property: For any sub-$\sigma$-algebra $\sH\subseteq\sG$,
    we have
    \begin{align*}
      \E\big[
        \,\E[X|\sG]\,
        \big|
        \sH
      \big]
      = \E[X|\sH]
    \end{align*}

  \item For $X$ independent of $\sG$, we have $\E[X|\sG]=\E[X]$.

  \item $X \geq Y$ $P$-a.s. implies that $\E[X|\sG]\geq\E[Y|\sG]$
    $P$-a.s.
  \item Conditional Jensen: For a convex function $\varphi:\R\ra \R$ suh
    such that $\varphi(X)\in L^1$, we have
    \begin{align*}
      \E[ \varphi(X) |\sG]
      \geq \varphi\left(\E[X|\sG]\right)
      \quad \text{a.s.}
    \end{align*}
    For a concave function, the inequality flips.

  \item $\lVert X\rVert_p\geq \lVert\,\E[X|\sG]\,\rVert_p$ (conditional
    expectation can only decrease in $p$-norm relative to the original
    $X$).
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn
\begin{enumerate}[label=(\roman*)]
  \item
  \item
  \item Here, we can apply conditional Jensen since $|\,\cdot\,|^p$ is
    a convex function so that
    \begin{align*}
      \E[\lvert X\rvert^p\,|\,\sG]
      &\geq
      \big\lvert\,\E[X|\sG]\,\big\rvert^p
    \end{align*}
    Now take the uconditional expectation of each side:
    \begin{align*}
      \E\big[
      \E[\lvert X\rvert^p\,|\,\sG]
      \big]
      &\geq
      \E\big[
      \lvert\,\E[X|\sG]\,\rvert^p
      \big] \\
      \iff\quad
      \E\left[ \lvert X\rvert^p\right]
      &\geq
      \E\big[
      \lvert\,\E[X|\sG]\,\rvert^p
      \big]
    \end{align*}
    But then raise each side to the power $1/p$ and recongnize that
    \begin{align*}
      \E\left[ \lvert X\rvert^p\right]^{\frac{1}{p}}
      &\geq
      \E\big[
      \lvert\,\E[X|\sG]\,\rvert^p
      \big]^{\frac{1}{p}} \\
      \iff\quad
      \lVert X \rVert_p
      &\geq
      \lVert \, \E[X|\sG] \,\rVert_p
    \end{align*}
\end{enumerate}
\end{proof}

\begin{defn}
Let $X$ be a $\bar{\R}$-valued random variable on $(\Omega,\sF,P)$ and
let $\sG\subseteq \sF$ be a sub-$\sigma$-algebra. Then
\begin{enumerate}[label=(\roman*)]
  \item $\E[X^-]<\infty$, define
    \begin{align*}
      \E[X|\sG] = \limn \E[X\wedge n|\sG]
    \end{align*}
  \item $\E[X^+]<\infty$, define
    \begin{align*}
      \E[X|\sG] = \lim_{n\ra-\infty} \E[X\vee n|\sG]
    \end{align*}
\end{enumerate}
\end{defn}



\clearpage
\subsection{Independence and Borel-Cantelli}

\begin{defn}(Independence)
Let $(\Omega,\sF,P)$ be a probability space and let $J$ be a non-empty
indexing set. We define the following types of independence
\begin{enumerate}
  \item \emph{Events}: A family $\{A_\alpha\}_{\alpha\in J}$ is
    independent if
    \begin{align*}
      P[A_{i_1} \cap \cdots \cap A_{i_m}]
      = P[A_{i_1}]\times \cdots \times P[A_{i_m}]
    \end{align*}
    for every finite sequence of distinct elements
    $\{i_1,\ldots,i_m\}\subseteq J$.

  \item \emph{Family of $\sigma$-algebras}: A family of
    $\sigma$-algebras $\{\sF_\alpha\}_{\alpha\in J}$
    (where each $\sF_\alpha\subseteq \sF$ for all $\alpha\in J$)
    is independent if the family of events
    $\{A_\alpha\}_{\alpha \in J}$ is independent where
    $A_{\alpha}\in \sF_{\alpha}$ $\alpha\in J$.

    In other words, construct the family of events
    $\{A_\alpha\}_{\alpha \in J}$ by plucking out some
    $A_\alpha \in \sF_\alpha\subseteq \sF$ for all $\alpha \in J$. The
    resulting family of events must be independent according to the
    first definition above for the family
    $\{\sF_\alpha\}_{\alpha \in J}$ to be independent.

  \item \emph{Random Variables}: A family of random variables
    $\{X_\alpha\}_{\alpha\in J}$ is independent if
    $\{\sigma(X_\alpha)\}_{\alpha \in J}$ is independent.
\end{enumerate}
\end{defn}

\begin{prop}\emph{(Characterizations of Independence of Random Variables)}
Let $\{X_n\}\nN$ be a sequence of random variables on probability space
$(\Omega,\sF,P)$. Then the following are equivalent
\begin{enumerate}
  \item $X_1,\ldots,X_N$ independent (by the definition above)
  \item For all \textbf{bounded} Borel functions (i.e.
    $f_n:(\R,\sB(\R)) \ra (\R,\sB(\R))$ measurable), we have
    \begin{align*}
      \E\left[\prod\nN f_n(X_n)\right]
      =
      \prod\nN \E\left[f_n(X_n)\right]
    \end{align*}
  \item For any $u\in \R^N$, we have
    \begin{align*}
      \E\left[e^{iu^T X}\right]
      =
      \prod\nN \E\left[e^{iu_n X_n}\right]
    \end{align*}
    where $X := \begin{pmatrix} X_1 &\cdots & X_N \end{pmatrix}'$.
\end{enumerate}
\end{prop}
\begin{cor}
If $X,Y\in L^2$ are independent, then
\begin{align*}
  \E[XY] &= \E[X]\E[Y] \\
  \Corr[X,Y] &= 0
\end{align*}
\end{cor}
\begin{proof}
For fixed $m$, define
\begin{align*}
  X_m := \max\{\min\{X,m\}, -m\}
\end{align*}
For each $m$, the statement should hold. Use Lebesgue's convergence
theorem.
\end{proof}

\begin{defn}(Limsup and Liminf of Events)
Given a sequence of sets $\{A_n\}\ninf$, define the limsup of events
\begin{align*}
  \limsup A_n := \bigcap\ninf \bigcup_{m \geq n} A_m
\end{align*}
If $x\in \limsup A_n$, then for any $N$, there will be an $m>N$ such
that $x\in A_m$. This is like ``$x$ shows up infinitely often.''

Define the liminf of events
\begin{align*}
  \liminf A_n := \bigcup\ninf \bigcap_{m \geq n} A_m
\end{align*}
If $x\in \liminf A_n$, then for some $N$, we will have $x\in A_m$ for
all $m > N$. It shows up in all sets, with only finitely many
exceptions.
\end{defn}

\begin{lem}\emph{(Borel-Cantelli)}
Let $\{A_n\}\ninf$ be a sequence of events in probability space
$(\Omega,\sF,P)$. Then
\begin{align*}
  \sumninf P(A_n) < \infty
  \quad\implies\quad
  P\left(\bigcap\ninf \bigcup_{m\geq n} A_n\right) = 0
\end{align*}
and
\begin{align*}
  \begin{rcases}
    \{A_n\}\ninf \; \text{\emph{independent}}\\
    \sumninf P[A_n] = \infty
  \end{rcases}
  \quad\implies\quad
  P\left(\bigcap\ninf \bigcup_{m\geq n} A_n\right) = 0
\end{align*}
\end{lem}

\clearpage
\subsection{Gaussian Processes}

\begin{defn}
(Symmetric Positive Semi-definite and Positive Definite Functions)
Given indexing set $J$, the function $C:J^2 \ra \R$ where we define
$c_{st}:=c(s,t)$ is
\begin{enumerate}
  \item \emph{Symmetric}: $c_{st}=c_{ts}$ for all $s,t \in J$.
  \item \emph{Positive (Semi-)definite}: The matrix
    \begin{align*}
      (c_{t_i,t_j})^d_{i,j=1}
      &=
      \begin{pmatrix}
        c_{t_1,t_1} &\cdots & c_{t_1,t_d} \\
        \vdots &\ddots & \vdots \\
        c_{t_d,t_1} &\cdots & c_{t_d,t_d} \\
      \end{pmatrix}
      \in \R^{d\times d}
    \end{align*}
    is positive (semi-)definite for every finite sequence
    $\{t_1,\ldots,t_d\}$ of different elements of $J$.
\end{enumerate}
\end{defn}

\begin{rmk}
Any inner product in a Hilbert Space is positive semi-definite. So
rather than take a function $C$, fix a sequence
$\{i_1,\ldots,i_d\}\subseteq J$, and check the resulting matrix for
positive (semi)definiteness, just see if it's an inner product in a
Hilbert space.
\end{rmk}


\clearpage
\section{Random Variables in Practice}
\subsection{Transformation Theory}

Throughout this subsection, we'll only consider continuous random
variables. So let's start with $n$ random variables, $X_1, \ldots, X_n$,
(not necessarily iid) which have jdf
   \[ f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) \]
Now let's suppose that we define a new random variable $Y$, which is
some function
   \[ Y = g(X_1, \ldots, X_n) \]
Let's review some reasons why we might want to construct such a $Y$:
\begin{enumerate}
  \item We might want to build up a more complex random variable $Y$
    by transforming relatively simple ones.
  \item Often, a maximum likelihood estimator for some parameter will be
    a function of random variables $X_i$.
  \item Many test statistics are, in fact, \emph{derived} statistics
    dependent on the variables $X_i$.
\end{enumerate}
So we'll need to be able to say something about the distribution of $Y$.
We'll start with the univariate case where $n=1$, so that $Y=g(X)$. From
there, we'll move onto the multivariate case.

\subsubsection{Univariate Case}

Throughout this section, we'll assume that $X$ has a known density
function, $f_X(x)$. For some differentiable function $g$, we then define
$Y = g(X)$. The goal is to find $f_Y(y)$.

\begin{thm}\emph{(Monotonic Transformations)}
Suppose that $g$ is a monotonic function. Then
\begin{align*}
   f_Y(y) &= \left[f_X(x) \left\lvert \frac{dx}{dy}\right\rvert
      \right]_{x = g^{-1}(y)} \\
    &= \left[ f_X(x) \left\lvert \frac{dy}{dx}\right\rvert^{-1}
      \right]_{x = g^{-1}(y)}
\end{align*}
\end{thm}

\begin{thm}\emph{(Non-Monotonic Transformations)}
Suppose $g$ is some non-monotonic function. Then split up the function
into monotonic segments. Suppose that there are $n$ such monotonic
segments of $g$, denoted $g_1,\ldots,g_n$. Then
\begin{align}
   \label{nonmontrans}
   f_Y(y)
   &= \sumin \left[ f_X(x_i) \left\lvert \frac{dx_i}{dy}
      \right\rvert \right]_{x_i=g_i^{-1}(y)}\\
   &= \sumin \left[ f_X(x_i) \left\lvert \frac{dy}{dx_i}
    \right\rvert^{-1} \right]_{x_i=g_i^{-1}(y)}
    \notag
\end{align}
\end{thm}


\subsubsection{Multivariate Case}

Suppose that we have
$X:=\begin{pmatrix}X_1 & \cdots & X_n\end{pmatrix}'$ with jdf
$f_{X}(x)$.
Now suppose that we have an invertible function $g:\Rn\ra\Rn$. The
invertibility assumption is similar to the monotonicity assumption in
the univariate case.  Let
\begin{align*}
  Y:= \begin{pmatrix} Y_1 \\ \vdots \\ Y_n \end{pmatrix}
  = g(X)
\end{align*}
We can compute the jdf for vector $Y$ as
\begin{align*}
  f_{Y}(y)
  %&= \left[ f_{X}(x) \lvert J \rvert \right]_{x=g^{-1}(y)} \\
   &= \left[
      f_{X}(x)\frac{1}{\lvert J\rvert}
      \right]_{x=g^{-1}(y)}
\end{align*}
where $|J|$ is the determinants of the Jacobian matrix
\begin{equation}
   %J = \begin{bmatrix} \frac{\partial x_1}{\partial y_1} &
      %\frac{\partial x_1}{\partial y_2} & \cdots &
      %\frac{\partial x_1}{\partial y_n} \\
      %\vdots & & \ddots & \vdots \\
      %\frac{\partial x_n}{\partial y_1} &
      %\frac{\partial x_n}{\partial y_2} & \cdots &
      %\frac{\partial x_n}{\partial y_n}
   %\end{bmatrix} \qquad \qquad
   J = \begin{bmatrix} \frac{\partial g_1}{\partial x_1} &
      \frac{\partial g_1}{\partial x_2} & \cdots &
      \frac{\partial g_1}{\partial x_n} \\
      \vdots & & \ddots & \vdots \\
      \frac{\partial g_n}{\partial x_1} &
      \frac{\partial g_n}{\partial x_2} & \cdots &
      \frac{\partial g_n}{\partial x_n}
   \end{bmatrix}
\end{equation}
where $\frac{\partial g_i}{\partial x_j}$ is the partial derivative of
the $i$th element of $g(X)$ with respect to the $j$th element of vector
$X$.

Often, we will even use this to do transformations that are not
1-to-1. That will involve doing a transformation that \emph{is}
1-to-1 using this method with dummy variables, then integrating
out those dummy variables.


\clearpage
\subsection{Select Probability Distributions}


\subsubsection{Multivariate Normal}

The univariate normal distribution is an extremely familiar concept
where some random variable $X$ can take values along the real with
probabilities that match the famouse bell-curve. Recall the probability
density function of
   \[ f_Y(y) = \frac{1}{\sigma \sqrt{2\pi}} \; e^{-\frac{1}{2\sigma^2}
      (y - \mu)^2} \]
However, that's limited to only one dimension, and we would like to
generalize to higher dimensions. In the next-simplest 2-dimensional
case, we'd like a distribution that actually looks like a bell---where
potentional values can range over the real plane, $\mathbb{R}^2$, with
the density clustered around some mean before tapering off in all
directions, as below.
\begin{figure}[h!]
   \centering
   %\includegraphics[scale=0.40]{multivariate.pdf}
\end{figure}
\\
This figure has mean zero for both $X_1$ and and $X_2$, and
which are independent, implying $\sigma = I_2$, the identity matrix.
It's easy to see that any vertical cuts parallel to $xz$ or $yz$ planes
will yield a traditional normal random variable. This of course
generalizes to higher dimensions, although we can't display it so nicely.
\\
\\
This generalization will give us the
\emph{Multivariate Normal (MVN) Distribution}.
This turns out to be an extremely useful distribution. Let's highlight
a few applications here:
\begin{enumerate}
  \item The MVN distribution has applications in Bayesian multivariate
    regressions (i.e. Bayesian regressions with more than one
    ``independent,'' ``predictor,'' or ``$X$'' variable).  In such
    applications, we hope to find the posterior distribution of the
    $\beta$ regression vector, and it's common to assume a MVN prior on
    $\beta$ that results in an MVN posterior for $p(\beta|y)$.

  \item Next, we often use the MVN distribution to describe
    \emph{recurrence relations}, where we try to forecast an MVN RV one
    period into the future, given current information about the RV, like
    it's current value and distribution. One example would include
    jointly forecasting height \emph{and} weight in the future given
    current height and weight.

  \item Finally, the MVN distribution forms the basis for VARs, which
    are used to forecast different economic variables one period into
    the future, expressing each future variable as a function of lags of
    itself and other economic variables.  However, VARs are too broad
    and interesting a subject to elicit only a cursory summary in this
    note.  I'll write a more extensive summary of them elsewhere.
\end{enumerate}

In this note, the multivariate distribution will apply to a
$d$-dimensional random vector
\[ X = \begin{pmatrix} X^{(1)} & X^{(2)} & \ldots & X^{(d)} \end{pmatrix}^T,
    \qquad X  \sim N_d(\mu, \Sigma) \]
where $\mu$ is the $n$-dimensional \emph{mean vector},
\[ \mu = \begin{pmatrix} EX^{(1)} & EX^{(2)} & \ldots & EX^{(d)} \end{pmatrix}^T,
      \]
and where $\Sigma$ is the $d\times d$ \emph{covariance matrix}, which
is defined and has in its $i,j$ entry
\[ \sigma^2 = E\left[ (X - \mu) (X-\mu)^T\right] \in
   \mathbb{R}^{N\times N} \]
\[ \Sigma_{ij} = Cov(X^{(i)}, X^{(j)}), \qquad i,j = 1, \ldots, d\]
Superscripts with parentheses, like ``$X^{(i)}$,'' will denote
$i$th element of the MVN RV ``$X$,'' while ``$X_t$'' will denote
an MVN RV at time $t$.


\begin{defn}(Multivariate Normal)
A random vector $X$ has a \emph{multivariate normal}
distribution if every linear combination of its components,
\[ Y = a_1 X^{(1)} + \ldots + a_d X^{(d)} \]
\[ \Leftrightarrow Y = {a} X, \qquad
    {a} \in \mathbb{R}^{d} \]
is \emph{univariate normally
distributed}, with a corresponding mean and variance. This gives a
joint density function of
\begin{equation}
    \label{pdf2}
    f_{X}({x}) = \frac{1}{(2\pi)^{d/2}\lvert \Sigma \rvert^{1/2}}
	\; e^{ -\frac{1}{2} ({x} - \mu)^T \;
	\Sigma^{-1} ({x} - \mu) }, \qquad \lvert\Sigma\rvert =
      \det\Sigma \qquad x \in \mathbb{R}^d
\end{equation}
Note, we impose the requirement that $\Sigma$ is symmetric and positive
definite.  Symmetric because the correlation of $X$ and $Y$ equals the
correlation of $Y$ and $X$.
\end{defn}
{\sl Terminology}: I'll often us MVN to refer to the case where $X$ is
a vector with $d\geq2$; however, it should be clear that the univariate
normal distribution is just a special case where $d=1$. Therefore,
when speaking about an RV that could be either MVN or univariate normal
\emph{or} properties that apply equally well to either
type of RV, I'll often use the term \emph{Gaussian}
both for convenience and out of reverence to long-dead German-speaking
mathematicians. Stimmt?


It's fairly common to consider linear transformations and functions
of a multivariate normally distributed random variable.  For instance,
we might have an economic or statistical model with a recurrence
relation to describe the dynamics of some process:
\[ X_{t+1} = A X_{t} + {V}_{t} \]
where ${V}_t$ is some innovation or random noise vector.
Therefore, it would be useful to be able to derive the distributions
of \emph{functions} or \emph{linear transformation} of multivariate
normal random variables.

So let's find the probability distribution of a linear transforamtion
of an MVN RV. Begin by assuming
\begin{align*}
    X &= A{Y}, \qquad {Y} \sim
	\text{MVN}(\mu, \Sigma) \\
    \Rightarrow \quad f(y) &= k \; \exp\left\{-\frac{1}{2} (y - \mu)^T
	\; \Sigma^{-1} (y-\mu)\right\}
\end{align*}
where $k$ is a constant.\footnote{The constant will come from the
definition of the distribution given above in Equation \ref{pdf}, but
it's boring algebra not that interesting, so I'll suppress the details.}
Assuming that $A$ is invertible, we substitute in, using Equation
, to get the distribution of $X$:
\begin{align}
    \label{midway}
    \Rightarrow \quad f_X(A^{-1}x) &= k' \exp\left\{-\frac{1}{2} (A^{-1}x - \mu)^T
	\Sigma^{-1} (A^{-1}x-\mu)\right\}
\end{align}
Next, since the expectation is a linear operator, we can use the fact
that
\begin{align*}
    EX &= E[A{Y}] = A E{Y} = A \mu \\
    \Rightarrow \quad \mu_* &= A \mu \\
    \Leftrightarrow \quad \mu &= A^{-1} \mu_*
\end{align*}
where $\mu'$ is the mean vector of $X$.  With that, we
can substitute back into Equation \ref{midway} and simplify even
further, using convenient matrix manipulations like the distributivity
property, associativity, etc.:
\begin{align*}
    f_X(A^{-1}x) &= k' \exp\left\{-\frac{1}{2} (A^{-1}x - \mu)^T \;
	\Sigma^{-1} (A^{-1}x-\mu)\right\} \\
    &= k' \exp\left\{-\frac{1}{2} (A^{-1}x - A^{-1}\mu_*)^T \;
	\Sigma^{-1} (A^{-1}x-A^{-1}\mu_*)\right\} \\
    &= k' \exp\left\{-\frac{1}{2} [A^{-1}(x - \mu_*)]^T \;
	\Sigma^{-1} [A^{-1}(x-\mu_*)]\right\} \\
    &= k' \exp\left\{-\frac{1}{2} (x - \mu_*)^T \; [(A^{-1})^T \;
	\Sigma^{-1} A^{-1}] (x-\mu_*)\right\} \\
    &= k' \exp\left\{-\frac{1}{2} (x - \mu_*)^T \;
	\Sigma_*^{-1} \; (x-\mu_*)\right\} \\
    \Rightarrow X &\sim  \text{MVN}(\mu_*, \Sigma_*),
    \qquad \text{where $\mu_* = A\mu$ and $\Sigma_* = A\Sigma A^T $}
\end{align*}
This is huge! It means \emph{linear transformations of MVN
RV's are themselves MVN.}


If you're familiar with the standard univariate normal RV,
then you can probably guess what the standard MVN RV is:
\begin{equation}
    {Z} \sim \text{MVN}(0, \; I_d)
\end{equation}
where $I_d$ is the $d\times d$ identity matrix.  This form for
the covariance matrix also suggests that the different components
of ${Z}$ (denoted $Z_1, Z_2, \ldots, Z_d$) are independent and
Gaussian.
\\
\\
Moreover, just like we can build from a standard univariate to
a general univariate.  To do so, we'll use the results from the
last section, since building from a standard MVN
to general MVN simply involves linear transformations of the components.
Specifically, we can express the MVN RV ${X}$ as follows
\begin{equation}
    {X} = \mu + A{Z}, \qquad {X}\sim
	\text{MVN}(\mu, AA^T)
\end{equation}
{\sl Computation}: Suppose we know that we want ${X}$ to
be MVN($\mu, \; \Sigma)$, and we can only generate ${Z}$.
How do we choose $A$ such that $AA^T = \Sigma$.  Typically, we'll
have to use something like a \emph{Cholesky Factorization}
algorithm to find the correct $A$ in the form of a lower triangular
matrix. And if $\Sigma$ is symmetric, positive definite, then
$A$ is guaranteed to exist and this approach will work.
The algorithm itself can be found in the appendix.


So to summarize, MVN (or Gaussian) RV's are particularly nice to work with because
of some convenient properties:
\begin{enumerate}
    \item Suppose that ${X}$ and ${Y}$ are \emph{univariate}
	normally distributed and independent. This then implies that they are
	\emph{jointly normally distributed}. In other words,
	$\begin{pmatrix} {X} & {Y} \end{pmatrix}$ must have a multivariate
	normal distribution.
    \item Linear functions of Gaussians are Gaussian. So if $A$
	is a constant matrix and ${X}$ is MVN, then $A{X}$
	is also MVN.
    \item Uncorrelated Gaussian random variables are independent.
    \item Conditions Gaussian's are normal. So if $X_1$ and $X_2$
	are two components of a MVN RV, ${X}$, then
	$X_1 | X_2$ is normal, and vice versa.
\end{enumerate}

\clearpage
\subsubsection{$\Chi^2$ Distribution}


\begin{defn}
The $\Chi^2_\nu$ distribution with $\nu$ degrees of freedom is the sum of
$\nu$ squared standard normal random variables:
\begin{align*}
  Y \sim \Chi^2_\nu
  \quad\implies\quad
	 Y = \sum^\nu_{i=1} Z^2_i
   \qquad Z_i \sim N(0,1)
\end{align*}
\end{defn}

\begin{prop}
\label{prop:chisum}
Given $Y_i \sim \Chi_{\nu_i}^2$, then
\begin{align*}
  \sum^n_{i=1} Y_i \sim \Chi^2_{\nu}
  \qquad
  \text{where}\;
  \nu=\sum^n_{i=1} \nu_i
\end{align*}
In words, sums of $\Chi^2$ RVs are also $\Chi^2$ distributed.
\end{prop}

\begin{cor}
\label{cor:chimu}
Suppose that $Y_i \sim N(\mu,\sigma^2)$. Then
\begin{align*}
  \sum^n_{i=1} \frac{(Y_i-\mu)^2}{\sigma^2}  \sim \chi^2_n
\end{align*}
\end{cor}

\begin{cor}
Suppose that $Y_i \sim N(\mu,\sigma^2)$. Then
\begin{align*}
  \sum^n_{i=1} \frac{(Y_i-\bar{Y})^2}{\sigma^2}  \sim \Chi^2_{n-1}
\end{align*}
In other words, if we estimate the mean by the sample average $\bar{Y}$,
we loose one degree of freedom.
\end{cor}
\begin{proof}
Start first with the sum assuming we have the true mean $\mu$. We'll
break it up
\begin{align*}
	 \sum^n_{i=1} \frac{(Y_i-\mu)^2}{\sigma^2} &= \frac{1}{
	    \sigma^2} \sum^n_{i=1} (Y_i - \bar{Y}+ \bar{Y}-\mu)^2 \\
	    &= \frac{1}{\sigma^2} \sum^n_{i=1}  (Y_i - \bar{Y})^2
	    + \frac{2}{\sigma^2} \sum^n_{i=1} (Y_i-\bar{Y})(\bar{Y}-\mu)
	    + \frac{1}{\sigma^2} \sum^n_{i=1} (\bar{Y}-\mu)^2
      %\\
			%&= \frac{1}{\sigma^2} \sum^n_{i=1}  (Y_i - \bar{Y})^2
			%+ \frac{2(\bar{Y}-\mu)}{\sigma^2} \sum^n_{i=1} (Y_i-\bar{Y})
			%+ \frac{n(\bar{Y}-\mu)^2 }{\sigma^2}  \\
\end{align*}
If you do out the algebra, the middle term is zero and drops out, so we
are left with
\begin{align*}
	 \sum^n_{i=1} \frac{(Y_i-\mu)^2}{\sigma^2}
	 &= \sum^n_{i=1}\frac{(Y_i - \bar{Y})^2 }{\sigma^2} +
	    \left(\frac{\bar{Y}-\mu }{\sigma /\sqrt{n}}\right)^2
\end{align*}
The LHS is $\Chi^2_n$ by Corollary~\ref{cor:chimu}.
On the RHS, the second term is the mean estimator divided by the
standard error of the mean; therefore, that term in parentheses is a
$N(0,1)$ random variable. When squared, the result is a $\Chi_1^2$
random variable. Taking that over to the LHS and applying
Proposition~\ref{prop:chisum}, we conclude that
\begin{align*}
  \sum^n_{i=1}\frac{(Y_i - \bar{Y})^2}{\sigma^2}
  \sim \Chi^2_{n-1}
\end{align*}
\end{proof}
\begin{rmk}
This result is extremely important for deriving the distribution of
different estimators, variance estimators in particular.
\end{rmk}

\begin{prop}
Suppose that $Y$ has a $\Chi^2$ distribution with $\nu$ degrees of
freedom. Then the distribution is a special case of the gamma
distribution, where $\alpha = \nu/2$ and $\beta = 1/2$ with the pdf and
descriptive statistics
\begin{equation}
   f_Y(y) = \frac{1}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)}
      x^{\frac{\nu}{2} - 1} e^{-\frac{x}{2}} \qquad
\text{Mean} = \nu, \qquad \text{Variance} = 2 \nu
\end{equation}
\end{prop}
\begin{proof}
Since we can generate a $\Chi^2(\nu)$ random variable by summing the
square of $\nu$ independent N(0,1) random variables, this suggests that
we can use an induction argument to get the distribution.
\end{proof}

\subsubsection{Student's t-Distribution}

\begin{defn}
Let $Z\sim N(0,1)$ and $Y\sim \Chi^2_n$ be independent random variables.
Then
\begin{align*}
  X = \frac{Z}{(Y/n)^{1/2}}
  \sim t_n
\end{align*}
where $n$ is the degrees of freedom.
\end{defn}
%In order to generate a variable that has $t$ distribution with
%$n-1$ degrees of freedom,
%we start by supposing that $Y_i\sim \text{NID}(\mu,\sigma^2)$. From
%there, we define
%\begin{align*}
   %\bar{Y} = \frac{1}{n} \sum^n_{i=1} Y_i \qquad
   %S^2 &= \frac{1}{n-1} \sum^n_{i=1} (Y_i - \bar{Y})^2 \\
   %&=\frac{1}{n-1} \left( Y_1^2 + \cdots + Y_n^2 - n \bar{Y}^2\right)
%\end{align*}
%The statistics $\bar{Y}$ and $S^2$ are useful because they are both
%unbiased estimators of $\mu$ and $\sigma^2$, respectively.
%And before we get to the derivation, we make one final note:
%\emph{Cochran's Theorem}
%says that $\bar{Y}$ and $S^2$ are independent for the normal
%distribution.\footnote{This holds for \emph{no} other distributions;
%it is unique to the normal.}
%\\
%\\
%Now let's define the statistic, $T$,
%whose distribution we hope to find:
   %\[ T = \frac{\bar{Y}-\mu}{S/\sqrt{n}} \qquad Y_i \sim \text{NID}(\mu,
      %\sigma^2) \]
%But after a bit of rearranging and variable definitions, we get
%\begin{align*}
   %T &= \frac{(\bar{Y}-\mu)\sqrt{n} }{S} = \frac{
      %\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{S^2}{\sigma^2}
      %\cdot \frac{n-1}{n-1}}} \\
      %&= \frac{W_1}{\sqrt{W_2 / (n-1)}}
%\end{align*}
%Now it's clear by how we defined $W_1$ and $W_2$ that $W_1$ has a
%N(0,1) distribution, while $W_2$ has a $\chi^2$ distribution with
%$n-1$ degrees of freedom. Since $W_1$ and $W_2$ are \emph{independent}
%by Cochran's theorem, the jdf of $W_1, W_2$ is
   %\[ f_{W_1, W_2}(w_1, w_2) = \left(
      %\frac{1}{\sqrt{2\pi}} \; e^{-\frac{1}{2} w_1^2}\right)\left(
      %\frac{1}{ 2^{\frac{n-1}{2}} \Gamma\left(\frac{n-1}{2}\right)}
      %\; w_2^{\frac{n-1}{2} -1} e^{-\frac{1}{2}
      %w_2}\right)
      %\]
%Now we just hope to find the jdf of $T$. Since transformations must be
%1-to-1 in the number of variables, we'll choose
%a dummy variable, $V$, and define to simplify the mess of fractions
%in the jdf:
%\[ V =W_2, \qquad k = \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{
   %2^{\frac{n-1}{2}} \Gamma\left(\frac{n-1}{2}\right)} \]
%This leads us to from the Jacobian matrix to go from $W_1$ and
%$W_2$ to $T$ and $V$:
%\[ J^* = \det \begin{bmatrix} \frac{\partial T}{\partial W_1} &
   %\frac{\partial T}{\partial W_2} \\
   %\frac{\partial V}{\partial W_1} &
   %\frac{\partial V}{\partial W_2} \end{bmatrix} =
   %\begin{vmatrix} \frac{1}{\sqrt{W_2/(n-1)}} && \frac{\partial T}{
      %\partial W_2} \\ 0 && 1
   %\end{vmatrix} = \frac{1}{\sqrt{W_2/(n-1)}}
   %\]
%Now we can get the jdf of $T$ and $V$:
%\begin{align*}
   %f_{T,V}(t,v) &= \left[ f_{W_1,W_2}(w_1, w_2) \cdot
      %\lvert J^* \rvert^{-1}
      %\right]^{w_1 = g_1^{-1}(t,v)}_{w_2 = g_2^{-1}(t,v)}\\
   %&= \left[ k \cdot e^{-\frac{1}{2} w_1^2}
      %w_2^{\frac{n-1}{2} - 1} e^{-\frac{1}{2} w_2}  \left(
      %\frac{1}{\sqrt{w_2/(n-1)}}\right)^{-1}
      %\right]^{w_1 = t \sqrt{\frac{v}{
      %n-1}}}_{w_2=v}\\
      %&= \frac{k}{\sqrt{n-1}} v^{\frac{n-1}{2} - \frac{1}{2}}
      %e^{-\frac{1}{2} v\left( 1 + \frac{t^2}{n-1}\right)}
%\end{align*}
%Now we just need to integrate out the dummy variable, $V$, to
%get the distribution of $T$, which will require some substitution:
%\[ y = \frac{1}{2} v \left( 1 + \frac{t^2}{n-1} \right), \qquad
   %dv = \frac{2}{\left( 1 + \frac{t^2}{n-1} \right)} \; dy \]
%\begin{align*}
   %\Rightarrow f_{T}(t) &=\frac{k}{\sqrt{n-1}} \int^\infty_0
      %v^{\frac{n-1}{2} - \frac{1}{2}}
      %e^{-\frac{1}{2} v\left( 1 + \frac{t^2}{n-1}\right)} \; dv \\
      %&= \frac{k}{\sqrt{n-1}} \left( \frac{2}{ 1 + \frac{t^2}{n-1} }
      %\right)^{\frac{n-1}{2} + \frac{1}{2}} \int^\infty_0
      %y^{\frac{n-1}{2} - \frac{1}{2}} e^{-y} \; dy
%\end{align*}
%Now we recognize the integral as the $\Gamma(n/2)$ function, so
%we can substitute that in and substitute in for $k$ to get
%\begin{align*}
   %\Rightarrow f_{T}(t) &=\frac{\Gamma(n/2)}{
      %\Gamma\left(\frac{n-1}{2}\right)\sqrt{\pi(n-1)}}
      %\left( 1 + \frac{t^2}{n-1}\right)^{-\frac{n}{2}}
%\end{align*}
%So for a given $n$, this is the $t$-distribution with $n-1$ degrees
%of freedom.
%\\
%\\
%Equivalently, we can also express the distribution in an equally
%common formulation, where the statistic has $\nu$ degrees of freedom:
%\begin{align*}
   %\Rightarrow f_{T}(t) &=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{
      %\Gamma\left(\frac{\nu}{2}\right)\sqrt{\nu\pi}}
      %\left( 1 + \frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2}}
%\end{align*}

\newpage
\subsubsection{F-Statistic and Distribution}


\begin{defn}
Closely related to the $t$-distribution (and just as important), we now
want to find the distribution of the $F$-statistic, defined as
\begin{align*}
  Y = \frac{ X_a / a}{X_b / b},\qquad\qquad
  X_a \sim \Chi^2_a
  \quad X_b \sim \Chi^2_b
\end{align*}
Then $Y\sim F_{a,b}$.
\end{defn}
%Since $X_a$ and $X_b$ are assumed independent,
%we can form the jdf as the product of the two density functions
%\begin{align*}
   %f_{X_a, X_b}(x_a, x_b) &= \left(\frac{1}{2^{a/2} \Gamma(a/2)}
      %\; x_a^{a/2 -1} e^{-x_a/2 }\right) \left(
      %\frac{1}{2^{b/2}  \Gamma(b/2)} \;
      %x_b^{b/2 -1} e^{- x_b /2} \right)\\
   %&= k \; x_a^{a/2 -1} \; x_b^{b/2 -1} e^{-\frac{1}{2}(x_a + x_b)} \\
   %\text{where } \quad k &= \frac{1}{ 2^{\frac{a+b}{2}} \Gamma(a/2)
      %\Gamma(b/2)}
%\end{align*}
%Now we want to find the jdf of $F$ and our chosen dummy variable,
%$G=X_b$. To do so, we start with the inverse functions and
%then form the Jacobian matrix:
   %\[ X_a = \frac{a FG}{b}, \qquad X_b = G, \quad\Rightarrow \quad
     %|J| = \begin{vmatrix} \frac{\partial X_a}{\partial F} &
      %\frac{\partial X_a}{\partial G} \\
      %\frac{\partial X_b}{\partial F} &
      %\frac{\partial X_b}{\partial G} \end{vmatrix} =
      %\begin{vmatrix}
	 %\frac{a G}{b} & \frac{a F}{b} \\ 0 & 1
      %\end{vmatrix}
      %= \frac{a}{b} G
      %\]
%This allows us to write the trnasformed jdf
%\begin{align*}
   %p_{F,G}(f,g) &= \left[
      %k x_a^{a/2 -1} x_b^{b/2 -1} e^{-\frac{1}{2}(x_a + x_b)}
      %\left( \frac{a}{b} g\right) \right]^{x_a = a f g/b}_{x_b = g}\\
   %&= k' f^{a/2 -1} g^{\frac{a+b}{2}-1} e^{-\frac{g}{2}\left(
      %\frac{a}{b}f +1 \right)} \\
   %\text{ Where} \qquad k' &= \frac{1}{ 2^{\frac{a+b}{2}} \Gamma(a/2)
      %\Gamma(b/2)} \left( \frac{a}{b}\right)^{\frac{a}{2}}
%\end{align*}
%To get the density function of $F$, we integrate out the dummy
%variable, $G=X_b$, which is a simple $\chi^2$ variable whose support
%is the positive real line:
%\begin{align*}
   %p_F(f) &= k' f^{a/2 -1} \int^\infty_0
      %g^{\frac{a+b}{2}-1} e^{-\frac{g}{2}\left(
      %\frac{a}{b}f +1 \right)} \; dg
%\end{align*}
%We'll make the substitution
   %\[ u = \frac{g}{2}\left(\frac{a}{b} f +1\right) , \qquad
      %g = 2u \left( \frac{a}{b} f +1\right)^{-1} \]
   %\[ dg = 2 \left( \frac{a}{b} f +1\right)^{-1} \; du
      %\]
%\begin{align*}
   %\Rightarrow \qquad p_F(f) &= k' f^{a/2 -1}  \left[
      %2 \left( \frac{a}{b} f +1\right)^{-1}\right]^{\frac{a+b}{2}}
     %\int^\infty_0 u^{\frac{a+b}{2} - 1}
     %e^{-u} \; du
%\end{align*}
%Finally,
%we recognize that the integral is simply the $\Gamma( (a+b)/2)$
%function. This allows us to simplify $p_F$ and substitute back for
%$k'$ to get
%\begin{align*}
   %p_F(f) &= k' f^{a/2 -1}  \left[
      %2 \left( \frac{a}{b} f +1\right)^{-1}\right]^{\frac{a+b}{2}}
      %\Gamma\left(\frac{a+b}{2}\right) \\
   %\Rightarrow \qquad
   %p_F(f) &= \frac{\Gamma\left(\frac{a+b}{2}\right)}{ \Gamma(a/2)
      %\Gamma(b/2)} \left( \frac{a}{b}\right)^{\frac{a}{2}}
      %f^{\frac{a}{2} -1} \left[
	 %1 + \frac{a}{b} f \right]^{-\frac{a+b}{2}}
%\end{align*}
%which is the density function for $F$.



\clearpage
\subsubsection{Beta Distribution}

One particularly useful random variable is the Beta Distribution, which
model proportions relatively well, as it only takes values between
$0$ and $1$, and which also retains the uniform distribution as a special
case.

A random variable $Y$ has a \emph{beta probability distribution} if
and only if it has density function
\begin{equation}
   \label{pdf}
   f_Y(y) = \begin{cases} \frac{y^{\alpha -1} (1-y)^{\beta-1}}{B(\alpha,
      \beta)}, & 0\leq y \leq 1 \\
	 0, & \text{otherwise}
   \end{cases} \qquad \alpha, \beta > 0
\end{equation}
where the function $B$ is defined
   \[ B(\alpha, \beta) = \int^1_0 y^{\alpha-1}(1-y)^{\beta-1} \; dy =
      \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} \]
Varying $\alpha$ and $\beta$ can lead to a vast array of different
shapes.

By a few easy manipulations, it can be shown that the beta distribution
has mean and variance
\begin{equation}
   \label{beta}
    \mu = \frac{\alpha}{\alpha+\beta}, \qquad \sigma^2 =
      \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)}
\end{equation}

The Beta Function can be defined as
\[ B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha
   + \beta)} \]
This happens to have several integral representations, two of which
we list:
\begin{align}
   B(\alpha, \beta) &= \int^{\infty}_0 t^{\alpha-1} (1+t)^{-(\alpha+
   \beta)} \; dt \label{int1} \\
   B(\alpha, \beta) = \int^1_0 t^{x-1} (1-t)^{y-1} dt \label{int2}
\end{align}
The proof\footnote{All proofs and further information can be found
   on the Statlect.com website:
   \url{http://www.statlect.com/subon2/betfun1.htm}. }
that Equation \ref{int1} does indeed equal that of \ref{int1}
requires a bit of clever manipulation, while the Equation \ref{int2} uses
Equation \ref{int1} and makes the substitution
   \[ s = \frac{t}{1+t} = 1-\frac{1}{1+t}, \qquad \Rightarrow
      t = \frac{s}{1-s} \]


\subsubsection{General Gamma Function}

The gamma function has many special cases, but let's consider it
in its most general form, along with the mean and variance:
\begin{equation}
   f_X(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1}
      e^{-\beta x}, \qquad x > 0
\end{equation}
\[ \text{Mean} = \frac{\alpha}{\beta} \qquad \text{Variance} =
   \frac{\alpha}{\beta^2} \]
Gamma random variables also have the nice property that
\[ X_i \sim \text{Gamma}(\alpha_i, \;\beta) \quad \Rightarrow \quad
   \sum^n_{i=1} X_i \sim \text{Gamma}\left(\sum^n_{i=1} \alpha_i,\;\beta
   \right) \]

\subsubsection{Exponential Random Variable}

This family has $\alpha=1$ with $\beta$ unrestricted. Changing the way
we denote the variables (set $\beta =\theta$), this simplifies to
   \[ f_X(x) = \theta\; e^{-\theta} \]
This is particularly useful for waiting times, as it is a memoryless
distribution. In addition, as it is technically a Gamma random
variable, sums of exponential variables will have a Gamma distribution:
\[ X_i \sim \text{Exp}(\theta) \quad \Rightarrow \quad
   \sum^n_{i=1} X_i \sim \text{Gamma}(n, \theta) \]
Another attraction is the intimate connection this distribution has
with the Poisson process.  Namely, suppose $N(t)$ is a Poisson process
with parameter $\theta$, in which case $N(t)$ is Poisson distributed
with parameter $\theta t$, then the interarrival times are
exponentially distributed with parameter $\theta$.

\clearpage
\subsubsection{Quadratic Form Results}

\begin{prop}
Suppose that $X\sim N_p(\mu,\Sigma)$ where $\rank(\Sigma)=p$. Then
\begin{align*}
  (X-\mu)'\Sigma^{-1}(X-\mu) \sim \Chi_p^2
\end{align*}
\end{prop}
\begin{proof}
Since $\Sigma$ is symmetric positive definite, we can factor
$\Sigma=\Sigma^{-1}\Sigma^{-1}$ via an eigenvalue decomposition. Then
define
\begin{align*}
  Z = \Sigma^{-1/2}(X-\mu)
  \quad\implies\quad
  (X-\mu)'\Sigma^{-1}(X-\mu)
  = Z'Z
\end{align*}
where the $Z\sim N(0,I_p)$. Hence, the result follows.
\end{proof}

\begin{prop}
Suppose that $M$ is an idempotent matrix $p\times p$ matrix with rank
$k$. Then $Z'MZ\sim \Chi^2_k$.
\end{prop}
\begin{proof}
\end{proof}




\clearpage
\section{Martingale Theory}

\subsection{Basics}
\begin{defn}(Stochastic Process)
A \emph{stochastic process} on probability space $(\Omega,\sF,P)$ is a
family of random variables $\{X_\alpha\}_{\alpha\in J}$, where $J$ is an
arbitrary nonempty index set.
If $J=\N$, then we call $\{X_t\}\tinfz$ a \emph{discrete time}
stochastic process.
\end{defn}

\begin{defn}(Filtration)
A \emph{filtration} is a sequence $\{\sF_t\}\tinfz$ of $\sigma$-algebras
on probability space $(\Omega,\sF,P)$ such that
\begin{align*}
  \sF_0 \subseteq \sF_1 \subseteq \cdots \subseteq\sF
\end{align*}
where
\begin{align*}
  \sF = \vee\tinfz \sF_t := \sigma\left(\cup\tinfz \sF_t\right)
\end{align*}
We will think about the filtration as a sequence of information sets,
while a single $\sF_t$ represents the collection of events that are
observable up until time $t$. As time progresses, the information sets
get finer and finer, captured by
$\sF_t\subseteq \sF_{t+1}\subseteq\cdots$, with more events being
observable (an in the successive $\sigma$-algebras) as time goes on.
\end{defn}

\begin{defn}(Adapted)
A stochastic process $\{X_t\}\tinfz$ is \emph{adapted} to the filtration
$\{\sF_t\}\tinfz$ if $X_t$ is $\sF_t$-measurable for all $t$.
In words, this just means that the value of $X_t$ is observable at time
$t$.
\end{defn}
%\begin{rmk}
%Recall the definition of measurability: real-valued RV $X_t$ is $\sF_t$
%measurable if and only if $\{X_t\leq x\}\in\sF_t$ for all $x\in\R$.  In
%words, this means that you can probabilistically answer questions about
%the values that $X_t$ takes on. So if someone asks you ``Is
%$X_t\leq x$'', the set of all $\omega$'s where this is true,
%$\{X_t\leq x\}$, is a set in the $\sigma$-algebra $\sF_t$, so you can
%give a valid probabilistic answer to that. Same for other values of $x$,
%other equalities or inequalities, or (more generally) whether
%$X_t$ is in some Borel set of $\R$. You can make valid probabilistic
%statements about all of those things. And what \emph{adapted} means is
%that \emph{at any time} $t$, you give probabilistic answers about the
%values $X_t$ might take on using the associated time $t$
%$\sigma$-algebra, $\sF_t$.
%\end{rmk}

\begin{defn}(Predictable Process)
We say that stochastic process $\{X_t\}\tinfz$ is \emph{predictable} if
$X_t$ is $\sF_{t-1}$-measurable for all $t\geq 1$.
\end{defn}
%\begin{rmk}
%$X_t$ is adapted to $\sF_t$ means that you can make statements about the
%value of $X_t$ in terms of the time $t$ information set $\sF_t$.
%For predictable processes, you can make statements about the value of
%$X_t$ given the time $t-1$ information set, $\sF_{t-1}$.
%\end{rmk}

\begin{defn}(Martingale)
Suppose we have a stochastic process $\{X_t\}\tinfz$ adapted to
filtration $\{\sF_t\}\tinfz$ on probability space $(\Omega,\sF,P)$ such
that $X_t\in L^1(\Omega,\sF_t,P)$ for all $t$. Then we define
\begin{enumerate}[label=(\roman*)]
  \item Martingale: $\E[X_{t+1}|\sF_t] = X_t$
  \item Sub-Martingale: $\E[X_{t+1}|\sF_t] \geq X_t$
  \item Super-Martingale: $\E[X_{t+1}|\sF_t] \leq X_t$
\end{enumerate}
Each must hold for all $t$. And to be especially pedantic, we make sure
to say that $\{X_t\}\tinfz$ is a (sub/super-)martingale \emph{with
respect to filtration} $\{\sF_t\}\tinfz$, since a process might be one
of those things with respect to \emph{some} filtration, but not another.
\end{defn}

\begin{prop}
Let $X\in L^1(\Omega,\sF,P)$ and $\{\sF_t\}$ be any filtration. Then
\begin{align*}
  X_t := \E[X|\sF_t] \qquad t=0,1,2,\ldots
\end{align*}
defines a uniformly integrable martingale.
\end{prop}


\begin{defn}(Martingale Sum/Integral)
Consider two stochastic processes $\{X_t\}\tinfz$ and $\{V_t\}\tinfz$ on
probability space $(\Omega,\sF,P)$. The define the discrete time
\emph{martingale sum} or \emph{martingale integral} as
\begin{align*}
  (V\circ X)_T
  :=
  \begin{cases}
    0 & T = 0 \\
    \sumtT V_t \Delta X_t & t \geq 1
  \end{cases}
\end{align*}
where $\Delta X_t:= X_t - X_{t-1}$.
\end{defn}

\begin{prop}\emph{(Doob Decomposition)}
Let $\{X_t\}\tinfz$ be a submartingale with respect to filtration
$\{\sF_t\}\tinfz$ Then there exists a martingale $\{M_t\}\tinfz$ and a
non-decreasing predictable process $\{A_t\}\tinfz$ such that $A_0=0$ and
\begin{align*}
  X_n = M_n + A_n
\end{align*}
Moreover this decomposition is unique.
\end{prop}
\begin{proof}
Let $A_0=0$ and $M_0=X_0$, then choose
\begin{align*}
  \Delta X_t
  = \underbrace{\Delta X_t - \E[\Delta X_t |\sF_{t-1}]}_{\Delta M_t}
  +
  \underbrace{\E[\Delta X_t |\sF_{t-1}]}_{\Delta A_t}
\end{align*}
\end{proof}


\begin{cor}
Let $\{X_t\}\tinfz$ be a submartingale and $\{V_t\}\tinf$ be a
non-negative predictable process with respetct $\{\sF_t\}\tinfz$
such that $\E[|V_t\Delta X_t|]<+\infty$ for all $t\geq 1$. Then
$(V\circ X)\tinfz$ is a submartingale with decomposition
\begin{align*}
  (V\circ X)_t =
  (V\circ M)_t +
  (V\circ A)_t
\end{align*}
\end{cor}


\clearpage
\subsection{Stopping Times}

\begin{defn}(Stopping Time)
A \emph{stopping time} $\tau$ with respect to filtration
$\{\sF_t\}\tinfz$ is a random variable
\begin{align*}
  \tau: (\Omega,\sF,P)\ra \N \cup \{+\infty\}
\end{align*}
such that $\{\tau=t\}\in \sF_t$ (or equivalently $\{\tau\leq
t\}\in\sF_t$) for all $t\in\N$.
\\
\\
In words, a stopping time maps every $\omega\in\Omega$ to number
$\tau(\omega)\in\N$ representing a time. The measurability condition
simply ensures that at time $t$, you can tell whether $\tau$ occurred or
not.
Qualifiers we might use to describe a particular stopping time:
\begin{itemize}
  \item \emph{Bounded}: if $P[\tau < T]=1$ for some $T<\infty$.
  \item \emph{Finite}: if $P[\tau=+\infty]=0$, which is weaker than
    bounded. It's like ``We know it will stop at some point, but we
    can't put a hard bound on when.''
\end{itemize}
\end{defn}

\begin{defn}(Stopping $\sigma$-Algebra)
Given a stopping time $\tau$, we define a
\emph{stopping $\sigma$-algebra}:
\begin{align*}
  \sF_\tau :=
  \{A\in\sF \;|\; A\cap \{\tau=t\} \in \sF_t \quad \forall t\in\N\}
\end{align*}
This is the information at random time $\tau$, just like $\sF_t$ is the
information at fixed, nonrandom time $t$. An event $A$ is in $\sF_\tau$
if we can say whether $A$ \emph{and} $\tau\leq t$ happened given
information up to time $t$.
\end{defn}

\begin{thm}\emph{(Debut Theorem/First Hitting Time)}
Given stochastic process $\{X_t\}\tinfz$ adapted to filtration
$\{\sF_t\}\tinfz$ and some $B\in\sB(\R)$, the random variable
\begin{align*}
  \tau(\omega) := \inf_t\{t\in\N \;|\; X_t(\omega) \in B\}
\end{align*}
is a stopping time with respect to $\{\sF_t\}\tinfz$ called the
\emph{first hitting time}.
\end{thm}

\begin{defn}\emph{(Stopped Process)}
Let $\{X_t\}\tinfz$ be a stochastic process adapted to
$\{\sF_t\}\tinfz$ and $\tau$ a stopping time with respect to
$\{\sF_t\}\tinfz$. Then
\begin{itemize}
  \item $X_\tau \cdot \one{\{\tau<+\infty\}}$ is $\sF_\tau$-measurable.
    It is the value of the stochastic process when it is stopped.
    We can think about $\{X_{\tau(\omega}(\omega)\}_{\omega\in\Omega}$
    as the set of values the process takes on at the stopping time for
    each different sample path realizations. It can be written as
    \begin{align*}
      X_\tau\cdot \one{\{\tau<+\infty\}}
      = \sumtinfz X_t \cdot \one{\tau=t}
    \end{align*}

  \item $X^\tau_t := X_{\tau \wedge t}$ for
    $t\in\N$ is adapted to $\{\sF_t\}\tinfz$.
    To be super explicit about the dependence on $\omega\in\Omega$, this
    would be written in its most verbose form as
    $X^\tau_t(\omega) := X_{\tau(\omega) \wedge t}(\omega)$. Note that
    we can write
    \begin{align*}
      X_t^\tau = X_0 + (V\circ X)_t
      \qquad V_t = \one{\{t\leq \tau\}}\quad\text{for $t\geq 1$}
    \end{align*}
\end{itemize}
\end{defn}


\begin{prop}\emph{(Poor Man's Optional Stopping)}
Suppose that the process $\{X_t\}\tinfz$ is a martingale with respect to
filtration $\{\sF_t\}\tinfz$. Then
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $\{X^\tau_t\}\tinfz$ is also a martingale with respect to
    $\{\sF_t\}\tinfz$.
  \item If $\tau$ is bounded, then $\E[X_\tau] = \E[X_0]$
\end{enumerate}
\end{prop}


\begin{prop}
From Poor Man's optional stopping, recall that $X^\tau_t$ is a
submartingale given submartingale $\{X_t\}\tinfz$. Then, using the
corollary to Doob's Decomposition,
\begin{align*}
  X_t^\tau = X_0 + (V\circ X)_t
  \qquad V_t = \one{\{t\leq \tau\}}\quad\text{for $t\geq 1$}
\end{align*}
is a submartingale with the above deocmposition for every stopping time
$\tau$.
\end{prop}

\begin{prop}\emph{(Optional Stopping/Sampling)}
Let $\{X_t\}\tinfz$ be a martingale and $\sigma \leq \tau$ stopping
times with respect to $\{\sF_t\}\tinfz$, where $\tau$ is also a bounded
stopping time. Then
\begin{align*}
  \E[X_\tau |\sF_\sigma] = X_\sigma
\end{align*}
If you set $\sigma=0$, the constant function, and take expectations, you
get $\E[X_\tau]=\E[X_0]$ as we had in the previous proposition.
\end{prop}

\clearpage
\subsection{Martingale Convergence Theorems}

\begin{defn}(Upcrossings)
Let $\{X_t\}\tinfz$ be a stochastic process. Fix numbers $a<b$. Then
define stopping times
\begin{align*}
  \tau_1 :=& \; \inf\{t\geq 0\;|\; X_t \leq a\} \\
  \tau_2 :=& \; \inf\{t> \tau_1\;|\; X_t \geq b\} \\
  \tau_3 :=& \; \inf\{t> \tau_2\;|\; X_t \leq a\} \\
  \vdots \quad & \quad \vdots
\end{align*}
or more generally, for $k=1,2,3,\ldots$
\begin{align*}
  \tau_{2k-1} &= \inf\{t\geq \tau_{2k-2)} \;|\; X_t \leq a\} \\
  \tau_{2k} &= \inf\{t\geq \tau_{2k-1} \;|\; X_t \geq b\}
\end{align*}
so that $\tau_{2k-1}$ is $k$th time you dip below $a$ (conditional on
having gone above $b$ since the last time you dipped below $a$), while
$\tau_{2k}$ is the $k$th time you punched above $b$ (conditional on
having dipped below $a$ since the last time you punched above $b$).

From these stopping times, we define the number of \emph{upcrossings} up
to and including $t$:
\begin{align*}
  \beta_t(a,b)
  :=
  \begin{cases}
    0 & t < \tau_2  \\
    \max\{k | \tau_{2k} \leq t\} & t \geq \tau_2
  \end{cases}
\end{align*}
This is the total number of times the process has dipped below $a$ then
punched above $b$ by the time $t$.
\end{defn}

\begin{thm}\emph{(Upcrossing Theorem)}
If $\{X_t\}\tinfz$ is a submartingale, then
\begin{align*}
  \E[\beta_t(a,b)]
  \leq
  \frac{\E[(X_t-a)^+]}{b-a}
  \qquad \forall t
\end{align*}
\end{thm}

\begin{thm}\emph{(Martingale Convergence Theorem)}
Let $\{X_t\}\tinfz$ be a $L^1$-bounded submartingale (i.e.
$\sup_t\lVert X_t\rVert_1<\infty$), then there exists a random variable
$X_\infty$ such that
\begin{align*}
  X_t \asto X_\infty
  \qquad\text{and}\qquad
  \lVert X_\infty\rVert_1 \leq \sup_t \lVert X_t\rVert_1 < \infty
\end{align*}
\end{thm}

\begin{cor}
Let $\{X_t\}\tinfz$ be a uniformly integrable submartingale. Then there
exists a limit $X_\infty\in L^1$ such that
\begin{align*}
  X_t\asto X_\infty
  \qquad
  X_t \Lqto{1} X_\infty
\end{align*}
Moreover
$\E[X_\infty|\sF_t] \geq X_t$.
\end{cor}
\begin{rmk}
In the case of a martingale, the last statement specializes to
$\E[X_\infty|\sF_t] = X_t$, which means that $X_\infty$ characterizes
the entire submartingale sequence in the case of a martingale.
\end{rmk}

\begin{cor}
Let $\{X_t\}\tinfz$ be a submartingale that is bounded above.
Then there exists a random variable $X_\infty\in L^1$ such that
$X_t\asto X_\infty$.
\end{cor}

\begin{cor}
Consider $X\in L^1(\Omega,\sF,P)$ and filtration $\{\sF_t\}\tinfz$.
Denote
\begin{align*}
  X_t &= \E[X|\sF_t] \\
  X_\infty &= \E[X|\sF_\infty]
  \qquad
  \text{where} \;
  \sF_\infty=\vee\tinfz \sF_n := \sigma(\cup\tinfz \sF_t)
\end{align*}
Then $X_n\asto X_\infty$ and $X_n\Lqto{1} X\infty$.
\end{cor}

\clearpage
\section{Markov Transition Dynamics}

\subsection{Transition Kernels and Conditional Probability}

\begin{defn}(Transition Kernel)
Let $(\Omega,\sF)$ and $(E,\sE)$ be measurable spaces. A mapping
\begin{align*}
  Q:\Omega\times \sE \ra \R_+ \cup\{\infty\}
\end{align*}
is a \emph{transition kernel} (or \emph{stochastic kernel}) from
$(\Omega,\sF)$ into $(E,\sE)$ if
\begin{enumerate}
  \item For any fixed $\omega \in \Omega$, the set function
    $Q(\omega, \cdot):(E,\sE)\ra\R_+\cup\{\infty\}$ defines a valid
    measure on $(E,\sE)$

    In words, ``I know where I am today---$\omega\in\Omega$. Where might
    I go tomorrow? What is the distribution?'' Well, that distribution
    is $Q(\omega,\cdot)$.

  \item For any fixed measurable set $A\in\sE$, the function
    $Q(\cdot,A)$ is $\sF$-measurable.

    This is like ``I don't know where I am today---I could be anywhere
    in $\Omega$. But suppose I think that I'll be in $A\in\sE$ tomorrow.
    What measurable sets in $\sF$ for my position \emph{today} are
    consistent with being in $A\in\sE$ \emph{tomorrow}?''
\end{enumerate}
\end{defn}

\begin{ex}
Let $Q:\Omega\times E\ra\R_+\cup\{\infty\}$ be a measurable function,
and let $\mu$ be a measure on $(E,\sE)$. Then
\begin{align*}
  Q(\omega,A) = \int_A Q(\omega,x) \; d\mu(x)
\end{align*}
defines a transition kernel.
\end{ex}

\begin{defn}(Finite and Probability Transitional Kernels)
Much like we had for probability measures, a kernel
$Q:\Omega\times E\ra\R_+\cup\{\infty\}$ is said to be a
\emph{finite transition kernel} if $Q(\omega,E)<\infty$ for all
$\omega\in\Omega$. It is said to be a
\emph{probability transition kernel} if  $Q(\omega,E)=1$ for all
$\omega\in\Omega$.
\end{defn}

\begin{thm}
Let $Q$ be a transition Kernel from $(\Omega,\sF)$ into $(E,\sE)$.
Then
\begin{enumerate}
  \item For $\sE$-measurable function $f:(E,\sE)\ra\bar{\R}_+$,
    then the following defines a $\sF$-measurable function
    $Qf:(\Omega,\sF)\ra\bar{\R}_+$:
    \begin{align*}
      Qf(\omega) = \int_E f(e) \; Q(\omega,de)
    \end{align*}

  \item If $\mu$ is a measure on $(\Omega,\sF)$ than the following is a
    measure on $(E,\sE)$.
    \begin{align*}
      \mu Q(A) = \int_\Omega Q(\omega,A) \;\mu(d\omega)
      \qquad A\in\sE
    \end{align*}

  \item Let $\mu$ be a measure on $(\Omega,\sF)$ and $Q$ a transition
    kernel from $(\Omega,\sF)$ into $(E,\sE)$. Then
    the following defines a measure on $(\Omega,\sF\otimes \sE)$.
    \begin{align*}
      \nu(A\times B) = \int_A Q(\omega,B)\;\mu(d\omega)
    \end{align*}
\end{enumerate}
\end{thm}

\subsection{Markov Chains}

Some of assumptions to make things more tractable:
\begin{enumerate}
  \item[i.] Discrete time.
  \item[ii.] Discrete (countable) space
  \item[iii.] Time Homogeneous: Transition probabilities don't depend
    upon time, just the state.
\end{enumerate}

\subsubsection{Basics}

\begin{defn}(Distribution)
Given a countable state space $I$, a vector $(\lambda_i)_{i\in I}$ is a
\emph{distribution} if
\begin{itemize}
  \item $\lambda_i\geq 0$ for all $i\in I$.
  \item $\sum_{i\in I} \lambda_i= 1$
\end{itemize}
\end{defn}

\begin{defn}(Stochastic Matrix)
A matrix $(Q_{ij})_{i,j\in I}$ is a a \emph{stochastic matrix} if all of
its rows are distributions.\footnote{%
  If the elements of each column added up to 1 as well, we would call
  the matrix {\sl doubly-stochastic}.
}
\\
\\
You should Think about the row $Q_{(i\cdot)}$ as the conditional
distribution for moving from the state $i$ to any of the other states
with probability $Q_{ij}$.
\end{defn}

\begin{defn}(Markov Chain)
A stochastic process $\{X_t\}\tinfz$ on a probability space
$(\Omega,\sF,P)$ is a \emph{Markov Chain} with initial distribution
$\lambda$ and transition matrix $Q$ if
\begin{enumerate}[label=(\roman*)]
  \item $P[X_0=i]=\lambda_i$ for all $i\in I$
  \item $P[X_{t+1} = i_{t+1} \; | \; X_0=i_0, \ldots, X_t=i_t]
    = p_{i_t,i_{t+1}}$ for all $t$
\end{enumerate}
In that case, we say that $\{X_t\}\tinfz$ is Markov$(\lambda,P)$.
\end{defn}

\begin{thm}\emph{(Markov Property)}
Let $\{X_t\}\tinfz$ be Markov($\lambda,P$). If $P[X_t=i]>0$, then
conditional on $X_t=i$, the stochastic process
$\{X_{t+s}\}_{s=0}^\infty$ is Markov($\delta_i,P$) and independent of
$(X_0,\ldots,X_t)$.
\end{thm}

\begin{defn}
Define $P_i = P[\;\cdot \;| X_0=i]$.
We say that
\begin{enumerate}[label=(\roman*)]
  \item $i\ra j$ or ``$i$ leads to $j$'' if, for some $t\geq 0$, we have
    $P_i[X_t = j]>0$. Note that $i\ra i$ always.
  \item $i\leftrightarrow j$ or ``$i$ communicates with $j$ if both
    $i\ra j$ and $j\ra i$.
\end{enumerate}
\end{defn}

\begin{thm}
For $i$ and $j$, the following are equivalent:
\begin{enumerate}
  \item $i\ra j$
  \item $p_{i,i_1}\cdots p_{i_{n-1},j}$ for some
    $i_1,\ldots,i_{n-1}\in I$
  \item $p^n_{ij}> 0$ for some $n\geq 1$.
\end{enumerate}
\end{thm}

\begin{defn}(Communicating Classes)
$I$ decomposes into a set of $N$ communicating classes
$I=\bigcup\nN C_n$ such that $C_n\cap C_m=\emptyset$ for all $m\neq n$.
Terminology:
\begin{enumerate}[label=(\roman*)]
  \item A class $C_n$ is closed if $i\in C_n$ and $i\ra j$ implies that
    $j\in C_n$.
  \item A state $i \in I$ is absorbing if $\{i\}$ is a closed class.
  \item A Markov chain or transition matrix is called \emph{irreducible}
    if the whole Markov chain is a single communicating class---it's
    possible to go from anywhere to anywhere.
\end{enumerate}
\end{defn}

\begin{defn}
Let $A\subseteq I$ be a collection of states and define
\begin{itemize}
  \item $H^A:=\inf\{t\geq 0 \;|\; X_t \in A\}$: A first-hitting time for
    process hitting $A$
  \item $h^A_i:= P_i[H^A<\infty]$: The probability of a finite first
    hitting time for $A$
  \item $k_i^A=\E_i[H^A]$: The expected time until the first hit of $A$
\end{itemize}
\end{defn}

\begin{thm}
The vector of hitting probabilities $(h_i^A)_{i\in I}$ is the minimal
nonnegative solution to the system of linear equations:
\begin{align*}
  h_i^A =
  \begin{cases}
    1 & i \in A \\
    \sum_{j\in I} p_{ij} h_j^A & \forall i \not \in A
  \end{cases}
\end{align*}
\end{thm}

\begin{thm}
The vector $(k_i^A)_{i\in I}$ is the minimal nonnegative solution to the
system of linear equations:
\begin{align*}
  k_i^A =
  \begin{cases}
    0 & i \in A \\
    1 + \sum_{j\in I} p_{ij}k_j^A & \forall i \not \in A
  \end{cases}
\end{align*}
\end{thm}

\begin{thm}\emph{(Strong Markov Property)}
Let $\{X_t\}\tinfz$ be a Markov$(\lambda,P)$ and $\tau$ a stopping time
with respect to $\{\sF_t^X\}\tinfz$ and $i\in I$ such that
$P[\tau<\infty, \; X_\tau=i]>0$. Then conditioned on $\tau<\infty$ and
$X_\tau=i$, $\{X_{\tau+s}\}_{s=0}^\infty$ is Markov$(\delta_i,P)$
independent of $\sF_\tau^X$.
\end{thm}

\begin{defn}
We call a state $i\in I$ \emph{recurrent} if
\begin{align*}
  P_i[X_n=i \;\; \text{for $\infty$ many $t$}]=1
\end{align*}
and we call a state \emph{transient} if
\begin{align*}
  P_i[X_n=i \;\; \text{for $\infty$ many $t$}]=0
\end{align*}
\end{defn}

\begin{defn}
Denote
\begin{align*}
  T_i^0 &= 0 \\
  T_i^1 &= T_i = \inf\{t \geq 1 \;|\; X_t=i\} \\
  T_i^2 &= \inf\{t \geq T_i^1 + 1 \;|\; X_t=i\} \\
  \vdots \quad &\qquad \vdots \\
  S_i^k &=
  \begin{cases}
    T_i^k- T_i^{k-1} & T_i^{k-1} < \infty \\
    0 & \text{o.w.}
  \end{cases}
\end{align*}
So $T^k_i$ is the time until the $k$th transition to state $i$.
\end{defn}


%\paragraph{Definition} $\{X_t\}$ is a {\sl non-homogeneous Markov Chain}
%when $\{X_t\}$ is an infinite sequence of random variables $X_0, X_1,\ldots$
%which satisfy the following properties
%\begin{enumerate}
   %\item[i.]{$X_n$ denotes the \emph{state number} of a subject at time
      %$t$.}
   %\item[ii.]{Each $X_t$ is a discrete-type random variable over $n$
      %possible discrete values.}
   %\item[iii.]{The {\sl transition probabilities} are history
      %independent when it comes to pre-$t$ states, 
      %but the transition probabilities may vary with time, $t$:
      %\[ Q_{ij}^{(t)} = P( X_{t+1}=j \; | \; X_t = i, \; X_{t-1}=k_{t-1},
	 %\ldots,\; X_0 = k_0 ) \]
      %\[ =P(X_{t+1} = j \; | \; X_t = i ) \]
      %So $Q_{ij}^{(t)}$ is the probability of moving from
      %state $i$ to state $j$ at time $t$.
      %}
%\end{enumerate}

%If the transition probabilities, $Q^{(t)}_{ij}$, do
%not depend on $t$, then they are denoted by $Q_{ij}$ and we have
%a {\sl homogeneous Markov Chain}. 
%\\
%\\
%For this type of Markov Chain, 
%only state or position, \emph{not} time, factors into the distribution for the
%next period. This differs from the definition above, in that we explicity allow $Q_{ij}^{(t_1)} \neq Q_{ij}^{(t_2)}$ for 
%$t_1\neq t_2$.


%\subsection{Order of a Markov Chain}
%We can generalize a bit and allow the Markov Chain to depend upon more
%than one previously observed state.  In particular, we define an 
%{\sl order}-${n}$ {\sl Markov Chain} to be a Markov Chain that 
%depends upon the previous $n$ values.  
%\\
%\\
%Above, we defined an order-1 Markov chain. If we want to consider 
%higher-order Markov Chains, we'll have to do a bit of work when it
%comes time to put the probabilities in the transition matrices that
%we next define. Namely, we will have to convert them to order-1.


%\newpage
%\subsection{Transition Matrices}

%Hopefully, the notation above clearly suggested that it will be
%convenient to place the many transition probabilities into a
%{\sl transition matrix}, whose $i,j$ entry is the transition
%probability for moving from state $i$ to state $j$. So if 
%there are $n$ states, define
%\[\mathbf{Q}^{(t)}=\begin{pmatrix} Q_{ij}^{(t)} \end{pmatrix}_{ij}
    %\quad \text{where $\mathbf{Q}^{(t)}$ is $n\times n$} \]
%And just a word about the funky notation you see above: it says
%``the matrix $\mathbf{Q}^{(t)}$ is a matrix populated
%with the values/probabilities $Q_{ij}^{(t)}$ at the $ij$ 
%entry.'' 
%\\
%\\


%\subsection{Longer Term Transition Probabilities and Matrices}

%Often, we'll want to look further ahead than just the next period, which
%is all that our current formulation allows. To do so, we'll define
%the {\sl longer term transition probabilities} as 
%\[ Q^{(t,k)}_{ij} = P(X_{t+k} = j \;|\; X_t = i ) \]
%where $Q^{(t,k)}_{ij}$ is the probability of 
%being in state $j$ in $k$ periods given that you are currently
%at time $t$ and in state $i$.\footnote{It's important to note that the transition
    %probability $Q^{(t,k)}_{ij}$ does \textbf{not} care what happens in between
    %time $t$ and time $t+k$.  It's just the probability that you were in
    %state $i$ at time $t$ and may be in state $j$ at time 
    %$t+k$. In fact, it's even entirely possible 
    %that you were \emph{already} in state $j$ sometime between
    %time $t$ and $t+k$! So if we're considering $Q^{(t,k)}_{ii}$, 
    %this is not the probability of staying of in state $i$
    %from time $t$ to $t+k$.  You can drift away and then come back.}
%The above sections dealt with the special case where $k=1$.
%\\
%\\
%Quite naturally, the {\sl longer term transition matrix} is defined
%\[\mathbf{Q^{(t,k)}} =
      %\begin{pmatrix} Q^{(t,k)}_{ij} \end{pmatrix}_{ij}
      %\]
%\paragraph{Theorem} In non-homogeneous Markov Chains, the longer-term
%probability $Q^{(t,k)}_{ij}$ can be computed as the $(i,j)$-entry of
%the matrix 
   %\[\mathbf{Q^{(t,k)}}= \mathbf{Q^{(t)}} \times \mathbf{Q^{(t+1)}} \times
      %\cdots \times \mathbf{Q^{(t+k-1)}} 
            %\]
%And for a homogeneous Markov Chain, this matrix is just 
%$\mathbf{Q^{(\cdot, k)}} = \left(\mathbf{Q}\right)^k$, dropping the
%$t$ in the superscript since the transition probabilities do not 
%change over time.

%\subsection{Chapman-Kolmogorov Equations (Homogeneous Case)}

%\paragraph{Theorem} If we restrict ourselves to the 
%homogeneous case, 
%the {\sl Chapman-Kolmogorov Equations} tell us that 
%\[Q^{(\cdot, \; k+\ell)}_{ij} = \sum^n_{s=1} 
    %Q^{(\cdot, \; k)}_{is} Q^{(\cdot,\; \ell)}_{sj} \]
%This equation captures the idea of ``interposing'' some
%intermediate time and state between now (time $t$, state
%$i$) and the future (time $t+k+\ell$, and state $j$).
%The logic goes as follows:
%\begin{enumerate}
    %\item Before you get to time $t+k+\ell$ and state $j$, you
	%have to stop at time $t+k$.   
    %\item At that time $t+k$, you'll be in any one of the states,
	%call it $s$, where $s$ could equal $1, \cdots, n$ (where
	%$n$ is the number of states.
    %\item So if you sum over the probability
	%of moving as follows: 
	%\[  \text{state $i$ at $t$} \quad \Rightarrow \quad
			%\text{state $s$ at $t+k$} \quad \Rightarrow \quad 
			%\text{state $j$ at $t+k+\ell$} \]
	%(where $s$ ranges over all possible intermediary states), 
	%you'll get the desired probability.
%\end{enumerate}
%{\sl Matrix Representation:}
%In matrix form, this result can be written more compactly as
    %\[ \mathbf{Q}^{(\cdot,\; k+\ell)} = \mathbf{Q}^{(\cdot,\; k)} \times 
      %\mathbf{Q}^{(\cdot,\; \ell)}\]
%restricting ourselves to the homogeneous case.
%Things are a little tougher if we want to consider the 
%non-homogeneous case, as we'll have to multiply more matrices 
%together and keep track of subscripts.


%\newpage
%\subsection{Marginal Distributions}

%Suppose we have:
%\begin{enumerate}
    %\item A homogeneous Markov Chain, $\{X_t\}$ with a transition matrix,
	%$\mathbf{Q}^{(\cdot)}$
    %\item A marginal distribution for $X_t$, denoted by row-matrix $\psi^{(t)}$ 
	%The $i$th entry of $\psi^{(t)}$ is
	%\[ \psi^{(t)}_i = P\left\{ X_t = i \right\} \]
%\end{enumerate}
%First, suppose we want to get the marginal distribution of $X_{t+1}$.
%This we can do by using the law of total probability:
%\begin{align*}
     %P\{X_{t+1} = j\} &= \sum_{i=1}^n 
	%P\{X_{t+1} = j \; | \; X_t = i \} \cdot 
	%P\{X_{t} = i\} \\
    %&= \sum_{i=1}^n 
	%Q^{(\cdot)}_{ij} \cdot 
	%\psi_i^{(t)} \\
    %\Rightarrow \qquad 
    %\psi^{(t+1)} &= \psi^{(t)} \mathbf{Q}^{(\cdot)}
%\end{align*}
%More generally, and still restricting to the homogeneous case, 
%this implies that for any arbitrary $m$, 
    %\[ \psi^{(t+m)} = \psi^{(t)} \mathbf{Q}^{(\cdot,\; m)} \]

%\paragraph{Note}: All of this math works whether we are 
%\emph{certain} of the initial distribution, $X_t$, or
%whether we aren't quite sure.  
%\begin{itemize}
    %\item[-] If we are \emph{certain} of the initial state $i$, 
	%$X_t$, or if we want to assume an initial state $i$,
	%$\psi^{(t)}$ will be a vector that is all zeros
	%\emph{except} for $i$th position, which will have a 1.
    %\item[-] If we want to think about things more probabilistically
	%or if we are uncertain of the state $X_t$, then
	%$\psi^{(t)}$ will be a vector that sums to one and
	%has, in each position $i=1,\ldots, n$, a probability
	%of being in state $i$ at time $t$.
%\end{itemize}


%\newpage
%\subsection{Stationary Distributions}

%In this section, we dispense with the non-homegeneous
%case, assuming $\mathbf{Q}^{(t)}$ is homegeneous
%over time. Therefore, we drop the superscript altogether
%and simply write $\mathbf{Q}$.

%\subsection{Definition}

%A distribution $\psi^*$ is called {\sl stationary} or 
%{\sl invariant} if $\psi^* = \psi^* \mathbf{Q}$,
%which implies that $\psi^* = \psi^* \left(\mathbf{Q}\right)^m$
%for all $m$ too.\footnote{Take note that a stationary 
    %\emph{distribution} differs from a stationary \emph{process}.
    %The former is the limiting distribution of a Markov Chain,
    %or the marginal distribution of a stationary process. The
    %latter is a stochastic process whose joint probability 
    %distribution does not change when shifted through time or
    %space.  Finally, a ``stationary process'' is \emph{not} the
    %same as a ``process with a stationary distribution.''}
%Mathematically, a stationary distribution
%is a fixed point if we think of $\mathbf{Q}$ as a map:
%\begin{align*}
    %P: \; &\mathbb{R}^n \rightarrow \mathbb{R}^n \\
    %&\psi \rightarrow \psi P
%\end{align*}
%At least one such distribution exists for each stochastic
%matrix, $\mathbf{Q}$.\footnote{Use Brouwer's fixed point theorem.}
%\\
%\\
%Importantly, if the distribution of $X_0$ is a stationary distribution,
%then $X_t$ will have this same distribution for all $t$.
%As a result stationary distributions have a natural
%interpretation of \emph{stochastic steady states}.


%\subsection{Solving for Stationary Distributions}

%We saw above that a stationary distribution, $\psi^*$, must solve 
%\begin{equation}
    %\label{psicond1}
    %\psi = \psi \mathbf{Q} \quad \Leftrightarrow \quad
    %\psi \left(I_n - \mathbf{Q}\right) = 0
%\end{equation}
%But note that this does not require $\psi^*$ to be a probability
%distribution since the zero vector happens to solve Equation
%\ref{psicond1}. So we want to impose the additional constraint
%\begin{align}
    %\sum_{i=1}^n \psi_i = 1 \quad &\Leftrightarrow \quad
    %\psi b = 1 \qquad \text{where $b_i \in \mathbb{R}^n$ and
				%$b_i=1$ for $i=1,\ldots,n$.} \nonumber \\ 
    %&\Leftrightarrow \quad \psi B = b 
    %\qquad \text{where $B$ is an $n\times n$ matrix of 1s}
    %\label{psicond2}
%\end{align}
%By adding together the two conditions, Equations \ref{psicond1}
%and \ref{psicond2}, we see that $\psi$ must satisfy 
%\begin{align*}
    %\psi \left(I_n - \mathbf{Q} + B\right) &= b \\
    %\Leftrightarrow \qquad
	%\left(I_n - \mathbf{Q} + B\right)^T \psi^T &= b^T 
%\end{align*}
%In this way, we can solve for the stationary distribution
%by inverting the matrix to get:
%\begin{equation}
    %\psi^* = \left[\left(I_n - \mathbf{Q} + B\right)^T\right]^{-1}
	%b^T
%\end{equation}


%\subsection{Uniform Ergodicity} 

%\paragraph{Definition}
%A stochastic matrix, $\mathbf{Q}$,
%is called uniformly ergodic if there is a positive
%integer $m$ such that all elements of $\left(\mathbf{Q}\right)^m$
%are \emph{strictly} positive.

%\paragraph{Uniqueness of the Stationary Distribution}
%Note that there may in fact be
%many stationary distributions for the stochastic matrix,
%$\mathbf{Q}$ (as in the case of the identity matrix).
%But one sufficient condition for uniqueness is 
%\emph{uniform ergodicity}.  

%\paragraph{Convergence to Steady-State}
%If uniform ergodicity holds, we also get the result that
%for any non-negative row vector, $\psi$, summing to one (so a
%proper distribution)
%\[ \psi \mathbf{Q}^t \rightarrow \psi^* \quad \text{ as }
	%t\rightarrow \infty \]
%where $\psi^*$ is the unique stationary distribution. So
%regardless of the distribution of $X_0$, the distribution 
%of $X_t$ converges to $\psi^*$.

%\paragraph{Time in States} To get another import interpretation
%and result, assume $\{X_t\}$ is a Markov Chain with stochastic 
%matrix $\mathbf{Q}$. Also assume that $\mathbf{Q}$ is 
%uniformly ergodic with stationary distribution $\psi^*$. Then
%\begin{equation}
    %\lim_{s\rightarrow\infty}
    %\frac{1}{s} \sum^s_{t=1} 1\{X_t = j\} \rightarrow
	%\psi^*_j  \qquad
	%\forall j \in \{1, \ldots, n\}
%\end{equation}
%This tells use that the fraction of time the Markov Chain
%$\{X_t\}$ spends in state $j$ converges to $\psi^*_j$ as
%time goes to infinity.  Therefore, if we consider many
%Markov chains with the same stochastic matrix, $\mathbf{Q}$,
%the long-run cross-sectional averages for a population will
%equal time-series averages for individual chains.



%\newpage
%\subsection{Application: Page Rank}

%Suppose we have a set of web-pages, $j \in \{1, 2, \dots,n \}$.
%We want to find the fanking $r_j$ for each page, $j$. To do 
%so, we'll set the ranking as
    %\[ r_j = \sum_{i \in L_j} \frac{r_i}{\ell_i} \]
%where $L_j$ is the set of pages linking to page $j$, and
%where $\ell_i$ is the total number of outbound links from 
%page $i$. 
%\\
%\\
%But we can think of this another way.  Specifically, imagine
%a jabroni on some webpage $i$ who is clicking links randomly
%and with equal probability.  So what's his probability of 
%landing on page $j$?  Well clearly, we can define this whole
%``ranking'' business in terms of Markov Chains and transition
%probabilities:
    %\[ r_j = \sum_{\text{all $i$}} Q_{ij} r_i, \qquad
	%\text{ where  } Q_{ij} = \frac{1\{i \rightarrow j\}}{\ell_i}
    %\]
%where $1\{i \rightarrow j\}$ is an indicator function that 
%equals 1 if page $i$ links to page $j$, zero otherwise.
%So $Q_{ij}$ is, as above, the transition probability for
%moving from state--or in this case ``page''--$i$ to 
%$j$.\footnote{We drop the superscript, since we'll assume 
%we're in the homeogeneous case.}
%\\
%\\
%Awesome, now let's write the ranking in vector and matrix 
%notation so we can describe the problem for all pages,
%$j \in \{1, \ldots, n\}$:
    %\[ \mathbf{r} = \mathbf{r} \mathbf{Q} \]
%where $\mathbf{r}$ is the vector of rankings and $\mathbf{Q}$
%is the transition matrix.  Well that looks familiar.  
%Specifically, $r$ is the stationary distribution of the
%stochastic matrix $\mathbf{Q}$---and we know how to solve for
%that!


%\newpage
%\subsection{Tauchen's Approximation Method}

%\subsection{AR(1) Approximiation}
%Often, we will approximate some continuous model with a 
%discrete model that utilizes Markov Chains. Specifically, the
%discrete model takes the form:
%\begin{equation}
    %y_{t+1} = \rho y_t + \varepsilon_{t+1} \qquad 
	%\varepsilon_{t+1} \sim \text{N}(0, \sigma_\varepsilon^2)
%\end{equation}
%Now the variance of the stationary probability distribution 
%of the stochastic process $\{y_t\}$ (which we hope to specify
%as a Markov Chain) can be easily shown to be
    %\[ \sigma_y^2 = \frac{\sigma^2_\varepsilon}{1-\rho^2} \]

%\subsection{Constructing the State Space}
%To discretize an otherwise continuous process, we choose
%\begin{itemize}
    %\item $n$, the number of states for the discrete 
	%approximation that $y_t$ can be in.
    %\item $m$, an integer that parameterizes the width of
	%the space of values $y_t$ can take on.
%\end{itemize}
%From there, we create a space $\{x_1, \ldots, x_n\}$ and
%transition matrix $\mathbf{Q}$ defined
%by the parameters $n$ and $m$ such that
%\begin{enumerate}
    %\item $\{ x_1, x_n \} = \{ -m \sigma_y, \; m \sigma_y \}$.
	%So we see how $m$ controls the width of the space.
    %\item $x_{i+1} = x_i + s$ where
	%\[ s = \frac{x_n - x_1}{n-1} \]
    %\item $Q_{ij}$ represents the probability of transition
	%from state $x_i$ to state $x_j$. 
%\end{enumerate}

%\subsection{Defining the Transition Probabilities}

%We've essentially divided the values $y_t$ can take into buckets
%that run from $[x_i, x_{i+1}]$.  So if we say that $y_t$
%is in state $i$, we really mean that $y_t$ is in that bucket.
%There are, of course, $n-1$ equal width buckets.


\clearpage
\section{Univariate Time Series}

%\subsection{Linear Difference Equations}

%\begin{defn}(Linear Difference Sequence)
%A linear difference sequence specifies a law of motion for some series.
%It generally involves taking some input sequence $\{\varepsilon_t\}$ and
%producing some output sequence $\{y_t\}$.
%\end{defn}

%\clearpage
\subsection{Stationarity, Ergodicity, and the Lag Operator}

\begin{defn}(Strict Stationarity)
A discrete stochastic process $\{y_t\}$ is \emph{strictly stationary} if
the distribution of $(y_t,\ldots,y_{t+k})$ is the same as the
distribution for $(y_s,\ldots,y_{s+k})$ for all $t$, $s$, and $k$. That
is, shifting time indices does not change the joint distribution of the
stochastic process.
\end{defn}

\begin{defn}(Covariance or Weak Stationarity)
A stochastic process $\{y_t\}$ is \emph{covariance stationary},
\emph{weakly stationary}, or just \emph{stationary} if the mean and the
autocovariance function does not depend upon time, i.e. for all $t$:
\begin{align*}
  \E[y_t] &= \mu \\
  \Cov(y_t,y_{t+k}) &= \gamma(k)
\end{align*}
where $\mu$ is a constant and $\gamma(k)$ is an autocovariance function
that does not depend upon time, just the lag length $k$.
\end{defn}
\begin{rmk}
``Stationary'' will generally refer to covariance or weak stationarity
since that's mostly what we care about. If I want to invoke strict
stationarity, I will explicitly use the qualification ``\emph{strictly}
stationary.''
\end{rmk}

\begin{defn}(Trend Stationary)
A stochastic process $\{y_t\}$ is \emph{trend stationary} if
$\{y_t-f(t)\}$ is stationary for some nonstochastic function $f(t)$ that
traces out a trend.
\end{defn}


\begin{defn}(White Noise)
A stochasic process $\{y_t\}$ is called \emph{white noise} if it
satisfies for all $t$
\begin{align*}
  \E[y_t] &= 0
  \\
  \E[y_t^2] &= \sigma^2
  \\
  \E[y_ty_{t-k}] &= 0 \qquad \forall k\neq 0
\end{align*}
This process is clearly covariance stationary, and it's one of the
simplest examples of a covariance stationary process.
\end{defn}

\begin{defn}(Random Walk)
A stochastic process $\{y_t\}$ is a \emph{random walk} if the process
$\{\Delta y_t\}$ is white noise, where $\Delta y_t = y_t-y_{t-1}$.
\end{defn}

\begin{defn}(Ergodicity and a Sufficient Condition for Ergodicity)
Intuitively, \emph{ergodicity} means that there is \textbf{no} common
component present in all elements of the time series $\{y_t\}$ that
does not average out over time.

A sufficient condition for ergodicity of stationary series $\{y_t\}$ is
that, for any two bounded functions $f:\R^k\ra \R$ and $g:\R^\ell\ra\R$,
we have
\begin{align*}
  \lim_{T\ra\infty}
  \big|
  \E[
    f(y_s,\ldots,y_{s+k})
    \cdot
    g(y_{s+T},\ldots,y_{s+T+\ell})
  ]
  \big|
  =
  \big|
  \E[
    f(y_s,\ldots,y_{s+k})
  ]\big|
    \cdot
  \big|
  \E[
  g(y_{s+T},\ldots,y_{s+T+\ell})]
  \big|
\end{align*}
Strict stationary does \emph{not} imply ergodicity.
\end{defn}

\begin{defn}(Ergodic for the Mean)
We say that $\{y_t\}$ is \emph{ergodic for the mean} if
\begin{align*}
  \frac{1}{T}\sum^T_{t=1}y_t \ra \E[y_t]
\end{align*}
In words, the time average equals the ensemble average. More explicitly,
for stochastic process $\{y_t(\omega)\}$ living in state space
$(\Omega,\sF_t,P)$, the average over time indices $t$ for any
realization $\omega^*$, $\frac{1}{T}\sumtT y_t(\omega^*)$, equals the
average across $\omega$, fixing the time $t$.
\end{defn}
\begin{rmk}
Later on, we'll see the Ergodic Theorem, which states that a stationary
ergodic process is ergodic for the mean.
\end{rmk}

\begin{ex}(Non-Ergodic Series)
Suppose that we have $y_t=x_t+Z$ where $x_t$ is iid. Then $\{y_t\}$ is
not an ergodic process because there is a common component that does not
average out. And that's true even though $\{y_t\}$ \emph{is} strictly
stationary, again reinforcing that strict stationarity
\emph{does not imply} ergodicity.
\end{ex}


\clearpage
\subsection{Filters, the Lag Operator, and Lag Polynomials}

\begin{defn}(Filter)
Let $\{h_j\}_{j=0}^\infty$ be sequence of numbers.
We define the associated \emph{filter} as the following function:
\begin{align*}
  h(z) :=&\; \sum_{j=0}^\infty h_j z^j
  = h_0 + h_1 z + h_2 z^2 + \cdots
  \qquad z\in\C
\end{align*}
\end{defn}

\begin{defn}(Lag Operator)
The \emph{lag operator} $L$ is an operator that maps sequences
into sequences such that $\{y_{t-1}\}=L\left(\{y_t\}\right)$. We often
simply write, as shorthand, $y_{t-1}=Ly_t$. We use powers of the lag
operator to express lags of more than one time unit:
\begin{align*}
  y_{t-k} = L^k y_t
\end{align*}
$L$ is a \emph{linear} operator.
\end{defn}

\begin{defn}(Lag Polynomial)
A \emph{lag polynomial} is a filter with the lag operator as its
argument. For example, every filter $h(z)=h_0+h_1z + h_2z^2+\cdots$ has
an associated lag polynomial
\begin{align*}
  h(L) = \sum_{j=0}^\infty h_j L^j = h_0 + h_1L + h_2 L^2 + \cdots
\end{align*}
and vice versa.
\end{defn}

\begin{defn}(Filtered Series)
Let $\{x_t\}$ be a covariance-stationary process and let
$\{h_j\}_{j=0}^\infty$ be an absolutely summable sequence,
$\sum_{j=0}^\infty |h_j|<\infty$.
Then we can create a new \emph{filtered series} $\{y_t\}$ by applying
filter $h(L)$ to the original series $\{x_t\}$:
\begin{align*}
  y_t =&\; h(L)x_t
  = h_0 x_t + h_1x_{t-1} + h_2x_{t-2} + \cdots
\end{align*}
Moreover, if the autocovariances of $\{x_t\}$ are absolutely summable,
then so are the autocovariances of $\{y_t\}$; therefore, filters map the
space of covariance-stationary processes into itself.
\end{defn}

\begin{ex}(ARMA Processes)
Virtually all processes below can be thought of as filtered processes.
For example, the MA($\infty$) process
\begin{align*}
  y_t = \psi(L)\varepsilon_t
  = \psi_0\varepsilon_t
  + \psi_1\varepsilon_{t-1}
  + \psi_2\varepsilon_{t-2}
  + \cdots
  \qquad
  \varepsilon_t \sim WN(0,\sigma^2)
\end{align*}
simply takes white noise process $\{\varepsilon_t\}$ and filters it
with $\{\psi_j\}$. Similarly, consider stationary ARMA($p$,$q$) process
\begin{align*}
  y_t
  &= \phi_1 y_{t-1}
  +\cdots
  + \phi_p y_{t-p}
  + \varepsilon_t
  + \theta_1\varepsilon_{t-1}
  + \cdots
  + \theta_q\varepsilon_{t-q}
\end{align*}
This can be rewritten in lag polynomial notation as
\begin{align}
  \iff\qquad
  (1 - \phi_1 L - \cdots - \phi_pL^p)y_t
  &=
  (1+ \theta_1 L + \cdots +\theta_qL^q )\varepsilon_t
  \notag
  \\
  \phi(L)y_t
  &=
  \theta(L)\varepsilon_t
  \label{lagarma}
  \\
  \text{where}\quad
  \phi(z) &:=
  1 - \phi_1 z - \cdots - \phi_pz^p
  \notag
  \\
  \theta(z)
  &:=
  1+ \theta_1 z + \cdots +\theta_qz^q
  \notag
\end{align}
Equation~\ref{lagarma} says ``process $\{y_t\}$ filtered by
$\phi(\,\cdot\,)$ equals white noise $\{\varepsilon_t\}$ filtered by
$\theta(\,\cdot\,)$.''
\end{ex}


\begin{defn}(Lag Polynomial Inverses)
Consider the following function: $\Psi(z)=(1-z)$. If we want to compute
its inverse for $|z|<1$, we could appeal to the geometric sum to write
\begin{align*}
  \Psi(z)^{-1} = \frac{1}{1-z}
  = \sumninfz z^n
  = 1+z+z^2 + z^3 + \cdots
\end{align*}
Now if instead, $\Psi$ defined a \emph{lag polynomial} $\Psi(L) =
(1-\psi L)$ where $|\psi|<1$, this logic still works, and we should
define the inverse of the simple lag polynomial as
\begin{align}
  \Psi(z)^{-1}
  = 1+\psi L+\psi L^2 + \psi L^3 + \cdots
  \label{laginv}
\end{align}
where, again, we need $|\psi|<1$ for this to make sense.\footnote{%
  The lag operator $L$ can usefully be thought of as having modulus of
  one so that $|\psi|<1$ is sufficient to ensure that $|\psi L|<1$,
  whatever that means.
}
We can invert more general lag polynomials by first factoring them as we
would any other polynomial:
\begin{align*}
  \Psi(L)
  &= 1 - \psi_1 L - \cdots \psi_pL^p \\
  &= (1-\lambda_1L)(1-\lambda_2L)\cdots(1-\lambda_pL)
\end{align*}
Then to invert $\Psi(L)$, we can just invert the individual
$(1-\lambda_iL)$ one at a time
\begin{align*}
  \Psi(L)^{-1}
  &= (1-\lambda_1L)^{-1}(1-\lambda_2L)^{-1}\cdots(1-\lambda_pL)^{-1}
\end{align*}
using the geometric sum Expression~\ref{laginv} above for each.
Of course, this makes clear that we need all roots $|\lambda_i|<1$ to do
this.
\end{defn}


\clearpage
\subsection{MA(q), AR(p), and ARMA(p,q) Models}

\begin{defn}(MA(q) Models)
We define an MA(q) process $\{y_t\}$ given white noise innovations
$\{\varepsilon_t\}$ as
\begin{align*}
  y_t &= \mu + \theta_0\varepsilon_t + \theta_1y_{t-1} + \cdots \theta_qy_{t-q}
  \qquad\qquad \varepsilon_t\sim(0,\sigma^2)
  \\
  \iff\quad
  y_t - \mu
  &= \theta(L) \varepsilon_t
  \\
  \text{where}\quad
  \theta(z) &= \theta_0 + \theta_1z + \cdots + \theta_qz^q
\end{align*}
It is easy to work out the mean and covariance function of the process
\begin{align*}
  \E[y_t] = \mu
  \qquad
  \gamma(k) = \E[y_ty_{t-k}] &=
    \sigma^2 \sum_{j=0}^{q-k} \theta_j \theta_{j+k}
\end{align*}
Since neither depends upon $t$, we conclude that \emph{any} MA(q)
process is covariance stationary. And since the autocovariance function
includes on finitely many terms, it will be absolutely summable
$\sum_{j=0}^\infty |\gamma(j)|<\infty$, which ensures that $y_t$ will be
ergodic for the mean.
\end{defn}
\begin{rmk}
Note that in practice we often take the innovations in an MA(q) process
to be mean-zero and \emph{iid}, which of course implies that the
innovations are white noise and satisfy this definition. \emph{However},
that's also slightly stronger than the definition we've given here since
we want to be able to define the MA(q) process given only white noise
(potentially non-iid) innovations.  Though white noise implies that the
$\{\varepsilon_t\}$ are uncorrelated, that will not necessarily imply
they are \emph{independent} (unless we assume they are Gaussian). This
whole remark will turn out to be important when we encounter the Wold
Representation.
\end{rmk}

\begin{defn}(MA($\infty$) Process)
Suppose we let $q\ra\infty$, we get the MA($\infty$) process
\begin{align*}
  y_t &= \mu + \sum_{i=0}^\infty \psi_i \varepsilon_{t-i}
  = \mu + \psi(L)\varepsilon_t
  \qquad \varepsilon_t\sim (0,\sigma^2)
  \\
  \text{where}\quad
  \psi(z) &= \psi_0 + \psi_1z + \psi_2z^2 + \cdots
\end{align*}
Given square summability of the coefficients, the mean and covariance
are given by
\begin{align*}
  \E[y_t] &= \mu\\
  \gamma(k) = \E[y_ty_{t-k}] &=
  \sigma^2
  \sum_{j=0}^{\infty}
  \psi_j \psi_{j+k}
  %\begin{cases}
    %\sigma^2\sum_{i=0}^\infty \psi_i^2 & k=0\\
    %\sigma^2
      %\sum_{j=0}^{\infty}
      %\psi_j \psi_{j+k}
    %& k\neq 0
  %\end{cases}
\end{align*}
Note that this object is only well-defined if the coefficients are
\emph{square-summable}: $\sum_{i=0}^\infty \psi_i^2<\infty$. Square
summability is implied by another stronger assumption we might
reasonably impose on the coefficients, \emph{absolute summability}:
$\sum_{i=0}^\infty |\psi_i|<\infty$.
Given either of these conditions, we can be assured that sum defining
$y_t$ will converge in mean-square to a random variable (making
$y_t$ a well-defined object) and that the variance of $y_t$ is finite,
which is necessary for the process to be covariance stationary.
Moreover, if we do impose absolute summability of the coefficients, we
also get that the covariance function is absolutely summable
$\sum_j |\gamma(j)|<\infty$ so that $y_t$ is ergodic for the mean.
\end{defn}

\begin{defn}(AR(p) Process)
Given white noise innovations $\{\varepsilon_t\}$, define the AR(p)
model
\begin{align*}
  y_t
  &= \phi_1 y_{t-1} + \cdots + \phi_p y_{t-p} + \varepsilon_t
  \qquad \varepsilon_t\sim(0,\sigma^2) \\
  \phi(L)y_t &= \varepsilon_t \\
  \text{where}\quad
  \phi(z)
  &= 1-\phi_1z - \cdots \phi_pz^p
\end{align*}
This can also be written in \emph{companion form}, expressing the scalar
valued AR(p) as a vector-valued AR(1):
\begin{align*}
  \begin{pmatrix}
    y_t \\ y_{t-1}\\\vdots \\ y_{t-p+2}\\y_{t-p+1}
  \end{pmatrix}
  &=
  \begin{pmatrix}
    \phi_1 & \phi_2 & \cdots& \phi_{p-1} & \phi_p \\
    1 & 0 & \cdots & 0 & 0\\
    0 & 1 & \cdots & 0 & 0\\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \cdots & 1 & 0\\
  \end{pmatrix}
  \begin{pmatrix}
    y_{t-1} \\ y_{t-2}\\\vdots \\ y_{t-p+1} \\y_{t-p}
  \end{pmatrix}
  +
  \begin{pmatrix}
     \varepsilon_t\\ 0\\\vdots \\ 0 \\0
  \end{pmatrix}
   \\
  \iff \qquad
  Y_t &= \Phi Y_{t-1} + e_t
\end{align*}
Though MA(q) processes are \emph{always} covariance-stationary, this is
decidedly \emph{not} the case for AR(p) processes. This motivates the
following proposition.
\end{defn}

\begin{prop}\emph{(Stationarity of AR(p) Processes)}
\label{prop:stationaryar}
Suppose that $\{y_t\}$ follows AR(p) with lag polynomial $\phi(z)$.  The
process is stationary if and only if any of the following equivalent
statements hold:
\begin{enumerate}[label=\emph{(\roman*)}]
  \item All roots of the lag polynomial $\phi(z)$ have modulus $>1$
  \item The lag polynomial has a well-defined inverse $\phi(z)^{-1}$
  \item All of the eigenvalues of $\Phi$ have modulus $<1$
\end{enumerate}
\end{prop}

\begin{prop}
\emph{(Yule-Walker and the Autocovariance Function of Stationary AR(p))}
If we multiply through the $p$th order difference equation by $y_{t-k}$
for $k=0,1,\ldots,p$ and take expectation, it follows that the
autocovariances satisfy the same difference equation as $y_t$:
\begin{align*}
  \gamma(k)
  &=
  \begin{cases}
    \sigma^2 + \sum_{j=1}^p \phi_j\gamma(j)
    & k = 0 \\
    \sum_{j=1}^p \phi_j\gamma(k-j)
    & k = 1,2,\ldots \\
  \end{cases}
\end{align*}
This is a system of $p+1$ equations in $p+1$ unknowns,
$\gamma(0),\ldots,\gamma(p)$.

We could have also derived this from the vector representation. So
define the unconditional variance $\Sigma=\Var(Y_t)$, which holds for
all $t$ (since $Y_t$ is stationary) and solve
\begin{align*}
  \Sigma = \Var(Y_t)
  &= \Var(\Phi Y_{t-1} + e_t)
  = \Phi \Sigma\Phi' + D_e \\
  \Sigma
  &= \Phi \Sigma\Phi' + D_e
\end{align*}
where $D_e=\diag(\sigma^2,0,\ldots,0)$. We can solve this system by
computing
\begin{align*}
  \vc(\Sigma) &= (\Phi\otimes\Phi)\vc(\Sigma) + \vc(D_e) \\
  \implies\quad
  \vc(\Sigma) &= \big(I_{p^2}-(\Phi\otimes\Phi)\big)^{-1}\vc(D_e)
\end{align*}

\end{prop}

\begin{ex}(Stationary AR(1) Model)
Consider the simple AR(1) model where
\begin{align*}
  y_t = \rho y_{t-1} + \varepsilon_t
  \qquad \varepsilon_t\sim(0,\sigma^2)
\end{align*}
Suppose that $|\rho|<1$. Then we we can solve for $y_t$ in terms of past
$\varepsilon_t$'s by backward recursive substitution:
\begin{align*}
  y_t
  &= \rho y_{t-1} + \varepsilon_t
  = \rho\big(\rho y_{t-2} + \varepsilon_{t-1}\big) + \varepsilon_t
  = \cdots \\
  \implies\quad
  y_t
  &=
  \sum_{j=0}^\infty \rho^j\varepsilon_{t-j}
\end{align*}
We have thus expressed the AR(1) as an MA($\infty$) model (more on that
in Proposition~\ref{prop:artoma}), and we see here why exactly we need
$|\rho|<1$ or, equivalently, the roots of $\phi(z) = 1-\rho z$ to be
$>1$ in modulus. Only this will ensure that the variance of $y_t$ is
finite and that the process is stationary.  From there, we can then
solve the Yule-Walker equations to compute the autocovariance function
\begin{align*}
  \gamma(k) = \E[y_ty_{t-k}] = \sigma^2 \frac{\rho^{|k|}}{1-\rho^2}
\end{align*}
\end{ex}

\begin{ex}(Explosive AR(1) Model)
Now consider the AR(1) model where $|\rho|>1$. We solve by forward
subsitution to get $y_t$ in terms of \emph{future} $\varepsilon_t$'s:
\begin{align*}
  y_t
  &= \rho^{-1}(y_{t+1} - \varepsilon_{t+1})
  = \rho^{-1}\big(
      \rho^{-1}(y_{t+2} - \varepsilon_{t+2}) - \varepsilon_{t+1}\big)
  = \cdots \\
  \implies\quad
  y_t
  &=
  -\sum_{j=1}^\infty \rho^{-j}\varepsilon_{t+j}
\end{align*}
\end{ex}

\begin{prop}\emph{(Stationary AR(p) to MA($\infty$))}
\label{prop:artoma}
Extending the simple AR(1) example above, if $y_t$ is a stationary AR(p)
model, then we know by Proposition~\ref{prop:stationaryar} that the
associated lag polynomial $\phi(z)$ is invertible. Therefore, we can
express $y_t$ as an MA($\infty$):
\begin{align*}
  \phi(L)y_t = \varepsilon_t
  \quad\iff\quad
  y_t = \phi(L)^{-1}\varepsilon_t
\end{align*}
\end{prop}

\begin{prop}\emph{(Invertible MA(q) to AR($\infty$))}
\label{prop:matoar}
Suppose that $y_t$ follows an MA(q) representation.  If the roots of the
associated MA lag polynomial $\theta(z)$ are all $>1$ in modulus, then
we call the MA(q) model \emph{invertible} and can express $y_t$ as an
AR($\infty$):
\begin{align*}
  y_t = \theta(L)\varepsilon_t
  \quad\iff\quad
  \theta(L)^{-1}y_t = \varepsilon_t
\end{align*}
The fact that all roots are $>1$ in modulus ensures that the
coefficients $\phi_i$ of the AR($\infty$) are absolutely summable
$\sum_{i=0}^\infty |\phi_i|<\infty$ so that the AR($\infty$)
representation converges in mean square to a random variable and is,
therefore, well-defined.
\end{prop}

\begin{rmk}
Note that while we could \emph{always} invert a \emph{stationary} AR(p),
we cannot always invert MA(q) models. There are such things as
non-invertible MA(q) models. So we really do need all roots of
$\theta(z)$ to lie outside the unit circle because only then will
$\theta(L)^{-1}$ be a well-defined object.
\end{rmk}

\begin{prop}\emph{(Observationally Equivalent MA Representations)}
Suppose that $y_t$ follows an invertible MA(q) process. Then there is an
associated non-invertible MA(q) process that is
\emph{observationally equivalent}, i.e.\ with the same first- and
second-order properities (mean and autocovariance function). The
opposite also holds true: For every non-invertible MA(q), there is an
associated invertible representation.
\end{prop}

\begin{defn}(ARMA(p,q) Model)
We define an ARMA(p,q) process $\{y_t\}$ given white noise innovations
$\{\varepsilon_t\}$ as
\begin{align*}
  \phi(L)y_t = \theta(L)\varepsilon_t
\end{align*}
If the roots of $\phi(z)$ are all $>1$ in modulus, then $\phi(L)$ has a
well defined inverse, the process $y_t$ is stationary, and $y_t$ has an
MA($\infty$) representation
\begin{align*}
  y_t = \frac{\theta(L)}{\phi(L)}\varepsilon_t
\end{align*}
If the roots of the $\theta(z)$ are all $>1$ in modulus, then
$\theta(L)$ has a well-defined inverse and $y_t$ has an AR($\infty$)
representation
\begin{align*}
  \frac{\phi(L)}{\theta(L)}y_t = \varepsilon_t
\end{align*}
\end{defn}

\clearpage
\subsection{Wold's Theorem}

\begin{thm}
Suppose $\{y_t\}_{t=-\infty}^\infty$ is a covariance stationary process.
Then it can be representated as
\begin{align*}
  y_t = \kappa_t + \sum_{s=0}^\infty c_s \varepsilon_{t-s}
  \qquad \varepsilon_t\sim(0,\sigma^2)
\end{align*}
where
\begin{enumerate}[label=(\roman*)]
  \item $\{\varepsilon_t\}$ are uncorrelated white noise (not
    necessarily independent)
  \item $\sum_{s=0}^\infty c_s^2<\infty$
  \item $\kappa_t$ is uncorrelated with $\varepsilon_{s}$ for any $s$
    and is perfectly linearly predictable from past $y_t$.
\end{enumerate}
\end{thm}
Therefore, treating any arbitrary covariance stationary processes as an
MA($\infty$) (with white noise errors) is without loss of generality.
Though it might be more convenient to work with and manipulate
\emph{other} representations (MA(q), AR(p), AR($\infty$), ARMA(p,q),
etc.) in practice, we lose nothing by working with the MA($\infty$)
representation when stating and proving results. And indeed, we will
make use of this result extensively in later subsections by mostly
working with the MA($\infty$) representation of processes.

To be especially pedantic, note how strong this theorem is. This theorem
doesn't just say ``we can find sequences $\{c_s\}$ and $\{\kappa_t\}$
along with white noise process $\{\varepsilon_t\}$ so that the resulting
MA($\infty$)
\begin{align}
  \kappa_t + \sum_{s=0}^\infty c_s \varepsilon_{t-s}
  \label{wold}
\end{align}
has the same mean and covariance function''---which
is a weaker statement about moments. No, the theorem is stronger and
says says that we can find $\{c_s\}$ and $\{\kappa_t\}$
along with white noise process $\{\varepsilon_t\}$ so that the resulting
MA($\infty$) as in Expression~\ref{wold} has the exact same
\emph{joint distribution} as $y_t$:
\begin{align*}
  \begin{pmatrix}
    y_t \\ \vdots \\ y_{t+k}
  \end{pmatrix}
  \overset{d}{=}
  \begin{pmatrix}
    \kappa_t + \sum_{s=0}^\infty c_s \varepsilon_{t-s}
    \\ \vdots \\
    \kappa_{t+k} + \sum_{s=0}^\infty c_s \varepsilon_{t+k-s}
  \end{pmatrix}
  \qquad \forall t,k
\end{align*}



\clearpage
\subsection{Optimal Forecasting}

\begin{prop}\emph{(Forecasting Given Past $\{\varepsilon_t\}$)}
Suppose that covariance-stationary $\{y_t\}$ can be written as
\begin{align*}
  y_t = \mu + \psi(L)\varepsilon_t
  \qquad \varepsilon_t \sim (0,\sigma^2)
\end{align*}
Then the lowest MSE forecast of $y_{t+s}$ given information up to time
$t$---i.e.\ given both $\{y_s\}_{s=-\infty}^t$ and
$\{\varepsilon_s\}_{s=-\infty}^t$---is just the conditional mean:
\begin{align*}
  \hat{y}_{t+s|t}
  = \E_t[y_{t+s}]
  &= \E_t[\mu + \psi(L)\varepsilon_{t+s}] \\
  &= \mu + \psi_s\varepsilon_t + \psi_{s+1}\varepsilon_{t-1} + \cdots
\end{align*}
which can be written as
\begin{align}
  \hat{y}_{t+s|t}
  &= \mu + \left[\frac{\psi(L)}{L^s}\right]_+\varepsilon_t
  \label{optforeeps}
\end{align}
where $[\,\cdot\,]_+$ is called the \emph{annihilation operator} and
says ``Ignore negative powers of $L$ in the expression $\psi(L)/L^s$.''
\end{prop}
\begin{rmk}
In practice, we obviously don't observe the infinite sequence
$\{\varepsilon_s\}_{s=-\infty}^t$ that this formula requires. There are
two options. First, we can recast the forecast in terms of past
$\{y_t\}$, but that still might require infinitely many past $\{y_t\}$.
Alternatively, one can also assume that $\varepsilon_s=0$ for all
$s\leq 0$, then solve for $\{\varepsilon_s\}_{s=1}^t$ sequentially by using
observations $\{y_s\}_{s=1}^t$ and the formula
$y_t = \mu + \psi(L)\varepsilon_t$ to back out first $\varepsilon_1$,
then $\varepsilon_2$, then $\varepsilon_3$, \ldots.
\end{rmk}


\begin{prop}\emph{(Wiener-Kolmogorov, Forecasting Given Past $\{y_t\}$)}
We don't usually have past $\{\varepsilon_t\}$ when forecasting (which
are typically model-based objects to be estimated), though we always
have access to past $\{y_t\}$. Therefore, suppose $y_t$ has an
AR($\infty$) representation, we rewrite the process for $y_t$ as
\begin{align*}
  y_t = \mu + \psi(L)\varepsilon_t
  \quad\iff\quad
  \psi(L)^{-1}(y_t - \mu) = \varepsilon_t
\end{align*}
This is an expression for $\varepsilon_t$, so ubstitute this expression
for $\varepsilon_t$ into Expression~\ref{optforeeps} to get
\begin{align}
  \hat{y}_{t+s|t}
  &= \mu + \left[\frac{\psi(L)}{L^s}\right]_+
  \psi(L)^{-1}(y_t - \mu)
  \label{wienerkolmogorov}
\end{align}
which is called the \emph{Wiener-Kolmogorov prediction formula}. We can
thus forecast any MA($\infty$) process using this formula and past
$\{y_t\}$ only, rather than past $\{\varepsilon_t\}$.
\end{prop}

\clearpage
\subsection{Autocovariance Generating Function}

\begin{defn}(Autocovariance Generating Function)
Let $\gamma(k)$ be the autocovariance function of some process $y_t$
Then the \emph{autocovariance function} for $y_t$ is defined
\begin{align*}
  g_y(z) = \sum_{k=-\infty}^\infty \gamma(k)z^k
  \qquad z\in\C
\end{align*}
For this to be well-defined, we require the $\gamma(k)$ to be absolutely
summable.
\end{defn}

\begin{prop}\emph{(AGF of MA($\infty$))}
Suppose that the process $y_t$ follows an MA($\infty$) representation
with associated lag polynomial $\psi(z)$.
Then its autocovariance function equals
\begin{align*}
  g_y(z) = \sigma^2 \psi(z)\psi(z^{-1})
\end{align*}
But for this to be a well defined object, we need the coefficients of
the lag polynomial to be absolutely summable
$\sum_{j=0}^\infty|\psi_j|<\infty$.
\end{prop}

\begin{prop}
Suppose we filter the MA($\infty$) data series $\{x_t\}$ to produce the
new series $\{y_t\} = \{h(L)x_t\}$. Then
\begin{align*}
  g_y(z) = h(z)h(z^{-1})g_x(z)
\end{align*}
where $g_y(z)$ and $g_x(z)$ are the autocovariance generating functions
of $y$ and $x$, respectively.
\end{prop}
\begin{proof}
Since $x_t$ has an MA($\infty$) representation, we can write it as
\begin{align*}
  x_t = \theta(L)\varepsilon_t
  \quad\implies\quad
  g_x(z) = \sigma^2 \theta(z)\theta(z^{-1})
\end{align*}
Now consider
\begin{align*}
  y_t = h(L)x_t = h(L)\theta(L)\varepsilon_t
  \quad\implies\quad
  g_y(z) &= \sigma^2 h(z)\theta(z)h(z^{-1})\theta(z^{-1})
  \\
  &= h(z)h(z^{-1})\big(\sigma^2 \theta(z)\theta(z^{-1})\big)
  \\
  &= h(z)h(z^{-1})g_x(z)
\end{align*}
where everything exists and is well-defined since we assume that
$\theta(z)$ and $h(z)$ have absolutely summable coefficients.
\end{proof}



\clearpage
\subsection{Spectral Analysis: Time Series in the Frequency Domain}


\subsubsection{Motivation}


\begin{defn}(Simple Cyclical Process)
Suppose we construct the cyclical process
for random $\alpha$ and $\beta$:
\begin{align*}
  y_t = \alpha \cos(\lambda t) + \beta \sin(\lambda t)
  \qquad
  \text{where}\quad
  \begin{cases}
    \E[\alpha] = \E[\beta] = \E[\alpha \beta] = 0
    \\
    \E[\alpha^2] = \E[\beta^2] = \sigma^2
    \\
    \lambda \in [0,\pi)
  \end{cases}
\end{align*}
Features of this very particular type of process
\begin{enumerate}[label=(\roman*)]
  \item Only $\alpha$ and $\beta$ are random. If you know them, you know
    \emph{exactly} how the process will evolve over time since the
    process $y_t$ is otherwise deterministic.
  \item The process $\{y_t\}$ is cyclical/periodic, since for any
    integer $k$
    \begin{align*}
      y_{t+2\pi k/\lambda}
      %&= \alpha \cos\big(\lambda(t+2\pi k/\lambda)\big)
      %+\beta \sin\big(\lambda(t+2\pi k/\lambda)\big) \\
      &= \alpha \cos\big(\lambda t+2\pi k\big)
      +\beta \sin\big(\lambda t +2\pi k\big)
      \\
      &= \alpha \cos(\lambda t)
      +\beta \sin(\lambda t)
      = y_t
    \end{align*}

  \item
    More specifically, the process $\{y_t\}$ has frequency
    $\lambda/(2\pi)$, i.e.\ completes $\lambda/(2\pi)$ cycles between
    any $t$ and $t+1$.  Equivalently, the process has period
    $2\pi/\lambda$, i.e.\ it takes $2\pi/\lambda$ units of $t$ to
    complete one cycle. So if $t$ indexes days, months, or years, the
    process takes $2\pi/\lambda$ days, months, or years (respectively)
    to complete a single cycle.

  \item The restriction $\lambda \in[0,\pi)$ is without loss of
    generality since
    \begin{align*}
      \forall t,k
      \quad
      \sin\big((\lambda + k\pi)t\big)
      =
      (-1)^{tk}\sin(\lambda t)
    \end{align*}
    and likewise for cosine, and I could always use $-\alpha$ and
    $-\beta$ in place of $\alpha$ and $\beta$, which have the same first
    and second moments.

  \item The process $\{y_t\}$ has mean zero $\E[y_t]=0$ (clear by
    inspection) and second moments:
    \begin{align*}
      \gamma(k) = \E[y_ty_{t+k}]
        &=
        \E\left[
          \big(
          \alpha \cos(\lambda t)+\beta \sin(\lambda t)
          \big)
          \big(
          \alpha \cos(\lambda (t+k))+\beta \sin(\lambda (t+k))
          \big)
        \right] \\
        \text{Since $\E[\alpha\beta]=0$}\quad
        &=
        \E[\alpha^2]
        \cos(\lambda t)\cos(\lambda (t+k))
        +
        \E[\beta^2]
        \sin(\lambda t)\sin(\lambda (t+k))
        \\
        &=
        \sigma^2
        \big(
        \cos(\lambda t)\cos(\lambda (t+k))
        + \sin(\lambda t)\sin(\lambda (t+k))
        \big)
        \\
        &=
        \sigma^2
        \cos(\lambda k)
    \end{align*}
    where the last inequality followed from
    $\cos(v)\cos(u) + \sin(v)\sin(u) =\cos(u-v)$

  \item
    The process is perfectly linearly predictable from past values.
    To show this, guess that $y_t$ can be predicted by just two lags,
    $y_{t-1}$ and $y_{t-2}$, since there are only two unknown parameters
    of $y_t$ to pin down, $\alpha$ and $\beta$. Then compute the
    population regression coefficients of $y_t$ on $(y_{t-1}\;y_{t-2})$
    to derive the optimal linear predictor of $y_t$ given
    $(y_{t-1}\;y_{t-2})$. They turn out to be $(2\cos(\lambda)\; -1)$,
    suggesting that we can write
    \begin{align*}
      y_t = 2\cos(\lambda) y_{t-1} - y_{t-2} + \varepsilon_t
      \qquad \E[\varepsilon_t] = 0
    \end{align*}
    To show that $y_t$ is indeed \emph{perfectly} linearly predictable,
    we need to show that $\varepsilon_t=0$, i.e.\ that the RHS is
    \emph{exactly} the LHS. So consider
    \begin{align*}
      RHS &= 2\cos(\lambda) y_{t-1} - y_{t-2} \\
      &= 2\cos(\lambda)\big(
          \alpha \cos(\lambda(t-1)) + \beta\sin(\lambda(t-1))
          \big) \\
        &\qquad
        -\big(
          \alpha \cos(\lambda(t-2)) + \beta\sin(\lambda(t-2))
        \big)
    \end{align*}
    That's a bit of a mess, but by using the identities
    \begin{align*}
      \sin(a\pm b) &= \sin(a)\cos(b) \pm \cos(a)\sin(b)
      \\
      \cos(a\pm b) &= \cos(a)\cos(b) \mp \sin(a)\sin(b)
    \end{align*}
    the expression above can be simplified to exactly
    \begin{align*}
      2\cos(\lambda) y_{t-1} - y_{t-2}
      = \alpha \cos(\lambda t) + \beta \sin(\lambda t)
      = y_t
    \end{align*}
    That implies that $y_t$ is, indeed, perfectly linearly predictable.
\end{enumerate}
\end{defn}

\begin{defn}(More General Cyclical Process)
Construct the following cyclical process
\begin{align*}
  y_t = \sum_{j=1}^J \big[
    \alpha_j \cos(\lambda_j t)
    + \beta_j \sin(\lambda_j t)
  \big]
  \qquad
  \text{where}\quad
  \begin{cases}
    \E[\alpha_j] = \E[\beta_j] = \E[\alpha_j \beta_k] = 0
    \quad \forall j,k
    \\
    \E[\alpha_j^2] = \E[\beta_j^2] = \sigma^2_j
    \\
    \E[\alpha_j\alpha_k] = \E[\beta_j\beta_k] = 0
    \\
    \lambda_j \in [0,\pi)
  \end{cases}
\end{align*}
This process is again mean zero, but has second moments
\begin{align*}
  \gamma(k) = \E[y_ty_{t-k}]
  &= \sum_{j=1}^J \sigma_j^2\cos(\lambda_j k) \\
  \gamma(0) = \E[y_t^2] &= \sum_{j=1}^J \sigma_j^2
\end{align*}
This process is therefore the sum of $J$ distinct versions of the
simpler cyclical process defined above, each with a differenct frequency
$\lambda_j/(2\pi)$.
Accordingly, the variance formula above says that we can
\emph{decompose} the total variance of $y_t$ into contributions from
cycles/oscillations at the different frequencies. In particular, the
cycle at frequency $\lambda_j/(2\pi)$ contributes $\sigma^2_j$ to the
total variance of $y_t$.
\end{defn}


\clearpage
\subsubsection{Spectral Representation Theorem}

This subsection generalizes the previous one. It was really
nice to think about the ``contributions to the variance from
oscillations at different frequencies''. We would now like to do that
for \emph{any} arbitrary covariance stationary process $\{y_t\}$, not
just processes that we rig to be cyclic from the get go. Surely,
even for general processes, we want to be able to quantify how much of
the variability of $y_t$ is due to long, slow moving cycles versus
high-frequency cycles.

This next theorem lets us do that by effectively taking the more general
cyclical process above, adding together uncountably many cycles (so that
sums become integrals), and generalizing the frequency-specific
variances from a sequence $\{\sigma_j^2\}_{j=1}^J$ to a function
$f(\lambda)$.

All of this this provides a stochastic analog to Fourier analysis of
determistic functions, which expresses any function as a combination of
sine and cosine oscillations. The main goal there is typically to find
the scalar coefficients on those sines and cosines. Here, we do the
same, except the coefficients will be random variables.

\begin{thm}\emph{(Spectral Representation Theorem)}
Any stationary process $\{y_t\}$ with absolutely summable
autocovariances and mean $\mu$ can be represented as
\begin{align}
  y_t &= \mu +
  \int_0^\pi \alpha(\lambda) \cos(\lambda t)\;d\lambda
  + \int_0^\pi \beta(\lambda) \sin(\lambda t)\;d\lambda
  \label{spectralrep}
\end{align}
where $0 = \E[\alpha(\lambda)] = \E[\beta(\lambda)] = \E[\alpha(\lambda)
\beta(\lambda')] \notag = \E[\alpha(\lambda)\alpha(\lambda')] =
\E[\beta(\lambda)\beta(\lambda')]$
for all $\lambda,\lambda' \in [0,\pi)$ such that $\lambda\neq\lambda'$.
Also,
\begin{align*}
    \E[\alpha(\lambda)^2] = \E[\beta(\lambda)^2] = 2f(\lambda)
    \qquad \forall \lambda \in [0,\pi)
\end{align*}
where $f(\lambda):[0,\pi]\ra \R_+$ is the nonnegative \emph{spectral
density function} that captures the contribution of cycles/oscillations
at each frequency $\lambda\in[0,\pi)$ to the total variance of $y_t$.
\end{thm}

\begin{rmk}
Both $\alpha(\lambda)$ and $\beta(\lambda)$ are \emph{random functions}.
Just as $y_t$ is implictly a function on the sample space $\Omega$ and
on the time domain $\Z$, so too are $\alpha$ and $\beta$ implicitly
functions on sample $\Omega$ and frequency domain $[0,\pi)$.
Therefore, each particular $\omega\in\Omega$ gives rise to not only an
entire time series realization $\{y_t(\omega)\}_{t=-\infty}^\infty$, but
also to corresponding $\alpha(\lambda)(\omega)$ and
$\beta(\lambda)(\omega)$ function realizations that make
Representation~\ref{spectralrep} true for that particulare
$\omega$.
\end{rmk}
%\begin{rmk}
%The integrals above are a little handwavey. They really should be
%interpreted as
%\begin{align*}
  %\int_0^\pi \alpha(\lambda)\cos(\lambda t)\;d\lambda
  %=
  %\int_0^\pi \cos(\lambda t)\;dA(\lambda)
%\end{align*}
%\end{rmk}


\begin{cor}(\emph{Alternative Spectral Representation})
Given process $y_t$ with spectral Representation~\ref{spectralrep}, we
can express the process slightly differently. In particular, we
enlarge the domain of $\alpha$ and $\beta$ from $[0,\pi)$ to
$[-\pi,\pi)$ by defining, for all $\lambda \in[0,\pi)$
\begin{align*}
  \alpha(-\lambda) :=& \;\alpha(\lambda)
  \qquad
  \beta(-\lambda) := \;-\beta(\lambda)
\end{align*}
which allows us to rewrite the spectral decomposition of $y_t$ as
\begin{align*}
  y_t-\mu =&\; \int_{-\pi}^\pi \delta(\lambda)e^{i\lambda t}\;d\lambda
  =\int_{-\pi}^\pi
  \delta(\lambda)\big(\cos(\lambda t)+i\sin(\lambda t)\big)\;d\lambda
  \\
  \text{where}\quad
  \delta(\lambda) :=& \;
    \frac{1}{2}\big(\alpha(\lambda)-i\beta(\lambda)\big)
\end{align*}
which implies $\delta(-\lambda)=\overline{\delta(\lambda)}$ and
\begin{align*}
\E\left[\delta(\lambda)\overline{\delta(\lambda)}\right] = f(\lambda)
\end{align*}
\end{cor}


\clearpage
\subsubsection{Spectral Density Function}

\begin{prop}\emph{(Spectral Density and the Autocovariance Function)}
The autocovariance function is the inverse Fourier transform of the
spectral density:
\begin{align}
  \gamma(k)
  &= \int_{-\pi}^\pi f(\lambda)e^{i\lambda k}\;d\lambda
  \label{inversefourier}
  \\
  \iff\qquad
  &= \int_{-\pi}^\pi f(\lambda)\cos(\lambda k)\;d\lambda
  \notag
\end{align}
where the $\sin$ part of $e^{i\lambda k}$ drops out of the integral
because it is even. Alternatively, the spectral density is the
discrete-time Fourier transform of the autocovariance function:
\begin{align}
  f(\lambda)
  &= \frac{1}{2\pi} \sum_{k=-\infty}^\infty
  \gamma(k) e^{-i\lambda k}
  \label{fourier}
  \\
  \iff\qquad
  &= \frac{1}{2\pi}
  \left[
    \gamma(0) + 2\sum_{k=1}^\infty \gamma(k)\cos(k\lambda)
  \right]
  \notag
\end{align}
Therefore, there is a one-to-one mapping between the spectral density
$f(\lambda)$ and the autocovariance function $\gamma(k)$.
All of this assumes absolute summability of $\gamma(k)$.
\end{prop}
\begin{proof}
First, the inverse Fourier transform:
\begin{align*}
  \gamma(k)=
  \E[y_ty_{t-k}] =
  \E[y_t\overline{y_{t-k}}]
  &=
  \E\left[
    \left(
    \int_{-\pi}^\pi \delta(\lambda)e^{\lambda t} \; d\lambda
    \right)
    \left(
    \int_{-\pi}^\pi \overline{\delta(\lambda')}e^{-\lambda' (t-k)} \;
    d\lambda'
    \right)
  \right]
  \\
  &=
  \int_{-\pi}^\pi \int_{-\pi}^\pi
  \E\left[
    \delta(\lambda) \overline{\delta(\lambda')}
  \right]
  e^{\lambda t}e^{-\lambda' (t-k)} \;
  \; d\lambda \; d\lambda'
\end{align*}
But recall from the definition of $\delta(\lambda)$ that
$\E[\delta(\lambda)\overline{\delta(\lambda')}]=0$ for all
$\lambda\neq\lambda'$ and that
$\E[\delta(\lambda)\overline{\delta(\lambda)}]=f(\lambda)$.
Therefore, the above integral simplifies precisely to
Expression~\ref{inversefourier}.
Conversely, to show Expression~\ref{fourier}, we will show that
we can plug Expression~\ref{fourier} into
Expression~\ref{inversefourier} for $f(\lambda)$ and get the correct
result, i.e.\ we want to show that
\begin{align*}
  \gamma(k)
  &= \int_{-\pi}^\pi
  \underbrace{%
  \left[
  \frac{1}{2\pi} \sum_{j=-\infty}^\infty
  \gamma(j) e^{-i\lambda j}
  \right]
  }_{\text{Candidate $f(\lambda)$}}
  e^{i\lambda k}\;d\lambda \\
  &= \frac{1}{2\pi}
  \sum_{j=-\infty}^\infty
  \gamma(j)
  \int_{-\pi}^\pi
  e^{i\lambda (j-k)}
  \;d\lambda
  \\
  &= \frac{1}{2\pi}
  \sum_{j=-\infty}^\infty
  \gamma(j)
  \cdot2\pi \mathbf{1}_{\{j=k\}}
  =\gamma(k)
\end{align*}
where we could switch the order of integration because
$|e^{i\lambda (k-j)}|<1$ and $\sum_k|\gamma(k)|<\infty$.
\end{proof}


\clearpage
\begin{cor}\emph{(Decomposing the the Variance with $f(\lambda)$)}
Setting $k=0$, we see that the total variance $\gamma(0)$ equals the
area under the population spectrum:
\begin{align*}
  \gamma(0) = \int_{-\pi}^\pi f(\lambda) \; d\lambda
\end{align*}
Therefore, if we integrate over frequency range
$[\lambda_1,\lambda_2]\subseteq[0,\pi]$,
using symmetry of $f(\lambda)$,
\begin{align*}
  2\int_{\lambda_1}^{\lambda_2} f(\lambda) \; d\lambda
\end{align*}
this integral tells us how much oscillations/cycles with frequencies
$\lambda \in[\lambda_1,\lambda_2]$ contribute to the overall variance
$\gamma(0)$ of the process.
\end{cor}


\begin{cor}
\emph{(Spectral Density and the Autocovariance Generating Function)}
Given the autocovariance generating function of a process
\begin{align*}
  g(z)=\sum_{k=-\infty}^\infty \gamma(k)z^k
\end{align*}
we can immediately recover the spectral density function $f(\lambda)$:
\begin{align*}
  f(\lambda) = \frac{1}{2\pi} g(e^{-i\lambda})
  = \frac{1}{2\pi} \sum_{k=-\infty}^\infty \gamma(k) e^{-i\lambda k}
\end{align*}
\end{cor}

\begin{cor}\emph{(Spectral Density of MA($\infty$))}
Suppose we have a covariance-stationary process $y_t$ that has an
MA($\infty$) representation:
\begin{align*}
  y_t = \psi(L)\varepsilon_t
\end{align*}
Then it has spectral density function
\begin{align*}
  f(\lambda) = \frac{\sigma^2}{2\pi}\cdot
  \psi(e^{i\lambda})\psi(e^{-i\lambda})
  = \frac{\sigma^2}{2\pi}\cdot
  \left\lvert
  \psi(e^{i\lambda})
  \right\rvert^2
  %\frac{\theta(e^{i\lambda})\theta(e^{-i\lambda})}{%
    %\phi(e^{i\lambda})\phi(e^{-i\lambda})}
  %= \frac{\sigma^2}{2\pi}\cdot
  %\left\lvert
  %\frac{\theta(e^{i\lambda})}{%
    %\phi(e^{i\lambda})}
  %\right\rvert^2
\end{align*}
where $|z|$ denotes the modulus of the complex number number $z$.
\end{cor}
\begin{proof}
This follows immediately from the previous corralary and the fact that
the autocovariance generating function $g_y$ of an MA($\infty$) process
is
\begin{align*}
  g_y(z) = \sigma^2 \psi(z^{-1})\psi(z)
  %g_y(z) = \sigma^2 \frac{\theta(z^{-1})\theta(z)}{\phi(z^{-1})\phi(z)}
\end{align*}
\end{proof}

\begin{cor}\emph{(Spectral Density of an ARMA(p,q) Process)}
Suppose that $y_t$ is a covariance stationary ARMA process
\begin{align*}
  \phi(L) y_t = \theta(L)\varepsilon_t
  \qquad
  y_t = \phi(L)^{-1}\theta(L)\varepsilon_t
\end{align*}
Then it has an MA($\infty$) representation and spectral density
function as in the previous corallary, taking
$\psi(z) = \phi(z)^{-1}\theta(z)$.
\end{cor}



\begin{cor}\emph{(White Noise)}
It is clear from the inverse Fourier transform in
Equation~\ref{inversefourier} that $f(\lambda)=c$ (i.e.\ the spectral
density is flat/constant) implies the process is white noise,
$\gamma(k)=0$ for $k\neq 0$.
It is also clear from that formula and the constant spectral density
that \emph{all frequencies} contribute equally to the variance
$\gamma(0)$.
\end{cor}


\clearpage
\subsection{Long-Run Variance}

With iid observations, the variance of a sum of observations
$\{y_t\}$ is the sum of the variances. When the observations are
correlated, that is no longer true. Instead, we need to define the
\emph{long-run variance}---the asymptotic variance of a sum of a series
(suitably scaled), allowing for correlation.

\begin{prop}\emph{(Long-Run Variance)}
Suppose $\{y_t\}$ is a stationary and ergodic process with absolutely
summable autocovariances and $\E[y_t]=\mu$. Define the
\emph{long-run variance} as
\begin{align}
  S := \limT \Var\left(\frac{1}{\sqrt{T}}\sumtT (y_t-\mu)\right)
  = \sum_{j=-\infty}^\infty \gamma(j)
  \label{clt:lrvar}
\end{align}
That last equality is not a definition; it's a result, which we now
(kinda) prove.
\end{prop}
\begin{proof}
First, start with expression for the finite-sample (of size $T$)
statistic:
\begin{align*}
 S_T
 := \frac{1}{T}\Var\left(\sumtT (y_t-\mu)\right)
\end{align*}
Since the observations are autocorrelated, the variance of the sum does
\emph{not} equal the sum of the variances. There are covariance terms,
and we have to account for them:
\begin{align*}
 T \cdot S_T
 &= \sumtT \sum_{s=1}^T\Cov((y_t-\mu), (y_s-\mu)) \\
 &= T\cdot \gamma(0) + \sum_{j=1}^T (T-|j|)\cdot \gamma(j)
\end{align*}
Dividing through on both sides by $T$ and taking the limit as
$T\ra\infty$ gives the desired expression, since $(T-|j|)/T \ra 1$ for
all $j$. Absolutely summable autocovariances ensures convergence.
\end{proof}


\begin{prop}
\emph{(Equivalent Expressions for the Long-Run Variance)}
Given stationary and ergodic $\{y_t\}$ with $\E[y_t]=\mu$, the long-run
variance $S$ as defined in Expression~\ref{clt:lrvar} satisfies
\begin{align*}
  S = \sum_{j=-\infty}^\infty \gamma(j) = g(1) = 2\pi f(0)
\end{align*}
where
\begin{enumerate}[label=(\roman*)]
  \item $\gamma(j)$ is the $j$th autocovariance
    $\E[(y_t-\mu)(y_{t+j}-\mu)]$
  \item $g(z)$ is the autocovariance generating function
    $g(z)=\sum_{j=-\infty}^\infty \gamma(j)z^j$
  \item $f(\lambda)$ is the spectral density of the process
\end{enumerate}
\end{prop}

\begin{cor}
For an ARMA process with MA($\infty$) representation has a very simple
long-run variance formula:
\begin{align*}
  \begin{rcases}
  y_t &= \psi(L)\varepsilon_t
  \qquad \varepsilon_t \sim (0,\sigma^2) \\
  \psi(z) &= \psi_0 + \psi_1z + \psi_2z^2 + \cdots
  \end{rcases}
  \quad\implies\quad
  S = \sigma^2 \psi(1)^2
\end{align*}
\end{cor}

\clearpage
\section{Multivariate Time Series}

\subsection{Generalizing the Univariate Concepts and Formulas}

\begin{defn}(Covariance Stationary Process)
A vector stochastic process $\{\bsy_t\}$ is
\emph{covariance stationary} if the mean and the autocovariance function
does not depend upon time. Now though, the mean is a vector and the
autocovariance function that maps into matrices:
\begin{align*}
  \E[\bsy_t] &= \bsmu
  \qquad
  \Cov(\bsy_t,\bsy_{t+k})
  = \E[(\bsy_t-\bsmu)(\bsy_t-\bsmu)']
  = \bsGamma(k)
\end{align*}
Again, $\bsGamma(k)$ is an autocovariance function that doesn't depend
upon time, just lag length $k$.
\end{defn}

\begin{defn}(Vector White Noise Process)
$\{\bsy_t\}$ a \emph{vector white noise process} if $\forall t$
\begin{align*}
  \E[\bsy_t] &= \bso
  \\
  \E[\bsy_t\bsy_t'] &= \bsSigma
  \quad\text{symmetric positive definite}
  \\
  \E[\bsy_t\bsy_{t-k}']
  &= \bso \qquad \forall k\neq 0
\end{align*}
We here place no other restriction on $\bsSigma$ except for symmetric
positive-definiteness. Therefore, we allow for contemporaneous
correlation between elements of $\bsy_t$.
\end{defn}

\begin{defn}(Multivariate Filter)
Given sequence of (possibly non-square) matrices
$\{\bsH_j\}_{j=0}^\infty$, the associated
\emph{multivariate filter}
is defined as
\begin{align*}
  \bsH(z)
  = \sum_{j=1}^\infty \bsH_j z^j
  = \bsH_0 + \bsH_1 z + \bsH_2z^2 + \cdots
\end{align*}
\end{defn}

\begin{defn}(Lag Polynomial Inverse)
Let $\bsH(L)$ be a lag polynomial formed from filter $\bsH(z)$ with
square-matrix elements. Define the lag polynomial inverse as the filter
$\bsH(L)^{-1}$ such that
\begin{align*}
  \bsH(L)^{-1}\bsH(L) =
  \bsH(L)\bsH(L)^{-1} = \bsI
\end{align*}
\end{defn}

\begin{defn}(Autocovariance Generating Function)
For vector process $\{\bsy_t\}$, we define the autocovariance generating
function
\begin{align*}
  \bsg_{\bsy}(z) = \sum_{j=-\infty}^\infty \bsGamma(j)z^j
\end{align*}
\end{defn}

\begin{defn}(Spectral Density)
Vector process $\{\bsy_t\}$ has spectral density function
\begin{align*}
  \bsf_{\bsy}(\lambda) = \frac{1}{2\pi} \bsg_{\bsy}(e^{-i\lambda})
\end{align*}
\end{defn}

\begin{prop}
Let $\{\bsx_t\}$ be a vector process and $\bsH(z)$ a conformable
multivariate filter. Then filtered series
$\bsy_t = \bsH(L)\bsx_t$
has autocovariance generating function
\begin{align*}
  \bsg_{\bsy}(z) = \bsH(z)\bsg_{\bsx}(z)\bsH(z^{-1})'
\end{align*}
\end{prop}

\clearpage
\subsection{VMA, VAR, and VARMA Models}

\begin{defn}(VMA Process)
Given vector white noise process $\{\bsvarepsilon_t\}$, mean $\bsmu$,
and multivariate lag polynomial $\bsPsi(L)$, we define the vector
MA($\infty$) process as
\begin{align*}
  (\bsy_t - \bsmu) = \bsPsi(L)\bsvarepsilon_t
  \qquad \bsvarepsilon_t \sim WN(\bso,\bsSigma)
\end{align*}
It has autocovariance function
\begin{align*}
  \bsGamma(j)
  &= \sum_{k=0}^\infty \bsPsi_{j+k}\bsSigma\bsPsi_k'
\end{align*}
\end{defn}

\begin{prop}\emph{(Invertibility of VMA Process)}
The VMA process with lag polynomial $\bsPsi(L)$ is invertible if and
only if all roots of $\text{\emph{det}}(\bsPsi(z))=0$ are outside the
complex unit circle.
\end{prop}

\begin{defn}(VAR($p$) Process)
$\{\bsy_t\}$ is a VAR($p$) process if it can be written
\begin{align*}
  \bsPhi(L)(\bsy_t-\bsmu) &= \bsvarepsilon_t
  \qquad\qquad\qquad\qquad\qquad \bsvarepsilon_t\sim WN(0,\bsSigma)
  \\
  \bsPhi(L) &= \bsI - \bsPhi_1 L - \cdots - \bsPhi_pL^p
\end{align*}
\end{defn}

\begin{prop}\emph{(Stability Condition)}
Suppose we have VAR($p$) process
\begin{align*}
  \bsPhi(L)\bsy_t &= \bsvarepsilon_t
  \qquad \bsvarepsilon_t \sim WN(0,\bsSigma)
  \\
  \text{\emph{where}}\quad
  \bsPhi(L) &= \bsI - \bsPhi_1 L - \cdots - \bsPhi_pL^p
\end{align*}
The process is covariance stationary if the following statements are
true (they are equivalent):
\begin{itemize}
  \item
    All roots of the following equation have modulus $>1$:
    $\det(\bsPhi(z)) = 0$
  \item
    All roots of the following equation have modulus $<1$:
    \begin{align*}
      \det\big(
        \bsI z^p - \bsPhi_1z^{p-1}
        - \cdots - \bsPhi_p z^0
      \big)
      =0
    \end{align*}
\end{itemize}
\end{prop}

\begin{defn}(VARMA($p,q$) Process)
$\{\bsy_t\}$ is a VARMA($p,q$) process if it can be written
\begin{align*}
  \bsPhi(L)(\bsy_t-\bsmu) &= \bsTheta(L)\bsvarepsilon_t
  \qquad\qquad\qquad\qquad\qquad \bsvarepsilon_t\sim WN(0,\bsSigma)
  \\
  \bsPhi(L) &= \bsI - \bsPhi_1 L - \cdots - \bsPhi_pL^p
  \\
  \bsTheta(L) &= \bsTheta_0 + \bsTheta_1 L + \cdots + \bsTheta_qL^q
\end{align*}
\end{defn}

\clearpage
\subsection{Impulse Response Function and Variance Decomposition}

In this section, we suppose that we have a model with VMA representation
\begin{align*}
  \bsy_t = \bsPsi(L)\bsvarepsilon_t
  \qquad
  \bsvarepsilon_t \wn(\bso,\bsSigma)
  \qquad\bsy_t,\bsvarepsilon_t\in \R^M
\end{align*}
The zero-mean assumption for $\bsy_t$ is without loss of generality.

\begin{defn}(Impulse Response Function)
The \emph{impulse respose function} of variable $i$ to shock $j$ after
$k$ periods is given by
\begin{align*}
  IRF_{ij}(k)
  &=
  \frac{\partial y_{t,i}}{\partial \varepsilon_{t-k,j}}
  =
  \frac{\partial y_{t+k,i}}{\partial \varepsilon_{t,j}}
  = \Psi_{k,ij}
\end{align*}
We think of this as a function of $k$.
\end{defn}

\begin{defn}(Variance Decomposition)
Assume that $\bsSigma$ is a diagonal matrix. As we will see below, this
is typical for structural models.
To obtain a (forecast error) \emph{variance decomposition}, we let
$R^2_{ij}(h)$ denote the fraction of $h$-period-ahead forecast error
variance for $i$th observable $y_{t,i}$ that comes from the $j$th
exogenous shock. It is given by
\begin{align}
  R^2_{ij}(h)
  %= \frac{\text{Contribution of $\varepsilon_{t,j}$}}{%
          %\text{Total for $y_{t,i}$}}
  %=
  %\frac{ }{%
    %\left[
    %\sum_{k=0}^{h-1} \bsPsi_k\bsSigma \bsPsi_k'
    %\right]_{ii}
  %}
  =
  \frac{%
    \sum_{k=0}^{h-1}
    \Sigma_{jj} \Psi_{k,ij}^2
  }{%
    \sum_{k=0}^{h-1} \sum_{m=1}^{M_\varepsilon}
    \Sigma_{mm} \Psi_{k,im}^2
  }
  \label{vardecomp}
\end{align}
To see where this comes from, note that the best linear forecast of
$\bsy_{t+h}$ given $\bsvarepsilon_t,\bsvarepsilon_{t-1},\ldots$ is given
by the conditional expectation
\begin{align*}
  \E_t[\bsy_{t+h}]
  =
  \E_t\big[
    \bsPsi_0\bsvarepsilon_{t+h}
    + \bsPsi_1\bsvarepsilon_{t+h-1}
    + \cdots
  \big]
  = [\bsPsi(L)L^{-h}]_+\;\bsvarepsilon_t
  = [\bsPsi(L)L^{-h}]_+\;L^h\bsvarepsilon_{t+h}
\end{align*}
where $[\;\cdot\;]_+$ denotes ``take only those terms where the lag term
is nonnegative,'' i.e.\ those $L^k$ where $k\geq 0$.
This implies that the forecast error is given by
\begin{align*}
  \bsy_{t+h} - \E_t[\bsy_{t+h}]
  &=
  \bsPsi(L)\varepsilon_{t+h}
  - [\bsPsi(L)L^{-h}]_+\;L^h\bsvarepsilon_{t+h}
  =
  \sum_{k=0}^{h-1}
  \bsPsi_k \varepsilon_{t+h-k}
\end{align*}
The total variance matrix for the forecast error is therefore given by
\begin{align}
  \Var\big(
  \bsy_{t+h} - \E_t[\bsy_{t+h}]
  \big)
  &=
  \Var\left(
  \sum_{k=0}^{h-1}
  \bsPsi_k \varepsilon_{t+h-k}
  \right)
  =
  \sum_{k=0}^{h-1} \bsPsi_k \Var\left(
  \varepsilon_{t+h-k}
  \right)\bsPsi_k'
  =
  \sum_{k=0}^{h-1}
  \bsPsi_k\bsSigma \bsPsi_k'
  \label{totalvar}
\end{align}
where we made use of the fact that the shocks $\bsvarepsilon_t$ are
white noise with zero autocorrelation.

To get to Expression~\ref{vardecomp}, first note that the $ii$ element
of the matrix in Expression~\ref{totalvar} represents the total amount
variance of the forecast error for the $i$th element of $\bsy_t$.
Naturally, we then ask ``What fraction of the forecast error variance
for observable $i$ in $\bsy_t$ is driven by shock $j$ in
$\bsvarepsilon_t$?'' Since $\bsSigma$ is a diagonal matrix, the formulas
simplify nicely to Expression~\ref{vardecomp} after writing everything
out.
\end{defn}



\clearpage
\subsection{Identification of Structural VMA($q$)}


Suppose that the data $\{\bsy_t\}$ follows MA($q$) process where
$\bsy_t\in\R^{M_y}$, $\bsvarepsilon_t\in\R^{M_\varepsilon}$, and
\begin{align*}
  \bsy_t &=
  \bsPsi_0\bsvarepsilon_t
  +
  \bsPsi_1\bsvarepsilon_{t-1}
  +\dots+
  \bsPsi_q\bsvarepsilon_{t-q}
  \qquad \bsvarepsilon_t \sim WN(\bso,\bsSigma)
\end{align*}
Our goal is to identify parameters in $\bsPsi_0,\ldots,\bsPsi_q$ and
$\bsSigma$ given observations of the data $\{\bsy_t\}$.
Since \emph{both} $\bsvarepsilon_t$ and the parameters are unknown, we
can reasonably suspect that we might have identification problems.
We start with reasonable restrictions that resolve some of this.
\\
\\
(\emph{Obvious and Innocuous Assumptions})
We start with non-controversial stuff:
\begin{enumerate}[label=(\roman*)]
  \item $M_y\geq M_\varepsilon$: Clearly, this restriction must be true.
    Otherwise, we would have an ``extra'' unobserved shock that we have
    no hope of identifying from the data.
  \item \emph{Number of Parameters to Identify}, $M:=M_y=M_\varepsilon$:
    This assumption is often used in practice, so I will also adopt it.
    Therefore, the number of parameters to estimate is
    \begin{align*}
      (\text{Number of Parameters to Estimate})
      &= (q+1) M^2
      + M(M+1)/2
    \end{align*}
    The first term counts the number of parameters in unknown matrices
    $\bsPsi_0,\ldots,\bsPsi_q$. The second term counts unknown
    parameters in symmetric covariance matrix $\bsSigma$.
  \item \emph{Information in the Data}: Given $\{\bsy_t\}$, all the
    information we have for identification comes from estimating the
    autocovariance structure of the data $\bsGamma(k)$, for
    $k=0,\ldots,q$ (the nonzero autocovariance matrices up to assumed
    order $q$). Therefore, the data pins down the following number of
    parameters
    \begin{align*}
      (\text{Number of Data-Identified Parameters})
      = q M^2
      + M(M+1)/2
    \end{align*}
    The first term counts the number of parameters in non-symmetric
    matrices $\bsGamma(1),\ldots,\bsGamma(k)$. The second term counts
    the number of parameters in symmetric $\bsGamma(0)$.

  \item \emph{The Problem}: The number of data-identified parameters
    is less than the number of parameters to idenify in the VMA model.
    The difference is $M^2$ parameters.

  \item \emph{Scale Normalization}: We often assume that
    $\Psi_{0,ii}=1$, i.e.\ ones on the diagonal. Thus a
    one-unit $\bsvarepsilon_t$ shock (i.e.\ where
    $\varepsilon_{t,i}=1$ for all $i$) translates into a one-unit
    increase in each element of $\bsy_t$. The scale of $\bsvarepsilon_t$
    and $\bsy_t$ are thus normalized to be the same. Consequently,
    $\sqrt{\Sigma_{ii}}$ is the standard deviation---thus the
    average size---of the $i$th shock.

    Alternatively, we could set $\Sigma_{ii}=1$ on the diagonal.
    Then $\bsSigma$ is a correlation matrix with $\sqrt{\Psi_{0,ii}}$
    giving the standard dev. of shock $i$. Either way, we've identified
    $M$ parameters.

  \item \emph{Diagonal $\bsSigma$}: This implies no contemporaneous
    correlation between exogenous shocks $\bsvarepsilon_t$. Even though
    we want contemporaneous correlation between elements of $\bsy_t$,
    this assumption makes sure that the correlation is driven by
    parameters in $\bsPsi_0$---by the endogenous response of
    the system---not by correlation between the \emph{fundamental},
    \emph{structural} shocks $\bsvarepsilon_t$.
    This means we have identified an extra $\frac{M(M+1)}{2} -
    M = \frac{M(M-1)}{2}$ parameters.
\end{enumerate}
(\emph{Remaining Scale \& Space of Underidentification})
Now that we've gotten all of the obvious and innocuous stuff out of the
way, we still have the following degree of underidentification:
\begin{align*}
  \underset{(iv)}{M^2\vphantom{M(M-1)/2}} -
  \underset{(v)}{M\vphantom{M(M-1)/2}}
  - \underset{(vi)}{M(M-1)/2}
  = M(M-1)/2
\end{align*}
Term (iv) is the initial number of unidentified parameters from point
(iv) above. Term (v) is the number of parameters we identified by scale
normalization. Term (vi) is the number of parameters we identified by
diagonal $\bsSigma$.

We can be more precise about all of this by saying ``the
\emph{space of underidentification} is spanned by the set of $M\times M$
orthogonal matrices (i.e. $\bsQ\bsQ'=\bsI_M$) with $M(M-1)/2$ free
parameters.'' To see this, assume diagonal $\bsSigma$ and use the
alternative scale normalization approach, which together imply
$\bsSigma=\bsI_M$ with $\bsPsi_0$ totally unrestricted.  Now take
\emph{any} $M\times M$ orthogonal matrix with $M(M-1)/2$ free
parameters, $\bsQ$. Then
\begin{align*}
  \Var(\bsQ\bsvarepsilon_t)
  &= \bsQ\Var(\bsvarepsilon_t)\bsQ'
  \underset{(a)}{=} \bsQ\bsI_M\bsQ'
  \underset{(b)}{=} \bsI_M
\end{align*}
where equality (a) followed from scale normalization and (b) from
orthogonality of $\bsQ$. In words, $\bsQ$ just rotates the errors while
still preserving their covariance matrix. Then
\begin{align*}
  \bsy_t
  &=
  \bsPsi_0\bsQ \bsvarepsilon_t
  + \bsPsi_1\bsQ \bsvarepsilon_{t-1}
  + \cdots +
  + \bsPsi_q\bsQ \bsvarepsilon_{t-q}
  \qquad \bsvarepsilon_t\sim WN(\bso,\bsI_M)
  \\
  &=
  \bstildePsi_0 \bsvarepsilon_t
  + \bstildePsi_1 \bsvarepsilon_{t-1}
  + \cdots +
  + \bstildePsi_q \bsvarepsilon_{t-q}
\end{align*}
has identical dynamics and 2nd order properties as the original
representation with $\bsPsi_0,\ldots,\bsPsi_q$.

Therefore, without further assumptions, we can rotate things by
orthogonal matrices in $\R^{M\times M}$ that have $M(M-1)/2$ free
parameters and \emph{not change anything}. This is the sense in which
the system is underidentified. We now consider the various approaches we
have to deal with this underidentification problem.
\\
\\
(\emph{Resolving Remaining Underidentification})
From this point, we can proceed by any of the following proposed routes
for resolving the underidentification problem.
\begin{itemize}
  \item \emph{Recursive/Zero Restrictions} (Sims):
    Assume certain elements of $\bsPsi_0$ equal zero. These kinds of
    assumptions are essentially ``We think a money supply shock hits
    measured M2 right away, but not GDP right away---that adjusts with a
    lag.'' These types of restrictions imply a triangular structure for
    $\bsPsi_0$, which establishes identification.
  \item \emph{Long-Run Restrictions} (Blanchard and Quah): Place
    restrictions on the long run effects of some shocks. In particular,
    we might assume that variable $i$ has no long-run response to an
    exogenous $j$ shock, i.e.\ that
    \begin{align*}
      \sum_{k=0}^q IRF_{ij}(k)
      \sum_{k=0}^q \Psi_{k,ij}
      = \Psi(1)_{ij}
      = 0
    \end{align*}
  \item \emph{Sign Restrictions} (Uhlig): Just impose restrictions on
    the signs of coefficients, rather than on their values or some
    function of their values. This only set-identifies the model, since
    there is clearly a range of parameter values for which the sign
    restriction is true. Therefore, Bayesian methods are typically used.
\end{itemize}


\clearpage
\subsection{Structural VARs}

\begin{defn}(SVAR Representation)
Assume that the lag polynomial of the VMA representation, $\bsPsi(L)$,
is invertible with $\bsPsi(L)^{-1}=\bsA(L)$. Then the SVAR
representation is
\begin{align*}
  \bsA_0\bsy_t
  - \bsA_1\bsy_{t-1}
  - \bsA_2\bsy_{t-2}
  - \cdots
  = \bsA(L)\bsy_t
  = \bsvarepsilon_t
  \qquad \bsvarepsilon_t \sim WN(\bso,\bsSigma)
\end{align*}
If $\bsA(L)$ is of order $p$, then it has SVAR($p$) representation
\begin{align}
  \bsA_0\bsy_t
  =
  \bsA_1\bsy_{t-1}
  + \cdots
  + \bsA_p\bsy_{t-p}
  + \bsvarepsilon_t
  \label{svarp}
\end{align}
Note that this requires invertibility of $\bsPsi(L)$, which is not
guaranteed. But even if we can't estimate this SVAR, we can estimate
the reduced form VAR.
\end{defn}

\begin{defn}(Identification of SVAR)
Just like with the SVMA, there are identification issues for the SVAR.
But everything we did in the SVMA has an analog here:
\begin{itemize}
  \item \emph{Diagonal $\bsSigma$}: This again rules out contemporaneous
    correlation between the fundamendal, structural shocks
    $\bsvarepsilon_t$, implying that contemporaneous correlation between
    elements of $\bsy_t$ is driven by the endogenous response of the
    system.
  \item \emph{Scale Normalization}: For the SVAR, this means
    $\bsA_{0,ii}=1$.
  \item \emph{Recursive, Triangular Structure}: Imposing triangular
    $\bsA_0$ is the same as imposing a triangular $\bsPsi_0$. It implies
    a recursive structure on the endogenous respose, with some variables
    being unable to respond contemporaneously to changes in other
    variables.
  \item \emph{Long Run Restrictions}: Imposing long-run restrictions is
    equivalent to imposing restrictions on $\bsA(1)^{-1}$, which sums up
    the IRFs associated with the structural VAR that has lag polynomial
    $\bsA(L)$.
\end{itemize}
\end{defn}

\begin{defn}(Reduced Form VAR($p$))
Given data $\{\bsy_t\}$, we can always estimate a
\emph{reduced form VAR($p$)},
\begin{align}
  \bsy_t =
  \bsPhi_1\bsy_{t-1}
  + \cdots + \bsPhi_1\bsy_{t-p} + \bsu_t
  \qquad
  \bsu_t \sim WN(\bso,\bsOmega)
  \label{varp}
\end{align}
If $\{\bsy_t\}$ does indeed have a VAR($p$) representation, then it's
clear from comparing Expressions~\ref{svarp} and \ref{varp} that
\begin{align*}
  \bsPhi_i = \bsA_0^{-1}\bsA_i
  \qquad
  \bsu_t = \bsA_0^{-1}\bsvarepsilon_t
  \qquad
  \bsOmega = \big(\bsA_0^{-1}\big)\bsSigma\big(\bsA_0^{-1}\big)'
\end{align*}
\end{defn}
\begin{rmk}
This is always possible to estimate---even if $\bsPsi(L)$ is not
invertible---and it poses no identification issues. The only downside is
that $\bsu_t\neq \bsvarepsilon_t$ and $\bsOmega\neq\bsSigma$ (with
$\bsOmega$ \emph{not} diagonal in general) so that the shocks have no
clear economic interpretation. This is why it's called
\emph{reduced form}, not \emph{structural}
\end{rmk}






\clearpage
\section{Large Sample Theory}


\subsection{Modes of Convergence}

Recall the usual Analysis definition of convergence of a sequence: A
sequence $\{x_n\}$ converges to limit $x$ if for all $\varepsilon> 0$,
there exists a finite $N$ such that
\begin{align*}
  n > N \implies |x_n - x| < \varepsilon
\end{align*}
But this, of course, is for a sequence of numbers $\{x_n\}$ in some
metric space. There's no randomness there. So what about a sequence of
random variables---something non-deterministic?

There are a few options when talking about convergence of random
variables, but all involve getting rid of the randomness somehow and
checking convergence of a sequence of \emph{functions} (random
variables) or a sequence of \emph{numbers} (where that number represents
a probability or expectation). And checking convergence of functions or
numerical sequences are both well within the domain of the usual
analysis machinery. More specifically, these alternatives give rise to
the following three different types of convergence:
\begin{enumerate}
  \item \emph{Almost Sure Convergence}: Check that the set of points
    $\omega\in\Omega$ where the sequence $X_n(\omega)\not\ra
    X(\omega)$ has probability zero.
  \item \emph{Convergence in Probability}: The sequence of probabilities
    that $X_n(\omega)$ is far from $X(\omega)$ over all $\omega$ goes to
    zero.
  \item \emph{Converge in $p$-Norm}: The sequence of expected distances
    $\E[|X_n-X|^p]$ goes to zero.
\end{enumerate}
In all of these cases, we've framed convergence of random variables in
terms of convergence of functions or numerical sequences. We can then
easily work with the usual analysis definitions. Now onto the formal
definitions.

\begin{defn}{(Almost Sure, Almost Everywhere, or Strong Convergence)}
Given a sequence of random variables $\{X_n\}$ and a random variable
$X$ on probability space $(\Omega,\sF,P)$, we say that ``$X_n$
\emph{converges almost surely} to $X$'' written
\begin{align*}
  X_n\asto X
  \quad \iff \quad
  \Prb\left[\limn X_n = X\right]
  = \Prb\left[\,\omega \,\big|\,\limn X_n(\omega) = X(\omega)\right]
  = 1
\end{align*}
This checks that the measure of the set of $\omega$'s where
$\limn X_n(\omega)\neq X(\omega)$ is zero, i.e.\ that $X_n$ matches $X$
almost everywhere on $\Omega$.
Note the function can differ arbitrarily on sets that have measure zero,
or zero probability of occurring. But on those events that will occur
with some nonzero probability, $X_n$ matches $X$.  If $X_n$ and $X$ are
vectors, we need almost surely convergence element-by-element.

This notion is called ``strong convergence'' because it it computes the
probability of the limit of a sequence of random variables, rather than
the limit of a sequence of probabilities. We're effectively asking that
the functions $X_n:\Omega\ra\R$ (the random variables in the sequence)
\emph{converge pointwise} to $X:\Omega\ra\R$ (the limiting random
variable) almost everywhere on the sample space $\Omega$.
\end{defn}


\begin{defn}{(Convergence in Probability)}
We say that a sequence of random variables $\{ X_n \}$ on measure space
$(\Omega,\sF,\Prb)$ \emph{converges in probability} to $X$ if
\begin{equation}
  \label{plim}
  X_n\pto X
  \quad\iff\quad
  \limn p_n(\varepsilon) :=
  \limn
  \Prb(\left\lvert X_n - X \right\rvert > \varepsilon) = 0
  \qquad \forall  \varepsilon> 0
\end{equation}
This is sometimes written as $\plim X_n = X$.
If $X_n$ and $X$ are vectors, we need convergence in probability
element-by-element.

Convergence in probability is also known as \emph{weak convergence}
because it doesn't ask the random variables (functions) $X_n$ to
converge to the random variable (function) $X$, i.e.\ that
$\limn X_n(\omega)=X(\omega)$ identically for almost all
$\omega\in\Omega$.
Instead, we just stipulate that the probability of $X_n$ being far from
$X$ over all $\Omega$ is small as $n\rightarrow\infty$.
\end{defn}

\begin{defn}{(Mean Square and $p$-norm or $L^p$ Convergence)}
We say that a sequence of random variables $\{ X_n \}$
\emph{converges in $p$-norm} or \emph{in $L^p$} to $X$, written
\begin{align*}
  X_n \Lpto X
  \quad\iff\quad
  \limn \E\left[|X_n-X|^p\right] &= 0 \\
  \iff\quad\;\;\;
  \limn ||X_n-X||_p &= 0
\end{align*}
The special case where $p=2$ is called \emph{mean square convergence} or
\emph{convergence in quadratic mean}, and is often written $X_n\msto X$.
If $X_n$ and $X$ are vectors, we need convergence in $p$-norm
element-by-element.
\end{defn}


\begin{defn}{(Convergence in Distribution or Weak Convergence)}
\label{defn:convergeInDistribution}
We say that a sequence of random variables $\{X_n\}$
\emph{converges in distribution} to random variable $X$, written
\begin{align*}
  X_n\dto X
  \quad\iff\quad
  \limn F_{X_n}(x) = F_X(x)
\end{align*}
for all values of $x$ where $F_x$ is continuous,
where $F_{X_n}$ and $F_X$ are the cdfs of $X_n$ and $X$, respectively.

One of the reasons this called weak convergence (aside from being
implied by all other kinds of convergence, as we'll see) is that each
$X_n$ can even be on a different probability space. We're just looking
at convergence of cdfs $F_{X_n}$ and $F_X$---functions from $\R$ to
$\R$.  We never compare $X_n$ and $X$ or $X_n$ and $X_m$ under a common
probability measure or integrate them together (as we did in all other
types of convergence). Therefore, the underlying probability spaces for
each can differ.

If $X_n$ and $X$ are vectors, then we can characterize convergence
in distribution as either
\begin{itemize}
  \item The joint cdf of $X_n$ converges to the joint CDF of $X$.
  \item Cramer-Wold Device: $a'X_n \dto a'X$ for any non-stochastic
    vector $a$.
\end{itemize}
\end{defn}
\begin{ex}
To see why we need to restrict convergence to points of continuity on
$F_X(x)$, consider the following example:
\begin{align*}
  F_{X_n}(x) &= \one{\left\{x\geq \frac{1}{n}\right\}}\\
  F_{X}(x) &= \one{\left\{x\geq 0\right\}}
\end{align*} In other words, random variable $X_n$ takes on value
$\frac{1}{n}$ with probability one, while $X$ takes on value zero with
probability one.  So if we check convergence of the CDFs at $x=0$, we
have a sequence $\{F_{X_n}(0)\}_{n=1}^\infty=\{0\}_{n=1}^\infty$, which
clearly does not converge to $F_X(0)=1$.

However, it is the case that $F_{X_n}$ is well-approximated by $F_X$ at
all other points, so we'd like to be able to say $X_n\dto X$. And we
can, if we simply restrict ourselves to points where $F_X$ is
continuous.
\end{ex}

\begin{prop}
If $X_n\dto X$ and $F_X$ is continuous, then
\begin{align*}
  \limn \sup_x |F_{X_n}(x)-F_X(x)| = 0
\end{align*}
\end{prop}

\begin{prop}
Sequence of random variables $\{X_n\} \dto X$ if and only if
\begin{align*}
  \E[g(X_n)] = \E[g(X)]
\end{align*}
for any continuous bounded function $g$.
\end{prop}
\begin{rmk}
This is another useful way to characterize to characterize convergence
in distribution aside from Definiton~\ref{defn:convergeInDistribution}.
It's extremely handy for a lot of results.

Moreover, it again highlights the fact that the $X_n$ and $X$ can all be
on different probability spaces. Each expectation in the sequence
$\{\E[g(X_n)]\}\ninf$ is computed on that $X_n$'s own probability space,
and we just get back a number. Convergence in distribution checks the
convergence of these numbers---we never compare the $X_n$ and $X$ under
a common probability measure or integral. It's all separate.
\end{rmk}

\begin{prop}
Sequence of random variables $\{X_n\} \dto X$ if and only if
$\varphi_{X_n}(x) \ra \varphi_X(x)$ pointwise where $\varphi_Z(z)$ is
the characteristic function of random variable $Z$.
\end{prop}

\begin{prop}
Given a sequence of random variables $\{X_n\}$ and random variable $X$,
the various convergence concepts are related in the following way:
\begin{enumerate}
  \item $X_n\asto X \implies X_n\pto X$. Converse not true in general.
  \item $X_n\Lpto X \implies X_n\pto X$
  \item Any one of
    \begin{align*}
      \begin{rcases}
        X_n&\asto &X \\
        X_n&\pto &X \\
        X_n&\Lpto &X
      \end{rcases}
      \implies X_n\dto X
    \end{align*}
    We see here even more explicitly that converge in distribution is
    \textsc{super weak}.
  \item Almost Surely and $p$-norm, undecidable.
  \item $X_n\pto X \implies$ There's a deterministic subsequence such
    that $X_{n_k}\asto X$ (but does not imply almost sure convergence
    for the original sequence).

  \item $X_n\dto c$, a constant, then $X_n\pto c$. However, in general,
    convergence in distribution does not imply convergence in
    probability.
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn.
\begin{enumerate}
  \item
    %Suppose that $X_n \asto X$. Suppose also that $\varepsilon$ and
    %$\delta$ are given. Convergence in probability means that we can
    %find an $N$ such that
    %\begin{align*}
      %n>N
      %\implies
      %P(|X_n-X|>\varepsilon) < \delta
    %\end{align*}
    %Since $X_n$ converges to $X$ almost surely, for any $\omega$, there
    %is an $N_\omega$ such that
    %\begin{align*}
      %n > N_\omega
      %\implies
      %|X_n(\omega)-X(\omega)| < \varepsilon
    %\end{align*}

  \item Define random variable $Z=X_n-X$. Suppose that $\varepsilon$ is
    given. From Markov's inequality,
    \begin{align*}
      P(|Z| \geq \varepsilon) \leq \frac{\E[|Z|^p]}{|\varepsilon|^p}
      \implies
      P(|X_n-X| \geq \varepsilon) \leq \frac{\E[|X_n-X|^p]}{|\varepsilon|^p}
    \end{align*}
    As $n\ra\infty$, the righthand side of the inequality goes to zero
    since $X_n$ converges to $X$ in $p$-norm. Hence, the lefthand side
    holds.
\end{enumerate}
\end{proof}

\clearpage
\subsection{$O_p$ and $o_p$ Notation}

In this subsection, we introduce Big-$O$ and Little-$o$ notation, along
with their probabilistic counterparts, $O_p$ and $o_p$ notation. While
the former deals with numerical sequences, while the latter deals with
sequences of random variables.

Loosely speaking, $x_n = O(b_n)$ means ``$x_n$ grows at the same
rate as $b_n$'' while $x_n=o(b_n)$ means ``$x_n$ is ultimately smaller
than $b_n$.''


\begin{defn}(Little-$o$ Notation)
Given numerical sequences $\{x_n\}_{n=1}^\infty$ and
$\{b_n\}_{n=1}^\infty$, we say that
\begin{align*}
  x_n &= o(b_n)
  \quad\text{or} \quad
  \frac{x_n}{b_n} = o(1)
  \quad \iff \quad
  \limn \frac{x_n}{b_n} = 0
\end{align*}
That is, sequence $b_n$ grows much more quickly than sequence $x_n$ so
that $x_n$ is \emph{ultimately smaller than} or
\emph{ultimately negligible compared to} $b_n$.
\end{defn}

\begin{defn}(Big-$O$ Notation)
Given $\{x_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$, we say that
\begin{align*}
  x_n = O(b_n)
  \quad\text{or} \quad
  \frac{x_n}{b_n} = O(1)
  \quad \iff \quad
  \exists M \; \text{s.t.} \;
  \left\lvert
  \frac{x_n}{b_n}
  \right\rvert
  < M
  \qquad \forall n
\end{align*}
That is, sequence $x_n$ grows at roughly the same rate as sequence
$b_n$, up to a scalar multiple.
\end{defn}

\begin{defn}(Little-$o_p$ Notation)
Given sequence of random variables $\{X_n\}_{n=1}^\infty$ and numerical
sequence $\{b_n\}_{n=1}^\infty$, we say that
\begin{align*}
  X_n = o_p(b_n)
  \quad\text{or} \quad
  \frac{X_n}{b_n} = o_p(1)
  \quad \iff \quad
  \frac{X_n}{b_n} \pto 0
\end{align*}
\end{defn}

\begin{defn}(Big-$O_p$ Notation)
Given sequence of random variables $\{X_n\}_{n=1}^\infty$ and
numerical sequence $\{b_n\}_{n=1}^\infty$, we say that
\begin{align*}
  X_n = O_p(b_n)
  \quad\text{or} \quad
  \frac{X_n}{b_n} = O_p(1)
  \quad \iff \quad
  \forall \varepsilon>0 \quad
  \exists M\quad
  \text{s.t.} \;
  \Prb\left(
  \left\lvert
  \frac{X_n}{b_n}
  \right\rvert
   \geq M\right)
  \leq \varepsilon
  \quad \forall n
\end{align*}
In other words, $\left\lvert\frac{X_n}{b_n}\right\rvert$ is almost surely
bounded by $M$. Hence, $X_n=O_p(1)$ if $X_n$ is almost surely bounded.
\end{defn}

\begin{defn}(Using other RVs)
The above definitiions used a numerical sequence $\{b_n\}\ninf$.
But we can generalize to say that $X_n$ grows more slowly than some
other random variable $Y_n$, i.e. $X_n = o_p(Y_n)$, or that $X_n$ is
bounded by $Y_n$, i.e.\ $X_n=O_p(Y_n)$.
\end{defn}

\clearpage
\begin{defn}(Operations and Relationships)
Throughout this definition, rather than write out and define $X_n =
O_p(b_n)$ and $Y_n=O_p(c_n)$, and then specify the order of $X_nY_n$ or
$X_n+Y_n$, we will simply write $O_p(b_n)O_p(c_n)$ directly and specify
the order of that. Though of course, it's implicit that we're really
doing multiplication and addition of \emph{random variables} that
happen to be of these orders. We now consider various cases:
\begin{enumerate}
  \item $o_p$-Operations:
    \begin{enumerate}
      \item $o_p(b_n)o_p(c_n)=o_p(b_nc_n)$
      \item $o_p(b_n)+o_b(c_n) = o_p(\max\{b_n, c_n\})$
    \end{enumerate}

  \item $O_p$-Operations
    \begin{enumerate}
      \item $O_p(b_n)O_p(c_n)=O_p(b_nc_n)$
      \item $O_p(b_n)+O_p(c_n) = O_p(\max\{b_n, c_n\})$
    \end{enumerate}

  \item Mixed Operations:
    \begin{enumerate}
      \item $o_p(1)+O_p(1) = O_p(1)$
      \item $o_p(1)O_p(1)=o_p(1)$
    \end{enumerate}
\end{enumerate}
We also have the following implications and rules of operations:
\begin{itemize}
  \item $X_n=o_p(1)$, then $X_n=O_p(1)$.
  \item $(1+o_p(1))^{-1}=O_p(1)$.
  \item $o_p(O_p(1))=o_p(1)$.
\end{itemize}
\end{defn}


\clearpage
\subsection{The Continuous Mapping Theorem and Slutsky's Theorem}

\begin{lem}
\label{lem:joint}
Suppose $X_n\dto X$ and $Y_n\pto c$, for constant vector $c$.
Then $(X_n,Y_n)\dto (X,c)$ jointly.
\end{lem}

\begin{thm}\emph{(Continuous Mapping Theorem)}
\label{thm:cmt}
Let $g(\,\cdot\,)$ be a function that is continuous on the support of
$X$.  Then
\begin{enumerate}
  \item $X_n\asto X \implies g(X_n) \asto g(X)$
  \item $X_n\pto X \implies g(X_n) \pto g(X)$
  \item $X_n\dto X \implies g(X_n) \dto g(X)$
  \item $X_n - Y_n\pto 0$ and $Y_n \dto Y$
    $\implies g(X_n) - g(Y_n)\pto 0$
\end{enumerate}
\end{thm}
\begin{rmk}
A word about the third requirement. We'd like to say that just
$X_n - Y_n\pto 0$ implies $g(X_n) - g(Y_n)\pto 0$, but we need to put
the extra requirement on at least one of the variables (WLOG, I chose
$Y_n$) converges in distribution to some $Y$. In a sense, this is a
requirement that $Y_n$ is bounded in probability and doesn't race off to
infinity, which would prevent us from having $g(X_n)-g(Y_n) \pto 0$ as
we want to conclude.
\end{rmk}

\begin{cor}{\emph{(Slutsky's Theorem)}}
Suppose that we have sequences of random variables $\{X_n\}$ and
$\{Y_n\}$ such that
\begin{align*}
  X_n &\dto X
  \qquad
  Y_n \pto c
\end{align*}
Then we have
\begin{enumerate}
  \item $X_n+Y_n\dto X+c$
  \item $X_n\cdot Y_n\dto Xc$
  \item $Y_n^{-1}X_n\dto c^{-1}X$ provided $c\neq 0$
\end{enumerate}
We can also let $X$ be a random vector and $c$ be a vector or matrix of
constants. In that case, the $c\neq 0$ requirement becomes ``$c$ full
rank.''
\end{cor}
\begin{proof}
We know from Lemma~\ref{lem:joint} that $(X_n,Y_n)\dto (X,c)$ jointly.
Since $g_1(x,y) = x+y$, $g_2(x,y)=x\cdot y$, and $g_3(x,y)=y^{-1}x$
(for $y\neq 0$) are all continuous functions of $(X_n,Y_n)$, Result 3 of
Continuous Mapping Theorem~\ref{thm:cmt} implies 1, 2, and 3.
\end{proof}

\begin{cor}
Suppose that $X_n - Y_n\pto 0$ and $Y_n\dto Y$. Then $X_n \dto Y$ as
well.
\end{cor}
\begin{proof}
Define $Z_n := X_n - Y_n$. We're told that $Z_n\pto 0$. We also know
that $Y_n\dto Y$. Then by Slutsky's Theorem, we know
\begin{align*}
  Z_n + Y_n &\dto 0 + Y \\
  \Leftrightarrow\quad (X_n-Y_n) + Y_n &\dto 0 + Y \\
  \Leftrightarrow\quad X_n &\dto Y
\end{align*}
\end{proof}

\begin{prop}(CMT for General Metric Spaces)
Let $h:S\ra U$ be a mapping between general metric spaces (not just $\R$
to $\R$) and let $D_h$ denote the set of discontinuity points of $h$.
Then
\begin{align*}
  \begin{rcases}
  X_n \dto X \\
  P[X\in D_h] = 0
  \end{rcases}
  \quad\implies\quad
  h(X_n) \dto h(X)
\end{align*}

\end{prop}


\clearpage
\subsection{Law of Large Numbers}

\begin{thm}\emph{(Weak Law of Large Numbers)}
Given a sequence $\{X_n\}$ of random variables such that
$\E[X_n]=\mu$, $\Var(X_n)=\sigma^2$, and $\Cov(X_i,X_j)=0$,
\begin{align*}
  \hat{\mu}_N \pto \mu
  \qquad\text{where} \quad
  \hat{\mu}_N := \frac{1}{N} \sumnN X_n
\end{align*}
\end{thm}
\begin{rmk}
We assume existence of the second moment here. In the Strong Law of
Large Numbers, we dispense with that assumption, and we also happen to
get \emph{almost sure} convergence, rather than the weaker notion of
convergence in probability as we had here.
\end{rmk}
\begin{proof}
Since $\hat{\mu}_N$ is a random variable, we can use Chebyshev's
inequality. Fix $\varepsilon$. Then
\begin{align*}
  \Prb(|\hat{\mu}_N -\mu| \geq \varepsilon)
  \leq \frac{\Var(\hat{\mu}_N)}{\varepsilon^2}
\end{align*}
Note that $\Var(\hat{\mu}_N) = \frac{\sigma^2}{N}$, which implies
\begin{align*}
  \Prb(|\hat{\mu}_N -\mu| \geq \varepsilon)
  \leq \frac{\sigma^2}{N\varepsilon^2}
\end{align*}
As $N\ra\infty$, the object on the right goes to zero. Since
$\varepsilon$ was arbitrary, we have exactly what we need to say that
$\hat{\mu}_N\ra\mu$.
\end{proof}

\begin{thm}\emph{(Strong Law of Large Numbers)}
Suppose that $\{X_n\}$ is a sequence of iid random variables with mean
$\E[X_n]=\mu<\infty$.
\begin{align*}
  \hat{\mu}_N \asto \mu
  \qquad\text{where} \quad
  \hat{\mu}_N := \frac{1}{N} \sumnN X_n
\end{align*}
\end{thm}

\begin{thm}\emph{(Ergodic Theorem)}
Suppose that $y_t$ is a stationary and ergodic (vector) process with
$\E[y_t]=\mu$. Then the process is ergodic for the mean, i.e.
\begin{align*}
  \frac{1}{T}\sumtT y_t \pto \mu
\end{align*}
In words, the $y_t$ can be correlated rather than iid---so long as they
are stationary and ergodic---and the resulting time average will
converge to the cross-sectional average.
\end{thm}


\clearpage
\subsection{Central Limit Theorems}

\begin{thm}\emph{(Lindberg-Levy)}
Let $\{X_n\}_{n=1}1^N$ be a sequence of iid random variables with
$\E[X_n] = \mu$ and $\Var(X_n)=\sigma^2$. Then
\begin{align*}
  \frac{\sqrt{N}}{\sigma}
  \left(\bar{X}_N - \mu\right) \dto N(0,1)
  \qquad
  \bar{X}_N := \frac{1}{N}\sumin X_n
\end{align*}
\end{thm}

\begin{thm}\emph{(CLT for Martingale Difference Sequence)}
Suppose that $\{u_t\}\ninf$ is a stationary and ergodic martingale
difference sequence with respect to the filtration $\{\sF_t\}\tinfz$,
i.e. $\E[u_t|\sF_{t-1}]=0$. Also suppose that $\E[u_tu_t']=\Sigma$. Then
\begin{align*}
  \frac{1}{\sqrt{T}}\sumtT u_t \dto N(0,\Sigma)
\end{align*}
\end{thm}

\begin{thm}\emph{(CLT for Linear Processes)}
Suppose that we have linear process $y_t=\mu+\sum_{j=0}^\infty \psi_j
\varepsilon_{t-j}$ where $\varepsilon_t\iid (0,\sigma^2)$ and
$\sum_{j=0}^\infty j^2\psi_j^2<\infty$. Then
\begin{align*}
  \sqrt{T}\left(
  \bar{y}_T
  - \mu
  \right)
  \dto N\left(0,S\right)
  \qquad
  \text{where}\quad
  \bar{y}_T =
  \frac{1}{T}
  \sumtT y_t
\end{align*}
where $S$ is the long-run variance of $\{y_t\}$.
\end{thm}

\begin{thm}\emph{(Functional CLT)}
Suppose that $y_t$ is stationary and ergodic with mean $\mu$. Then
\begin{align*}
  W_T(s) :=
  \frac{1}{\sqrt{T}}
  \sum_{t=1}^{\lfloor s T\rfloor}
  (y_t-\mu) \dto \sqrt{S}\cdot W(s)
  \qquad
  s\in [0,1]
\end{align*}
where $S$ is the long-run variance of $\{y_t\}$ and $W(s)$ is Brownian
motion.
\end{thm}


\clearpage
\subsection{Delta Method}

We start with some motivation.
Suppose we have some sequence of vector-valued random variables
$\{\bsX_n\}\ninf$ such that
\begin{align*}
  \bsX_n \dto \bsX
\end{align*}
In words, we know that the asymptotic distribution of sequence
$\{\bsX_n\}\ninf$ is the random variable $\bsX$.
Often, this limiting distribution will be derived through some version
of the Central Limit Theorem.

That's great, but now consider a transformation of the original
sequence, denoted $\{\bsg(\bsX_n)\}\ninf$ for some continuously
differentiable function $\bsg$. What is the asymptotic distribution of
that guy? Since we know that $\bsX$ is the limiting distribution of
$\{\bsX_n\}$, it seems like we should be able to figure out the
asymptotic distribution of the transformed sequence
$\{\bsg(\bsX_n)\}\ninf$ and that the result should probably be some
function of $\bsX$. Both of those things are true. The
\emph{delta method} lets us show that rigorously and derive an explicit
distribution.

\begin{thm}\emph{(Delta Method)}
Given random variable $\bsX_N$ and continuously differentiable
transformation $\bsg:\R^m\rightarrow \R^p$, the \emph{delta method}
states that
\begin{align*}
  \sqrt{N}\left(
  \bsX_N
  -\bsmu
  \right)
  \dto \bsX
  \quad\implies\quad
  \sqrt{N}(
  \bsg(\bsX_N)
  &-
  \bsg(\bsmu)
  )
  \dto
  \bsG \bsX
  \\\\
  \text{where}\quad
  \bsJ(\bsu)
  &:=\frac{\partial}{\partial \bsup}
  \left[\bsg(\bsu)\right]\\
  \bsG &:= \bsJ(\bsmu)
\end{align*}
In words, suggestively-named $\bsJ$ is the Jacobian of $\bsg$, while
$\bsG$ is shorthand for the Jacobian evaluated at $\bsmu$.
\\
\\
If $\bsX$ is normal $N(0,\bsSigma)$, then this specializes to
\begin{align*}
  \sqrt{N}\left(
  \bsg(\bsX_N)
  -
  \bsg(\bsmu)
  \right)
  &\xrightarrow{d}
  N(0,\bsG\bsSigma\bsG')
\end{align*}
\end{thm}
\begin{proof}
Start by doing a Taylor series expansion of $\bsg$ about $\bsmu$:
\begin{align*}
  \bsg(\bsX_N)
  &=
  \bsg(\bsmu)
  + \bsJ(\tilde{\bsmu}) \left( \bsX_N - \bsmu \right)
  \\
  \Leftrightarrow\quad
  \bsg(\bsX_N) - \bsg(\bsmu)
  &=
  \bsJ(\tilde{\bsmu}) \left( \bsX_N - \bsmu \right)
\end{align*}
where $\tilde{\bsmu}$ is in between $\bsmu$ and $\bsX_N$.
But we know that
\begin{alignat*}{3}
  \sqrt{N}(\bsX_N-\bsmu)
  \dto \bsX
  \quad\overset{(1)}{\implies}&&\quad
  (\bsX_N-\bsmu)
  &\dto 0 \\
  \quad\implies&&\quad
  \bsX_N
  &\dto \bsmu \\
  \quad\overset{(2)}{\implies}&&\quad
  \bsX_N
  &\pto \bsmu \\
  \quad\overset{(3)}{\implies}&&\quad
  \tilde{\bsmu}
  &\pto \bsmu
\end{alignat*}
Implication (1) followed because anything else would imply that the
first limit would have raced off to $+\infty$. Implication (2) followed
because convergence in distribution to a constant implies convergence in
probability to that constant.
Finally, Implication (3) followed because $\tilde{\bsmu}$ is pinned in
between $\bsmu$ and $\bsX_N$---which we just said goes to $\bsmu$ in
probability.

Next, since $\bsg$ is continuously differentiable, the Jacobian is
continuous, so by the Continuous Mapping Theorem
\begin{align*}
  \tilde{\bsmu} \pto \bsmu
  \quad\implies\quad
  \bsJ(\tilde{\bsmu})
  \pto \bsJ(\bsmu) = \bsG
\end{align*}
From there, we can use this in Slutsky's Theorem:
\begin{align*}
  \begin{rcases}
  \sqrt{N}\left(\bsX_N-\bsmu\right) &\dto \bsX \\
  \bsJ(\tilde{\bsmu}) &\pto \bsG
  \end{rcases}
  \quad\implies\quad
  \bsG \sqrt{N} (\bsX_N-\bsmu) \dto
  \bsG \bsX
\end{align*}
But notice by the Taylor Series expansion we did above, we have now
enough to conclude that
\begin{align*}
  \sqrt{N} \bsG (\bsX_N-\bsmu)
  = \sqrt{N}(\bsg(\bsX_N)-\bsg(\bsmu))
  \dto \bsG\bsX
\end{align*}
which is exactly what we wanted to prove.
The normal case follows directly.
\end{proof}

\begin{thm}\emph{(Second-Order Delta Method, Normal Case)}
Given random variable $\bsX_N$ and continuously differentiable
transformation $\bsg:\R^m\rightarrow \R^p$
such that $\bsg'(\bsmu)=0$, the \emph{second-order delta method} states
\begin{align*}
  \sqrt{N}\left( \bsX_N -\bsmu \right)
  \dto N(0,\sigma^2)
  \quad\implies\quad
  \sqrt{N}(
  g(X_N) &- g(\mu))
  \dto
  N(0, \frac{1}{2}g''(\mu) \sigma^2 G')
  \\\\
  \text{where}\quad
  J(u)
  &:=\frac{\partial}{\partial up}
  \left[g(u)\right]\\
  G &:= J(mu)
\end{align*}
In words, suggestively-named $\bsJ$ is the Jacobian of $\bsg$, while
$\bsG$ is shorthand for the Jacobian evaluated at $\bsmu$.
\end{thm}
\begin{rmk}
In this theorem, we limited ourselves to the normal case since the
transformation then converges to a nice tractable $\Chi^2_1$
distribution. Moreover, the proof will be rather brief, relying on
intuition and details filled in during the previous theorem.
\end{rmk}
\begin{proof}
Start by doing a second-order Taylor series expansion of $g$ about $\mu$:
\begin{align*}
  \bsg(X_N)
  &=
  g(\mu)
  + g'(\mu) \left( X_N - \mu \right)
  + \frac{1}{2}g''(\tilde{\mu}) \left( X_N - \mu \right)^2
  \\
  \iff\quad
  g(X_N) - g(\mu)
  &=
  g'(\mu) \left( X_N - \mu \right)
  + \frac{1}{2}g''(\tilde{\mu}) \left( X_N - \mu \right)^2
\end{align*}
where $\tilde{\mu}$ is in between $X_N$ and $\mu$.
Next, by assumption, $g'(\mu)=0$, which leaves
\begin{align*}
  g(X_N) - g(\mu)
  &=
  \frac{1}{2}g''(\tilde{\mu}) \left( X_N - \mu \right)^2
\end{align*}
Since $\tilde{\mu}$ is in between $X_N$ and $\mu$ and
$X_N\pto \mu$, that means $\tilde{\mu}\pto \mu$.
By the continuous mapping theorem, $g''(\tilde{\mu})\pto g''(\mu)$.
Now apply Slutsky and use the distribution of $X_N$ to conclude that
\begin{align*}
  N\left(g(X_N) - g(\mu)\right)
  &=
  \frac{g''(\mu)}{2}
  \left(\sqrt{N}  \left( X_N - \mu \right) \right)^2
  \dto
  \sigma^2 \frac{g''(\mu)}{2} Z^2
\end{align*}
\end{proof}




\clearpage
\section{Estimation}

Throughout this section, we consider the task of estimating some
parameter vector of interest, $\bstheta$. This parameter vector should
be interepreted very generally. It can represent moments of the data,
regression coefficients, parameters of a conditional expectation
function, etc.

This section divides into two main themes: \emph{assessing} and
\emph{constructing} estimators, where estimators are functions that map
the sample/data $\{\bsy_n\}\nN$ into a guess $\bshattheta$ for the true
parameter vector of interest, $\bstheta$.  We won't discuss how one can
obtain that mapping at first.  Rather, we first start by introducing
terminology and methods to assess the quality of estimators.  Then with
those tools in hand, we move on to the second goal: coming up with good
estimators (the mappings) in the first place. In doing so, this section
will introduce nonlinear least squares (NLS), maximum likelihood
estimation (MLE), and generalized method of moments (GMM)---all of which
are general frameworks designed to produce estimators. In many cases,
the estimators that emerge from these different approaches overlap or
nest each other. Most especially, we'll see that virtually all
estimators of interest can be written as a special case GMM estimator
which is, in some sense, the most general framework for constructing
estimators.


\subsection{Estimators, Estimates, and Notation}

Throughout this section, our goal is to estimate the true parameter
value $\bstheta\in\R^P$.  To do so, we will form an estimator/estimate
$\bshattheta$ (see formal definition below). We will let $\bstildetheta$
denote an arbitrary vector in $\R^P$, which we'll often use to
distinguish any arbitrary vector in $\R^P$ of \emph{potential} parameter
values from the true, honest-to-goodness, written-in-God's-notebook
parameter vector $\bstheta$.  To form our estimator $\bshattheta$, we
typically use sample data $\{\bsy_n\}\nN$, where $\bsy_n\in\R^K$ refers
to the $n$th observation.  We will often use the shorthand $\bsy_{1:N}$
to refer to the whole sample $\{\bsy_n\}\nN$.

Finally, when we talk about maximum likelihood, we suppose that the
sample $\bsy_{1:N}$ is drawn from the \emph{true} joint likelihood
function $f(\bsy_{1:N}|\bstheta)$ which uses the true parameter value
$\bstheta$. In the special case of an iid sample, the individual
$\bsy_n$ are drawn from true likelihood $f(\bsy_n|\bstheta)$ which
implies joint likelihood
$f(\bsy_{1:N}|\bstheta)=\prod\nN f(\bsy_n|\bstheta)$.

Of course, we often don't know the true parameter value $\bstheta$ (or,
by implication, the true likelihood), though we will typically assume
that we know the functional form $f(\bsy_n|\bstildetheta)$, called
simply the \emph{likelihood} function (ditching the ``true'' modifier).
We sometimes restrict $\bstildetheta$ to lie in some subspace
$\bsTheta\subseteq\R^p$ to implement nonnegativity constraints or other
parameter resrictions---though in principle, $\bsTheta$ could be all of
$\R^P$). Therefore, we will think of
$\{f(\bsy_n|\bstildetheta)\}_{\bstildetheta\in\bsTheta}$ as a family of
possible true likelihoods. The goal of maximum likelihood estimation
(MLE) will be to use the sample $\bsy_{1:N}$ to find the correct
likelihood in that family, $f(\bsy_n|\bstheta)$.

\begin{defn}(Estimator and Estimate)
An \emph{estimator} $\bshattheta$ of parameter $\bstheta$ is a function
$\bshattheta = \bsg(\bsy_{1:N})$ that maps observations into a guess for
$\bstheta$.
Given a sample realization $\bsy_{1:N}$, we can plug in the realized
values and compute a number, which we call an \emph{estimate}. In an
abuse of notation, we will also use $\bshattheta$ to denote both an
estimat\emph{e} (a number computed from a realized sample), as well as
an estimat\emph{or} (the more general function/mapping).

This section is concerned more with estimat\emph{ors} than
estimat\emph{es}. We don't particularly care about plugging a single
realized sample into a function to generate a single number/estimate.
Rather, we care more about finding good functions/mappings (estimators)
in the first place. But to understand what makes a ``good'' estimator,
we need think of estimators as random variables with some sampling
distribution that we can study and analyze. In particular, we treat the
sample $\bsy_{1:N}$ as a collection of unrealized random variables with
some joint distribution; since $\bshattheta$ is a function of random
$\bsy_{1:N}$, our estimator $\bshattheta$ will \emph{also} a random
variable. Its sampling distribution can be deduced from the joint
sampling distribution of the underlying RV's $\bsy_{1:N}$ and the
mapping itself. Therefore, we can make \emph{ex ante} probability
statements (before a sample is ever realized) about the distribution of
values $\bshattheta$ might take on, its mean, its variance, etc.

So to reiterate: The concern of this section is \emph{estimators} and
their sampling distributions. We begin by examining the topics of bias,
consistency, loss, and risk which help identify good estimators. We then
put these concepts to work by looking at general frameworks for
estimation that provide good estimators.
\end{defn}


\clearpage
\subsection{Assessment of Estimators}

\subsubsection{Loss and Risk}

In this section, we again consider the quality of estimators.  The key
concepts are \emph{loss} which defines a quantitative penalty for poor
estimates, and \emph{risk} which characterizes expected loss---an
average of the loss using some suitable weighting function. Computing
risk (expected loss) can be carried out in a few different ways, but all
of them involve averaging the loss with some weighting function (that
will differ depending upon the particular definition of risk) to arrive
at expected loss. Of course, computing risk requires choice of a loss
function, and there are many possible loss functions that an
econometrician could reasonably use. We will see a few examples.

\begin{defn}(Loss Function)
The \emph{loss function} $L(\bshattheta,\bstheta)$ is a function that
maps the true parameter value $\bstheta$ and an estimate $\bshattheta$
into negative utility. It returns the ``loss''---a number---quantifying
how bad it was to use $\bshattheta$ when the true value was $\bstheta$.
We can think of it as a penalty function.
\end{defn}

\begin{defn}{(Classical or Frequentist Risk Function)}
Given loss function $L(\bshattheta,\bstheta)$, the \emph{classical} or
\emph{frequentist risk function} is the expected value of the loss:
\begin{align*}
  R(\bshattheta,\bstheta)
  = \E_{\bsy|\bstheta}\big[L(\bshattheta,\bstheta)\big]
\end{align*}
where $\E_{\bsy|\bstheta}$ indicates that the expectation is taken over
realizations of $\bsy_{1:N}$ generated from true likelihood
$f(\bsy_{1:N}|\bstheta)$.
The loss tells us the penalty or negative utility \emph{given} an
estimate $\bshattheta$ and the true value $\bstheta$. But an estimate
$\bshattheta$ is a random variable that depends on the sample.
So by averaging over sample draws of $\bsy_{1:N}$ (generated from true
$\bstheta$), we are indirectly averaging over the values $\bshattheta$
takes on to compute the risk.

The ``Classical'' and ``Frequentist'' qualifiers follow from the fact
that $R(\bshattheta,\bstheta)$ does not integrate over different values
that $\bstheta$ can take on since frequentists want to treat $\bstheta$
as a fixed constant, not a random variable.
\end{defn}

\begin{ex}(Quadratic Loss and Mean Squared Error)
Often, a good loss function is the \emph{quadratic loss} function
\begin{align*}
  L(\bshattheta,\bstheta) = (\bshattheta-\bstheta)^2
\end{align*}
which penalizes more steeply the further away you are from the true
value. It has associated \emph{mean squared error} risk:
\begin{align*}
  R(\bshattheta,\bstheta) = \E_{\bsy|\bstheta}[(\bshattheta-\bstheta)^2]
\end{align*}
\end{ex}

\clearpage
\begin{defn}(Average or Bayes Risk)
\label{defn:bayesrisk}
Given a prior distribution $w(\bstheta)$ for parameter $\bstheta$, the
\emph{Average} or \emph{Bayes risk} denoted $r(\bshattheta)$ averages
the classical or frequentist risk over the values of $\bstheta$, weighted
by $w(\bstheta)$:
\begin{align*}
  r(\bshattheta)
  = \int R(\bshattheta,\bstheta) w(\bstheta) \; d\bstheta
  = \E_{\bstheta}\big[ R(\bshattheta,\bstheta)\big]
\end{align*}
Note that we can express that a few different ways in terms of
expectation operators, all of which offer different type of intuition:
\begin{align*}
  r(\bshattheta)
  = \E_{\bstheta}\big[ R(\bshattheta,\bstheta)\big]
  = \E_{\bstheta}\left[
      \E_{\bsy|\bstheta}\left[
      L(\bshattheta,\bstheta)
      \right]
    \right]
  = \E_{\bstheta,\bsy}\left[ L(\bshattheta,\bstheta) \right]
\end{align*}
In words, frequentist risk $R(\bshattheta,\bstheta)$ gives expected loss
conditional on a value of $\bstheta$, denoted
$\E_{\bsy|\bstheta}[L(\bshattheta,\bstheta)]$. If you take the
expectation of that, by the law of iterated expectations, you get
$\E_{\bstheta,\bsy}[L(\bshattheta,\bstheta)]$, which is the expected
loss \emph{jointly} averaging out over both data $\bsy_{1:N}$
uncertainty \emph{and} parameter $\bstheta$ uncertainty.
\end{defn}
\begin{rmk}
In general, different priors or $w(\bstheta)$ functions give
different Bayes risk estimates.
%Choose wisely.
\end{rmk}

\begin{defn}{(Posterior Risk or Posterior Expected Loss)}
This is the expected loss conditional on a sample realization:
\begin{align*}
  \E_{\bstheta|\bsy}\big[
    L(\bshattheta,\bstheta)\;|\;\bsy_{1:N}
  \big]
  =
  \int L(\bshattheta,\bstheta)
  f_{\bstheta|\bsy}(\bstheta|\bsy_{1:N}) \; d\bstheta
\end{align*}
Intuitively, you have a posterior
$f_{\bstheta|\bsy}(\bstheta|\bsy_{1:N})$ that characterizes the
distribution of $\bstheta$ after observing data $\bsy_{1:N}$. The
posterior risk is an average of the loss, but fixing that sample/data
and using the posterior as the appropriate weighting function. As a
result, you weight the loss more heavily in areas where the
$f_{\bstheta|\bsy}(\bstheta|\bsy_{1:N})$ is largest.
\end{defn}

\begin{rmk}
To be more concrete, suppose the sample $\bsy_{1:N}$ suggests that
$\bstheta$ is most likely positive.  Then if your estimate has huge loss
in negative territory (i.e. $\bshattheta$ is far from $\bstheta$ when
$\bstheta$ is negative), whatever. You don't care. Discount that loss
because $\bstheta$ probably isn't over there. We shouldn't penalize
$\bshattheta$ if it's does a shitty job estimating $\bstheta$ in regions
where $\bstheta$ is really unlikely to be.
\end{rmk}

\begin{prop}\emph{(Average or Bayes Risk, Revisited)}
\label{prop:bayesrisk}
Definition~\ref{defn:bayesrisk} defines the Bayes risk as the average of
classical or frequentist risk over all values of $\bstheta$ weighted by
the prior. We can equivalently characterize Bayes risk as the posterior
risk averaged over all samples $\bsy_{1:N}$. In other words
\begin{align*}
  r(\bshattheta)
  &= \E_{\bstheta,\bsy}[L(\bshattheta,\bstheta)]
  =
  \begin{cases}
    \E_{\bstheta}\left[
      \E_{\bsy|\bstheta}\big[
        L(\bshattheta,\bstheta)
      \big]
    \right]
    &\text{Ave.\ of classical/frequentist risk over $\bstheta$}
    \\
    \\
    \E_{\bsy}\left[
      \E_{\bstheta|\bsy}\big[L(\bshattheta,\bstheta)\big]
    \right]
    &\text{Ave.\ of posterior risk over samples $\bsy_{1:N}$} \\
  \end{cases}
\end{align*}
\end{prop}
%\begin{proof}
%Write out the definition or $r(\hat{\theta})$
%\begin{align*}
  %r(\hat{\theta})
  %&= \E_\theta[R(\hat{\theta},\theta)] \\
  %&= \int R(\hat{\theta},\theta)  w(\theta) \; d\theta
%\end{align*}
%\end{proof}

\begin{defn}(Bayes Estimator)
\label{defn:bayesest}
Given a loss function $L(\bshattheta,\bstheta)$, the
\emph{Bayes estimator} is the estimator that minimizes posterior risk
conditional on the sample $\bsy_{1:N}$:
\begin{align*}
  \bshattheta_{\text{Bayes}}
  = \argmin_{\bshattheta} \;
  \E_{\bstheta|\bsy}\big[
    L(\bshattheta,\bstheta)\;|\;\bsy_{1:N}
  \big]
\end{align*}
In words, the Bayes Estimator looks at the sample and minimizes the risk
of being far what the posterior suggests is the true value.
\end{defn}

\begin{prop}
The Bayes Estimator minimizes Bayes risk.
\end{prop}
\begin{proof}
Definition~\ref{defn:bayesest} defines the Bayes estimator as the
estimator that minimizes posterior risk given $\bsy_{1:N}$ fixed.
Proposition~\ref{prop:bayesrisk} defines Bayes risk as the average of
the posterior risk over all samples $\bsy_{1:N}$. Clearly, the Bayes
estimator must minimize Bayes risk.
\end{proof}


\begin{ex}
Given quadratic loss $L(\bshattheta,\bstheta) =
(\bshattheta-\bstheta)^2$, the Bayes estimator is the posterior mean:
\begin{align*}
  \bshattheta_{\text{Bayes}}
  &= \E_{\bstheta|\bsy}[\bstheta|\bsy_{1:N}]
\end{align*}
\end{ex}

A word about interpretation and intuition. Classical risk fixes
$\bstheta$ and averages the loss over different samples. Writing out the
definition in especially verbose notation:
\begin{align*}
  R(\bshattheta,\bstheta)=
  \E_{\bsy|\bstheta}\big[
    L(\bshattheta,\bstheta)
  \big] =
  \int L\big(\bshattheta(\bsy_{1:N}),\bstheta\big)
    \cdot f_{\bsy|\bstheta}(\bsy_{1:N}|\bstheta)\; d\bsy_{1:N}
\end{align*}
You want an estimator that does well for samples with the largest
likelihood. As you integrate over different samples, the value of your
estimator is changing to reflect the different sample. You want to make
sure that the value of that estimator $\bshattheta(\bsy_{1:N})$ is
spot on in regions where $\bsy_{1:N}|\theta$ is super likely.

On the other hand, posterior risk fixes the data at $\bsy_{1:N}$ (hence
the estimate $\bshattheta(\bsy_{1:N})$ as well) and averages the loss
over different values of $\bstheta$.
Again, writing out the definition in especially verbose notation:
\begin{align*}
  \E_{\bstheta|\bsy}\big[
    L(\bshattheta,\bstheta)
  \big] =
  \int L\big(\bshattheta(\bsy_{1:N}),\bstheta\big)
    \cdot f_{\bstheta|\bsy}(\bstheta|\bsy_{1:N})\; d\bstheta
\end{align*}
You want an estimator that, given the data, is pretty close to values of
$\bstheta$ that are especially likely according to the posterior.  For
this reason, it makes sense that the Bayes Estimator with Mean Square
Error loss is the posterior mean, since that's the balancing point of
the posterior distribution. Hence, it does the best job of balancing
``pleasing everybody'' in the sense of trying to be as close as possible
(in an $L^2$ sense) to all of the values of $\bstheta$ we expect to see.

\begin{defn}(Admissability)
An estimator $\bshattheta$ is said to be \emph{inadmissable} if there
exists another estimator $\bstildetheta$ such that
\begin{align*}
  R(\bstildetheta,\bstheta)
  \leq
  R(\bshattheta,\bstheta)
\end{align*}
and with the inequality strict for some $\bstheta$.
\end{defn}

\begin{prop}
Bayes Estimators are admissable.
\end{prop}
\begin{proof}
Bayes estimators minimize Bayes risk, i.e.
\begin{align*}
  \bshattheta_\text{Bayes}
  = \argmin_{\bshattheta} r(\bshattheta)
  = \argmin_{\hat{\theta}}\E_\theta[R(\bshattheta,\bstheta)]
\end{align*}
If there were another estimator $\bstildetheta$ such that
$R(\bstildetheta,\bstheta)\leq R(\bshattheta_\text{Bayes},\bstheta)$
with the inequality strict for some $\bstheta$, that would contradict
the definition of $\bshattheta_\text{Bayes}$ as the minimizer.
\end{proof}

\begin{thm}\emph{(Complete Class Theorem)}
Any admissable estimator can be interpretated as a Bayes esitmator.
\end{thm}


\clearpage
\subsubsection{Bias, Consistency, and Asymptotic Normality}

In this section, we will mostly try to construct \emph{consistent}
estimators with a normal asymptotic distribution as the sample grows
large. These are both large sample properties. In finite samples, we
might also ask the estimator to be unbiased. We now define these terms.

\begin{defn}(Bias)
An estimator is \emph{unbiased} if
\begin{align*}
  \E_{\bsy|\bstheta}\big[\,\bshattheta\,\big]
  =
  %\underbrace{\int \cdots \int}_{n\; times}
  \int
    \bshattheta(\bsy_{1:N}) \;
    f(\bsy_{1:N}|\bstheta) \; d\bsy_{1:N} = \bstheta
\end{align*}
The \emph{bias} is defined as
\begin{align*}
  \text{Bias} = \E_{\bsy|\bstheta}\big[\bshattheta-\bstheta\big]
\end{align*}
\end{defn}

\begin{prop}\emph{(MSE Bias-Variance Tradeoff)}
Mean square error loss of an estimator can be decomposed into its bias
and its variance:
\begin{align*}
  \E_{\bsy|\bstheta}\big[(\bshattheta-\bstheta)^2\big]
  =\Var_{\bsy|\bstheta}(\bshattheta) +
  \left(\E_{\bsy|\bstheta}\big[\bshattheta-\bstheta\big]\right)^2
\end{align*}
\end{prop}

\begin{defn}(Consistency and Strong Consistency)
An estimator $\bshattheta_N$ constructed using $N$ sample observations
is \emph{consistent} if $\bshattheta_N\pto \bstheta$ as $N\ra\infty$.

If the convergence is \emph{almost sure} rather than ``in probability'',
i.e.\ $\bshattheta_N\asto \bstheta$ as $N\ra\infty$, we say that
$\bshattheta_N$ is \emph{strongly consistent}.
\end{defn}


\begin{defn}(Asymptotic Normality and $\sqrt{N}$-consistency)
An estimator $\bshattheta_N$ is asymptotically normal if there exists
and increasing function $v(N)$ such that
\begin{align*}
  v(N)(\bshattheta_N -\bstheta) \dto N(0,\bsSigma)
\end{align*}
for some asymptotic variance $\bsSigma$. If $v(N)=\sqrt{N}$, then we
call the estimator \emph{$\sqrt{N}$-consistent}.
\end{defn}



\clearpage
\subsubsection{Score and Information}

More on notation. In this section, $f(\bsy|\bstildetheta)$ for
$\bstildetheta\in\bsTheta$ is the likelihood function as a function of
$\bstildetheta$, while $f(\bsy|\bstheta)$ is the true likelihood
function.  To avoid clutter, this notation will be used as shorthand to
refer to either the \emph{joint} likelihood
$f(\bsy_{1:N}|\bstildetheta)$ or the likelihood of a single observation
$f(\bsy_n|\bstildetheta)$.  So it should be understood that $\bsy$ is
either a single observation or a sample of $N$ observations.  But this
section is about likelihood functions, not about samples. So the results
hold whether $f(\bsy|\bstildetheta)$ is the likelihood function for a
single draw or the joint likelihood for a sample draw.

Again, data is generated according to a \emph{true likelihood} function
$f(\bsy|\bstheta)$. Hence expectations involving functions of $\bsy$
should be computed using the true likelihood function
$f(\bsy|\bstheta)$, which is why we will explicitly write
$\E_{\bsy|\bstheta}[\;\cdot\;]$ whenever we have to compute expectations
below.

%Most derivations will assume that $\bstildetheta$ is a scalar, though I
%will also restate results for cases where $\bstildetheta$ is a vector of
%parameters. However, I won't derive those cases.

\begin{defn}(Score Function)
The score function $\bsS(\bstildetheta,\bsy)$ is the derivative of the
log-likelihood function with respect to $\bstildetheta$:
\begin{align}
  \bsS(\bstildetheta,\bsy) &:=
  \frac{\partial \ln f(\bsy|\bstildetheta)}{\partial \bstildetheta}
  \label{score1}
\end{align}
By the above definition, we can simplify the derivative of the log and
write
\begin{align}
  \bsS(\bstildetheta,\bsy)
  &=
  %\frac{\partial \ln f(\bsy|\bstildetheta)}{\partial \bstildetheta}
  %=
  \frac{1}{f(\bsy|\bstildetheta)}
  \frac{\partial f(\bsy|\bstildetheta)}{\partial \bstildetheta}
  \label{score2} \\
  \implies\quad
  f(\bsy|\bstildetheta) \cdot \bsS(\bstildetheta,\bsy)
  &=
  \frac{\partial f(\bsy|\bstildetheta)}{\partial \bstildetheta}
  \label{score3}
\end{align}
Since the score depends upon random variable $\bsy$, the score function
is itself a random variable. Again, it is also the case that
$\bstildetheta$ can vary. Don't assume $\bstildetheta=\bstheta$, the
true value, unless I make that explicit.
\end{defn}

\begin{defn}(Score)
The \emph{score} is the score function evaluated at true value:
$\bsS(\bstheta,\bsy)$.
\end{defn}
\begin{rmk}
In the results to follow, always note whether I am referring to the
\emph{score function} $\bsS(\bstildetheta,\bsy)$ (which depends on both
$\bstildetheta$ and $\bsy$) or the \emph{score}, $\bsS(\bstheta,\bsy)$
which depends only on $\bsy$ since $\bstildetheta=\bstheta$, the true
value.
\end{rmk}

\begin{prop}
The score has expectation zero:
$\E_{\bsy|\bstheta}[\bsS(\bstheta,\bsy)]=\bso$.
\end{prop}
\begin{proof}
Start with an identity that holds for any $\bstildetheta$ (whether it
equals $\bstheta$ or not):
\begin{align*}
  1 &= \int f(\bsy|\bstildetheta) \; d\bsy
\end{align*}
Provided the support doesn't depend upon $\bstildetheta$, differentiate
with respect to $\bstildetheta$ and take the derivative operator inside:
\begin{align*}
  \frac{d}{d\bstildetheta}
  [\, 1 \,]
  &=
  \frac{d}{d\bstildetheta}
  \left[
    \int f(\bsy|\bstildetheta) \; d\bsy
  \right] \\
  \iff\qquad
  \bso
  &=
  \int \frac{df(\bsy|\bstildetheta)}{d\bstildetheta}  \; d\bsy
\end{align*}
Now use Equation~\ref{score3} from above to substitute in for the
partial derivative term:
\begin{align}
  \bso &= \int \bsS(\bstildetheta,\bsy)\cdot f(\bsy|\bstildetheta)
  \; d\bsy
  \label{trueonly}
\end{align}
If we evaluate this at the true $\tilde{\theta}=\theta$, then
\begin{align*}
  \bso &= \int \bsS(\bstheta,\bsy)\cdot f(\bsy|\bstheta) \; d\bsy
\end{align*}
But then just identify the integral as
\begin{align*}
  \bso &= \int \bsS(\bstheta,\bsy)\cdot f(\bsy|\bstheta) \; d\bsy
  = \E_{\bsy|\bstheta}\big[\bsS(\bstheta,\bsy)\big]
\end{align*}
\end{proof}
\begin{rmk}
It's important that this works \emph{only} if we evaluate the score at
the true value $\bstildetheta=\bstheta$ because the data $\bsy$ are
generated by \emph{true} likelihood function $f(\bsy|\bstheta)$.
%Therefore, the expectation of the score function is
%\begin{align*}
  %\E_{\bsy|\bstheta}\big[\bsS(\bstildetheta,\bsy)\big]
  %= \int \bsS(\bstildetheta,\bsy) \cdot f(\bsy|\bstheta) \; d\bsy
%\end{align*}
In the key step in the proof, Equation~\ref{trueonly}, to get the
integral to be the corresponding integral for
$\E_{\bsy|\bstheta}[\;\cdot\;]$, we had to choose
$\bstildetheta=\bstheta$ so that the likelihood was $f(\bsy|\bstheta)$.
But that choice of $\bstildetheta$ also had to apply to the score
function $\bsS(\bstheta,\bsy)$. So we only get the ``expectation equals
zero'' result if $\bstildetheta=\bstheta$.
\end{rmk}

\begin{defn}(Information)
The \emph{information} is defined
\begin{align*}
  \calI
  =
  -\E_{\bsy|\bstheta}\left[
    \frac{\partial \bsS(\bstheta,\bsy)}{\partial \bstildetheta'}
  \right]
  =
  -\E_{\bsy|\bstheta}\left[
    \frac{\partial^2 \ln f(\bsy|\bstheta)}{%
      \partial\bstildetheta\;\partial\bstildetheta'}
  \right]
\end{align*}
where the derivatives are first taken with respect to $\bstildetheta$
and then evaluated at $\bstheta$.
\end{defn}
\begin{rmk}
Information $\calI$ is \emph{not} a function. It is always evaluated at
the true $\bstheta$. It only made sense to define a score
\emph{function} which depends upon $\bstildetheta$ because we want to
set that to zero in MLE estimation---doing so sets sets the first
derivative of the log-likelihood to zero so that we maximize the
log-likelihood. It's perfectly natural to think of that first-derivative
function. However, there's no similar motivation or justification for
defining an information function.  The concept only really makes sense
when we look at the second derivative of the log-likelihood evaluated at
the true $\bstheta$.
\end{rmk}

\begin{prop}
The information is both the variance and the second moment of the score.
\begin{align*}
  \calI
  = \E_{\bsy|\bstheta}\left[\bsS(\bstheta,\bsy)\bsS(\bstheta,\bsy)'\right]
  = \Var_{\bsy|\bstheta}[\bsS(\bstheta,\bsy)]
\end{align*}
\end{prop}
\begin{proof}
Start with the expectation of the score function for any
$\bstildetheta$, which will equal some vector of constants (we don't
care about the values):
\begin{align*}
  \bsC
  &= \E_{\bsy|\bstheta}\big[\bsS(\bstildetheta,\bsy)\big]
\end{align*}
Differentiate with respect to $\bstildetheta$ again:
\begin{align*}
  \frac{\partial}{\partial\bstildetheta'}
  \left[ \bsC \right]
  &=
  \frac{\partial}{\partial\bstildetheta'}
  \bigg[
  \E_{\bsy|\bstheta}\big[S(\bstildetheta,\bsy)\big]
  \bigg] \\
  \bso &=
  \int
  \left[
  \frac{\partial \bsS(\bstildetheta,\bsy)}{\partial\bstildetheta'}
  f(\bsy|\bstildetheta)
  +
  \bsS(\bstildetheta,\bsy)
  \frac{\partial f(\bsy|\bstildetheta)}{\partial\bstildetheta'}
  \;
  d\bsy
  \right]
  \\
  \text{By Equation~\ref{score3}}
  \qquad
  \bso &=
  \int
  \frac{\partial \bsS(\bstildetheta,\bsy)}{\partial\bstildetheta'}
  f(\bsy|\bstildetheta)
  \; d\bsy
  +
  \int
  \bsS(\bstildetheta,\bsy)\bsS(\bstildetheta,\bsy)' f(\bsy|\bstildetheta) \;
  d\bsy
\end{align*}
Now evaluate these expressions at the true value,
$\bstildetheta=\bstheta$:
\begin{align*}
  \bso &=
  \int
  \frac{\partial \bsS(\bstheta,\bsy)}{\partial\bstildetheta'}
  f(\bsy|\bstheta)
  \; d\bsy
  +
  \int
  \bsS(\bstheta,\bsy)\bsS(\bstheta,\bsy)' f(\bsy|\bstheta) \;
  d\bsy
  \\
  \iff\quad
  \bso
  &=
  \E_{\bsy|\bstheta}\left[
  \frac{\partial \bsS(\bstheta,\bsy)}{\partial\bstildetheta'}
  \right]
  +
  \E_{\bsy|\bstheta}\big[
  \bsS(\bstheta,\bsy)\bsS(\bstheta,\bsy)'
  \big]
\end{align*}
Rearrange:
\begin{align*}
  -\E_{\bsy|\bstheta}\left[
  \frac{\partial \bsS(\bstheta,\bsy)}{\partial\bstildetheta'}
  \right]
  &=
  \E_{\bsy|\bstheta}\big[
  \bsS(\bstheta,\bsy)\bsS(\bstheta,\bsy)'
  \big] \\
  \iff \qquad
  \calI
  &=
  \Var_{\bsy|\bstheta}\big[
  \bsS(\bstheta,\bsy)
  \big]
\end{align*}
The LHS expecation simplified to $\calI$ by definition of $\calI$. The
RHS simplified to the variance of the score because the mean of the
score function evaluated at true $\bstheta$ is zero, as shown above, so
the variance equals the second moment.
\end{proof}


\clearpage
\subsubsection{Cramer-Rao Lower Bound}

In general, we hope to find minimum-variance unbiased (MVU) estimators
of parameters. One approach is the Cramer-Rao inequality approach.
Given one regularity assumption, it establishes a lower bound on the
variance of \emph{any} unbiased estimator.
Note also that the result is \emph{very} general, as it does not assume
independence or identical distribution of the different $\bsy_n$ that we
use to estimate the parameter.

\begin{thm}\emph{(Cramer-Rao)}
Provided the support of the likelihood does \emph{not} depend upon the
value of $\bstheta$, then for \emph{any} unbiased estimator
$\bshattheta$, we have
\begin{align*}
  \Var(\bshattheta)
  =
  \E\big[ (\bshattheta-\bstheta) (\bshattheta-\bstheta)' \big]
  &\geq
  \big[\Var(\bsS(\bstheta,\bsy))\big]^{-1}
  =
  \calI^{-1}
\end{align*}
i.e.\ the difference $\Var(\bshattheta)-\calI^{-1}$ is a postive
semidefinite matrix.
\end{thm}
\begin{proof}
Let $\bshattheta$ be any unbiased estimator of $\bstheta$, i.e.\ it
satisfies
\begin{align*}
  \bstheta = \int \bshattheta f(\bsy|\bstheta) \; d\bsy
\end{align*}
Next, note that $\bshattheta$ is a function/mapping from sample
observations to a vector in $\R^P$. Though the data generating process
depends upon $\bstheta$, the estimator $\bshattheta$ is, mechanically,
just a function from data/numbers to a vector. Thus, it is a constant
with respect to $\bstheta$. So when we differentiate both sides of the
above equation with respect to $\bstheta$ and use Equation~\ref{score3},
we get
\begin{align*}
  \frac{\partial}{\partial\bstheta'}
  [\,
  \bstheta
  \,]
  &=
  \frac{\partial}{\partial\bstheta'}
  \left[
  \int \bshattheta f(\bsy|\bstheta) \; d\bsy
  \right]
  \\
  \bsI_P
  &= \int \bshattheta \bsS(\bstheta,\bsy)' f(\bsy|\bstheta)\; d\bsy \\
  \iff\quad
  \bsI_P
  &= \E\big[\bshattheta \bsS(\bstheta,\bsy)'\big]
\end{align*}
keeping in mind that both $\bshattheta$ and $\bsS(\bstheta,\bsy)$ are
random variables. But since $\E[\bsS(\bstheta,\bsy)]=\bso$, we can say
$\E[\bshattheta \bsS(\bstheta,\bsy)'] =
\Cov[\bshattheta,\bsS(\bstheta,\bsy)]$, therefore
\begin{align*}
  \bsI_P &=
  \Cov[\bshattheta,\bsS(\bstheta,\bsy)]
  \\
  \text{Standard Result} \qquad
  &\leq
  \Var[\bshattheta]\Var[\bsS(\bstheta,\bsy)]
\end{align*}
Therefore, we can rearrange to get
\begin{align*}
  \Var[\bshattheta]
  &\geq
  \Var[\bsS(\bstheta,\bsy)]^{-1}
  =\calI^{-1}
\end{align*}
\end{proof}


\clearpage
\subsection{Extremum Estimators, M- (``Sandwich'') Estimators}

Extremum estimators are quite important. Virtually all estimators of
interest---MLE, NLS, GMM---are (or can be cast as) extremum
estimators. We also introduce a particular type of extremum estimator
called the \emph{M-estimator}, which nests as special cases MLE and
NLS.


\begin{defn}(Extremum Estimator)
We call $\bshattheta$ an \emph{extremum estimator} if it satisfies
\begin{align*}
  \bshattheta = \argmax_{\bstildetheta\in\bsTheta}\;
  Q_N(\bstildetheta)
\end{align*}
for some sample objective function $Q_N(\bstildetheta)$, which is
constructed from sample $\{\bsy_n\}\nN$ and converges to corresponding
population objective function $Q(\bstildetheta)$ as $N\ra\infty$ where
\begin{align*}
  \bstheta = \argmax_{\bstildetheta\in\bsTheta}\;
  Q(\bstildetheta)
\end{align*}
This ensures that $\bshattheta$ is a consistent estimator of true
$\bstheta$, the unique maximizer of $Q(\bstildetheta)$.
\end{defn}
\begin{rmk}
Maximum is WLOG because, if we want to minimize, just max the negative.
\end{rmk}

\begin{defn}(M-Estimators)
M-estimators are a special type of extremum estimator. More
specifically, we call $\bshattheta$ an \emph{M-estimator} if it can be
written as
\begin{align*}
  \bshattheta =
  \argmax_{\bstildetheta\in\bsTheta}\;
  \underbrace{%
  \frac{1}{N}
  \sumnN q(\bsy_n,\bstildetheta)
  }_{Q_N(\bstildetheta)}
  \qquad\implies\qquad
  \bso
  = \underbrace{%
    \frac{1}{N}\sumnN q'(\bsy_n, \bshattheta)
  }_{Q'_N(\bshattheta)}
\end{align*}
for some function $q(\bsy_n,\bstildetheta)$.
This means the true $\bstheta$ we hope to estimate satisfies
$Q(\bstheta)=\E[q(\bsy_n,\bstheta)]$ and $\bso=\E[q'(\bsy_n,\bstheta)]$
where $q'(\,\cdot\,,\bstheta)$ is
$\frac{\partial}{\partial\bstildetheta}[q(\bsy_n,\bstildetheta)]$
evaluated at true $\bstheta$.
\end{defn}

\begin{prop}\emph{(Asymptotic Distribution of M-Estimator)}
\label{prop:masymp}
Let $\bshattheta_N$ denote the M-estimator constructed with sample size
$N$. Then
\begin{align*}
  \sqrt{N}(\bshattheta_N-\bstheta)
  \quad\dto\quad
  \calN\big(0,\;
  \E[q''(\bsy_n,\bstheta)]^{-1}
  \;
  \Var[q'(\bsy_n,\bstheta)]
  \;
  \E[q''(\bsy_n,\bstheta)]^{-1}
  \big)
\end{align*}
where
\begin{align*}
  q'(\bsy_n,\bstildetheta)
  =
  \frac{\partial q(\bsy_n,\bstildetheta)}{%
    \partial\bstildetheta}
  \qquad
  q''(\bsy_n,\bstildetheta)
  =
  \frac{\partial^2 q(\bsy_n,\bstildetheta)}{%
    \partial\bstildetheta\,\partial\bstildetheta'}
\end{align*}
Note that the derivatives in the asymptotic variance expression are
evaluated at true $\bstheta$.  Since we don't know the true $\bstheta$
in general, we will need to estimate these components. We do so by
approximating the expectation and variance terms with
\begin{align}
  \frac{1}{N}\sumnN q'(\bsy_n,\bshattheta_N)q'(\bsy_n,\bshattheta_N)'
  \quad&\pto\quad
  \E\big[q'(\bsy_n,\bstheta)q'(\bsy_n,\bstheta)'\big]
  =
  \Var\big[q'(\bsy_n,\bstheta)\big]
  \label{mestvar}
  \\
  \frac{1}{N}\sumnN q''(\bsy_n,\bshattheta_N)
  \quad&\pto\quad
  \E\big[q''(\bsy_n,\bstheta)\big]
  \notag
\end{align}
In Expression~\ref{mestvar}, second moment and the variance are equal
because $\E[q'(\bsy_n,\bstheta)]=0$.
\end{prop}
\begin{rmk}
This is sometimes called a \emph{Sandwich Estimator} because the
asymptotic variance looks like a sandwich.
\end{rmk}

\begin{proof}
For $\bstheta\in\R^P$, then $\bshattheta$ must satisfy the following
$P$ first order conditions for a maximum (ommitting unnecessary $1/N$):
\begin{align*}
  \bso &=
  \sumnN q'(\bsy_n,\bshattheta)
\end{align*}
where the elements of the sum are the derivative of $q$ with respect to
vector $\bstildetheta$, evaluated at $\bsy_n$ and $\bshattheta$. This is
a system of $P$ equations in $P$ unknowns.  We can do a Taylor Expansion
of \emph{each} equation (separately) about the unknown, true value
$\bstheta$:
\begin{align*}
  0 &=
  \sumnN q'(\bsy_n,\bshattheta)
  =
  \sumnN
  \big[
  q'(\bsy_n,\bstheta)
  +
  q''(\bsy_n,\bsbartheta)
  (\bshattheta-\bstheta)
  \big]
\end{align*}
where $\bsbartheta$ lies in between true $\bstheta$ and estimated
$\bshattheta$. Note that this is a slight abuse of notation. Since the
Taylor expansion was done separately for each equation, there is a
separate $\bsbartheta$ for each expansion/equation. But it won't matter
because as we send $N\ra\infty$, each $\bsbartheta$ (sandwiched
in between consistent $\bshattheta$ and true $\bstheta$) will converge
to $\bstheta$ anyway.
So to finish up, we rearrange,
\begin{align*}
  \sqrt{N}(\bshattheta-\bstheta)
  =
  -
  \bigg[
  \frac{1}{N}
  \sumnN
  q''(\bsy_n,\bsbartheta)
  \bigg]^{-1}
  \bigg[
  \frac{1}{\sqrt{N}}
  \sumnN
  q'(\bsy_n,\bstheta)
  \bigg]
\end{align*}
By the LLN and the CMT, the first term converges in probability to
$\E[q''(\bsy_n,\bstheta)]^{-1}$. By the CLT, the second converges to in
distribution to $\calN\big(0,\Var[q'(\bsy_n,\bstheta)]\big)$. Then, by
Slutsky,
\begin{align*}
  \sqrt{N}(\bshattheta-\bstheta)
  \quad\dto\quad
  \calN\big(0,
  \E[q''(\bsy_n,\bstheta)]^{-1}
  \;
  \Var[q'(\bsy_n,\bstheta)]
  \;
  \E[q''(\bsy_n,\bstheta)]^{-1}
  \big)
\end{align*}

\end{proof}


\begin{ex}(Mini-Max Estimator)
As a final example of an extremum estimator, suppose we know the true
value $\bstheta$ lies in $\bsTheta$, then we might choose an estimator
to minimize the maximum risk
\begin{align*}
  \bshattheta_{\text{mini-max}}
  =
  \argmin_{\bshattheta}
  \left(
  \max_{\bstildetheta \in \bsTheta}
  R(\bshattheta,\bstildetheta)
  \right)
\end{align*}
\end{ex}

\clearpage
\subsection{Nonlinear Least Squares Estimation}

Suppose we have random variables $y_n$ and $\bsx_n$, and we want to use
$\bsx_n$ to forecast $y_n$. Then we want to find some function
$g(\bsx_n)$ that does a ``good'' job predicting $y_n$. We say that
the mapping $g(\,\cdot\,)$ is ``good'' if it minimizes the Mean-Square
Error (MSE) of the forecast:
\begin{align}
  \min_{g(\,\cdot\,)} \; \E\big[(y_n - g(\bsx_n))^2\big]
  =
  \min_{g(\,\cdot\,)}
  \E\left[\E\big[(y_n - g(\bsx_n))^2\big] \;|\;\bsx_n\right]
  \label{nlsmotivation}
\end{align}
Using the second expression where we iterated expectations, it can be
shown that the choice of $g(\,\cdot\,)$ that minimizes the MSE is the
conditional expectation function (CEF): $g(\bsx_n)=\E[y_n|\bsx_n]$.
This motivates the following definition.

\begin{defn}(Nonlinear Least Squares Estimator)
Given sample $\{y_n,\bsx_n\}\nN$ and any arbitrary parametric function
$g(\bsx_n,\bstildetheta)$, the
\emph{nonlinear least squares (NLS) estimator} solves
\begin{align}
  \bshattheta_{NLS}
  = \argmin_{\bstildetheta\in\bsTheta}
  \frac{1}{N}\sumnN \big(y_n-g(\bsx_n,\bstildetheta)\big)^2
  \label{nls}
\end{align}
Note that this has first order condition
$\bso = \sumnN \hat{\varepsilon}_n g'(\bsx_n,\bshattheta)$
where $\hat{\varepsilon}_n := y_n-g(\bsx_n,\bshattheta)$.

We should think of $g(\bsx_n,\bstildetheta)$ as a parametric form that
\emph{actually is} or \emph{approximates} the true population CEF.
Then, since the true population CEF $\E[y_n|\bsx_n]$ necessarily
minimizes the population objective function in
Expression~\ref{nlsmotivation}, we are fitting a \emph{sample} CEF (or
CEF approximation) $g(\bsx_n,\bshattheta)$ to minimize the sample
objective function in Expression~\ref{nls}.
\end{defn}

\begin{defn}(Ordinary Least Squares)
If $g(\bsx_n,\bstildetheta)$ is \emph{linear} in $\bsx_n$ (as in
regression), we instead call the NLS estimator the \emph{ordinary least
squares (OLS) estimator}.
\end{defn}


\begin{defn}(Weighted NLS)
Given sample $\{y_n,\bsx_n\}\nN$, let $\{w_n\}\nN$ denote a set of
weights for the observations. Then the \emph{weighted NLS estimator}
solves
\begin{align}
  \bshattheta_{WNLS}
  = \argmin_{\bstildetheta\in\bsTheta}
  \frac{1}{N}\sumnN w_n^2\big(y_n-g(\bsx_n,\bstildetheta)\big)^2
  \label{wnls}
\end{align}
\end{defn}


\begin{prop}\emph{(Asymptotic Distribution of NLS Estimator)}
The NLS estimator has asymptotic distribution
\begin{align*}
  \sqrt{N}(\bshattheta_{NLS}-\bstheta)
  \quad\dto\quad
  \E\big[
  g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'
  \big]^{-1}
  \cdot
  \calN\big(0,\;
  \underbrace{%
  \E\big[\varepsilon_n^2g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'\big]
  }_{=\Var[\varepsilon_ng'(\bsx_n,\bstheta)]}
  \big)
\end{align*}
where $\varepsilon_n:=y_n-g(\bsx_n,\bstheta)$.  We see that the second
moment of $\varepsilon_n g'(\bsx_n,\bstheta)$ equals the variance
because the population first order conditions require
$\bso = \E[\varepsilon_n g'(\bsx_n,\bstheta)]$.

Given $\bshattheta_{NLS}$, we can estimate population quantities in
asymptotic variance by their sample counterparts
\begin{align*}
  \frac{1}{N}\sumnN
  g'(\bsx_n,\bshattheta_{NLS})g'(\bsx_n,\bshattheta_{NLS})'
  \quad&\pto\quad
  \E\big[
  g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'
  \big]
  \\
  \frac{1}{N}\sumnN
  %\underbrace{%
  \big(y_n-g(\bsx_n,\bshattheta_{NLS})\big)^2
  %}_{\hat{\varepsilon}_n}
  g'(\bsx_n,\bshattheta_{NLS})g'(\bsx_n,\bshattheta_{NLS})'
  \quad&\pto\quad
  \E\big[\varepsilon_n^2
  g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'
  \big]
  \notag
\end{align*}
\end{prop}
\begin{cor}\emph{(Asymptotic Distribution of the OLS Esimtator)}
In the special case where $g$ is linear,
$g(\bsx_n,\bstildebeta)=\bsx_n'\bstildebeta$ so that
$g'(\bsx_n,\bstildebeta)=\bsx_n$, the associated OLS estimator has
asymptotic distribution
\begin{align*}
  \sqrt{N}(\bshatbeta_{OLS}-\bsbeta)
  \quad\dto\quad
  \calN\big(0,\;
  \E[ \bsx_n\bsx_n' ]^{-1}
  \E[\varepsilon_n^2 \bsx_n\bsx_n' ]
  \E[ \bsx_n\bsx_n' ]^{-1}
  \big)
\end{align*}
Note that these are the heteroskedasticity-robust standard errors. If we
are willing to assume homoeskedasticy, then
$\E[\varepsilon_n^2\bsx_n\bsx_n']=\E[\varepsilon_n^2]\cdot\E[\bsx_n\bsx_n']$
and the standard errors simplify further.
\end{cor}
\begin{proof}
Notice that the NLS estimator is an $M$-estimator:
\begin{align*}
  \bshattheta_{NLS}
  = \argmin_{\bstildetheta\in\bsTheta}
  \frac{1}{N}\sumnN q(y_n,\bsx_n,\bstildetheta)
  \qquad\text{where}
  \quad
  q(y_n,\bsx_n,\bstildetheta):=
  \big(y_n-g(\bsx_n,\bstildetheta)\big)^2
\end{align*}
%Recall the asymptotic distribution for a general M-estimator in terms of
%the derivatives of $q$:
%\begin{align*}
  %\sqrt{N}(\bshattheta_N-\bstheta)
  %\quad\dto\quad
  %\calN\big(0,\;
  %\E\big[q''(y_n,\bsx_n,\bstheta)\big]^{-1}
  %\;
  %\Var\big[q'(y_n,\bsx_n,\bstheta)\big]
  %\;
  %\E\big[q''(y_n,\bsx_n,\bstheta)\big]^{-1}
  %\big)
%\end{align*}
We now just need to compute the derivatives of our particular $q$ and
apply Proposition~\ref{prop:masymp}, which gave the asymptotic
distribution for general M-estimators in terms of derivatives of $q$.
\begin{align*}
  q'(y_n,\bsx_n,\bstheta)
  &= \frac{\partial}{\partial \bstildetheta}
  \left[
  \big(y_n-g(\bsx_n,\bstildetheta)\big)^2
  \right]_{\bstildetheta=\bstheta}
  \\
  &=
  -2
  \big(y_n-g(\bsx_n,\bstheta)\big)
  g'(\bsx_n,\bstheta)
  \\
  q''(y_n,\bsx_n,\bstheta)
  &= \frac{\partial}{\partial \bstildetheta'}
  \left[
  -2
  \big(y_n-g(\bsx_n,\bstildetheta)\big)
  g'(\bsx_n,\bstildetheta)
  \right]_{\bstildetheta=\bstheta}
  \\
  &=
  -2
  \big(y_n-g(\bsx_n,\bstheta)\big)
  g''(\bsx_n,\bstheta)
  +2 g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'
\end{align*}
Define $\varepsilon_n:=y_n-g(\bsx_n,\bstheta)$, substite that in, and
compute moments:
\begin{align}
  \Var[q'(y_n,\bsx_n,\bstheta)]
  &=
  \E\big[q'(y_n,\bsx_n,\bstheta)q'(y_n,\bsx_n,\bstheta)'\big]
  \notag\\
  &=
  4\E\big[\varepsilon_n^2g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'\big]
  \notag\\
  \E\big[q''(y_n,\bsx_n,\bstheta)\big]
  &=
  \E\left[
  -2
  \varepsilon_n
  g''(\bsx_n,\bstheta)
  +2 g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'
  \right]
  \notag\\
  &=
  2 \E\big[
  g'(\bsx_n,\bstheta)g'(\bsx_n,\bstheta)'
  \big]
  \label{nlsproof}
\end{align}
And that is enough to plug into Proposition~\ref{prop:masymp} for the
desired result.

Final note: the simplification to Line~\ref{nlsproof} required
elimination of the $-2\varepsilon_ng''(\bsx_n,\bstheta)$ term. This
followed because $\bstheta$ must satisfy the population first order
necessary conditions, of the minimization problem, which say that
\begin{align*}
  \bso =
  \E\big[q'(y_n,\bsx_n,\bstheta)\big]
  &=
  -2\E\big[\varepsilon_ng'(\bsx_n,\bstheta)\big]
\end{align*}
Iterating expectations, this last line says
\begin{align*}
  \bso &=
  -2\E\left[
    \E\big[\varepsilon_ng'(\bsx_n,\bstheta)\;|\bsx_n\big]
  \right]
  =
  -2\E\big[
    g'(\bsx_n,\bstheta)\cdot \E[\varepsilon_n|\bsx_n]
  \big]
\end{align*}
In general, $g'(\bsx_n,\bstheta)\neq \bso$, so this implies that
$\E[\varepsilon_n|\bsx_n]=0$. That is sufficient, since then
\begin{align*}
  \E\big[
  \varepsilon_n
  g''(\bsx_n,\bstheta)
  \big]
  =
  \E\left[
  \E\big[
  \varepsilon_n
  g''(\bsx_n,\bstheta)
  \;|\bsx_n
  \big]
  \right]
  =
  \E\left[
  g''(\bsx_n,\bstheta)
  \cdot
  \E[ \varepsilon_n |\bsx_n ]
  \right]
  =\bso
\end{align*}
\end{proof}


\clearpage
\subsection{Maximum Likelihood Estimation (MLE)}

Maximum likelihood is another framework for estimating parameters from
the data. The big insight of MLE is to turn the joint probability
density function on its head and instead think in terms of likelihoods.
To draw this distinction, we now contrast the density and the
likelihood:
\begin{enumerate}[label=(\roman*)]
  \item Given \emph{fixed}, \emph{known}, true parameter $\bstheta$,
    $f(\bsy_{1:N}|\bstheta)$ is a \emph{density} function (of
    $\bsy_{1:N}$) that defines the data generating process.  Given that
    density, you can make probability statements about the distribution
    and expected realizations of sample $\bsy_{1:N}$ \emph{ex ante},
    before observing any data. The density is simply a map from some
    hypothetical sample realization $\bsy_{1:N}$ to the probablity of
    observing that realization if you draw a sample.

  \item The \emph{likelihood} inverts that thinking. Specifically, we
    treat our \emph{sample} as fixed, supposing that
    \emph{we have an already-realized sample} $\bsy_{1:N}$ but we don't
    know $\bstheta$.  We therefore think about
    $f(\bsy_{1:N}|\bstildetheta)$ as a map from hypothetical
    $\bstildetheta$ to the the probability of
    \emph{having observed realized sample $\bsy_{1:N}$},
    given that $\bstildetheta$ was the true parameter vector all along.
    Maximum likelihood estimation then says to choose as $\bshattheta$
    the vector that would have delivered the highest \emph{ex ante}
    probability of observing the actual realized sample $\bsy_{1:N}$.
\end{enumerate}
Now this is all well and good, but we wouldn't care about this logic if
maximum likelihood estimation produced garbage estimators. But it
doesn't. In fact, it produces estimators with really desireable
properties, as we'll see below. But first, we have to define MLEs.

\subsubsection{MLE Definition and Properties}

\begin{defn}(Maximum Likelihood Estimator)
Suppose we think that the joint likelihood of our sample $\bsy_{1:N}$
has parameteric form $f(\bsy_{1:N}|\bstildetheta)$. Then the
\emph{maximum likelihood estimator} solves
\begin{align}
  \bshattheta_{MLE}
  &=
  \argmax_{\bstildetheta} f(\bsy_{1:N}|\bstildetheta)
  =
  \argmax_{\bstildetheta}\;
  \ln f(\bsy_{1:N}|\bstildetheta)
  \label{mlelik}
\end{align}
In practice, we almost always work with the log-likelihood. First, in
the special case where $\bsy_{1:N}$ is an iid sample so that
$f(\bsy_{1:N}|\bstildetheta)=\prod\nN f(\bsy_n|\bstildetheta)$, the
product simplifies to a sum in the log-likelihood. Second, when we
actually program up this maximization problem on a computer, the
numerical value of the likelihood will often be very, very tiny given
many observations, so working with the log-likelihood is more stable
numerically.

Finally, for simple cases, we can often solve the maximization problem
analytically. We do so by looking at the first order conditions to
Maximization Problem~\ref{mlelik}, then solving those first order
conditions to write the parameter(s) as a function of of the sample
observations. That will generate a map from the sample to the MLE
parameter estimates. This approach can also incorporate constraints,
since we can just use constrained optimization techniques.
\end{defn}
\begin{rmk}
In finite samples, MLEs aren't guaranteed to unbiased at all, but often
they will be, and since they're consistent in large samples (as we'll
show), that's fine enough.
\end{rmk}


\begin{prop}\emph{(Properties of Maximum Likelihood Estimators)}
Given MLE estimator $\bshattheta_{MLE}$ and iid sample $\{\bsy_n\}\nN$
with likelihood $f(\bsy_{1:N}|\bstheta) =\prodnN f(\bsy_n|\bstheta)$,
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $\bshattheta_{MLE}$ is consistent, i.e.\
    $\bshattheta_{MLE}\pto\bstheta$.
  \item $\bshattheta_{MLE}$ is asymptotically normal with distribution
    \begin{align*}
      \sqrt{N}(\bshattheta_{MLE}-\bstheta)
      \quad\dto\quad
      \calN\big(0,\;
      \calI^{-1}
      \big)
    \end{align*}
    where $\calI$ is the information matrix
    \begin{align*}
      \calI
      = -\E\left[
        \frac{\partial^2 \ln f(\bsy_n|\bstheta)}{%
          \partial \bstildetheta \,\partial \bstildetheta'}
        \right]
      = \Var\left[
        \frac{\partial \ln f(\bsy_n|\bstheta)}{%
          \partial \bstildetheta}
        \right]
    \end{align*}
    where the derivatives are derivatives of log-likelihood function
    $\ln f(\bsy_n|\bstildetheta)$ with respect to $\bstildetheta$ and
    evaluated at true $\bstheta$.
\end{enumerate}
\end{prop}
\begin{rmk}
Property (ii) says that, asymptotically, MLE achieves the Cramer-Rao
lower bound---the lowest possible variance for any estimator. For that
reason, MLE is an \emph{asymptotically efficient} or
\emph{asymptotically lowest-variance} estimator. You simply can't do
better than MLE as your sample grows large.

But there's a catch: misspecification.
MLE relies heavily on the assumption that the true likelihood
\emph{really is} $f(\bsy_{1:N}|\bstildetheta)$, and that all you need to
do is find the true $\bstheta$ to plug in for $\bstildetheta$. It
doesn't allow for the fact that you've misspecified your model, i.e.\
that $f(\bsy_{1:N}|\bstildetheta)$ was a flat-out wrong likelihood
function. So in the case of misspecification, it might \emph{not} be the
lowest-variance efficient estimator, and we might prefer other
alternatives. Below, we will see a test for misspecification and a word
about interpreting the MLE under misspecification.
\end{rmk}
\begin{proof}
Before proving each point, we first note that the maximum likelihood
estimator can be written as an M-estimator if we work with the
log-likelihood:
\begin{align*}
  \bshattheta_{MLE}
  &=
  \argmax_{\bstildetheta} \; \ln f(\bsy_{1:N}|\bstildetheta)
  =
  \argmax_{\bstildetheta} \;
  \ln\left(\prodnN f(\bsy_n|\bstildetheta)\right)
  \\
  \iff\qquad
  \bshattheta_{MLE}
  &=
  \argmax_{\bstildetheta}
  \sumnN
  \underbrace{%
  \ln f(\bsy_{n}|\bstildetheta)
  }_{q(\bsy_n,\bstildetheta)}
\end{align*}
where
\begin{align*}
  q'(\bsy_n,\bstildetheta)
  =
  \frac{\partial \ln f(\bsy_n|\bstildetheta)}{\partial \bstildetheta}
  = S(\bstildetheta,\bsy_n)
  \qquad
  q''(\bsy_n,\bstildetheta)
  =
  \frac{\partial^2 \ln f(\bsy_n|\bstildetheta)}{%
    \partial\bstildetheta\,\partial\bstildetheta'}
\end{align*}
where $S(\bstildetheta,\bsy_n)$ is the score function for a single
observation.

The maximization problem has FOCs for the sample and the population,
respectively, of
\begin{align*}
  \bso = \sumnN q'(\bsy_n,\bshattheta_{MLE})
  =
  \sumnN S(\bshattheta_{MLE},\bsy_n)
  \qquad
  \bso
  = \E\big[q'(\bsy_n,\bstheta)\big]
  = \E\big[S(\bstheta,\bsy_n)\big]
\end{align*}
As we saw before, the latter condition says the score function
evaluated at true $\bstheta$ has mean 0.
\clearpage
We have now set up enough to move onto the proofs.
\begin{enumerate}[label=(\roman*)]
  \item
    In a finite sample, we construct the MLE estimator by solving the
    maximization problem
    \begin{align*}
      \bshattheta_{MLE}
      &=
      \argmax_{\bstildetheta}
      \frac{1}{N}
      \sumnN \ln f(\bsy_{n}|\bstildetheta)
    \end{align*}
    Note that as the sample size grows, the LLN gives us
    \begin{align*}
      %\frac{1}{N}\sumnN q(\bsy_n,\bstildetheta)
      %=
      \frac{1}{N} \sumnN \ln f(\bsy_n|\bstildetheta)
      \quad\pto\quad
      \E_{\bsy|\bstheta}\big[\ln f(\bsy_n|\bstildetheta)\big]
    \end{align*}
    where $\E_{\bsy|\bstheta}[\;\cdot\;]$ denotes the expectation,
    averaging over $\bsy_n$, using true likelihood function
    $f(\bsy_n|\bstheta)$.
    For $\bshattheta_{MLE}$ to be consistent, we need to check that
    true $\bstheta$ is a maximizer of this limiting expectation.
    Otherwise, the MLE estimator would be converging to another
    parameter vector entirely.

    The true $\bstheta$ is a maximizer of
    $\E_{\bsy|\bstheta}\big[\ln f(\bsy_n|\bstildetheta)\big]$ if and
    only if
    \begin{align*}
      \E_{\bsy|\bstheta}\big[\ln f(\bsy_n|\bstheta+\bsdelta)\big]
      &\leq
      \E_{\bsy|\bstheta}\big[\ln f(\bsy_n|\bstheta)\big]
      \qquad\forall \bsdelta
    \end{align*}
    We can rearrange and alternatively say that $\bstheta$ is a
    maximizer if and only if
    \begin{align*}
      \E_{\bsy|\bstheta}\left[
        \ln\left(\frac{f(\bsy_n|\bstheta + \bsdelta)}{f(\bsy_n|\bstheta)}\right)
      \right]
      &\leq 0
      \qquad\forall \bsdelta
    \end{align*}
    But that is immediate if we apply Jensen's inequality to the LHS of
    the inequality to get:
    \begin{align*}
      \forall\bsdelta\qquad
      \E_{\bsy|\bstheta}\left[
        \ln\left(\frac{f(\bsy_n|\bstheta + \bsdelta)}{f(\bsy_n|\bstheta)}\right)
      \right]
      &\leq
      \ln \left(
      \E_{\bsy|\bstheta}\left[
        \frac{f(\bsy_n|\bstheta + \bsdelta)}{f(\bsy_n|\bstheta)}
      \right]
      \right)
      \\
      &=
      \ln \left(
      \int
      \frac{f(\bsy_n|\bstheta + \bsdelta)}{f(\bsy_n|\bstheta)}
      \cdot f(\bsy_n|\bstheta)
      \; d\bsy_n
      \right) \\
      &=
      \ln \left(
      \int
      f(\bsy_n|\bstheta + \bsdelta)
      \; d\bsy_n
      \right)
      = \ln(1) = 0
      %\\
      %\implies\quad
      %\E_{\bsy|\bstheta}\left[
        %\ln\left(\frac{f(\bsy_n|\bstheta + \bsdelta)}{f(\bsy_n|\bstheta)}\right)
      %\right]
      %&\leq 0
    \end{align*}
    So we have $\leq 0$, which is precisely the condition we described
    for $\bstheta$ to be the maximizer.

  \item
    We already wrote the MLE as an M-estimator, so we can just plug in
    the derivatives of $q$ from above into the expression for the
    asymptotic distribution of M-estimators:
    \begin{align*}
      \sqrt{N}(\bshattheta_{MLE}-\bstheta)
      \;\dto\;
      \calN\left(0,\;
      \E\left[
      \frac{\partial^2 \ln f(\bsy_n|\bstheta)}{%
        \partial\bstildetheta\,\partial\bstildetheta'}
      \right]^{-1}
      \;
      \Var\left[
        \frac{\partial \ln f(\bsy_n|\bstheta)}{\partial \bstildetheta}
      \right]
      \;
      \E\left[
      \frac{\partial^2 \ln f(\bsy_n|\bstheta)}{%
        \partial\bstildetheta\,\partial\bstildetheta'}
      \right]^{-1}
      \right)
    \end{align*}
    The only further step is recalling a result we proved
    above---namely, that information also equals the variance of the
    score
    \begin{align*}
      \calI
      := -\E\left[
        \frac{\partial^2 \ln f(\bsy_n|\bstheta)}{%
          \partial \bstildetheta \,\partial \bstildetheta'}
        \right]
      =
      \Var S(\bstheta,\bsy_n)
      = \Var\left[
        \frac{\partial \ln f(\bsy_n|\bstheta)}{%
          \partial \bstildetheta}
        \right]
    \end{align*}
    So the above expression for asymptotic variance is
    $\calI^{-1}\calI\calI^{-1}$, which simplifies to $\calI^{-1}$.
\end{enumerate}
\end{proof}
%\begin{enumerate}
   %\item {\sl Invariance Property}: Suppose that $\tau = \tau(\theta)$
      %is a monotonically increasing or decreasing function of
      %$\theta$. Then
      %\[ \hat{\tau}_{\text{MLE}} = \tau\left(\theta_{\text{MLE}}\right)
   %\]
      %where $\hat{\tau}_{\text{MLE}}$ is the MLE for $\tau(\theta)$
      %and $\theta_{\text{MLE}}$ is the MLE for $\theta$.
   %\item {\sl Relation to MNTSS's}: If a scalar MNTSS exists,
      %then it is a function of the maximum likelihood estimator.

      %\begin{proof} Here's an informal sketch. We know by the
   %factorization criterion that the jdf, and hence the likelihood,
   %will look like
      %\[ f = L = g(y_1, \ldots, y_n) h(\hat{\theta}_\text{MNTSS},
         %\theta) \]
   %where $g$ is independent of $\theta$.
   %Well, to max L with respect to $\theta$, we will be maxing
   %$h$ with respect to $\theta$, which implies that $\theta$
   %is some function of the MNTSS.
      %\end{proof}
%\end{enumerate}

\subsubsection{Misspecification and Kullback-Leibler Divergence}

Throughout this section, we consider what happens when we have a random
variable $\bsy$, but we misspecify $\bsy$'s likelihood function.
In particular, we assume the \emph{wrong} parameteric form
$g(\bsy|\bstildetheta)$ for the likelihood function, instead of the true
parameteric functional form $f(\bsy|\bstildetheta)\neq
g(\bsy|\bstildetheta)$. While there is a true $\bstheta$ for the correct
likelihood function $f(\bsy|\bstildetheta)$, there is no such ``true''
parameter for the misspecified likelihood. Nonetheless, it will be
useful to define a vector $\bstheta_{KL}$ (that we will justify and
explain below) to put in all the places where we would normally use true
$\bstheta$.

\begin{prop}\emph{(Possible Misspecification)}
The pseudo-``information matrix'' will, in general, not equal the
variance of the pseudo-score
\begin{align*}
  \calI_p
  =
  -\E_{\bsy|f}\left[
    \frac{\partial^2 \ln g(\bsy|\bstheta_{KL})}{%
      \partial\bstildetheta\;\partial\bstildetheta'}
  \right]
  \neq
  \Var_{\bsy|f}\left[
    \frac{\partial \ln g(\bsy|\bstheta_{KL})}{%
      \partial\bstildetheta}
  \right]
  =
  \Var_{\bsy|f}[\bsS_p(\bstheta_{KL},\bsy)]
\end{align*}
where $\E_{\bsy|f}[\;\cdot\;]$ is the expectation over $\bsy$ computed
with the true likelihood $f(\bsy|\bstheta)$.
\end{prop}
\begin{proof}
No real proof except to say that we have no reason to expect the
variance of the log of $g$ to equal the mean of the second derivative of
$g$. The equality of these two objects for the true likelihood relied
heavily upon the fact that we were, in fact, working with the true
likelihood. No such luck now.
\end{proof}

\begin{prop}(Asymptotic Distribution of the Misspecified Estimator)
Suppose that we have an iid sample $\{\bsy_n\}\nN$, but we run ``maximum
likelihood'' on the \emph{wrong} parameteric functional form
$g(\bsy_n|\bstildetheta)$. Then the resulting estimator has asymptotic
distribution
\begin{align*}
  \sqrt{N}(\bshattheta-\bstheta_{KL})
  \quad\dto\quad
  \calN\left(0,\;
  \E\left[
  \frac{\partial^2 \ln g(\bsy_n|\bstheta_{KL})}{%
    \partial\bstildetheta\,\partial\bstildetheta'}
  \right]^{-1}
  \;
  \Var\left[
    \frac{\partial \ln g(\bsy_n|\bstheta_{KL})}{\partial \bstildetheta}
  \right]
  \;
  \E\left[
  \frac{\partial^2 \ln g(\bsy_n|\bstheta_{KL})}{%
    \partial\bstildetheta\,\partial\bstildetheta'}
  \right]^{-1}
  \right)
\end{align*}
And since $g$ is the wrong likelihood function, we can't use the
``information matrix equals variance of the score'' trick on the middle
term to simplify the asymptotic variance expression (as we did for
correctly specified MLE).
\end{prop}
\begin{proof}
Even though we have the wrong likelihood function, the maximization
problem still characterizes a well-defined M-estimator
\begin{align*}
  \bshattheta
  &=
  \argmax_{\bstildetheta}
  \sumnN
  \underbrace{%
  \ln g(\bsy_{n}|\bstildetheta)
  }_{q(\bsy_n,\bstildetheta)}
\end{align*}
where
\begin{align*}
  q'(\bsy_n,\bstildetheta)
  =
  \frac{\partial \ln g(\bsy_n|\bstildetheta)}{\partial \bstildetheta}
  \qquad
  q''(\bsy_n,\bstildetheta)
  =
  \frac{\partial^2 \ln g(\bsy_n|\bstildetheta)}{%
    \partial\bstildetheta\,\partial\bstildetheta'}
\end{align*}
From there, we just apply the usual formulas.
\end{proof}


\clearpage
\begin{defn}(Kullback-Leibler Divergence)
Let $f$ and $g$ be densities. Define the
\emph{Kullback-Leibler Divergence} as
\begin{align*}
  K(g,f)
  &= \int \ln\left(\frac{g(z)}{f(z)} \right) g(z)\;dz
  =: \E_g\left[
  \ln\left(\frac{g(z)}{f(z)} \right)
  \right]
\end{align*}
This is \textbf{not} a \emph{metric} since it is not symmetric. It is,
however, a divergence since
\begin{align*}
  K(g,f)
  &= \E_g\left[
  -\ln\left(\frac{f(z)}{g(z)}\right)
  \right] \\
  \text{By Jensen's Inequality}\quad
  &\geq
  -\ln\left(
  \E_g\left[
  \frac{f(z)}{g(z)}
  \right]  \right)
  =
  -\ln\left(
  \int
  \frac{f(z)}{g(z)}
  g(z)
  \right)
  \\
  \implies\quad
  K(g,f)
  &\geq
  -\ln\left(
  \int
  f(z)
  \right)
  = -\ln(1) = 0
\end{align*}
\end{defn}

\begin{prop}\emph{(Misspecified MLE Minimizes KL Divergence)}
Suppose that the true density for random variable $\bsy_n$ is
$g(\bsy_n)$, but we mistakenly assume the density is in parametric
family $f(\bsy_n|\bstildetheta)$. Then all we can say about the pseudo
``MLE'' estimate $\bshattheta_{KL}$---computed assuming
$f(\bsy_n|\bstildetheta)$---is that it is a consistent estimator for the
parameter vector $\bstheta_{KL}$ that minimizes the KL divergence
between densities $g$ and $f(\,\cdot\,|\bstildetheta)$.
\end{prop}
\begin{proof}
Let's start by looking at the objective function for the misspecified
pseudo-MLE:
\begin{align*}
  \bshattheta_{KL}
  &=
  \argmax_{\bstildetheta}\;
  \frac{1}{N} \sumnN
  \ln f(\bsy_n|\bstildetheta)
  \quad
  \pto\quad
  \argmax_{\bstildetheta}\;
  \E_g\left[
  \ln f(\bsy_n|\bstildetheta)
  \right]
\end{align*}
Where the expectation is take with respect to the true data density,
$g(\bsy_n)$. Now consider the KL divergence between $g$ and $f$:
\begin{align*}
  K\big(g,f(\,\cdot\,|\bstildetheta)\big)
  &= \E_g\left[
  \ln\left(\frac{g(\bsy_n)}{f(\bsy_n|\bstildetheta)}\right)
  \right]
  =
  \E_g\left[
  \ln g(\bsy_n)
  \right]
  -
  \E_g\left[
  \ln f(\bsy_n|\bstildetheta))
  \right]
\end{align*}
The first term is a constant with respect to $\bstildetheta$. Therefore,
it's obvious that the minimizer of this expression---the $\bstildetheta$
that minimizes the KL divergence between $g$ and
$f(\,\cdot\,|\bstildetheta)$---must also necessarily solve the
maximization problem above that defined $\bshattheta_{KL}$.
\end{proof}


\clearpage
\subsection{Generalized Method of Moments}

\subsubsection{Method of Moments and GMM Defined}

A model and its assumptions typically imply certain \emph{moment
conditions} which state that some function
$\bsf:\R^K\times\R^P \ra \R^M$ of random variable $\bsy_n\in\R^K$ and
true parameter vector
$\bstheta \in\R^P$ must equal (without loss of generality) the constant
zero vector:
\begin{align}
  \label{populationMomentConditions}
  \bsg(\bstheta) := \E[\bsg_n(\bstheta) ]
  := \E[\bsf(\bsy_n,\bstheta)] = \bso
\end{align}
Notice we introduced shorthand notation
$\bsg_n(\bstildetheta):=\bsf(\bsy_n,\bstildetheta)$
and $\bsg(\bstildetheta):=\E[\bsg_n(\bstildetheta)]$.
Our goal is to estimate true $\bstheta$. Quite sensibly, we can estimate
parameters by drawing a sample $\{\bsy_n\}\nN$ and constructing the
sample analog of the population moment conditions:
\begin{align}
  \label{sampleMomentConditions}
  \bsbarg_N(\bstildetheta) :=
  \frac{1}{N} \sumnN \bsg_n(\bstildetheta)
  =
  \frac{1}{N} \sumnN \bsf(\bsy_n,\bstildetheta)
\end{align}
This is a function of parameter vector $\bstildetheta\in\R^P$. We will
naturally choose our estimator to be the parameter vector that sets this
sample analog equal to zero or ``close'' to zero.

\begin{defn}(Method of Moments Estimator)
When the dimensionality of $\bstheta$ equals that of $\bsg(\bstheta)$
(when there are as many parameters as moment conditions $P=M$), we can
always set the sample analog of the moment conditions equal to zero
exactly. Therefore, we define the \emph{method of moments estimator} as
the solution $\bshattheta$ to the following system of
equations:
\begin{align*}
  \bsbarg_N(\bshattheta) &= \bso
  \qquad\text{when}\quad
  \bsbarg_N,\bshattheta \in\R^P
\end{align*}
\end{defn}


\begin{defn}(GMM Estimator)
\label{defn:gmm}
When there are more moment conditions than parameters, i.e.\ when $M>P$
so that the model is \emph{overidentified}, we cannot generally set the
sample analog of the moment conditions $\bsbarg_N(\bstildetheta)$ to zero
\emph{exactly} using some $P$-dimensional $\bstildetheta$.
Therefore, we instead look for the parameter vector that gets the
squared-weighted elements of vector $\bsbarg_N(\bstildetheta)$ jointly as
\emph{close} to the scalar zero as possible:
\begin{align}
  \bshattheta(\bshatW) :=&\;
  \argmin_{\bstildetheta} \; J(\bstildetheta,\bshatW)
  \label{eq:bhatmin} \\
  \text{where}\quad
  J(\bstildetheta,\bshatW)
  :=&\;
  N\cdot \bsbarg_N(\bstildetheta)' \, \bshatW \, \bsbarg_N(\bstildetheta)
  \notag
\end{align}
where $\bshattheta(\bshatW)$ is called
the \emph{Generalized Method of Moments} (GMM) estimator using
symmetric, positive definite weighting matrix $\bshatW$ such that
$\bshatW\pto \bsW$ as $N\ra\infty$.

We index $\bshattheta$ by $\bshatW$ because the estimate depends
crucially on the choice of weighting matrix, which can possibly weight
the different moment conditions asymmetrically, emphasizing some over
others.
We'll talk about choices of the weighting matrix
later on. For now though, you might reasonably just
choose $\bshatW=\boldsymbol{I}$, treating all moments
equally.
But later on, we'll see that most estimators that we care about can be
expressed as a GMM estimator given an appropriate choice of moment
conditions and weighting matrix.
\end{defn}


\begin{rmk}
We typically use the ``generalized'' quantifier to refer to methods
where there are more conditions than parameters and so we have to ``do
something close'' rather than exactly.
\end{rmk}


\begin{defn}(GMM via Moment Selection)
Another way to think about GMM is to deal with overidentification by
employing a selection matrix $\bshata\in \R^{P\times M}$ to
select/emphasize important moments or linear combinations of moments.
In particular, rather than solve minimization Problem~\ref{eq:bhatmin},
the GMM estimator $\bshattheta(\bshata)$ solves this modified, but
exactly-identified variation on the original moment conditions:
\begin{align}
  \bshata \; \bsbarg_N\big(\bshattheta(\bshata)\big) &= \bso
  \qquad\text{where}\quad
  \bshattheta(\bshata) \in\R^P\quad \bshata \in \R^{P\times M}
  \label{eq:bhatmina}
\end{align}
We see that $\bshata\,\bsbarg_N(\bstildetheta)$ is like a new set of moment
conditions, but with the dimensionality reduced from $M>P$ to $P$ so
that we \emph{can} set this to zero for some
$\bshattheta(\bshata)\in\R^P$. And we again use notation
$\bshattheta(\bshata)$ since the estimate will depend crucially upon our
choice of $\bshata$.

This selection matrix can depend upon the data---hence the hat in
$\bshata$. It could also, for example, be mostly ones and zeros to pick
out the moments in $\bsbarg_N(\bstildetheta)$ (or form linear combinations
of moments) that we think are most important and definitely want to
force to zero. But in general, it really can be any arbitrary matrix in
$\R^{P\times M}$.

Lastly, just to tie things together, we can make this
approach/definition equaivalent to Definition~\ref{defn:gmm} by choosing
$\bshata$ in a particular way. Specifically, consider the FOCs to the
minimization problem defined by Equation~\ref{eq:bhatmin}:
\begin{align}
  \bso
  =
  \underbrace{\frac{\partial \bsbarg_N'}{\partial \bstildetheta} \bshatW}_{\bshata}
  \;\bsbarg_N(\bstildetheta)
  \label{eq:equivrep}
\end{align}
If, we choose
$\bshata=\frac{\partial \bsbarg_N'}{\partial \bstildetheta} \bshatW$,
then $\bshattheta(\bshatW)=\bshattheta(\bshata)$.
\end{defn}


\clearpage
\subsubsection{Asymptotic Distribution of GMM Estimator}

\begin{prop}
The GMM estimator $\bshattheta(\bshatW)$ that solves
\begin{align*}
  \bshattheta(\bshatW)
  &=
  \argmin_{\bstildetheta}\;
  N \cdot \bsbarg_N(\bstildetheta)' \,\bshatW \,\bsbarg_N(\bstildetheta)
  \qquad
  \text{where}\quad
  \bsbarg_N(\bstildetheta)
  := \frac{1}{N}\sumnN
  \underbrace{\bsg_n(\bstildetheta)}_{\bsf_n(\bsy_n,\bstildetheta)}
\end{align*}
for $\bshatW\pto\bsW$ (a positive definite matrix)
has asymptotic distribution
\begin{align*}
  \sqrt{N}\big(\bshattheta(\bshatW)-\bstheta\big)
  \quad&\dto\quad
  \big( \bsG' \bsW \bsG \big)^{-1}
  \bsG \bsW
  \cdot
  \calN\big(\bso,\bsOmega\big)
  \\
  \text{where}\quad
  \bsOmega &:= \Var[\bsg_n(\bstheta)]
  = \E[\bsg_n(\bstheta)\bsg_n(\bstheta)']
  \\
  \bsG &:= \E\left[
    \frac{\partial \bsg_n(\bstheta)}{\partial \bstildetheta'}
  \right]
\end{align*}
To be clear, $\bsG$ is the expected Jacobian matrix of
$\bsg_n(\bstildetheta)$ evaluated at true $\bstheta$.
\end{prop}
\begin{proof}
For notational convenience, we will simply write $\bshattheta$, rather
that $\bshattheta(\bshatW)$ throughout this proof. Though it should be
understood that the estimator depends upon the choice of weighting
matrix $\bshatW$.
We also define the Jacobian matrix of $\bsbarg_N$, evaluated at arbitrary
$\bsbartheta$:
\begin{align*}
  \bsG_N(\bsbartheta)
  =
  \frac{\partial \bsbarg_N(\bsbartheta)}{\partial \bstildetheta'}
  =
  \frac{1}{N}
  \sumnN
  \frac{\partial \bsg_n(\bsbartheta)}{\partial \bstildetheta'}
  \in \R^{M\times P}
\end{align*}
Now begin by noting that the GMM estimator $\bshattheta$ must satisfy
the first order necessary conditions of its defining maximization
problem, i.e.\ we must have:
\begin{align*}
  \bso &=
  \frac{\partial \bsbarg_N(\bshattheta)'}{\partial \bstildetheta}
  \, \bshatW \, \bsbarg_N(\bshattheta)
  =
  \bsG_N(\bshattheta)'
  \, \bshatW \, \bsbarg_N(\bshattheta)
\end{align*}
This is $P$ equations in $P$ unknowns (the elements of $\bshattheta$).
To derive the asymptotic distribution, do a first order Taylor expansion
of $\bsbarg_N(\bshattheta)$ for each equation about the true value
$\bstheta$:
\begin{align}
  \bso &=
  \bsG_N(\bshattheta)'
  \, \bshatW \,
  \big(
  \bsbarg_N(\bstheta)
  +
  \bsG_N(\bsbartheta)
  (\bshattheta-\bstheta)
  \big)
  \notag
  \\
  \iff\quad
  \sqrt{N}(\bshattheta-\bstheta)
  &=
  -
  \big(
  \bsG_N(\bshattheta)'
  \, \bshatW \,
  \bsG_N(\bsbartheta)
  \big)^{-1}
  \big(
  \bsG_N(\bshattheta)'
  \, \bshatW \,
  \big)
  \big(
  \sqrt{N}
  \bsbarg_N(\bstheta)
  \big)
  \label{gmm:proof}
\end{align}
for some $\bsbartheta$ in between true $\bstheta$ and estimate
$\bshattheta$. Note that this is an abuse of notation since there's
actually a separate $\bsbartheta$ for each of the $P$ equations.
However, it won't matter asymptotically---they'll all get arbitrarily
close together to each other and to $\bstheta$ as we'll see---so we'll
ignore that caveat.

We now consider the asymptotic properties of the elements on the RHS of
Equation~\ref{gmm:proof}. First, by the CLT
\begin{align*}
  \sqrt{N}
  \bsbarg_N(\bstheta)
  =
  \frac{1}{\sqrt{N}}
  \sumnN
  \bsg_n(\bstheta)
  \quad\dto\quad
  \calN\big(\bso,\Var[\bsg_n(\bstheta)]\big)
\end{align*}
Next, consistent $\bshattheta$ means that $\bshattheta\pto\bstheta$. And
with $\bsbartheta$ sandwiched in between $\bshattheta$ and $\bstheta$,
we also get $\bsbartheta\pto\bstheta$. Therefore,
\begin{align*}
  \bsG_N(\bshattheta)
  \pto \bsG
  \qquad
  \bsG_N(\bsbartheta)
  \pto \bsG
\end{align*}
Finally, we've already assumed that $\bshatW$ was chosen such that
$\bshatW\pto \bsW$. Putting all of this together, we can apply these
results to the terms in Expression~\ref{gmm:proof} individually,
\begin{align*}
  \sqrt{N}(\bshattheta-\bstheta)
  &=
  -
  \big(
  \underbrace{%
    \bsG_N(\bshattheta)'
  }_{%
    \pto \bsG
  }
  \quad
  \underbrace{%
    \bshatW
    \vphantom{\bsG(\bshattheta)'}
  }_{%
    \pto\bsW
  }
  \quad
  \underbrace{%
    \bsG_N(\bsbartheta)
  }_{%
    \pto \bsG
    \vphantom{\bsG(\bshattheta)'}
  }
  \quad
  \big)^{-1}
  \underbrace{%
    \bsG_N(\bshattheta)'
  }_{%
    \pto \bsG
    \vphantom{\bsG(\bshattheta)'}
  }
  \quad
  \underbrace{%
    \bshatW
    \vphantom{\bsG(\bshattheta)'}
  }_{%
    \pto\bsW
  }
  \quad
  \underbrace{%
  \sqrt{N}
  \bsbarg_N(\bstheta)
  }_{\dto\calN\big(\bso,\bsOmega\big)}
\end{align*}
From there, we can invoke the continuous mapping theorem to string
everything together and arrive at the asymptotic distribution:
\begin{align*}
  \sqrt{N}(\bshattheta-\bstheta)
  \quad\dto\quad
  \big( \bsG' \bsW \bsG\big)^{-1}
  \bsG \bsW
  \cdot
  \calN\big(\bso,\bsOmega\big)
\end{align*}
\end{proof}


\clearpage
\subsubsection{Efficient GMM}

GMM is always consistent, but can we do better?
Often our goal will be to obtain not just consistent estimators, but
also \emph{efficient} GMM estimators, i.e.\ estimators $\bshattheta$
with the lowest possible asymptotic variance.

But \textbf{note}: This section is only relevant when $M>P$, i.e.\ there
are more moment conditions than parameters. When $M=P$, we can set
$\bsbarg_N(\bshattheta)=\bso$ exactly to get the method of moments
estimator. This estimator is efficient \emph{always} and has
asymptotic variance $(\bsG'\bsOmega^{-1}\bsG)^{-1}$ regardless of
the weighting matrix (which is moot anyway).

\begin{prop}\emph{(Efficient GMM under Optimal Weighting Matrix)}
The choice of weighting matrix $\bsW$ that delivers the lowest asymptotic
variance is $\bsW=\bsOmega^{-1}=\Var[\bsg_n(\bstheta)]^{-1}$, in which
case the asymptotic variance formulas simplify to
\begin{align*}
  \sqrt{N}(\bshattheta-\bstheta)
  \quad\dto\quad
  \calN\big(\bso,(\bsG'\bsOmega^{-1}\bsG)^{-1}\big)
\end{align*}
\end{prop}

\begin{rmk}
Here's the intuition for why
$\bsW=\bsOmega^{-1}=\Var[\bsg_n(\bstheta)]^{-1}$ is the efficient
choice:
We're weighting each moment condition by its inverse variance.
Therefore, when computing $\bshattheta(\bshatW)$, we overweight moment
conditions with low variance, i.e.\ those moment conditions we have the
best chance of estimating precisely.  We underweight the high variance
moment conditions, which would likely just contribute noise to our
choice of $\bshattheta$.
\end{rmk}


\begin{defn}(Two-Step Efficient GMM Estimators)
In practice, we often cannot obtain the efficient estimate right off the
bat.
The first problem is that we typically don't know the population
quantity $\bsOmega=\Var[\bsg_n(\bstheta)]$. Therefore, we
can't just set our weighting matrix to be $\bsW=\bsOmega^{-1}$.
Of course, we can do the next-best thing: use a consistent estimator
$\bshatOmega^{-1}\pto\bsOmega^{-1}$ as our weighting matrix instead. But
that often brings us to a second problem: constructing a consistent
estimator for $\bsOmega$ typically requires the true $\bstheta$, which
we don't know. Again, we can get around that by using a consistent
estimator for $\bstheta$.

All of this suggests the following procedure for obtaining the
so-called \emph{Two-Step Efficient GMM Estimator}:
\begin{enumerate}
  \item Choose some arbitrary $\bshatW_1$ ($\bsI$ often works) and solve
    for a preliminary ``inefficient'' (but still consistent) estimator
    $\bshattheta(\bshatW_1)$.
  \item Use $\bshattheta(\bshatW_1)$ to compute a second-stage weighting
    matrix $\bshatW_2$ that is a consistent estimator for
    $\bsOmega^{-1}=\Var[\bsg_n(\bstheta)]^{-1}$. Then use this to solve
    for the two-step efficient GMM estimator $\bshattheta(\bshatW_2)$.
\end{enumerate}
Note that this is not quite ``efficient''---it's only ``two-step
efficient''---because our final weighting matrix $\bshatW_2$ is not
true $\bsOmega^{-1}$, but rather a consistent estimator for that.
Moreover, our consistent estimator $\bshatW_2$ depends on our initial
choice of $\bshatW_1$, which was ad-hoc. Therefore, the ``quality'' of
$\bshatW_2\pto\bsOmega^{-1}$ (i.e. whether it really is a good
approximation of $\bsOmega^{-1}$) will depend on our initial ad-hoc
choice of $\bshatW_1$. The next section offers an alternative that
remedies this slightly undesireable feature.
\end{defn}
\begin{rmk}
The above approach was described for the $\bshatW$ weighting-matrix
approach of Expression~\ref{eq:bhatmin}. But it also applies analogously
to the moment selection approach with $\bshata$.
\end{rmk}


\clearpage
\subsubsection{Continuously Updated GMM Estimator (CUE)}

We can think of iterating on the two-step procedure of the last section
to obtain higher-quality consistent estimators of the efficient
weighting matrix $\bsOmega^{-1}=\Var[\bsg_n(\bstheta)]^{-1}$. In
particular, we might guess a weighting matrix $\bshatW_i$, solve for
$\bshattheta(\bshatW_i)$, use it to compute/update
$\bshatW_{i+1}\pto\bsOmega^{-1}$, and repeat until $\bshatW_i$ converges
to some really good approximation of efficient $\bsOmega^{-1}$. This
next GMM estimator effectively formalizes that and does it all in one
step (without iterating).

\begin{defn}(Continuously Updating GMM Estimator)
We define the \emph{Continuously Updating GMM Estimator} (CUE)
as
\begin{align}
  \bshattheta_{CUE}
  &=
  \argmin_{\bstildetheta}\;
  \bsbarg_N(\bstildetheta)'
  \left(
  \frac{1}{N}
  \sumnN
  \bsg_n(\bstildetheta)
  \bsg_n(\bstildetheta)'
  \right)^{-1}
  \bsbarg_N(\bstildetheta)'
  \label{gmm:cue}
  \\
  \text{where}\quad
  \bsbarg_N(\bstildetheta)
  &:= \frac{1}{N}\sumnN \bsg_n(\bstildetheta)
  \notag
\end{align}
Notice that, by the LLN, the weighting matrix that we're using converges
to $\E[\bsg_n(\bstheta)\bsg_n(\bstheta)']^{-1}$, which precisely equals
$\bsOmega^{-1}=\Var[\bsg_n(\bstheta)]^{-1}$ since,
by assumption, $\E[\bsg_n(\bstheta)]=\bso$.
Advantages of this approach include
\begin{enumerate}[label=(\roman*)]
  \item Using a weighting matrix that is consistent for the efficient
    weighting matrix $\bsOmega^{-1}$, but \emph{without} using an ad-hoc
    first stage weighting matrix
  \item Invariant to rotation of $\bsg_n(\bstildetheta)$.
\end{enumerate}
\end{defn}

\begin{defn}(CUE Alternative)
We can also use a different estimate of the efficient weighting matrix
to define a CUE alternative that might be more appropriate in finite
samples:
\begin{align}
  \bshattheta
  &=
  \argmin_{\bstildetheta}\;
  \bsbarg_N(\bstildetheta)'
  \,
  \bsW(\bstildetheta)
  \,
  \bsbarg_N(\bstildetheta)'
  \label{gmm:cue2}
  \\
  \text{where}\quad
  \bsW(\bstildetheta)
  &=
  \left(
  \frac{1}{N}
  \sumnN
  \big(
  \bsg_n(\bstildetheta)
  -
  \bsbarg_N(\bstildetheta)
  \big)
  \big(
  \bsg_n(\bstildetheta)
  -
  \bsbarg_N(\bstildetheta)
  \big)'
  \right)^{-1}
  \notag
\end{align}
Why is this useful?
As we said before, we can't generally get $\bsbarg_N(\bshattheta)=\bso$
when $M>P$, i.e. when there are more moment conditions that parameters.
Therefore, $\bsbarg_N(\bstildetheta)\neq\bso$ generally and the weighting
matrix in Expression~\ref{gmm:cue} is the inverse sample \emph{second
moment}, not the inverse sample \emph{variance}.
By de-meaning, Expression~\ref{gmm:cue2} uses the inverse sample
variance as the weighting matrix instead.
\end{defn}

\clearpage
\subsubsection{Example: OLS as a GMM Estimator}

As a first example, take simple OLS, which can be written as a GMM
estimator. In particular, take the model
\begin{align*}
  y_n &= \bsx_n' \bsbeta + \varepsilon_n \\
  \text{where} \quad
  \E[\varepsilon_n\bsx_n] &=
  \E[(y_n-\bsx_n'\bsbeta)\bsx_n] = \bso
\end{align*}
The orthogonality condition $\bsg(\bsbeta)=\E[\varepsilon_n\bsx_n']=\bso$
will be the moment condition to use. Therefore, we form $\bsbarg_N$, the
sample analog of $\bsg$,
\begin{align*}
  \bsbarg_N(\bsb) &= \frac{1}{N}\sumnN (y_n-\bsx_n'\bsb)\bsx_n
  = \frac{1}{N}\bsX'(\bsy - \bsX\bsb)
\end{align*}
and estimate the parameters using identity weighting matrix $\bsW =
\bsI_K$ (ignoring constants like $N$ or $\frac{1}{N}$). Note that the
weighing matrix won't matter anyway since there are as many parameters
as moment conditions.
\begin{align*}
  \bshatbeta(\bsI_K)
  &= \argmin_{\bsb} \; \bsbarg_N(\bsb)' \, \bsI_K \, \bsbarg_N(\bsb) \\
  &= \argmin_{\bsb} \;
    (\bsy - \bsX\bsb)'\bsX \bsI_K \bsX'(\bsy - \bsX\bsb)
\end{align*}
Ignoring terms constant terms that are not a function of $\bsb$, we can
simplify the minimization problem to
\begin{align*}
  \bshatbeta(\bsI_K)
  &= \argmin_{\bsb} \;
    -\bsb'\bsX'\bsX \bsX'\bsy
    - \bsy'\bsX \bsX'\bsX\bsb + \bsb'\bsX'\bsX \bsX'\bsX\bsb
\end{align*}
The first order conditions of the minimization problem are
\begin{align*}
  0 &=
  -2\bsX'\bsX \bsX'\bsy
  + 2\bsX'\bsX \bsX'\bsX\bsb \\
  \implies\quad
  \bsb
  &= (\bsX'\bsX)^{-1} \bsX'\bsy
\end{align*}
which is exactly the OLS estimator formula.


\clearpage
\subsection{Estimating Spectral Densities and Long-Run Variance}

\subsubsection{Sample Periodogram}

In this section, given observations $\{y_t\}\tT$, we construct an
estimate, called the \emph{sample periodogram}, for the spectral density
$f(\lambda)$ (a population concept).
But first, recall that the spectral representation theorem lets us
express any covariance-stationary process $\{y_t\}$ as the ``sum'' of
oscillations at different frequencies, as follows
\begin{align*}
  y_t =&\; \int_{-\pi}^\pi \delta(\lambda)e^{i\lambda t}\;d\lambda
\end{align*}
First, we are integrating over $\lambda\in(-\pi,\pi]$---the set of all
frequencies that can be identified (i.e.\ excluding unidentifiable
multiples). Next, for a fixed $t$, $e^{i\lambda t}$ represents
oscillations at frequency $\lambda$, and as we integrate over $\lambda$,
we are adding up oscillations at \emph{all} frequencies in $(-\pi,\pi]$.
Next, $\delta(\lambda)$ (which is unique to $\{y_t\}$) gives the weight
to place on oscillations at frequency $\lambda$.  Finally, recall that
there was a one-to-one mapping between $\delta(\lambda)$ and the
spectral density, $f(\lambda)$.  Namely,
$\E[\delta(\lambda)\overline{\delta(\lambda)}] = f(\lambda)$.
Therefore, if we can estimate $\delta(\lambda)$, we can presumably use
that estimate to construct an estimate of $f(\lambda)$, as we will see.

Okay, we now want a sample counterpart to all of this. To do so, we
appeal to the discrete Fourier transform, writing a discrete version of
the above spectral representation:
\begin{align}
  y_t =
  %\frac{1}{\sqrt{T}}
  \sum_{\lambda_\ell\in (-\pi,\pi]} \delta_\ell \; e^{i\lambda_\ell t}
  \label{discretespectralrep}
\end{align}
Instead of integrating over all $\lambda\in(-\pi,\pi]$, we add over a
finite set of \emph{Fourier frequencies} (denoted by $\lambda_\ell$'s)
within that range. And instead of the function $\delta(\lambda)$, we now
have weights $\delta_\ell$, each associated with a corresponding
frequency $\lambda_\ell$. Now for the details.

\begin{defn}(Fourier Frequencies)
Depending upon whether $T$ is odd or even, define Fourier integers
\begin{align*}
  F_T^{odd}
  &= \left\{-\frac{T-1}{2},\ldots,\frac{T-1}{2}\right\}
  \subseteq\Z
  \qquad
  F_T^{even}
  = \left\{-\frac{T-2}{2},\ldots,\frac{T}{2}\right\}
  \subseteq\Z
\end{align*}
Letting $F_T$ denote the appropriate choice of integers (depending on
whether $T$ is odd or even), the \emph{Fourier Frequencies} are given by
\begin{align*}
  \lambda_\ell = (2\pi\ell)/T
  \qquad \ell \in F_T
\end{align*}
The choice of Fourier integers and specification of the frequencies
ensures that, for all $T$, we have $\lambda_\ell\in(-\pi,\pi]$.
These are the frequencies at which we will estimate the weights and the
spectral density.
By construction, there are always $T$ such frequencies.
\end{defn}

\begin{ex}
Just to give an example of the Fourier integers to make thing concrete:
\begin{align*}
  F_{51} = \{-25,-24,\ldots,24,25\}
  \qquad
  F_{50} = \{-24,-23,\ldots,24,25\}
\end{align*}
\end{ex}

\begin{defn}(Some Orthonormal Bases for $\C^T$ and $\R^T$)
\label{defn:periobasis}
Stack the data $\{y_t\}\tT$ into a column vector and denote this vector
as $\bsy_{1:T}=(y_1,\ldots,y_T)'\in\R^T\subset\C^T$.  We will construct
two orthonormal bases: one relevant for when we think of
$\bsy_{1:T}\in\C^T$, and one for when we think of $\bsy_{1:T}\in\R^T$.
Even though all of the observations are real-valued, it will still be
useful to work with $\C^T$ in this section, which is why we go to the
trouble.
\begin{itemize}
  \item $\C^T$:
    For the space $\C^T$, we define the following collection of vectors:
    \begin{align*}
      \bse_\ell = \frac{1}{\sqrt{T}}
      \begin{pmatrix}
        e^{i\lambda_\ell} &
        e^{i2\lambda_\ell} &
        e^{i3\lambda_\ell} &
        \cdots &
        e^{iT\lambda_\ell}
      \end{pmatrix}'
      \qquad \ell \in F_T
    \end{align*}
    This collection of vectors $\{\bse_\ell\}_{\ell\in F_T}$ forms an
    orthonormal basis for $\C^T$ (recall $F_T$ has exactly $T$ elements,
    by construction). There is one basis vector $\bse_\ell$ for each
    Fourier frequency $\lambda_\ell$.

  \item $\R^T$:
    Next (and we'll see where these definition come from later), define
    \begin{align*}
      \bsu_\ell^{\cos} &=
      \sqrt{\frac{2}{T}}
      \begin{pmatrix}
        \cos(\lambda_\ell) &
        \cos(2\lambda_\ell) &
        \cdots &
        \cos(T\lambda_\ell)
      \end{pmatrix}'
      \qquad \ell = 1,\ldots,\left\lfloor\frac{T-1}{2}\right\rfloor
      \\
      \bsu_\ell^{\sin} &=
      \sqrt{\frac{2}{T}}
      \begin{pmatrix}
        \sin(\lambda_\ell) &
        \sin(2\lambda_\ell) &
        \cdots &
        \sin(T\lambda_\ell)
      \end{pmatrix}'
      %\qquad \ell = 1,\ldots,\left\lfloor\frac{T-1}{2}\right\rfloor
    \end{align*}
    Using $e^{ix}=\cos x + i\sin x$, this lets us write for all
    $\ell\in\{1,\ldots,\lfloor (T-1)/2\rfloor\}\subset\Z$
    \begin{align*}
      \bse_\ell
      &= \frac{1}{\sqrt{T}}
      \begin{pmatrix}
        e^{i\lambda_\ell} \\
        e^{i2\lambda_\ell}\\
        \cdots \\
        e^{iT\lambda_\ell}
      \end{pmatrix}
      = \frac{1}{\sqrt{T}}
      \begin{pmatrix}
        \cos(\lambda_\ell) + i \sin(\lambda_\ell)\\
        \cos(2\lambda_\ell) + i \sin(2\lambda_\ell)\\
        \cdots \\
        \cos(T\lambda_\ell) + i \sin(T\lambda_\ell)
      \end{pmatrix}
      = \frac{%
      \left(
      \bsu_\ell^{\cos} + i\bsu_\ell^{\sin}
      \right)
      }{\sqrt{2}}
    \end{align*}
    Note that since $\cos$ is an even and $\sin$ an odd function and
    since $\lambda_\ell=-\lambda_{-\ell}$, we also have that
    \begin{align*}
      \bsu^{\cos}_\ell = \bsu^{\cos}_{-\ell}
      \qquad
      \bsu^{\sin}_\ell = -\bsu^{\sin}_{-\ell}
      \quad\implies\quad
      \bse_{-\ell} =
      \frac{(\bsu_\ell^{\cos} - i\bsu_\ell^{\sin})}{\sqrt{2}}
    \end{align*}
    We will take as our orthonormal basis for $\R^T$
    \begin{align*}
      \left\{\bse_0,\bsu^{\cos}_1,\bsu^{\sin}_1,\ldots,
        \bsu^{\cos}_{\lfloor (T-1)/2\rfloor},
        \bsu^{\sin}_{\lfloor (T-1)/2\rfloor},
        \bse_{T/2}\right\}
    \end{align*}
    with the last vector excluded if $T$ is odd.
    Note that this also has exactly $T$ elements by construction, as we
    need to have a proper basis for $\R^T$.
    Oh, and don't worry about the fact that $\bse_{T/2}$ is in $\C^T$,
    not $\R^T$.
\end{itemize}
\end{defn}


%\begin{proof}
%We now prove $\{\bse_\ell\}_{\ell\in F_T}$ is, indeed, an orthonormal
%basis. So compute inner product
%\begin{align*}
  %\langle \bse_\ell, \bse_k\rangle
  %&=
  %\frac{1}{T}
  %\sumtT
  %e^{i\lambda_\ell t}
  %\cdot
  %\overline{ e^{i\lambda_k t}}
  %=
  %\frac{1}{T}
  %\sumtT
  %e^{i(\lambda_\ell -\lambda_k) t}
%\end{align*}
%First, if $k=\ell$, we do indeed have that the inner product equals one: \begin{align*}
  %\langle \bse_\ell, \bse_\ell\rangle
  %=
  %\frac{1}{T}
  %\sumtT
  %e^{i(\lambda_\ell -\lambda_\ell) t}
  %= 1
%\end{align*}
%For $k\neq \ell$, write out the $\lambda_\ell$ terms and exponential
%more fully:
%\begin{align*}
  %\langle \bse_\ell, \bse_k\rangle
  %=
  %\frac{1}{T}
  %\sumtT
  %e^{i(\lambda_\ell -\lambda_k) t}
  %&=
  %\frac{1}{T}
  %\sumtT
  %\cos\left(
  %(\lambda_\ell -\lambda_k) t
  %\right)
  %+i
  %\sin\left(
  %(\lambda_\ell -\lambda_k) t
  %\right)
  %\\
  %&=
  %\frac{1}{T}
  %\sumtT
  %\cos\left(
  %\frac{2\pi}{T}(\ell -k) t
  %\right)
  %+i
  %\sin\left(
  %\frac{2\pi}{T}(\ell -k) t
  %\right)
%\end{align*}
%\end{proof}

\clearpage
\begin{defn}(Discrete Fourier Transform, $\bsy_{1:T}\in\C^T$)
Since $\{\bse_\ell\}_{\ell\in F_T}$ forms a orthonormal basis for
$\C^T$, we know from Fourier analysis that we can express the vector
$\bsy_{1:T}\in\C^T$ in terms of this basis by doing a discrete Fourier
transform:
\begin{align*}
  \bsy_{1:T} &= \sum_{\ell\in F_T} \delta_\ell \bse_\ell
  \qquad
  \text{where}\quad
  \delta_\ell
  = \langle \bsy_{1:T},\bse_\ell \rangle
  = \frac{1}{\sqrt{T}}\sum_{t=1}^T y_t e^{-i\lambda_\ell t}
\end{align*}
If we look at element $t$ of $\bsy_{1:T}$, i.e. at observation $y_t$,
the above says that
$y_t = \sum_{\ell\in F_T} \delta_\ell e^{i\lambda_\ell t}$.
This is essentially Representation~\ref{discretespectralrep} above
(rewritten slightly but equivalent), which was our goal from the
start.
\end{defn}

\begin{defn}(Discrete Fourier Transform, $\bsy_{1:T}\in\R^T$)
\label{defn:realdft}
The previous DFT imagined $\bsy_{1:T}\in\C^T$.
But really $\bsy_{1:T}\in\R^T$, which allows us to rewrite the DFT into
an equivalent representation that deals with only real numbers (almost).
In particular, if we use orthonormal basis for $\R^T$ from
Definition~\ref{defn:periobasis}, we can write
\begin{align*}
  \bsy_{1:T}
  &=
  \delta_0 \bse_0
  +
  \sum_{\ell=1}^{\left\lfloor \frac{T-1}{2} \right\rfloor}
  \big[
  z_\ell^{\cos}
  \bsu_\ell^{\cos}
  -
  z_\ell^{\sin}
  \bsu_\ell^{\sin}
  \big]
  + \delta_{\frac{T}{2}}\bse_{\frac{T}{2}}
    \cdot\mathbf{1}\{\text{$T$ Even}\}
\end{align*}
where the coefficients are given for
$\ell\in\{1,\ldots,\lfloor (T-1)/2\rfloor\}\subset\Z$
are given by projecting the data $\bsy_{1:T}$ onto the basis elements:
\begin{alignat*}{3}
  z_{\ell}^{\cos}
  &= \langle \bsy_{1:T}, \bsu_{\ell}^{\cos}\rangle
  = \sqrt{\frac{2}{T}}\sum_{t=1}^T y_t\cos(t\lambda_\ell)
  \qquad\quad
  &z_{\ell}^{\sin}
  &= \langle \bsy_{1:T}, \bsu_{\ell}^{\sin}\rangle
  = \sqrt{\frac{2}{T}}\sum_{t=1}^T y_t\sin(t\lambda_\ell)
  \\
  \delta_0
  &= \langle \bsy_{1:T}, \bse_0\rangle
  = \frac{1}{\sqrt{T}} \sumtT y_t
  \qquad
  &\delta_{T/2}
  &= \langle \bsy_{1:T}, \bse_{T/2}\rangle
  %= \frac{1}{\sqrt{T}}\sum_{t=1}^T
    %y_t e^{-it\lambda_{T/2}}
  = \frac{1}{\sqrt{T}}\sum_{t=1}^T
    y_t e^{-it\pi}
\end{alignat*}
\end{defn}

\begin{defn}(Periodogram)
Given the DFT coefficients $\{\delta_\ell\}_{\ell\in F_T}$, we can
construct the periodogram $I(\lambda_\ell)$ which is defined at each
Fourier frequency $\lambda_\ell$ as
\begin{align*}
  I(\lambda_\ell)
  := |\delta_\ell|^2
  = \delta_\ell \overline{\delta}_\ell
\end{align*}
If we used the alternate DFT in $\R^T$, we can instead compute
\begin{align*}
  I(\lambda_\ell) =
  \frac{\left(z_\ell^{\cos}\right)^2+\left(z_\ell^{\sin}\right)^2}{2}
\end{align*}
\end{defn}

\begin{rmk}
We can usefully think of the periodogram as an ``analysis of variance''
that decomposes the sample second moment
$\lVert \bsy_{1:T}\rVert^2
=\langle \bsy_{1:T},\bsy_{1:T}\rangle
=\sum_{t=1}^T y_t^2$ into periodogram pieces:
\begin{align*}
  \lVert\bsy_{1:T}\rVert^2
  &= \langle \bsy_{1:T},\bsy_{1:T}\rangle
  \underset{(i)}{=}
  \left\langle \sum_{\ell \in F_T}\delta_\ell \bse_\ell,\bsy_{1:T}\right\rangle
  \underset{(ii)}{=}
  \sum_{\ell \in F_T}\delta_\ell \left\langle \bse_\ell,\bsy_{1:T}\right\rangle
  \underset{(iii)}{=}
  \sum_{\ell \in F_T}\delta_\ell \overline{\delta}_\ell
  \\
  &\underset{(iv)}{=}
  \sum_{\ell \in F_T}I(\lambda_\ell)
\end{align*}
Equality (i) followed by substituting in the DFT representation of
$\bsy_{1:T}$ described above.  Equality (ii) used linearity of inner
products. Equality (iii) followed from the definition of $\delta_\ell$
and the complex inner product. Equality (iv) was simply the definition
of $I(\lambda_\ell)=\delta_\ell\overline{\delta}_\ell$.
\end{rmk}


\begin{proof}
Now we just need to justify all of this shit.
The DFT in $\C^T$ was clear. That's just basic Fourier analysis. We now
derive the DFT in $\R^T$ from the one in $\C^T$.
So first, start by regrouping terms in the sum from the original DFT in
$\C^T$:
\begin{align*}
  \bsy_{1:T}
  = \sum_{\ell\in F_T} \delta_\ell \bse_\ell
  &=
  \delta_0 \bse_0
  +
  \sum_{\ell=1}^{\left\lfloor (T-1)/2 \right\rfloor}
  \left(
  \delta_\ell \bse_\ell
  +
  \delta_{-\ell} \bse_{-\ell}
  \right)
  + \delta_{\frac{T}{2}}\bse_{\frac{T}{2}}
    \cdot\mathbf{1}\{\text{$T$ Even}\}
\end{align*}
Next, since since $\bsy_{1:T}\in \R^T$, we know that
\begin{align*}
  \delta_\ell =
  \langle \bsy_{1:T}, \bse_\ell \rangle
  = \frac{1}{\sqrt{T}} \sumtT y_t e^{-i\lambda_\ell t}
  = \overline{%
      \left(\frac{1}{\sqrt{T}} \sumtT y_t e^{i\lambda_\ell t}\right)
    }
  = \overline{\langle \bsy_{1:T}, \bse_{-\ell} \rangle}
  = \overline{\delta}_{-\ell}
\end{align*}
which also implies that also implies
$\delta_{-\ell}=\overline{\delta}_\ell$, so we use this to rewrite the
sum terms
\begin{align}
  \bsy_{1:T}
  &=
  \delta_0 \bse_0
  +
  \sum_{\ell=1}^{\left\lfloor (T-1)/2 \right\rfloor}
  \left(
  \delta_\ell \bse_\ell
  +
  \overline{\delta}_{\ell} \bse_{-\ell}
  \right)
  + \delta_{\frac{T}{2}}\bse_{\frac{T}{2}}
    \cdot\mathbf{1}\{\text{$T$ Even}\}
  \label{realdft}
\end{align}
Next, we want to simplify the terms of the sum further.
First, since $\delta_\ell$ is a complex number, we know it has polar
representation $\delta_\ell = r_\ell e^{i\theta_\ell}
= r_\ell(\cos\theta_\ell+i\sin \theta_\ell)$ for some $r_\ell$ and
$\theta_\ell$. We will use that. Next, recall from
Definition~\ref{defn:periobasis} that our orthonormal basis for $\R^T$
satisfies
$\bse_\ell = (\bsu_\ell^{\cos}+i\bsu_\ell^{\cos})/\sqrt{2}$
and
$\bse_{-\ell} = (\bsu_\ell^{\cos}-i\bsu_\ell^{\cos})/\sqrt{2}$.
So use both of these facts to simplify the expression for the terms in
the sum of Equation~\ref{realdft}:
\begin{align*}
  \delta_\ell \bse_\ell
  +
  \overline{\delta}_{\ell} \bse_{-\ell}
  &=
  \big(r_\ell(\cos\theta_\ell+i\sin \theta_\ell)\big)
  (\bsu_\ell^{\cos} + i \bsu_\ell^{\sin})/ \sqrt{2}
  +
  \big(r_\ell(\cos\theta_\ell-i\sin \theta_\ell)\big)
  (\bsu_\ell^{\cos} - i \bsu_\ell^{\sin})/ \sqrt{2}
  \\
  &=
  \frac{r_\ell}{\sqrt{2}}
  \big[
  \cos \theta_\ell
  \big(
  (\bsu_\ell^{\cos} + i \bsu_\ell^{\sin})
  + (\bsu_\ell^{\cos} - i \bsu_\ell^{\sin})
  \big)
  +
  i\sin \theta_\ell
  \big(
  (\bsu_\ell^{\cos} + i \bsu_\ell^{\sin})
  -
  (\bsu_\ell^{\cos} - i \bsu_\ell^{\sin})
  \big)
  \big]
  \\
  &=
  \sqrt{2}r_\ell
  \big[
  \cos \theta_\ell
  \bsu_\ell^{\cos}
  -
  \sin \theta_\ell
  \bsu_\ell^{\sin}
  \big]
\end{align*}
Therefore, we can rewrite Expression~\ref{realdft} as
\begin{align}
  \bsy_{1:T}
  &=
  \delta_0 \bse_0
  +
  \sum_{\ell=1}^{\left\lfloor (T-1)/2 \right\rfloor}
  \sqrt{2}r_\ell
  \big[
  \cos \theta_\ell
  \bsu_\ell^{\cos}
  -
  \sin \theta_\ell
  \bsu_\ell^{\sin}
  \big]
  + \delta_{\frac{T}{2}}\bse_{\frac{T}{2}}
    \cdot\mathbf{1}\{\text{$T$ Even}\}
    \notag
  %\\
  %&=
  %\delta_0 \bse_0
  %+
  %\sum_{\ell=1}^{\left\lfloor \frac{T-1}{2} \right\rfloor}
  %\big[
  %z_\ell^{\cos}
  %\bsu_\ell^{\cos}
  %-
  %z_\ell^{\sin}
  %\bsu_\ell^{\sin}
  %\big]
  %+ \delta_{\frac{T}{2}}\bse_{\frac{T}{2}}
    %\cdot\mathbf{1}\{\text{$T$ Even}\}
\end{align}
Lastly, we can get exactly what we had in Definition~\ref{defn:realdft}
if we define
\begin{align*}
  z_\ell^{\cos} = \sqrt{2}r_\ell\cos\theta_\ell
  \qquad\qquad
  z_\ell^{\sin} = \sqrt{2}r_\ell\sin\theta_\ell
\end{align*}
\end{proof}

\begin{proof}
Next, we show how to compute $I(\lambda_\ell)$ from $z_\ell^{\cos}$ and
$z_\ell^{\sin}$ values. Recall $\delta_\ell\in\C$ has polar
representation $\delta_\ell=r_\ell(\cos\theta_\ell+i\sin \theta_\ell)$.
Therefore, the definition of $I(\lambda_\ell)$ implies
\begin{align*}
  I(\lambda_\ell)
  &= |\delta_\ell|^2
  = |r_\ell (\cos\theta_\ell+i\sin\theta_\ell)|^2
  = r_\ell^2 (\cos^2\theta_\ell+\sin^2\theta_\ell)
  = r_\ell^2 \cdot 1
\end{align*}
But recall that
\begin{align*}
  \begin{rcases}
  z_\ell^{\cos} = \sqrt{2}r_\ell\cos\theta_\ell
  \\
  z_\ell^{\sin} = \sqrt{2}r_\ell\sin\theta_\ell
  \end{rcases}
  \implies\quad
  \frac{\left(z_\ell^{\cos}\right)^2 + \left(z_\ell^{\sin}\right)^2}{2}
  =
  \frac{%
    \left(\sqrt{2}r_\ell\cos\theta_\ell\right)^2
    +
    \left(\sqrt{2}r_\ell\sin\theta_\ell\right)^2
  }{2}
  = r_\ell^2
  = I(\lambda_\ell)
\end{align*}
\end{proof}


\clearpage
\subsubsection{Periodogram as Estimate of Spectral Density, Whittle
Likelihood}

\begin{prop}\emph{(Similar Relationship to Autocovariance Function)}
The periodogram for $\bsy_{1:T}$ satisfies
\begin{align*}
  I(\lambda_\ell)
  =
  \begin{cases}
  \sum_{|k|<T} \hat{\gamma}(k)e^{-ik\lambda_\ell}
  & \lambda_\ell \neq 0
  \\
  T \bar{y}^2 & \lambda_\ell=0
  \end{cases}
  \qquad
  \text{where}\quad
  \begin{cases}
  \hat{\gamma}(k)
  = \frac{1}{T}\sum_{t=1}^{T-k}
    (y_{t+k}-\bar{y})
    (y_{t}-\bar{y})
  \\
  \bar{y} = \frac{1}{T} \sumtT y_t
  \end{cases}
\end{align*}
\end{prop}
\begin{rmk}
This looks a lot like the definition of the spectral density function we
saw before,
$f(\lambda) =
\frac{1}{2\pi}\sum_{k=-\infty}^\infty \gamma(k)
e^{-ik\lambda}$
so it seems natural that they would be related.
\end{rmk}
\begin{proof}
Start with the definition of
$I(\lambda_\ell) = \delta_\ell \overline{\delta}_\ell =
\langle \bsy_{1:T}, \bse_\ell\rangle$
\begin{align*}
  I(\lambda_\ell)
  &=
  \left(
  \frac{1}{\sqrt{T}}
  \sumtT y_t e^{-it\lambda_\ell}
  \right)
  \overline{%
  \left(
  \frac{1}{\sqrt{T}}
  \sumtT y_t e^{-it\lambda_\ell}
  \right)
  }
  =
  \frac{1}{T}
  \left(
  \sumtT y_t e^{-it\lambda_\ell}
  \right)
  \left(
  \sumtT y_t e^{it\lambda_\ell}
  \right)
\end{align*}
For $\lambda_\ell=0$, obvious. To tackle $\lambda_\ell\neq0$, note that
for any $\lambda_\ell\neq 0$, we have
\begin{align}
  S =
  \sum_{t=1}^T \exp\left({it\lambda_\ell}\right)
  =
  \sum_{t=1}^T \exp\left({\frac{i2\pi \ell}{T}t}\right)
  = 0
  \label{orthzero}
\end{align}
Therefore, we can add zero within both parentheses by adding
$-m\sum_{t=1}^T e^{it\lambda_\ell}$---for some $\ell\neq0$---and
maintain equality (including $-\lambda_\ell=\lambda_{-\ell}$:
\begin{align*}
  I(\lambda_\ell)
  &=
  \frac{1}{T}
  \left(
  \sumtT y_t e^{-it\lambda_\ell}
  - m\sumtT e^{-it\lambda_\ell}
  \right)
  \left(
  \sumtT y_t e^{it\lambda_\ell}
  - m\sumtT e^{it\lambda_\ell}
  \right)
  \\
  &=
  \frac{1}{T}
  \left(
  \sumtT
  (y_t - m)e^{-it\lambda_\ell}
  \right)
  \left(
  \sumtT
  (y_t - m)e^{it\lambda_\ell}
  \right)
  \\
  \implies\quad I(\lambda_\ell)
  &=
  \frac{1}{T}
  \sumtT
  \sum_{s=1}^T
  (y_t - m)
  (y_s - m)e^{-i(t-s)\lambda_\ell}
  = \sum_{|k|<T} \hat{\gamma}(k)e^{-ik\lambda_\ell}
\end{align*}
as desired.
Lastly, just to tie up the only loose end.
Equality~\ref{orthzero} follows from the even spacing of the
Fourier frequencies $\lambda_\ell$'s:
\begin{align*}
  S
  &=
  \left[
  \exp\left({i\frac{2\pi\ell}{T}}\right)
  + \exp\left(i2\frac{2\pi\ell}{T}\right)
  +\cdots+
  \exp\left(iT\frac{2\pi\ell}{T}\right)
  \right]
  \\
  \implies\quad
  \exp\left({i\frac{2\pi \ell}{T}}\right)S
  &=
  \left[
  \exp\left({i2\frac{2\pi\ell}{T}}\right)
  + \exp\left({i3\frac{2\pi\ell}{T}}\right)
  +\cdots+
  \exp\left({i(T+1)\frac{2\pi\ell}{T}}\right)
  \right]
\end{align*}
Subtract the second line from the first and factor out a
$\exp(i(2\pi\ell)/T)$ on the RHS
\begin{align*}
  S\left[1- \exp\left({i\frac{2\pi \ell}{T}}\right)\right]
  &=
  \exp\left({i\frac{2\pi\ell}{T}}\right)
  \left[
  1-\exp\left({iT\frac{2\pi\ell}{T}}\right)
  \right]
  %\\
  %&=
  %\exp\left({i\frac{2\pi\ell}{T}}\right)
  %\left[
  %1-
  %(\cos(2\pi\ell)+i\sin(2\pi\ell))
  %\right]
\end{align*}
Since $\ell$ is an integer, $\exp(i2\pi\ell)=1$.  Therefore, the RHS of
the above equality is zero, so the LHS must be as well. Since
$\lambda_\ell\neq 0$ by assumption, the bracketed term on the LHS cannot
be zero; therefore it must be that $S=0$, as desired.
\end{proof}


\begin{prop}
For any mean-zero stationary series with absolutely summable
autocovariances, pairwise expected \emph{correlations} between the
$T$ random variables constructed in the DFT converge to zero as
$T\ra\infty$:
\begin{align*}
  \sup_{\ell\neq\ell'} \E[z^{\cos}_\ell z^{\cos}_{\ell'}]
  \ra 0
  \qquad
  \sup_{\ell\neq\ell'} \E[z^{\sin}_\ell z^{\cos}_{\ell'}]
  \ra 0
  \qquad
  \sup_{\ell,\ell'} \E[z^{\cos}_\ell z^{\cos}_{\ell'}]
  \ra 0
\end{align*}
i.e.\ the RV are asymptotically uncorrelated.
Moreover, the expected second moments converges to the spectral density:
\begin{align*}
    \sup_{\ell\leq (T-1)/2}
    \big\lvert\;
      \E[(z^{\cos}_\ell)^2]
      - 2\pi f(\lambda_\ell)
    \;\big\rvert
    &\ra 0
    \qquad
    \sup_{\ell\leq (T-1)/2}
    \big\lvert\;
      \E[(z^{\sin}_\ell)^2]
      - 2\pi f(\lambda_\ell)
    \;\big\rvert
    \ra 0
\end{align*}
Therefore, the DFT converts autocovariance in $\{y_t\}\tT$ into
heteroskedasticity (across $\ell$) of $\{z_\ell^{\cos},z_\ell^{\sin}\}$
terms, with the shape of heteroskedasticity determined by the spectral
density $f(\lambda)$.
\end{prop}

\begin{cor}
Let $I_T(\lambda_\ell)$ denote the periodogram constructed from $T$
observations.
\begin{align*}
  \text{As $T\ra\infty$}\quad
  \E\left[
    I_T(\lambda_\ell)
  \right]
  \ra
  2\pi f(\lambda_\ell)
\end{align*}
This follows from the previous proposition since the expectations of the
DFT coefficients (which are used to construct
$I_T=[(z_\ell^{\cos})^2+(z_\ell^{\cos})^2]/2$) converge.
\end{cor}

\begin{note}
(Important)
Convergence in the above proposition and corollary is \emph{not} in
probability; it is in the analysis sense of convergence of sequences.
Specifically, we're saying that sequences of expectations (which are
sequences of \emph{numbers}, not RV's) are converging.
We're \emph{not} saying that $z^{\cos}_\ell$, $z^{\sin}_\ell$, and
$I_T(\lambda_\ell)$ (which are random variables) converge in probability
something.  No, we're saying that \emph{expectations} involving these
random variables are converging.

It is for this reason that we \emph{smooth} the periodogram.
Essentially, we should think of the periodogram as the underlying
spectral density plus noise. The above convergence result says only
that, as $T\ra\infty$, that noise will be \emph{mean-zero}---\emph{but}
there's still noise there that we can't get rid of, except by smoothing
across Fourier frequencies to improve the quality of our estimates. And
that's the best we can do.
\end{note}



\begin{cor}\emph{(Whittle Likelihood)}
If $\{y_t\}$ is an mean-zero stationary Gaussian
process, then $z^{\cos}_\ell$ and $z^{\sin}_\ell$---as weighted averages
of $\{y_t\}$---will also be Gaussian s.t.
\begin{align*}
  z_\ell^{\cos} &\sim N(0,2\pi f(\lambda_\ell))
  \qquad
  z_\ell^{\sin} \sim N(0,2\pi f(\lambda_\ell))
\end{align*}
Now let $f(\lambda;\theta)$ denote the the spectral density function at
frequency $\lambda$ as a function of parameters $\theta$.
%Also let $\phi(z;\sigma^2)$ denote the density of normal with mean zero
%and variance $\sigma^2$.
Then we can write the likelihood of the model as
\begin{align*}
  \calL(\theta)
  %&= \prod_{\ell=1}^{\lfloor(T-1)/2\rfloor}
  %\phi\big(
    %z_\ell^{\cos},\;2\pi f(\lambda_\ell;\theta)
  %\big)
  %\phi\big(
    %z_\ell^{\sin},\;2\pi f(\lambda_\ell;\theta)
  %\big)
  %\\
  &= \prod_{\ell=1}^{\lfloor(T-1)/2\rfloor}
  \left[
  \frac{1}{2\pi \sqrt{f(\lambda_\ell;\theta)}}
  \exp\left(
    -\frac{(z_\ell^{\cos})^2}{2\cdot 2\pi f(\lambda_\ell;\theta))}
  \right)
  \right]
  \left[
  \frac{1}{2\pi \sqrt{f(\lambda_\ell;\theta)}}
  \exp\left(
    -\frac{(z_\ell^{\sin})^2}{2\cdot 2\pi f(\lambda_\ell;\theta))}
  \right)
  \right]
  \\
  &= \prod_{\ell=1}^{\lfloor(T-1)/2\rfloor}
  \frac{1}{(2\pi)^2 f(\lambda_\ell;\theta)}
  \exp\left(
    -\frac{I_T(\lambda_\ell)}{2\pi f(\lambda_\ell;\theta))}
  \right)
\end{align*}
where we used that
$I_T(\lambda_\ell)=[(z_\ell^{\cos})^2+(z_\ell^{\sin})^2]/2$ to simplify
the exponential.
\clearpage
Now take logs, ignore the constant term, and arrive at the Whittle
log-likelihood:
\begin{align*}
  \ln\calL(\theta)
  &=
  %\frac{T}{2}\log(2\pi)
  -\sum_{\ell=1}^{\lfloor(T-1)/2\rfloor}
    \log\big(2\pi f(\lambda_\ell;\theta)\big)
  -\sum_{\ell=1}^{\lfloor(T-1)/2\rfloor}
  \frac{I_T(\lambda_\ell)}{2\pi f(\lambda_\ell;\theta)}
\end{align*}
We can maximize this object (even if the data is not Gaussian) for
approximate model estimation.
\end{cor}

\begin{ex}
If we suppose $y_t$ follows an AR(1)
\begin{align*}
  y_t=\rho y_{t-1}+\varepsilon_t
  \quad\iff\quad
  y_t = (1-\rho L)^{-1}\varepsilon_t
\end{align*}
where the $\varepsilon_t$ innovations are Gaussian, then we know that
\begin{align*}
  f(\lambda; \rho,\sigma^2)
  =
  \frac{\sigma^2}{2\pi}
  (1-\rho e^{i\lambda})^{-1}
  (1-\rho e^{-i\lambda})^{-1}
\end{align*}
Then, given data $\{y_t\}\tT$, we can compute $I_T(\lambda_\ell)$ at the
Fourier frequencies, plug $I_T(\lambda_\ell)$ and this expression for
$f(\lambda;\rho,\sigma^2)$ into the log likelihood above, and maximize
for $\rho$ and $\sigma^2$.
\end{ex}


\clearpage
\subsubsection{Estimating the Long-Run Variance}
\label{sec:hac}

In this subsection, we suppose that we have a sample $\{y_t\}\tT$ for
some covariance stationary process.
We want to estimate the long-run variance, which can be written in any
of the following three equivalent ways in terms of the autocovariances,
the autocovariance generating function $g(z)$, or the spectral density
$f(\lambda)$, respectively:
\begin{align*}
  S &= \sum_{j=-\infty}^\infty \gamma(j)
  = g(1)
  = 2\pi f(0)
\end{align*}

\begin{defn}(Non-Parametric Estimator)
The non-parametric approach estimates sample covariances
$\{\hat{\gamma}(j)\}$ and then constructs
\begin{align*}
  \hat{S} = \sum_{j=-m}^m \hat{\gamma}(j)
  \qquad \text{where}\quad
  \hat{\gamma}(j) = \frac{1}{T-j}\sum_{t=j+1}^T y_t y_{t-j}
\end{align*}
where $m<T-1$ is some truncation parameter.
\end{defn}

\begin{defn}(Kernel Based Estimators and the Newey-West Estimator)
Here we define a class of nonparametric estimators that still uses estimates
of the sample covariances $\{\hat{\gamma}(j)\}$, but applies a weighting
kernel to prevent sample error from imprecisely estimated (far-apart,
$j$ close to $T$) covariances influencing $\hat{S}$ too much or inducing
a negative long-run variance:
\begin{align*}
  \hat{S} = \sum_{j=-(T-1)}^{T-1} k(j/b_T)\hat{\gamma}(j)
\end{align*}
where $\{b_T\}$ is a sequence of bandwith parameters such that
$b_T\ra \infty$ and where $k$ is a weighting kernel that satisfies
\begin{enumerate}[label=(\roman*)]
  \item Even, $k(-x)=k(x)$
  \item $k(0)=1$
  \item $\lim_{x\ra\infty}k(x)=0$
\end{enumerate}
Therefore, $\hat{S}$ is a class of estimators indexed by different
choices of $k$ and $b_T$.
In practice, $k$ is often chosen to be the \emph{Bartlett Kernel}
\begin{align*}
  k(x) &=
  \begin{cases}
    (1-|x|) & |x|<1 \\
    0 &\text{otherwise}
  \end{cases}
\end{align*}
and the resulting estimator is called the \emph{Newey-West Estimator}.
Rule of thumb from Stock and Watson for a series with moderate
persistence: $b_T=0.75 T^{1/3}$.
\end{defn}


\begin{defn}(Parametric Estimator)
If we're willing to assume that $\{y_t\}$ is an ARMA process
\begin{align*}
  \phi(L)(y_t-\mu) = \theta(L)\varepsilon_t
  \qquad \varepsilon_t\sim(0,\sigma^2)
\end{align*}
we can estimate the parameters $\hat{\sigma}^2$, $\hat{\phi}(z)$, and
$\hat{\theta}(z)$ and then compute the implied spectral density at zero
$2\pi f(0)$ or (equivalently) the autocovariance generating function at
unity $g(1)$:
\begin{align*}
  \hat{S} = \hat{\sigma}^2 \frac{\hat{\theta}(1)^2}{\hat{\phi}(1)^2}
  = \hat{\sigma}^2
  \frac{(1+\hat{\theta}_1 + \cdots + \hat{\theta}_q)^2}{%
    (1-\hat{\phi}_1 - \cdots - \hat{\phi}_p)^2}
\end{align*}
\end{defn}

\begin{defn}(Pre-Whitened Estimator)
This next approach combines both the parametric and non-parameteric
approaches. Suppose that $y_t$ follows an MA($\infty$) process
\begin{align*}
  (y_t -\mu) = \psi(L)\varepsilon_t
  \qquad \varepsilon_t\sim (0,\sigma^2_\varepsilon)
\end{align*}
An MA($\infty$) process can be reasonably well approximated by an AR(1),
so factor out an AR(1) polynomial from $\psi(L)$ to write $y_t$ as
\begin{align}
  (y_t  - \mu) &=
  \underbrace{(1-\rho L)^{-1} \theta(L)}_{\psi(L)}
  \varepsilon_t
  \qquad
  \varepsilon_t\sim (0,\sigma^2_\varepsilon)
  \notag
  \\
  &=
  (1-\rho L)^{-1} u_t
  \qquad\qquad
  u_t := \theta(L)\varepsilon_t
  \label{prewhiten}
  \\
  \iff\quad
  (y_t -\mu) &= \rho (y_{t-1}-\mu) + u_t
  %\quad\quad\;\,
  %u_t := \theta(L)\varepsilon_t
  %\label{prewhiten}
  \notag
\end{align}
Then we can construct an estimate of the long-run variance for $\{y_t\}$
by computing
\begin{align*}
  \hat{S}_y = \frac{\hat{S}_u}{(1-\hat{\rho})^2}
\end{align*}
where $\hat{\rho}$ is the coefficient from OLS regression of
$(y_t-\mu)$ on $(y_{t-1}-\mu)$ and $\hat{S}_u$ is a nonparametric
estimate (probably a Newey-West estimator) of the long-run variance of
the residuals.

The idea is that the first stage OLS regression of $(y_t-\mu)$ on
$(y_{t-1}-\mu)$ will remove a lot of the autocorrelation while still
only estimating one parameter, $\hat{\rho}$. As a result, the
nonparametric estimate of the residual long-run variance
$\sum_{j=-\infty}^\infty\gamma_u(j)$ (an infinite sum we approximate
with a finite sum) will be much higher quality since the autocovariances
for long langs won't contribute as much---the first state removed lots
of the autocorrelation already.
\end{defn}
\begin{proof}
To see why the formula for $\hat{S}_y$ takes that form, we work with the
autocovariance generating functions. First recall that the long-run
variance of $\{x_t\}$ is $S_x=g_x(1)$ where $g_x(z)$ is the
autocovariance generating function of $\{x_t\}$.

Next, if we write $y_t$ as in expression \ref{prewhiten} where $\{u_t\}$
is a covariance stationary process with autocovariance generating
function $g_u(z)$, then we can view $\{y_t\}$ as simply $\{u_t\}$
filtered by $h(z)=(1-\rho z)^{-1}$. But we know to compute the
autocovariance generating functions of filtered processes:
\begin{align*}
  g_y(z) = h(z)h(z^{-1}) g_u(z)
  = (1-\rho z)^{-1}(1-\rho z^{-1})^{-1}g_u(z)
\end{align*}
Given this expression, we can compute the long-run variance of $y$:
\begin{align*}
  S_y = g_y(1) = \frac{g_u(1)}{(1-\rho)^{2}}
\end{align*}
But notice that $g_u(1)$ is simply the long-run variance of $u$, denoted
$S_u$.  Therefore,
\begin{align*}
  S_y = g_y(1) = \frac{S_u}{(1-\rho)^{2}}
\end{align*}
Notice that we don't actually care about the parametric form that $S_u$
takes. It's just a number we want to estimate nonparametrically.
\end{proof}

\begin{defn}(Parametric GLS Approach)
We suppose that the process follows
\begin{align*}
  y_t &= x_t'\beta + u_t \\
  u_t &= \psi(L)\varepsilon_t
  \qquad \varepsilon_t\sim (0,\sigma^2)
\end{align*}
where $x_t$ is a vector of predetermined values (maybe just a constant)
and $u_t$ is some covariance stationary process with MA($\infty$)
representation as above with lag polynomial $\psi(L)$.  As stated, the
errors $u_t$ are correlated across observations. We can turn this into a
regression with uncorrelated errors by inverting $c(L)$ to get
\begin{align*}
  \psi(L)^{-1}y_t &= \psi(L)^{-1}x_t'\beta + \varepsilon_t \\
  \iff\qquad
  \tilde{y}_t &= \tilde{x}_t\beta + \varepsilon_t
\end{align*}
Then the OLS estimator from the regression of $\tilde{y}$ on $\tilde{x}$
will be the best linear unbiased estimator by Gauss-Markov since the
errors are now iid.

Of course, this supposes that we know $\psi(L)$. In general we don't, so
we have to estimate it and do feasible GLS. Therefore, we can do the
following:
\begin{enumerate}[label=(\roman*)]
  \item Run the regression $y_t=x_t'\beta +u_t$ to get a vector of
    estimated residuals $\hat{u}_t$.
  \item Using $\hat{u}_t$, estimate the parameters $\hat{\psi}(z)$
  \item Use the estimated $\hat{\psi}(z)$ to construct $\tilde{y}_t$ and
    $\tilde{x}_t$
  \item Run OLS regression $\tilde{y}=\tilde{x}\beta + \varepsilon_t$.
\end{enumerate}
The result is a better estimate of $\beta$ and resulting asymptotic
variance
\begin{align*}
  \hat{\beta}_{GLS}
  &=
  \left[ \sumtT \tilde{x}_t\tilde{x}_t'\right]^{-1}
  \left[ \sumtT \tilde{x}_t\tilde{y}_t\right]
\end{align*}
However, we need $\E[\tilde{x}_t\varepsilon_t]=0$ for consistency of the
FGLS estimator. In general that means we need not only
\emph{predetermined} (aka \emph{weakly endogenous}) regressors
$\E[u_t|x_t,x_{t-1},\ldots]=0$, but also \emph{strictly endogenous}
regressors $\E[u_t|x_{t+1},x_t,x_{t-1},\ldots]=0$, which is generally a
bad assumption.


\end{defn}










\clearpage
\section{Classical Hypothesis Testing}

There are two main approaches to \emph{hypothesis testing}: the
\emph{Classical Approach} and the \emph{Bayesian Approach}.  This
section will focus exclusively on the classical approach. To do so, we
introduce a table that will be of use in our discussion that documents
the types of outcomes we might observe in hypothesis testing:
\begin{center}
   \begin{tabular}{r c}
      $\quad\qquad$ & $\qquad$ {\sl What We Claim}\\
   \end{tabular}
   \\
   \begin{tabular}{l | c c}
      {\sl True Situation} & $H_0$ True & $H_0$ False \\\hline
      $H_0$ True & \checkmark & \textbf{Type I Error}\\
      $H_0$ False & \textbf{Type II Error} & \checkmark
   \end{tabular}
\end{center}
Our hypothesis testing will invariably require us to collect
\emph{data}, which we explicitly define as observed
values, or realizations, of random variables; therefore,
randomness is built into the decision.


\subsection{Five Basic Steps}

In the classical approach to hypothesis testing, there are five
main steps which we will summarize here:
\begin{enumerate}
   \item Set up a null hypothesis, denoted $H_0$, \emph{explicitly}.
      Also, set up an explicit alternative hypothesis, denoted
      $H_1$. There are two forms a hypothesis, either null or
      alternative, can take:
      \begin{itemize}
	 \item[-] {\sl Simple}: All parameters are specified
	    numerically. For example
	    \[ H_0: \beta = 0, \qquad H_1:\theta = 0.6\]
	 \item[-] {\sl Composite}: Not all parameters are specified
	    numerically. Examples include:
	    \begin{align*}
	       \text{One-Sided Composite } \qquad& \theta > 0.5,
	       \qquad \theta < 0.5\\
	       \text{Two-Sided Composite } \qquad& \theta \neq 0.5
	    \end{align*}
      \end{itemize}
   \item We next adopt our Type I error rate.
      on the basis of observed data,  which equals the
      $\alpha$ level. Just to be explicit:
	 \[ \alpha = P(\text{Type I error}) \]
   \item Next, we decide on the \emph{test-statistic} that we
      will compute in order to decide whether to accept or reject
      $H_0$ depending upon the resulting numerical value. Ideally,
      this test statistic will be chosen based on some
      ``optimality properties'' which we will consider below.
   \item Next, we establish the \emph{critical region} within
      which we will reject the null in favor of the alternative
      hypothesis.
   \item Finally, we will collect data, calculate the value of the
      test statistic, and accept or reject the null based on
      whether the value of the test statistic falls within some
      critical region.
\end{enumerate}

\subsection{Using p-values}

{\sl Definition}: You can alternatively conduct Hypothesis Tests using
p-values. A p-value is the probability of getting the
\emph{actually observed} value (given the data) of the test
statistic---or a value more extreme than the observed value---when
$H_0$ is true. We accept or reject using the rule
\[ \text{p-value} \leq P(\text{Type I error}) \quad
   \Rightarrow \quad \text{Reject $H_0$} \]
Note that when working with p-values, the alternative hypothesis
specification is Duncan Hines irrelevant. You work only with the
values specified in the null.\footnote{Note that this process doesn't
really take into account model uncertainty.}


\subsection{Power}

Our Type I error is fixed at whatever value of $\alpha$ that we
chose in Step 2. But given that value of $\alpha$, we would like
to know the \emph{power} of the test, defined as
\begin{align*}
   \text{Power} &= P(\text{Reject $H_0$} | H_0 \text{ False})
   =1 - P(\text{Type II Error})
\end{align*}
In the case where $H_1$ is \emph{simple}, the
power is a number, but if $H_1$ is \emph{composite}, there is
no unique value for the power.  Rather, we will have a
\emph{power curve}
or \emph{power surface}. So suppose without loss
of generality, that the \emph{true} value of the parameter, $\theta^*$,
is greater than the \emph{null} value, $\theta_0$.
In that case the power curve
will look something like the following.
Notice that the probability of rejection increases as
$\theta^*-\theta_0$ increases:
%\begin{figure}[h!]
   %\centering
   %\includegraphics[scale=0.7]{SamplePowerCurve.pdf}
%\end{figure}

\subsection{Neyman-Pearson Lemma}

{\sl Simple $H_0$, Simple $H_1$}: We first consider the case where
both the null and alternative are simple, and we denote the jdfs
implied by $H_0$ and $H_1$ as $f_0$ and $f_1$, respectively. We
then form the \emph{likelihood ratio}:
\[ \text{LR} = \frac{f_0(y_1, \ldots, y_n)}{f_1(y_1, \ldots, y_n)}=
   \frac{P(\text{data under $H_0$})}{P(\text{data under $H_1$})}
   \]
The Neyman-Pearson Lemma states that the uniformly most powerful
test of $H_0$ versus $H_1$ rejects $H_0$ whenever LR $\leq K$,
where $K$ is ``sufficiently small'' given the chosen value of $\alpha$.
And by ``uniformly most powerful,'' we mean that this test
maximizes the power (i.e. is \emph{optimal})
after controlling for the Type I error---i.e.
after setting the Type I error rate to $\alpha$.
\\
\\
{\sl Connection to Sufficient Statistics}: If a sufficient statistic,
$W$, exists for the parameter, $\theta$, that's being tested, then
$W$ \emph{will} be the test statistic in the hypothesis test. This
follows from the fact that we can factorize the jdf into two
components: a function of the data only and a function of the sufficient
statistic and $\theta$.

\subsection{Lambda Ratio Test}

\subsubsection{Justification and Intuition}

Recall that the Neyman-Pearson lemma stated that the method gave
the uniformly most powerful tests. However, recall that power\footnote{
Unlimited power!} is defined as
   \[ \text{Power} = P(\text{Reject $H_0$} | \text{$H_1$ true}) \]
Moreover the likelihood Ratio procedure required a specification
of the probability of the data under the alternative, $H_1$. BUT,
and here's the rub, is you're working with a \emph{composite}
alternative, where not all parameters are specified
\begin{enumerate}
   \item Power is not a single number, but a range of values
      depending upon the values in $H_1$, which can vary.
   \item The probability of the data under the alternative \emph{also}
      takes on a range of values.
\end{enumerate}
How to cope?
\\
\\
Well, if you wanted to know who had the best Hockey players---the US
or Canada---it's not very efficient to have all teams consisting of
all hockey players compete against each other. So you take the
all-stars, and have the best of the US play against the best Canadian
players.  We'll essentially do the same here.

\subsubsection{Procedure}

So suppose the null and alternative hypothesis are composite.
We now form the Lambda Ratio by taking
\[ \lambda =
   \frac{\max P(\text{data under $H_0$})}{
      \max P(\text{data under $H_1$})}
   \]
To find the maximum probabilities under $H_0$ and $H_1$, we
will have to differentiate the likelihood---or equivalently, the
log-likelihoods---with respect to any unspecified parameters, set
the first order conditions equal to zero, and solve.
\\
\\
After we have the solutions, we plug back into the $\lambda$ formula
and use the same criterion:
reject $H_0$ whenever $\lambda\leq K$,
where $K$ is ``sufficiently small'' given the chosen value of $\alpha$.
This maximizes the power after controlling for the Type I error---i.e.
after setting the value of the Type I error equal to $\alpha$.



\clearpage
\section{Classical Linear Regression in Finite Samples}

\subsection{Setup and Assumptions}

In this section, I follow Hayashi and assume that the $N\times K$
regressor matrix $\bsX$ is \emph{random} rather than fixed, since that
seems more appropriate for a non-experimental discipline like economics
where the RHS ``independent'' variables are not under the control of the
econometrician.


\begin{defn}(Classical Linear Regression Model)
For a sample of $N$ observations and $K$ regressors, the
\emph{classical linear regression model} satisfies the following
assumptions. Note that the sample $\{(y_n,\bsx_n)\}\nN$ is not
necessarily assumed iid. The joint distribution of $(y_n,\bsx_n)$ could
depend on $n$, as it likely would for time series data, though the
assumptions we now state are not necessarily \emph{good} assumptions for
such applications.
\begin{enumerate}
  \item Linearity: We assume the model can be written
    \begin{align}
      \underset{(N\times 1)}{\bsy}
      =
      \underset{(N\times K)}{\vphantom{\bsy} \bsX}
      \underset{(K\times 1)}{\vphantom{\bsy} \bsbeta}
      + \underset{(N\times 1)}{\vphantom{\bsy} \bsvarepsilon}
      \label{regyx}
    \end{align}
  \item Strict Exogeneity: $\E[\varepsilon_n |\bsX]=0$ for
    $n=1,\ldots,N$.

    Note that error $\varepsilon_n$ for observation $n$ is
    mean zero conditional on \emph{all} observations in $\bsX$, not just
    $\bsx_n$.
    And it doesn't matter here that we're restricting them to the
    constant zero rather than some other constant, so long as we have an
    intercept in the model.

    Consequences of this assumption are that the errors are
    unconditinally mean zero and \emph{each} error term is orthogonal to
    the regressors of \emph{any} observation, hence uncorrelated:
    \begin{align*}
      \E[\varepsilon_n] &= \E[\E[\varepsilon_n|\bsX]] = 0 \\
      \E[\bsx_m \cdot \varepsilon_n] &=
      \E[\E[\bsx_m \cdot \varepsilon_n|\bsX]] =
      %\E[\bsx_m\E[ \varepsilon_n|\bsX]] = \underbrace{\bso}_{K\times 1}
      \E[\bsx_m\E[ \varepsilon_n|\bsX]] = \bso
      \qquad \forall n,m = 1,\ldots,N \\
      \Cov(\varepsilon_n, x_{mk})
      &=
      \E[\varepsilon_n x_{mk}] - \E[\varepsilon_n]\E[x_{mk}] = 0
    \end{align*}
    In general, strict exogeneity is a bad assumption for time series
    models.

  \item No multicolinearity: $P[\rank(\bsX)=K]=1$, full column
    rank.\footnote{%
      This implies that $\bsX'\bsX$ is positive definite. First, define.
      $\bsb:=\bsX \bsa$. By the full rank assumption, there is no linear
      combination of columns such that $\bsb=\bsX\bsa = \bso$.
      Therefore, for all $\bsa\neq \bso$, we have
      $\bsa'\bsX'\bsX\bsa=\bsb'\bsb = \sum_{k=1}^K b_k^2 > 0$, so we can
      say that $\bsX'\bsX$ is positive definite.
    }

  \item Spherical Error Variance (Optional): Errors are both
    homoskedastic and uncorrelated:
    \begin{align*}
      \Var(\bsvarepsilon|\bsX) =
      \E[\bsvarepsilon\bsvarepsilon'|\bsX] = \sigma^2\bsI_N
    \end{align*}
    where $\bsI_N$ is the $N$-dimensional identity matrix.

    Coupled with strict exogeneity, homoeskedasticy implies constant
    conditional error variance $\Var(\varepsilon_n|\bsX)=\sigma^2$,
    while uncorrelated errors implies no serial correlation
    in the error terms $\Cov(\varepsilon_n,\varepsilon_m)=0$.
\end{enumerate}
\end{defn}

\clearpage
\subsection{Discussion of Random Sampling and Heteroskedasticity}
\label{sec:discusshetero}

\begin{defn}(Classical Regression Model with Random Samples)
The classical regression model with random sampling assumes that the
sample $\{(y_n,\bsx_n)\}\nN$ is iid across observations.

First, Assumption 1 along with the iid assumption on
$(y_n,\bsx_n)$ implies that $(\varepsilon_n, \bsx_n)\perp \bsx_m$ for
$n\neq m$.
So Assumptions 1 and 4 on $\varepsilon_n$ (which we always
conditioned on $\bsX$) will simplify to conditioning on individual
$\bsx_n$ as follows:
\begin{alignat*}{5}
  \text{Assumption 2} &&\qquad
   \E[\varepsilon_n | \bsX] &=
   \E[\varepsilon_n | \bsx_n] &&= 0\\
  \text{Assumption 4} &&\qquad
   \E[\varepsilon_n^2 | \bsX] &=
   \E[\varepsilon_n^2 | \bsx_n] &&= \sigma^2\\
  \text{} &&\qquad
   \E[\varepsilon_n\varepsilon_m|\bsX] &=
   \E[\varepsilon_n|\bsx_n] \E[\varepsilon_m|\bsx_m]
   &&=0
\end{alignat*}
\end{defn}

\begin{defn}(Conditional and Unconditional Homoskedasticity)
Suppose that we have an iid sample, so conditioning on $\bsX$ can be
replaced with conditioning on $\bsx_n$.
We make a distinction here (which will be reused again later in the OLS
Asymptotics section) about two types of homo-/heteroskedasticity:
\emph{conditional} and \emph{unconditional} (where the conditioning
refers to $\bsx_n$). They are defined as follows:
\begin{enumerate}[label=(\roman*)]
  \item \emph{Unconditional Homoskedasticity}:
    $\E[\varepsilon_n^2] = \sigma^2$ for all $n$.
  \item \emph{Unconditional Heteroskedasticity}:
    $\E[\varepsilon_n^2] = \sigma^2_n$, i.e. depends upon the index $n$
  \item \emph{Conditional Homoskedasticity}:
    $\E[\varepsilon_n^2 | \bsX]
    =\E[\varepsilon_n^2 | \bsx_n] = \sigma^2$, a constant
  \item \emph{Conditional Heteroskedasticity}:
    $\E[\varepsilon_n^2 | \bsX] =\E[\varepsilon_n^2 | \bsx_n] =
    \sigma^2(\bsx_n)$, which is a deterministic function of $\bsx_n$.
\end{enumerate}
They are related as follows.
\begin{itemize}
  \item Both (i) and (ii) \emph{cannot} be true at the same time
  \item Both (iii) and (iv) \emph{cannot} be true at the same time
  \item (iii) $\implies$ (i) by iterated expectations:
    $\E[\varepsilon_n^2]=
    \E[\E[\varepsilon_n^2|\bsx_n]]
    \underset{\text{By (iii)}}{=}
    \E[\sigma^2] = \sigma^2$

  \item (iv) $\implies$ (i) by iterated expectations:
    $\E[\varepsilon_n^2]=
    \E[\E[\varepsilon_n^2|\bsx_n]]
    \underset{\text{By (iv)}}{=}
    \E[\sigma^2(\bsx_n)] = \sigma^2$
    where the final expectation was take by averaging over $\bsx_n$.
\end{itemize}
The model we will use throughout this section and in the next on OLS
asymptotics will generally allow for Conditional Heteroskedasticity as
in (iv). That is, assuming a random sample and using Assumptions 1-3
above is only enough to get us to case (iv) where we still allow for
\emph{conditional heteroskedasticity}. The random sampling assumption
plus Assumptions 1-3 say only that there is a deterministic function
$\sigma^2(\bsx_n)$ that is \emph{identical across $n$} such that
$\E[\varepsilon_n^2|\bsx_n]=\sigma^2(\bsx_n)$. That's it.

Sometimes, however, we will choose to use additionally Assumption 4
above, which switches us to case (iii) and says
$\sigma^2(\bsx_n)=\sigma^2$, a constant. Regardless though, as shown, we
will have (i) for free. And when we refer to ``heteroskedasticity robust
standard errors'' below, we are referring to \emph{conditional}
heteroskedasticity robustness as in (iv).
\end{defn}


\clearpage
\subsection{Estimation Formulas}

\begin{defn}(Sum of Squared Residuals)
Define the \emph{sum of squared residuals} as
\begin{align*}
  SSR(\bsb) = (\bsy - \bsX \bsb)'(\bsy - \bsX \bsb)
\end{align*}
\end{defn}


\begin{defn}(OLS Estimates)
The \emph{OLS Estimator} minimizes the SSR:
\begin{align*}
  \bshatbeta_{OLS}
  =
  \argmin_{\bsb} \;
  (\bsy - \bsX \bsb)'(\bsy - \bsX \bsb)
\end{align*}
The FOCs of $K$ equations and $K$ unknowns are called the
\emph{normal equations}, where the second expression highlights the
orthogonality of the regressors $\bsX'$ to the residual vector
$\bsy-\bsX\bsb$.  Since the second derivative is the positive definite
matrix $\bsX'\bsX$, we have a minimum.
\begin{align*}
  \bsX'\bsX\bsb &= \bsX'\bsy
  \quad\iff\quad
  0= \bsX'(\bsy-\bsX\bsb)
\end{align*}
Solving, we get the OLS estimator, which has the following mean and
variance:
\begin{align*}
  \hat{\bsbeta}_{OLS}
  &= (\bsX'\bsX)^{-1}\bsX'\bsy \\
  &= (\bsX'\bsX)^{-1}\bsX' (\bsX \bsbeta + \bsvarepsilon)
  = \bsbeta + (\bsX'\bsX)^{-1}\bsX' \bsvarepsilon
  \\
  \implies\quad
  \hat{\bsbeta}_{OLS} - \bsbeta
  &= (\bsX'\bsX)^{-1}\bsX' \bsvarepsilon \\
  \E[\hat{\bsbeta}_{OLS}|\bsX]
  %&=
  %\bsbeta +
  %\E[(\bsX'\bsX)^{-1}\bsX' \bsvarepsilon |\bsX]
  %=
  %\bsbeta +
  %(\bsX'\bsX)^{-1}\bsX' \cdot \E[\bsvarepsilon |\bsX]
  %\\
  &= \bsbeta \\
  \Var(\hat{\bsbeta}_{OLS}|\bsX)
  &=
  \E[(\hat{\bsbeta}_{OLS}-\bsbeta)(\hat{\bsbeta}_{OLS}-\bsbeta)'|\bsX]
  \\
  %&=
  %\E\left[
    %(\bsX'\bsX)^{-1}\bsX'\bsvarepsilon)
    %(\bsX'\bsX)^{-1}\bsX'\bsvarepsilon)'
  %|\bsX\right]
  %\\
  &=
  \left((\bsX'\bsX)^{-1}\bsX'\right)
  \E[\bsvarepsilon\bsvarepsilon' |\bsX]
  \left(\bsX(\bsX'\bsX)^{-1}\right)
\end{align*}
With spherical errors, the variance simplifies to
\begin{align*}
  \Var(\hat{\bsbeta}_{OLS}|\bsX)
  &=
  \left((\bsX'\bsX)^{-1}\bsX'\right)
  \sigma^2 \bsI_N
  \left(\bsX(\bsX'\bsX)^{-1}\right)
  =\sigma^2 (\bsX'\bsX)^{-1}
\end{align*}
Now define the following objects
\begin{alignat*}{3}
  \text{Projection Matrix}&\quad
  \bsP &&= \bsX(\bsX'\bsX)^{-1}\bsX' \\
  \text{Annihilator Matrix}&\quad
  \bsM &&= \bsI_n - \bsP \\
  \text{Residual Vector}&\quad
  \bse &&= \bsX\bsb - \bsy \\
  \text{Fitted Values}&\quad
  \bshaty &&= \bsX\bsb
\end{alignat*}
Both $\bsP$ and $\bsM$ are easily shown to be symmetric and idempotent.
Moreover,
\begin{itemize}
  \item $SSR(\bsb) = \bse'\bse$
  \item $\bsP\bsX=\bsX$: The projection of $\bsX$ onto itself is $\bsX$
  \item $\bsM\bsX = \bso$: This is where the annihilator matrix gets
    it's name.
  \item $\bse = \bsM\bsy = \bsM \bsvarepsilon$: Applied to $\bsy$, the
    annihilator matrix returns the residuals
\end{itemize}
\end{defn}

\begin{defn}(Distribution of Coefficients)
If we want the mean and variance of individual coefficients or linear
combinations of coefficients, we have
\begin{align*}
  \E[r'\bshatbeta_{OLS}|\bsX]
  &= r'\bsbeta \\
  \Var(r'\bshatbeta_{OLS}|\bsX)
  &=
  r'\left((\bsX'\bsX)^{-1}\bsX'\right)
  \E[\bsvarepsilon\bsvarepsilon' |\bsX]
  \left(\bsX(\bsX'\bsX)^{-1}\right) r \\
  &=
  \sigma^2r'(\bsX'\bsX)^{-1} r
\end{align*}
where the second formula for the variance assumed spherical errors. The
first expression is more general.
\end{defn}

\begin{defn}(Estimator for $\sigma^2$ and $\Var[\hat{\bsbeta}_{OLS}|\bsX]$)
Assuming spherical errors, we form the estimator for $\sigma^2$:
\begin{align*}
  s^2 &= \frac{SSR(\bsb)}{(n-K)} = \frac{\bse'\bse}{(N-K)}
\end{align*}
That let's us define an estimator for the covariance matrix of the
coefficients:
\begin{align*}
  \bshatV(\hat{\bsbeta}_{OLS}|\bsX) = s^2(\bsX'\bsX)^{-1}
\end{align*}
\end{defn}

\begin{prop}
Assuming spherical errors, $s^2$ is an unbiased estimator:
$\E[s^2|\bsX]=\sigma^2$.
\end{prop}
\begin{proof}
Recall that $\bse=\bsM\bsvarepsilon$ where $\bsM$, the annihilator
matrix, is idempotent. Then
\begin{align*}
  \E[\bse'\bse|\bsX]
  &= \E[\bsvarepsilon'\bsM'\bsM\bsvarepsilon|\bsX]\\
  &= \E[\bsvarepsilon'\bsM\bsvarepsilon|\bsX] \\
  &= \sumin \sumjn m_{ij}\E[\varepsilon_i\varepsilon_j|\bsX]
\end{align*}
But since we assumed $\E[\varepsilon_i\varepsilon_j|\bsX]=0$, all cross
terms drop, leaving
\begin{align*}
  \E[\bse'\bse|\bsX]
  &= \sumin m_{ii}\E[\varepsilon_i\varepsilon_i|\bsX]
  = \sumin m_{ii}\sigma^2 \\
  &= \sigma^2 \cdot \trace(\bsM)
\end{align*}
Almost there. Just use the definition of $\bsM$:
\begin{align*}
  \E[\bse'\bse|\bsX]
  &= \sigma^2 \cdot \trace(\bsM) \\
  &= \sigma^2 \trace(\bsI_N - \bsX(\bsX'\bsX)^{-1}\bsX') \\
  &= \sigma^2 \trace(\bsI_N) - \trace(\bsX(\bsX'\bsX)^{-1}\bsX') \\
  \text{By $\trace(AB)=\trace(BA)$}\qquad
  &= \sigma^2 \left(N - \trace(\bsX'\bsX(\bsX'\bsX)^{-1})\right) \\
  \E[\bse'\bse|\bsX]
  &= \sigma^2 \left(N - \trace(\bsI_K)\right)
  = \sigma^2 \left(N - K\right)
\end{align*}
Therefore,
\begin{align*}
  \E[\bse'\bse|\bsX] = \sigma^2(N-K)
  \quad\implies\quad
  \E[s^2|\bsX] = \frac{\E[\bse'\bse|\bsX]}{N-K} = \sigma^2
\end{align*}




\end{proof}



\clearpage
\subsection{Gauss-Markov Theorem}

\begin{thm}\emph{(Gauss-Markov)}
Given the classical linear regression model assuming spherical errors
$\Var(\bsvarepsilon|\bsX)=\sigma^2\bsI$, the OLS estimator is
\emph{efficient} in the class of linear unbiased estimators. That is,
\begin{align*}
  \bstildebeta = \bsA\bsy
  \quad \text{s.t}\quad \E[\bstildebeta]=\bsbeta
  \quad\implies\quad
  \Var(\bstildebeta|\bsX) \geq \Var(\bshatbeta|\bsX)
\end{align*}
where $\bsA$ is some constant (nonrandom) matrix that can depend upon
$\bsX$, and $\bshatbeta$ is the OLS estimator.
\end{thm}
\begin{proof}
($\bstildebeta$ Unbiased $\implies$ $\bsA\bsX=\bsI$)
First, we want to show a mini-result. Start by writing out the
expectation of $\bstildebeta$:
\begin{align*}
  \E[\bstildebeta|\bsX]
  &= \E[\bsA\bsy|\bsX]
  = \E[\bsA(\bsX\bsbeta +\varepsilon)|\bsX] \\
  \implies\quad
  \E[\bstildebeta|\bsX]
  &= \bsA\bsX\bsbeta
\end{align*}
Therefore, if $\bstildebeta$ is to be an unbiased estimator of
$\bsbeta$, we need must have $\bsA\bsX=\bsI$.
\\
\\
($\Cov(\bshatbeta, \bstildebeta-\bshatbeta|\bsX)=0$)
Next, we show another mini-result. To do so, we consider
$\bstildebeta-\bshatbeta$ and use the result that $\bsA\bsX=\bsI$ along
with regression equation $\bsy=\bsX\bsbeta+\bsvarepsilon$ to simplify
the difference:
\begin{align}
  \bstildebeta - \bshatbeta
  &= \bsA\bsy - ((\bsX'\bsX)^{-1}\bsX'\bsy) \notag\\
  %&= (\bsA-(\bsX'\bsX)^{-1}\bsX)\bsy \\
  &= (\bsA-(\bsX'\bsX)^{-1}\bsX')(\bsX\bsbeta + \bsvarepsilon) \notag\\
  &= \bsA\bsX\bsbeta-(\bsX'\bsX)^{-1}\bsX'\bsX\bsbeta
    + (\bsA-(\bsX'\bsX)^{-1}\bsX')\bsvarepsilon \notag \\
  %&= \bsI\bsbeta-\bsI\bsbeta
    %+ (\bsA-(\bsX'\bsX)^{-1}\bsX)\bsvarepsilon \\
  \implies\quad
  \bstildebeta - \bshatbeta
  &= (\bsA-(\bsX'\bsX)^{-1}\bsX')\bsvarepsilon
  \label{betatilde-betahat}
\end{align}
Moreover, since both $\bshatbeta$ and $\bstildebeta$ are unbiased
estimators, we have
\begin{align*}
  \E[\bshatbeta|\bsX] = \E[\bstildebeta|\bsX] &= \bsbeta \\
  \E[\bshatbeta-\bstildebeta|\bsX] &= 0
\end{align*}
We now compute the covariance between $\bshatbeta$ and
$\bstildebeta-\bshatbeta$. We will use the simplified formula from above
for $\bstildebeta-\bshatbeta$, the means just stated, and the formula
derived in the estimation section above that
$\bshatbeta-\bsbeta=(\bsX'\bsX)^{-1}\bsX'\bsvarepsilon$ in the covariance
formula:
\begin{align*}
  \Cov(\bstildebeta - \bshatbeta,\bshatbeta|\bsX)
  &=
  \E\left[
    (\bstildebeta - \bshatbeta)
    (\bshatbeta-\bsbeta)'
    |\bsX\right]
  \\
  &=
  \E\left[
    \left((\bsA-(\bsX'\bsX)^{-1}\bsX')\bsvarepsilon\right)
    \left((\bsX'\bsX)^{-1}\bsX'\bsvarepsilon\right)'
    |\bsX\right]
  \\
  &=
  (\bsA-(\bsX'\bsX)^{-1}\bsX')
    \E\left[ \bsvarepsilon \bsvarepsilon'|\bsX\right]
    \left(\bsX(\bsX'\bsX)^{-1}\right)
\end{align*}
Under spherical errors
$\E\left[\bsvarepsilon\bsvarepsilon'|\bsX\right]=\sigma^2\bsI$,
therefore
\begin{align*}
  \Cov(\bstildebeta - \bshatbeta,\bshatbeta|\bsX)
  &=
  \sigma^2
  (\bsA-(\bsX'\bsX)^{-1}\bsX')
    \left(\bsX(\bsX'\bsX)^{-1}\right)
  \\
  &=
  \sigma^2
  \bsA \bsX(\bsX'\bsX)^{-1}
  -(\bsX'\bsX)^{-1}
  \\
  \text{Since $\bsA\bsX=\bsI$}\qquad
  &=
  \sigma^2
  \bsI(\bsX'\bsX)^{-1}
  -(\bsX'\bsX)^{-1} \\
  \implies\quad
  \Cov(\bstildebeta - \bshatbeta,\bshatbeta|\bsX)
  &= 0
\end{align*}
(Final Step)
Okay, now compute the variance of $\bstildebeta$:
using the fact that covariance equals zero
\begin{align*}
  \Var(\bstildebeta|\bsX)
  &=
  \Var(\bshatbeta + (\bstildebeta-\bshatbeta)|\bsX) \\
  &=
  \Var(\bshatbeta|\bsX) + \Var(\bstildebeta-\bshatbeta)|\bsX)
  + 2\Cov(\bstildebeta-\bshatbeta,\,\bshatbeta|\bsX)\\
  \implies\quad
  \Var(\bstildebeta|\bsX)
  &=
  \Var(\bshatbeta|\bsX) + \Var(\bstildebeta-\bshatbeta)|\bsX)
\end{align*}
Therefore, the variance of $\bstildebeta$, an arbitrary linear unbiased
estimator, is greater than the variance of $\bshatbeta$ by the positive
semidefinite matrix $\Var(\bstildebeta-\bshatbeta|\bsX)$.
\\
\\
If we want to answer ``How much bigger?'', we can simplify the
expression for the variance using Expression~\ref{betatilde-betahat}:
\begin{align*}
  \Var(\bstildebeta-\bshatbeta)|\bsX)
  &=
  \Var((\bsA-(\bsX'\bsX)^{-1}\bsX')\bsvarepsilon|\bsX) \\
  &=
  (\bsA-(\bsX'\bsX)^{-1}\bsX')
  \;\Var(\bsvarepsilon|\bsX)\;
  (\bsA-(\bsX'\bsX)^{-1}\bsX')' \\
  &=
  \sigma^2 (\bsA-(\bsX'\bsX)^{-1}\bsX')
  (\bsA-(\bsX'\bsX)^{-1}\bsX')'
\end{align*}
Where the last line used the assumption of spherical errors
$\Var(\bsvarepsilon|\bsX)=\sigma^2\bsI$.
\end{proof}


\clearpage
\subsection{$R^2$ and Relatives}

Let $\bsy$ denote the vector of observations for the dependent value,
$\bshaty$ the vector of fitted values, $\bar{\bsy}$ a vector with each
entry equal to the sample mean $\frac{1}{N}\sumnN y_n$, and $\bse$ equal
to the estimated residuals.

\begin{defn}(Uncentered $R^2$)
First, We define the \emph{uncentered $R^2$} via the decomposition
\begin{align*}
  \bsy' \bsy
  &=
  (\bshaty +\bse)'
  (\bshaty +\bse) \\
  &=
  \bshaty' \bshaty
  +2 \bshaty'\bse
  +\bse'\bse \\
  \text{From orthogonality condtion} \quad
  &=
  \bshaty' \bshaty
  +\bse'\bse \\
  \implies\quad
  R^2_{uc}
  &=
  \frac{\bshaty' \bshaty}{\bsy' \bsy}
  = 1- \frac{\bse'\bse}{\bsy' \bsy}
\end{align*}
This is guaranteed to lie in $[0,1]$.
\end{defn}

\begin{defn}(Centered $R^2$)
Next, we define the \emph{centered $R^2$} or
\emph{coefficient of determination}.
\begin{align*}
  SST &= SSM + SSR \\
  (\bsy-\bar{\bsy})'(\bsy-\bar{\bsy})
  &=
  (\bshaty-\bar{\bsy})'(\bsy-\bar{\bsy})
  +
  \bse'\bse \\
  \implies\quad
  R^2_c
  &=
  1-
  \frac{\bse'\bse}{(\bsy-\bar{\bsy})'(\bsy-\bar{\bsy})}
  =
  \frac{(\bshaty-\bar{\bsy})'(\bsy-\bar{\bsy})}{%
      (\bsy-\bar{\bsy})'(\bsy-\bar{\bsy})}
  =
  \frac{\bshaty'\bshaty - \bar{\bsy}'\bar{\bsy}}{%
      \bsy'\bsy - \bar{\bsy}'\bar{\bsy}}
  \\
  &= 1- \frac{SSR}{SST} = \frac{SSM}{SST}
\end{align*}
Note that this guy can go negative if you don't include an intercept
term in the regression.
\end{defn}

\begin{defn}
We now define \emph{adjusted $R^2$}, which penalizes the number of
regressors:
\begin{align*}
  R^2_A
  &= 1 -
  \frac{\bse'\bse/(N-K)}{(\bsy-\bar{\bsy})'(\bsy-\bar{\bsy})/(N-1)}
\end{align*}
Given plain old centered $R^2_c$, it can be derived as
\begin{align*}
  R^2_A =
  1 -
  \left(\frac{N-1}{N-K}\right)
  \left(1 - R^2_c\right)
\end{align*}
\end{defn}

\clearpage
\subsection{Heteroskedasticity Robust Standard Errors}

Without assuming spherical errors (homoskedasticity), recall that
the variance of the OLS estimator is
\begin{align*}
  \Var(\hat{\bsbeta}_{OLS}|\bsX)
  &=
  \left((\bsX'\bsX)^{-1}\bsX'\right)
  \E[\bsvarepsilon\bsvarepsilon' |\bsX]
  \left(\bsX(\bsX'\bsX)^{-1}\right) \\
  %&=
  %(\bsX'\bsX)^{-1}
  %\E[\bsX'\bsvarepsilon\bsvarepsilon'\bsX\;|\;\bsX]
  %(\bsX'\bsX)^{-1}
  %\\
  &=
  (\bsX'\bsX)^{-1}
  \E\left[
  \left(
  \sumnN \varepsilon_n \bsx_n
  \right)
  \left(
  \sumnN \varepsilon_n \bsx_n
  \right)'
  \;\bigg|\;\bsX
  \right]
  (\bsX'\bsX)^{-1} \\
  &=
  (\bsX'\bsX)^{-1}
  \E\left[
  \sumnN
  \sum_{m=1}^N
  \varepsilon_n\varepsilon_m \bsx_n\bsx_m'
  \big|\bsX
  \right]
  (\bsX'\bsX)^{-1} \\
  \implies\quad
  \Var(\hat{\bsbeta}_{OLS}|\bsX)
  &=
  (\bsX'\bsX)^{-1}
  \E\left[
  \sumnN
  \varepsilon_n^2 \bsx_n\bsx_n'
  \;\big|\;\bsX
  \right]
  (\bsX'\bsX)^{-1}
\end{align*}
where got to the last line by using the fact that
$\E[\varepsilon_n\varepsilon_m|\bsX]=0$ for $n\neq m$.
\\
\\
To esitmate this object, we form the natural estimator called the
\emph{sandwich estimator}:
\begin{align*}
  \bshatV(\hat{\bsbeta}_{OLS}|\bsX)
  =
  (\bsX'\bsX)^{-1}
  \left(
  \sumnN
  e_n^2 \bsx_n\bsx_n'
  \right)
  (\bsX'\bsX)^{-1}
\end{align*}
using the estimated errors $e_n$.


\clearpage
\subsection{Omitted Variable Bias}

Suppose that the true model is
\begin{align*}
  \bsy = \bsX\bsbeta + \bsZ \bsgamma + \bsvarepsilon
\end{align*}
but we run the regression
\begin{align*}
  \bsy = \bsX\bsbeta + \bsvarepsilon
\end{align*}
Then the OLS estimates are
\begin{align*}
  \bshatbeta
  &= (\bsX'\bsX)^{-1}\bsX' \bsy \\
  &= (\bsX'\bsX)^{-1}\bsX'
    \left(\bsX\bsbeta + \bsZ \bsgamma + \bsvarepsilon\right) \\
  \bshatbeta - \bsbeta
  &= (\bsX'\bsX)^{-1}\bsX'
    \left(\bsZ \bsgamma + \bsvarepsilon\right)
\end{align*}
This implies bias
\begin{align*}
  \E\left[\bshatbeta - \bsbeta\;|\;\bsX,\bsZ\right]
  &= (\bsX'\bsX)^{-1} \bsX'\bsZ \bsgamma
\end{align*}
The bias is zero if and only if $\bsgamma=0$ (variables in $\bsZ$ do not
affect $\bsy$) or $\bsX'\bsZ=0$ (the ommited variables are orthogonal to
those included).



\clearpage
\subsection{Coefficient Restrictions}

Suppose that we have linear regression model
\begin{align*}
  \bsy = \bsX\bsbeta + \bsvarepsilon
\end{align*}
and we want to estimate $\bsbeta$ while also imposing some restrictions
$\bsR'\bsbeta = \bsr$, where $\bsR\in\R^{K\times C}$ is a matrix of $C$
constraints on the $K$ coefficients. We assume that $\rank(\bsR)=C$
(full column rank), otherwise there would be redundant constraints that
could be removed without effect.
\\
\\
We estimate by constrained least squares
\begin{align*}
  \min_{\bsb} \; &(\bsy-\bsX\bsb)'(\bsy-\bsX\bsb)
  \\
  \text{s.t.} \; &
    \bsR'\bsb = \bsr
\end{align*}
whose solution we call $\bshatbeta_*$, while $\bshatbeta_{OLS}$
represents the unrestricted OLS estimator.
The basic estimates are
\begin{align*}
  \bshatbeta_*
  &=
  \bshatbeta_{OLS} - \left(\bsX'\bsX\right)^{-1}  \bsR
  \left(\bsR'\left(\bsX'\bsX\right)^{-1}  \bsR\right)^{-1}
  \left(\bsR'\bshatbeta_{OLS} -  \bsr\right)
  \\
  \Var[\bshatbeta_*\;|\;\bsX]
  &=
  \underbrace{\sigma^2(\bsX'\bsX)^{-1}}_{\Var(\bshatbeta_{OLS}|\bsX)}
  -
  \sigma^2(\bsX'\bsX)^{-1}  \bsR
  \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'(\bsX'\bsX)^{-1}
  \\
  \Var\left[ \bshatbeta_*-\bshatbeta_{OLS} \;|\;X\right]
  &=
  \sigma^2
  \left(\bsX'\bsX\right)^{-1}  \bsR
  \left(\bsR'\left(\bsX'\bsX\right)^{-1}  \bsR\right)^{-1} \bsR'
  \left(\bsX'\bsX\right)^{-1}
\end{align*}
Since the term subtracted from $\Var[\bshatbeta_{OLS}\;|\;\bsX]$ in the
expression for $\Var[\bshatbeta_*\;|\;\bsX]$ is a positive semidefinite
matrix, the constrained estimates have a smaller variance than the OLS
estimates (though they are biased).


\begin{proof}
Form the Lagrangian
\begin{align*}
  \sL
  &=
  (\bsy-\bsX\bsb)'(\bsy-\bsX\bsb)
  + 2\bslambda' (\bsR'\bsb - \bsr)
\end{align*}
where $\bslambda$ is a vector the same size as $\bsr$ (possibly also length
one, i.e.\ a scalar).
The first order conditions of the Lagrangian with respect to $\bsb$:
\begin{align*}
  \frac{d}{d\bsb}
  \left[
    (\bsy-\bsX\bsb)'(\bsy-\bsX\bsb)
    + 2\bslambda' (\bsR'\bsb - \bsr)
  \right]
  &= -2\bsX'\bsy +2\bsX'\bsX\bsb + 2\bsR\bslambda
  =0
\end{align*}
Solve for $\bsb$
\begin{align}
  \bsb &=
  \left(\bsX'\bsX\right)^{-1} \bsX'\bsy
  - \left(\bsX'\bsX\right)^{-1} \bsR\bslambda
  \notag\\
  \implies\quad
  \bsb &=
  \bshatbeta_{OLS} - \left(\bsX'\bsX\right)^{-1}  \bsR\bslambda
  \label{restricted-b}
\end{align}
To eliminate $\bslambda$, we will multiply through the above expression
by $\bsR'$ and use the constraint $\bsR'\bsb=\bsr$ on the LHS to get
\begin{align}
  %\bsR'\bsb
  %&=
  %\bsR'\bshatbeta_{OLS}
  %- \bsR'\left(\bsX'\bsX\right)^{-1} \bsR\bslambda \notag\\
  %\iff\quad
  \bsr
  &=
  \bsR'\bshatbeta_{OLS}
  - \bsR'\left(\bsX'\bsX\right)^{-1} \bsR\bslambda \notag\\
  \implies\quad
  \bslambda
  &=
  \left(\bsR'\left(\bsX'\bsX\right)^{-1}  \bsR\right)^{-1}
  \left(\bsR'\bshatbeta_{OLS} -  \bsr\right)
  \label{restricted-lambda}
\end{align}
where $\bsR'(\bsX'\bsX)^{-1}\bsR$ is invertible because
$(\bsX'\bsX)^{-1}$ is positive definite and $\bsR$ has full column rank.
Now sub Expression~\ref{restricted-lambda} for $\bslambda$ into
Expression~\ref{restricted-b} for $\bsb$:
\begin{align*}
  \bshatbeta_*
  =
  \bshatbeta_{OLS} - \left(\bsX'\bsX\right)^{-1}  \bsR
  \left(\bsR'\left(\bsX'\bsX\right)^{-1}  \bsR\right)^{-1}
  \left(\bsR'\bshatbeta_{OLS} -  \bsr\right)
\end{align*}
This estimator has expectation
\begin{align*}
  \E\left[ \bshatbeta_* \;|\; \bsX \right]
  &=
  \bsbeta
  -
  \left(\bsX'\bsX\right)^{-1}  \bsR
  \left(\bsR'\left(\bsX'\bsX\right)^{-1}  \bsR\right)^{-1}
  \left( \bsR' \bsbeta -  \bsr\right)
\end{align*}
Next, we compute the variance of the estimator:
\begin{align*}
  \Var[\bshatbeta_*\;|\;\bsX]
  &=
  \E\left[
    \left(\bshatbeta_*- \E[\bshatbeta_*|\bsX]\right)^2
  \;|\;\bsX \right] \\
  &=
  \E\bigg[
  \bigg\{
  \left[
  \bshatbeta_{OLS} - (\bsX'\bsX)^{-1}  \bsR
  \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1}
  \left(\bsR'\bshatbeta_{OLS} -  \bsr\right)
  \right]\\
  &\qquad\quad
  -
  \left[
  \bsbeta -
  (\bsX'\bsX)^{-1}  \bsR \left(\bsR'(\bsX'\bsX)^{-1}\bsR\right)^{-1} \left(\bsR' \bsbeta -\bsr\right)
  \right]
  \bigg\}^2
  \;\big|\;\bsX
  \bigg] \\
  \Var[\bshatbeta_*\;|\;\bsX]
  &=
  \left(
  \bsI - (\bsX'\bsX)^{-1}  \bsR \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'
  \right)
  \E\left[ (\bshatbeta_{OLS}-\bsbeta)^2 \;\big|\;\bsX \right]
  \left(
  \text{Leading term}
  \right)' \\
  &= (\bsI - \bsA)\bsV(\bsI-\bsA)'
  \\
  \text{where}
  \quad
  \bsA &:=
    (\bsX'\bsX)^{-1}  \bsR \big(\bsR'(\bsX'\bsX)^{-1}  \bsR\big)^{-1} \bsR' \\
  \bsV
  &:= \E\left[ (\bshatbeta_{OLS}-\bsbeta)^2 \;\big|\;\bsX \right]
\end{align*}
Note that
$(\bsI-\bsA)\bsV(\bsI-\bsA)'=(\bsV-\bsA\bsV)-(\bsV\bsA'-\bsA\bsV\bsA')$.
We can calculate each of these objects, assuming spherical errors,
otherwise the result doesn't work out nicely
\begin{align*}
  \bsV
  &=
  \E\left[ (\bshatbeta_{OLS}-\bsbeta)^2 \;\big|\;\bsX \right]
  =
  \sigma^2(\bsX'\bsX)^{-1} \\
  \bsA\bsV = (\bsV\bsA')'
  &=
  \sigma^2(\bsX'\bsX)^{-1}  \bsR \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'
  (\bsX'\bsX)^{-1} \\
  \bsA\bsV\bsA'
  &=
  \sigma^2(\bsX'\bsX)^{-1}  \bsR \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'
  (\bsX'\bsX)^{-1}
  \bsR \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'(\bsX'\bsX)^{-1}
  \\
  &=
  \sigma^2(\bsX'\bsX)^{-1}  \bsR
  \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'(\bsX'\bsX)^{-1}
\end{align*}
Hence the variance of the estimator is
\begin{align*}
  \Var[\bshatbeta_*\;|\;\bsX]
  &=
  \underbrace{\sigma^2(\bsX'\bsX)^{-1}}_{\Var(\bshatbeta_{OLS}|\bsX)}
  -
  \sigma^2(\bsX'\bsX)^{-1}  \bsR
  \left(\bsR'(\bsX'\bsX)^{-1}  \bsR\right)^{-1} \bsR'(\bsX'\bsX)^{-1}
\end{align*}
The last one is just a bunch of messy algebra again. It's
straightforward to derive.
\end{proof}


\subsection{Inference under Normal Errors}

\begin{prop}
If we assume
$\bsvarepsilon|\bsX\sim N(0,\sigma^2\bsI)$, then the variance estimator
$s^2$ of $\sigma^2$ has distribution
\begin{align}
  s^2
  = \frac{\bse'\bse}{N-K}
  \sim \frac{\sigma^2}{N-K} \Chi^2_{N-K}
  \label{s2dist}
\end{align}
\end{prop}
\begin{proof}
Since $\bse=\bsM\bsvarepsilon$ where $\bsM$ is the annihilator matrix,
the definition of $s^2$ gives us
\begin{align*}
  (N-K) s^2
  &= \bse' \bse
  = (\bsM\bsvarepsilon)' (\bsM\bsvarepsilon)\\
  &= \sigma^2 \left(\frac{\bsvarepsilon'}{\sigma}\right)\bsM'
    \bsM\left(\frac{\bsvarepsilon}{\sigma}\right)
\end{align*}
Since the matrix $\bsM$ is idempotent, we get
\begin{align*}
  (N-K) \frac{s^2}{\sigma^2}
  &= \left(\frac{\bsvarepsilon'}{\sigma}\right)
    \bsM\left(\frac{\bsvarepsilon}{\sigma}\right)
\end{align*}
Then $\bsvarepsilon/\sigma$, the standardized error vector, is a
multivariate normal distribution.Therefore, the above quadratic form is
$\Chi^2$ with $\rank(\bsM)$ degrees of freedom. But since
$\rank(\bsM)=\trace(\bsM)$ for idempotent matrices like $\bsM$ and
$\trace(\bsM)=N-K$, the desired result follows.
\end{proof}


\begin{defn}(Testing Individual Coefficients)
If we assume $\bsvarepsilon|\bsX \sim N(0,\sigma^2\bsI_N)$, then
\begin{align*}
  \bshatbeta - \bsbeta
  \sim
  N(0,\sigma^2 (\bsX'\bsX)^{-1})
\end{align*}
So we can test whether the $k$th regression coefficient equals
$\beta_{k}^0$ by forming a Z-statistic because, under the null,
\begin{align}
  z_k = \frac{\hat{\beta}_{k} - \beta_{k}^0}{%
      \sqrt{\sigma^2\cdot r_k'(\bsX'\bsX)^{-1}r_k}}
      \sim N(0,1)
  \label{ztest}
\end{align}
where $r_k$ is the $k$th basis element for $\R^K$.

Since $\sigma^2$ is generally unknown, we instead construct a
$t$-statistic using $s^2$, which has a $T_{N-K}$ distribution (see
proof):
\begin{align*}
  t_k = \frac{\hat{\beta}_{k} - \beta_{k}^0}{%
      \sqrt{s^2 \cdot r_k'(\bsX'\bsX)^{-1}r_k}}
    \sim T_{N-K}
\end{align*}
\end{defn}
\begin{proof}
Write out the $t$-statistic:
\begin{align*}
  t_k =
  \frac{\hat{\beta}_{k} - \beta_{k}^0}{%
      \sqrt{s^2 \cdot r_k'(\bsX'\bsX)^{-1}r_k}}
  &=
  \frac{\hat{\beta}_{k} - \beta_{k}^0}{%
      \sqrt{\sigma^2 \cdot r_k'(\bsX'\bsX)^{-1}r_k}}
  \cdot
  \left(
  \sqrt{%
  \frac{s^2}{\sigma^2}
  }
  \right)^{-1} \\
  \text{By Expressions~\ref{s2dist} and \ref{ztest}}\qquad
  &\sim
  N(0,1)
  \cdot
  \left(
  \frac{\Chi^2_{N-K}}{N-K}
  \right)^{-1}
\end{align*}
But the normal and the $\Chi^2$ distribution above are independent
conditional on $\bsX$, so we're done because that's exactly the
definition of a $T_{N-K}$-distribution.
\end{proof}

\begin{defn}($F$-Test)
Suppose we want to test the linear hypothesis
$H_0: \bsR'\bsbeta^0 = \bsr$,
where $\bsR$ is of full column rank and $\bsr$ has dimension $p$,
corresponding to $p$ restrictions.
\\
\\
If we assume spherical errors,
$\bsvarepsilon|\bsX\sim N(0,\sigma^2\bsI_N)$, then under the null, we
know that
\begin{align*}
  \bsR'(\bshatbeta-\bsbeta^0)
  =
  \bsR'\bshatbeta-\bsr
  &\sim N(0,\sigma^2 \bsR'(\bsX'\bsX)^{-1}\bsR)
  %\\
  %\iff\qquad
  %\bsR'\bshatbeta-\bsr
  %&\sim N(0,\sigma^2 \bsR'(\bsX'\bsX)^{-1}\bsR)
\end{align*}
So then we could jointly test whether the following quadratic form is
close to zero
\begin{align*}
  \left(\bsR'\bshatbeta-\bsr\right)'
  \left[\sigma^2 \bsR'(\bsX'\bsX)^{-1}\bsR)\right]^{-1}
  \left(\bsR'\bshatbeta-\bsr\right)
  \sim \Chi_p^2
\end{align*}
But in general, we don't know $\sigma^2$, so we form the $F$-statistic,
which has the following distribution:
\begin{align*}
  F &=
  \frac{%
  \left(\bsR'\bshatbeta-\bsr\right)'
  \left[\bsR'(\bsX'\bsX)^{-1}\bsR)\right]^{-1}
  \left(\bsR'\bshatbeta-\bsr\right)/p
  }{s^2}
  \sim F_{p,N-K}
\end{align*}
\end{defn}

\begin{proof}
Write out the definition of the $F$-statistic:
\begin{align*}
  F &=
  \frac{%
  \left(\bsR'\bshatbeta-\bsr\right)'
  \left[\bsR'(\bsX'\bsX)^{-1}\bsR)\right]^{-1}
  \left(\bsR'\bshatbeta-\bsr\right)/p
  }{s^2} \\
  &=
  \frac{%
  \left(\bsR'\bshatbeta-\bsr\right)'
  \left[\sigma^2\bsR'(\bsX'\bsX)^{-1}\bsR)\right]^{-1}
  \left(\bsR'\bshatbeta-\bsr\right)/p
  }{s^2/\sigma^2} \\
  &=
  \frac{%
    \Chi^2_p/p
  }{\Chi^2_{N-K}/(N-K)}
\end{align*}
But the fraction is precisely the definition of a $F_{p,N-K}$ distribution.
\end{proof}

\begin{prop}
Alternatively, we can motivate the $F$-stat as follows. Consider the sum
of squared errors from an \emph{unrestricted} minimization problem
\begin{align*}
  SSR_U = \min_{\bsb} (\bsy-\bsX\bsb)'(\bsy-\bsX\bsb)
\end{align*}
and the sum of squared errors from an \emph{restricted} minimization
problem with $p = \dim(\bsr)$ constraints:
\begin{align*}
  SSR_R &= \min_{\bsb} (\bsy-\bsX\bsb)'(\bsy-\bsX\bsb) \\
  \text{s.t.} \; \bsR' \bsb &= \bsr
\end{align*}
Then we can compute the $F$ statistic as
\begin{align*}
  F = \frac{(SSR_R-SSR_U)/p}{SSR_U/(N-K)}
\end{align*}
\end{prop}

\begin{prop}
An $F$-distributed random variable has a $\Chi^2$-distributed asymptotic
distribution in the second degree of freedom:
\begin{align*}
  \limn F_{k,n} \quad\dto \quad\Chi_k^2 / k
\end{align*}
\end{prop}
\begin{proof}
Start by writing out the definition of an $F_{k,n}$ random variable:
\begin{align}
  F_{k,n} =
  \frac{%
    \Chi^2_k/k
  }{\Chi^2_{n}/n}
  \label{Ffrac}
\end{align}
Write out the denominator:
\begin{align*}
  \Chi^2_{n}/n
  = \frac{1}{n} \sumin Z_i^2
  \quad\underset{\text{By LLN}}{\pto}\quad
  \E[Z_i^2] = 1
\end{align*}
We can then use Slutsky on Expression~\ref{Ffrac} since the numerator
trivially converges in distribution to $\Chi_k^2/k$, while the
denominator converges in probability to one, hence the whole thing
converges in distribution to $\Chi_k^2/k$.

\end{proof}

\clearpage
\subsection{Weighted and Generalized Least Squares (WLS, GLS)}

\begin{defn}
While OLS minimized the sum of squared residuals, the
\emph{weighted least squares (WLS) estimator} $\bshatbeta_{WLS}$
minimizes the \emph{weighted} sum of squared residuals:
\begin{align}
  \bshatbeta_{WLS}
  %:=&\; \argmin_{\bsb}
  %\sumnN w_n^2(y_n - \bsx_n'\bsb)^2
  = \argmin_{\bsb}
  (\bsy - \bsX\bsb)'\bsW(\bsy - \bsX\bsb)
  \label{wlsmax}
\end{align}
for some (typically symmetric) positive definite weighting matrix
$\bsW$. Note that it could depend on or be constructed from the data.

As a special case, we might choose diagonal $\bsW :=
\diag\{w_1^2,\ldots,w_N^2\}$ given sample weights $\{w_n\}\nN$ so that
\begin{align*}
  \bshatbeta_{WLS}
  = \argmin_{\bsb}
  (\bsy - \bsX\bsb)'\bsW(\bsy - \bsX\bsb)
  :=&\; \argmin_{\bsb}
  \sumnN w_n^2(y_n - \bsx_n'\bsb)^2
\end{align*}
Solving the first order conditions to maximization Problem~\ref{wlsmax}
for the WLS estimator gives
\begin{align*}
  \bshatbeta_{WLS}
  = \left( \bsX'\bsW\bsX \right)^{-1} \bsX'\bsW \bsy
\end{align*}
Substituting in $\bsy=\bsX\bsbeta+\bsvarepsilon$, we get
\begin{align*}
  \bshatbeta_{WLS} - \bsbeta
  = \left( \bsX'\bsW\bsX \right)^{-1} \bsX'\bsW \bsvarepsilon
\end{align*}
The estimator is consistent if the RHS $\pto \bso$.
In the case of a diagonal weighting matrix, this occurs if
$\E[w_n^2\bsx_n\varepsilon_n]=0$, which is implied by
$\E[\varepsilon_n|\bsx_nw_n]=0$.
\end{defn}

Naturally, we might ask ``What is a good choice for $\bsW$?'' Enter GLS.
Gauss-Markov says that under spherical errors
$\Var(\bsvarepsilon|\bsX)=\sigma^2\bsI$ (i.e. conditional
homoskedasticity), the OLS estimator is \emph{efficient}. However, if
the errors are \emph{heteroskedastic} and/or correlated (they often are)
this is not the case. GLS therefore employs a special choice of the
weighting matrix effectively to ``uncorrelate and make homoskedastic''
the data.

But first, a word of caution: OLS has nice asymptotic properties, while
GLS does not. GLS can only really be motivated by the fact that it
\emph{might} perform better in finite samples---and even then, you often
have to know a lot about the data generating process or parameters for
that to be the case. So GLS should only be used with care. OLS, in
general, does something approximately correct most of the time, while
GLS has no such gaurantee.


\begin{prop}\emph{(GLS Estimator)}
Suppose the errors correlated with mean and covariance
\begin{align*}
  \E[\bsvarepsilon|\bsX]= \bso
  \qquad
  \Var(\bsvarepsilon|\bsX)
  = \bsLambda
  = (\bsP'\bsP)^{-1} \neq \sigma^2\bsI
\end{align*}
Then the GLS estimator, defined as,
\begin{align*}
  \bshatbeta_{GLS} =
  \big(\bsX'\bsLambda^{-1}\bsX)^{-1}\bsX'\bsLambda^{-1}\bsy
\end{align*}
has lower variance than the OLS estimator.
\end{prop}
\begin{proof}
Since $\Var(\bsvarepsilon|\bsX)\neq\sigma^2\bsI$, we cannot apply
Gauss-Markov to say that OLS has lowest variance. So let's multiply
through the regression equation by $\bsP$ to get
\begin{align}
  \underbrace{\bstildey}_{\bsP \bsy}
  &=
  \underbrace{\bstildeX\vphantom{\bstildey}}_{\bsP \bsX}
  \bsbeta
  + \underbrace{\bstildevarepsilon\vphantom{\bstildey}}_{\bsP \bsvarepsilon}
  \label{glsmod}
\end{align}
Notice that
\begin{align*}
  \Var(\bstildevarepsilon|\bsX)
  &=
  \Var(\bsP\bsvarepsilon|\bsX)
  =
  \bsP\Var(\bsvarepsilon|\bsX)\bsP'
  =
  \bsP (\bsP'\bsP)^{-1} \bsP'
  = \bsI
\end{align*}
Therefore, the errors in Equation~\ref{glsmod} \emph{are} spherical, so
if we do OLS regression of $\bstildey$ on $\bstildeX$, Gauss-Markov says
the resulting estimator will have the lowest variance:
\begin{align*}
  \bshatbeta
  &= [\bstildeX'\bstildeX]^{-1}\bstildeX'\bstildey
  = [(\bsP\bsX)'(\bsP\bsX)]^{-1}
      (\bsP\bsX)'(\bsP\bsy)
  = [\bsX'\bsP'\bsP\bsX]^{-1}
      \bsX'\bsP'\bsP\bsy
\end{align*}
Making the substitution that $\bsLambda^{-1}=\bsP'\bsP$, we get exactly
$\bshatbeta_{GLS}$.
\end{proof}

\begin{cor}\emph{(GLS with Heteroskedasticity Only)}
Suppose the errors are uncorrelated, but conditionally heteroskedastic
with mean and covariance matrix
\begin{align*}
  \E[\bsvarepsilon|\bsX]= \bso
  \qquad
  \Var(\bsvarepsilon|\bsX)
  = \bsLambda = \text{\emph{diag}}\{\sigma_1^2,\ldots,\sigma^2_N\}
\end{align*}
Then we can see from he above formula that GLS is exactly like doing WLS
with weights $w_n=\sigma^{-1}_n$ in weighting matrix
$\bsW=\bsLambda^{-1}$.
\end{cor}
\begin{rmk}
The weighting structure $w_n=\sigma_n^{-1}$ clearly \emph{overweights}
those observations that have small conditional variance. Intuitively, we
use those observations that are measured most precisely to construct a
more efficient estimator than plain OLS.
\end{rmk}

\begin{defn}(FGLS Estimator)
Of course in practice, we don't know $\bsLambda$ or (in the case of
heteroskedasticity only) the $\{\sigma_n^2\}\nN$ terms that we need to
construct the weighting matrix. So we instead specify functional form
\begin{align*}
  \bsLambda = \bsg(\bsX,\bstheta)
\end{align*}
for some $\bstheta$ that we can either observe or estimate.
Then we plug in the data $\bsX$ along with our estimates of
the parameters $\bshattheta$ to construct
$\bsW=\bshatLambda^{-1}=\bsg(\bsX,\bshattheta)^{-1}$ and compute the GLS
estimator. This is called the \emph{Feasible Generalized Least Squares
(FGLS) Estimator}.
\end{defn}


\clearpage
\section{OLS Asymptotics}


\subsection{Model Assumptions and OLS Estimator}

In the previous section on classical linear regression, we specified the
joint distribution of $(\bsy,\bsX)$ for a fixed, finite sample size. The
distributions were exact.
In this section, we instead specify just enough assumptions about the
underlying data-generating process to derive joint
\emph{asymptotic} distributions which hold only approximately in a
fixed, finite sample.

\begin{defn}(Assumptions)
Here, we state assumptions to be referenced below:
\begin{enumerate}
  \item \emph{Linearity}: $y_n = \bsx_n' \bsbeta + \varepsilon_n$
    where $\bsx_n$ and $\bsbeta$ are $(K\times 1)$ vectors
  \item \emph{Ergodic Stationarity}:
    The $(K+1)$-vector $(y_n, \bsx_n)$ is jointly stationary and
    ergodic. This would be implied by iid, but is a weaker assumption.

  \item \emph{Predetermined Regressors}:
    For all $n$, we have
    \begin{align*}
      \E[\bsx_n (y_n - \bsx'_n \bsbeta)]=\E[\bsx_n \varepsilon_n] =
      \E[\bsg_n]= \bso
      \qquad
      \text{where}\quad
      \bsg_n := \bsx_n\varepsilon_n
    \end{align*}
    This is implied by (but weaker than) assuming
    $\E[\varepsilon_n|\bsx_n]=0$.  It is also implied by (but weaker
    than) assuming strict exogeneity, which requires
    $\E[x_{nk}\varepsilon_m]=0$
    for all $n,m$.  In fact, we rule out only contemporaneous ($n=m$)
    correlation between regressor $\bsx_n$ and error $\varepsilon_n$.
    This allows us to accommodate the AR(1) model and other time series
    models.

  \item \emph{Rank and Finiteness Condition}:
    %$\bsSigma_{\bsx\bsx}:=\E[\bsx_n\bsx_n']$ exists, is finite, and has
    $\E[\bsx_n\bsx_n']$ exists, is finite, and has full rank

  \item \emph{Martingale Difference Sequence
    $\bsg_n:=\bsx_n\varepsilon_n$, i.e. no autocorrelation in $\bsg_n$}:
    We strengthen Assumption 3 above. In particular,
    we suppose $\bsg_n=\bsx_n\varepsilon_n$ has finite variance/second
    moment, i.e.\
    $\Var[\bsg_n]=\E[\bsg_n\bsg_n']=\E[\varepsilon_n\bsx_n\bsx_n']<\infty$, and is a
    martingale difference sequence relative to some filtration
    (the natural filtration works):
    \begin{align}
      \E[\bsg_n|\;
      \bsg_{n-1},\bsg_{n-2},\ldots]
      = \bso
      \label{guncorr}
    \end{align}
    A sufficient condition for $\bsg_n$ to be a MDS is
    \begin{align*}
      \E[\varepsilon_n\;|\;
      \varepsilon_{n-1},
      \varepsilon_{n-2}, \ldots,
      \bsx_n,
      \bsx_{n-1}, \ldots]
      = \bso
    \end{align*}
    By iterated expectations,
    Statement~\ref{guncorr} implies $\E[\bsg_n\bsg_{n-k}']=\bso$ for all
    $k>0$. Together with Assumption 3, Assumption 5 amounts to saying
    that $\bsg_n$ is \emph{not} autocorrelated.
    We will not maintain this assumption for all of the analysis below,
    but it's useful to have a labeled condition that we can refer to
    when we do want to invoke it.
    %For later, we define
    %\begin{align*}
      %\bsS := \E[\bsx_n\varepsilon^2_n\bsx_n']
    %\end{align*}
\end{enumerate}
\end{defn}

\begin{rmk}
This model also allows for \emph{conditional heteroskedasticity}, as
discussed in Section~\ref{sec:discusshetero}. Therefore, we will
develop conditional heteroskedasticity-robust standard errors.

However, we will also have \emph{un}conditional \emph{homo}skedasticity.
This follows because $(y_n,\bsx_n)$ is assumed stationary, which implies
the error term $\varepsilon_n$ is consequently also stationary.
For that to be the case, it must be that the unconditional error
variance is constant across $n$, i.e. $\E[\varepsilon^2_n]=\sigma^2$, a
constant that does not depend upon $n$. That is the definition of
unconditional homoskedasticity.
\end{rmk}


\clearpage
\subsection{%
  Discussion of Autocorrelation in $\bsg_n=\bsx_n\varepsilon_n$ and
  of Asymptotic Variance $\bsOmega$
}

When we derive the asymptotic properities of the OLS estimator below, we
will use a Central Limit Theorem on $\bsg_n=\bsx_n\varepsilon_n$ to say
that
\begin{align*}
  \frac{1}{\sqrt{T}}\sumtT \bsg_n\bsg_n'
  = \frac{1}{\sqrt{T}}\sumtT \varepsilon_n^2\bsx_n\bsx_n'
  \quad\dto\quad \calN(0,\bsOmega)
\end{align*}
The correlation structure of $\bsg_n$ will determine what we take as
the asymptotic variance $\bsOmega$. In particular, if we are willing to
impose Assumption 5, then we are saying $\bsg_n$ has no autocorrelation,
as discussed. In that case
\begin{align*}
  \frac{1}{\sqrt{T}}\sumtT \bsg_n\bsg_n'
  =
  \frac{1}{\sqrt{T}}\sumtT \varepsilon_n^2\bsx_n\bsx_n'
  \quad&\dto\quad
  \calN\big(\bso,
        \underbrace{\E[\bsg_n\bsg_n']}_{=\E[\varepsilon_n^2\bsx_n\bsx_n']}
        \big)
\end{align*}
If, on the other hand, we want to allow for $\bsg_n$ to be
autocorrelated, we know that
\begin{align*}
  \frac{1}{\sqrt{T}}\sumtT \bsg_n\bsg_n'
  \quad&\dto\quad
  \calN(\bso,\bsS)
\end{align*}
where $\bsS$ is the long-run variance.

Because the asymptotic distribution will depend on our assumptions about
the autocorrelation structure of $\bsg_n$, I will simply use $\bsOmega$
from here on out to denote the appropriate asymptotic variance. Whether
$\bsOmega$ is $\E[\bsg_n\bsg_n']$ or the long-run variance is up to the
econometricition to decide. But after that, the econometrics is the
same.


\clearpage
\subsection{Asymptotic Properties of the OLS Estimator}

\begin{prop}\emph{(OLS Estimator)}
The OLS estimator has the following properties
\begin{enumerate}
  \item \emph{Consistent}: Under Assumptions 1-4 above,
    $\bshatbeta_{OLS}\pto \bsbeta$
  \item \emph{Asymptotic Normality}: Under assumptions 1-4
    \begin{align*}
      \sqrt{N}(\bshatbeta_N- \bsbeta) \dto
      N(0,\E[\bsx_i\bsx_i']^{-1}\;\bsOmega\;\E[\bsx_i\bsx_i']^{-1})
      %N(0,\E[\bsx_i\bsx_i']^{-1}\;\E[\varepsilon^2_n\bsx_n\bsx_n']\;\E[\bsx_i\bsx_i']^{-1})
    \end{align*}
    Adding Assumption 5 and assuming conditional homoeskedasticy
    $\E[\varepsilon_n^2|\bsx_n]=\E[\varepsilon_n^2]=\sigma^2$, the
    distribution simplifies
    \begin{align*}
      \sqrt{N}(\bshatbeta_N- \bsbeta) \dto
      N(0,\sigma^2\E[\bsx_i\bsx_i']^{-1})
    \end{align*}

  \item Using Assumptions 5, since $\bshatbeta_{OLS}$ is a consistent
    estimator for $\bsbeta$, the estimated residuals
    $e_n = y_n - \bsx_n' \bshatbeta$ can be used to construct a
    consistent estimator for $\E[\varepsilon_n^2\bsx_n\bsx_n']$:
    \begin{align}
      \frac{1}{N} \sumnN e^2_n \bsx_n \bsx_n'
      \quad\pto\quad
      \E[\varepsilon_n^2\bsx_n\bsx_n']
      \label{consistentomega}
    \end{align}
    provided $\E[\varepsilon_n^2\bsx_n\bsx_n']$ exists and is finite,
    and the regressors have finite fourth moments
    $\E[(x_{ni}x_{nj})^2]<\infty$ for all $i,j$.

  \item
    Let $\bshatOmega$ denote a consistent estimator for the asymptotic
    variance $\bsOmega$.\footnote{%
      If we use Assymption 5, that consistent estimator can be
      Line~\ref{consistentomega} above. If not, see
      Subsection~\ref{sec:hac} on how to construct $\bshatOmega$ for
      long-run variance $\bsOmega$.
    }
    Then we can form a consistent, heteroskedasticity and
    autocorrelation robust (HAC) estimator of the asymptotic variance
    for $\bshatbeta_{OLS}$:
    \begin{align*}
      \bigg[
        \frac{1}{N} \underbrace{\sumnN \bsx_n\bsx_n'}_{\bsX'\bsX}
      \bigg]^{-1}
      \;
      \bshatOmega\;
      \bigg[
        \frac{1}{N} \underbrace{\sumnN \bsx_n\bsx_n'}_{\bsX'\bsX}
      \bigg]^{-1}
      \;\pto\;
      \E[\bsx_i\bsx_i']^{-1}\;\bsOmega\;\E[\bsx_i\bsx_i']^{-1}
    \end{align*}

  \item Under Assumptions 1-4, the OLS error variance estimator $s^2$
    for $\E[\varepsilon_n^2]$ is consistent:
    \begin{align*}
      s^2_N := \frac{\bse' \bse}{N-K}
      =\frac{1}{N-K} \sumnN (y_n-\bsx_n\bshatbeta_N)^2
      \quad\pto\quad \sigma^2 = \E[\varepsilon_n^2]
    \end{align*}
    and asymptotically normal
    \begin{align*}
      \sqrt{N}(s^2_N - \sigma^2)
      \quad\dto\quad
      N\left(0,\E[\varepsilon_n^4]-\E[\varepsilon^2_n]^2\right)
    \end{align*}
    Therefore, if we assume homoskedasticity, i.e.
    $\E[\varepsilon_n^2|\bsx_n]=\E[\varepsilon_n^2]=\sigma^2$, we
    can construct a consistent estimator of the asymptotic variance of
    $\bshatbeta_{OLS}$:
    \begin{align*}
      s_N^2\left(\frac{1}{N} \bsX'\bsX\right)^{-1}
      \quad\pto\quad
      \sigma^2\E[\bsx_i\bsx_i']^{-1}
    \end{align*}
\end{enumerate}
\end{prop}

\clearpage
\begin{proof}
We prove each in turn
\begin{enumerate}
  \item
    Write out the definition of the OLS estimator:
    \begin{align}
      \bshatbeta_N
      &= (\bsX'\bsX)^{-1}\bsX'\bsy
      %= (\bsX'\bsX)^{-1}\bsX'\left(\bsX\bsbeta + \bsvarepsilon\right)
      = \bsbeta + (\bsX'\bsX)^{-1}\bsX'\bsvarepsilon \\
      &= \bsbeta + \left[\frac{1}{N}\sumnN \bsx_n\bsx_n' \right]^{-1}
        \left[\frac{1}{N} \sumnN \bsx_n\bsvarepsilon_n \right]
      \label{beta-asymptotic}
    \end{align}
    By the LLN and the CMT, the two sums converge in probability to
    $\E[\bsx_n\bsx_n']$ and $\E[\bsx_n\bsvarepsilon_n]$, respectively.
    The first is finite by Assumption 4, while the second is zero by
    Assumption 3, so they drop and we have almost sure convergence of
    $\bshatbeta_N$ to $\bsbeta$ by LLN.

  \item
    Rearrange Equation~\ref{beta-asymptotic}, replace
    $\bsx_n\varepsilon_n$ with $\bsg_n$, and multiply by $\sqrt{N}$ to
    get
    \begin{align}
      \sqrt{N}(\bshatbeta_N - \bsbeta)
      &= \left[\frac{1}{N}\sumnN \bsx_n\bsx_n' \right]^{-1}
        \left[\frac{1}{\sqrt{N}} \sumnN \bsg_n \right]
    \end{align}
    Again, the first term on the RHS converges in probability to
    $\E[\bsx_n\bsx_n']$, while the second term
    converges in distribution to $N(0,\bsOmega)$ by an appropriate CLT.
    Then by Slutsky, the desired result follows.

  \item
    Recall that
    \begin{align*}
      (N-K) s^2_N
      &= \bse' \bse
      = \bsvarepsilon'\bsM' \bsM\bsvarepsilon
      = \bsvarepsilon'\bsM\bsvarepsilon \\
      &= \bsvarepsilon'(\bsI - \bsX(\bsX'\bsX)^{-1}\bsX')\bsvarepsilon
      \\
      &= \bsvarepsilon'\bsvarepsilon
        - \bsvarepsilon'\bsX(\bsX'\bsX)^{-1}\bsX'\bsvarepsilon \\
    \end{align*}
    Rearrange and write things out as sums, the above is equivalent to
    \begin{align}
      s^2_N
      &= \underbrace{\frac{N}{N-K}}_{\pto 1}
      \bigg(
        \bigg[
        \underbrace{\frac{1}{N} \sumnN \varepsilon_n^2}_{%
          \pto \E[\varepsilon^2_n]}
         \bigg]
        - \bigg[
        \underbrace{\frac{1}{N} \sumnN \varepsilon_n \bsx_n}_{%
          \pto\E[\varepsilon_n\bsx_n]=0}
        \bigg]'
        \bigg[
        \underbrace{\frac{1}{N} \sumnN \bsx_n\bsx_n'}_{%
          \pto \E[\bsx_n\bsx_n']}
        \bigg]^{-1}
        \bigg[
        \underbrace{\frac{1}{N} \sumnN \varepsilon_n \bsx_n}_{%
          \pto\E[\varepsilon_n\bsx_n]=0}
        \bigg]
      \bigg)
      \label{s2consistent}
    \end{align}
    where $\E[\varepsilon_n\bsx_n]=0$ by predetermined regressiors
    Assumption 3. So in applying the continuous mapping theorem a few
    times, the second term in parentheses will drop and we end up with
    $s^2_N\pto \E[\varepsilon_n^2]$.

    To show that it's asymptotically normal, start with
    Expression~\ref{s2consistent}, subtract
    $\sigma^2=\E[\varepsilon_n^2]$ from both sides, multiply by
    $\sqrt{N}$, and rearrange the RHS a bit to get
    \begin{align*}
      \sqrt{N}(s^2_N - \sigma^2)
      &= \frac{N}{N-K}
      \bigg(
        \bigg[
        \frac{1}{\sqrt{N}} \sumnN \varepsilon_n^2
         \bigg]
      - \frac{N-K}{N}\sqrt{N}\sigma^2
      \\
      &\qquad\qquad\quad
      - \bigg[
        \frac{1}{\sqrt{N}} \sumnN \varepsilon_n \bsx_n
        \bigg]'
        \bigg[
        \frac{1}{N} \sumnN \bsx_n\bsx_n'
        \bigg]^{-1}
        \bigg[
        \frac{1}{N} \sumnN \varepsilon_n \bsx_n
        \bigg]
      \bigg)
    \end{align*}
    Break up the $\sigma^2$ term in parentheses and bring a chunk back
    outside the parentheses
    \begin{align*}
      \sqrt{N}(s^2_N - \sigma^2)
      &= \frac{N}{N-K}
      \bigg(
        \sqrt{N}\bigg[
        \frac{1}{N} \sumnN \varepsilon_n^2
        -\sigma^2
         \bigg]
      \\
      &\qquad\qquad\quad
      - \bigg[
        \frac{1}{\sqrt{N}} \sumnN \varepsilon_n \bsx_n
        \bigg]'
        \bigg[
        \frac{1}{N} \sumnN \bsx_n\bsx_n'
        \bigg]^{-1}
        \bigg[
        \frac{1}{N} \sumnN \varepsilon_n \bsx_n
        \bigg]
      \bigg)
      + \frac{K\sqrt{N}}{N-K}\sigma^2
    \end{align*}
    Accounting for all of the convergences in distribution and
    probability, plus applying slutsky and the continuous mapping
    theorem a bit, we get
    \begin{align*}
      \sqrt{N}(s^2_N - \sigma^2)
      &=
        \sqrt{N}
        \left(
        \frac{\sumnN \varepsilon_n^2}{N}
        -\sigma^2
        \right)
      \quad\dto\quad
      N\left(0,\E[\varepsilon_n^4]-\E[\varepsilon^2]^2\right)
    \end{align*}
\end{enumerate}
\end{proof}

\clearpage
\subsection{Inference}

\begin{prop}
Suppose we have OLS estimator $\bshatbeta_{OLS}$ and we construct the
above-proposed consistent estimator for its asymptotic variance:
\begin{align}
  \bshatSigma :=
  N\cdot
  \left(\bsX'\bsX\right)^{-1}
  \bshatOmega
  \left(\bsX'\bsX\right)^{-1}
  \quad&\pto\quad
  \bsSigma :=
  \E[\bsx_i\bsx_i']^{-1}\;\bsOmega\;\E[\bsx_i\bsx_i']^{-1}
  \notag\\
  \text{where}\quad
  \sqrt{N}(\bshatbeta_{OLS} - \bsbeta)
  \quad &\dto \quad
  N(0,\bsSigma) \label{trobust}
\end{align}
These results and estimators can be used to conduct the following tests:
\begin{enumerate}
  \item \emph{Robust $t$-Ratio for Testing Coefficients}: We can
    test the null hypothesis $H_0:\beta_k = \beta_k^0$ (where $\beta_k$
    is the $k$th element of $\bsbeta$) by constructing the
    \emph{robust $t$-ratio}:
    \begin{align*}
      t_k =
      \frac{\sqrt{N}(\hat{\beta}_k-\beta_k^0)}{\sqrt{\hat{\Sigma}_{kk}}}
      \quad\dto \quad N(0,1)
    \end{align*}
    where $\hat{\Sigma}_{kk}$ is the $k,k$th element of $\bshatSigma$.

  \item \emph{Wald Statistic for Testing Linear Hypotheses}:
    Suppose we want to test null hypothesis
    $H_0: \bsR'\bsbeta = \bsr$
    where $\bsR \in \R^{K \times C}$
    and has full column rank,
    $\rank(\bsR) = C$, i.e.\ no redundant constraints.
    Then
    \begin{align*}
      W = N\cdot
      (\bsR'\bshatbeta-\bsr)'
      \big(
      \bsR'\bshatSigma\bsR
      \big)^{-1}
      (\bsR'\bshatbeta-\bsr)
      \quad\dto\quad
      \Chi^2_C
    \end{align*}
    where $W = N\cdot F$, the $F$-ratio in the finite-sample, exact
    section under homoskedasticity.

  \item \emph{Wald Statistic for Testing Non-Linear Hypotheses}:
    Any nonlinear hypothesis can be written (w.l.o.g.) as
    $H_0: \bsa(\bsbeta) = \bso$ for some function $\bsa:\R^K\ra \R^C$.
    Then
    \begin{align*}
      W :=
      N \cdot \bsa(\bshatbeta_{OLS})'
      \big(
      \bsA \bshatSigma \bsA'
      \big)^{-1}
      \bsa(\bshatbeta_{OLS})
      \quad&\dto\quad
      \Chi^2_C
      \qquad
      \text{where} \quad
      \bsA := \; \frac{\partial}{\partial \bsbeta'}[\bsa(\bsbeta)]
    \end{align*}
    i.e. $\bsA$ is the derivative of $\bsa$ evaluated at true parameter
    vector $\bsbeta$.

  \item \emph{``LR'' Test for Non-Linear Hypotheses}:
    Given Null Hypothesis $H_0: \bsa(\bsbeta) = \bso$ for some function
    $\bsa:\R^K\ra \R^C$ above,  we consider restricted and unrestricted
    efficient GMM estimates
    \begin{align*}
      \bshatbeta
      :=&\; \argmin_{\bsb}\; J(\bsb, \bshatS^{-1})
      %\\
      \qquad
      \bsbarbeta
      :=\; \argmin_{\text{$\bsb$ \emph{:} $\bsa(\bsb) = \bso$}}\;
      J(\bsb, \bshatS^{-1})
    \end{align*}
    where
    $J(\bsb, \bshatW) := N\cdot \bsbarg_N(\bsb)' \bshatW \bsbarg_N(\bsb)$
    and $\bshatS$ is a consistent estimator
    of $\E[\varepsilon_n^2\bsx_n\bsx_n']$, the inverse of the efficient
    weighting matrix. Then
    \begin{align*}
      LR := J(\bsbarbeta, \bshatS^{-1}) - J(\bshatbeta, \bshatS^{-1})
      \quad \dto \quad \Chi^2_C
    \end{align*}
    i.e.\ it has the same asymptotic distribution as the Wald Statistic
    $W$ in Test 3 above.
\end{enumerate}
Tests 3 and 4 both test the nonlinear null hypothesis
$H_0:\bsa(\bsbeta)=\bso$. However, Test 3 is \emph{not} invariant to
different (but equivalent) ways of writing the same null hypothesis.
Test 4 \emph{is} invariant to such rewriting.
\end{prop}
\begin{proof}
Note/recall that $\bshatSigma$ is a consistent estimator of $\bsSigma$.
Now we consider each in turn:
\begin{enumerate}
  \item Convergence in distribution of the $t$-ratio follows from
    Expression~\ref{trobust} above, consistency of $\bshatSigma$ in
    estimating $\bsSigma$, and Slutsky.

  \item
    By Expression~\ref{trobust} and a property of MVN RV's, we get
    \begin{align*}
      \sqrt{N}(\bshatbeta - \bsbeta)
      \; &\dto \;
      N(0,\bsSigma)
      \quad\implies\quad
      \bsR'\sqrt{N}(\bshatbeta - \bsbeta)
      \; \dto \;
      N(0,\bsR'\bsSigma\bsR)
    \end{align*}
    But under the null, $\bsR'\bsbeta = \bsr$, so that
    $\bsR'\sqrt{N}(\bshatbeta - \bsbeta)
    =
    \sqrt{N}(\bsR'\bshatbeta - \bsr)$,
    hence
    \begin{align*}
      \sqrt{N}(\bsR'\bshatbeta - \bsr)
      \; \dto \;
      N(0,\bsR'\bsSigma\bsR)
    \end{align*}
    Moreover, we can write $W$ as
    \begin{align*}
      W &=
      %N\cdot
      %(\bsR'\bshatbeta-\bsr)'
      %\big(
      %\bsR'\bshatSigma\bsR
      %\big)^{-1}
      %(\bsR'\bshatbeta-\bsr) \\
      %&=
      \sqrt{N}
      (\bsR'\bshatbeta-\bsr)'
      \big(
      \bsR'\bshatSigma\bsR
      \big)^{-1}
      \sqrt{N}
      (\bsR'\bshatbeta-\bsr)
    \end{align*}
    This is just a quadratic form of a MVN RV wrapped around its inverse
    variance so that the whole thing converges to a $\Chi^2_C$ variable
    since $C$ is the rank of the variance.

  \item
    Given Expression~\ref{trobust}, Straightforward application of the
    the Delta Method gets us
    \begin{align*}
      \sqrt{N}\big(
        \bsa(\bshatbeta_{OLS})
        -
        \bsa(\bsbeta)
      \big)
      \dto N(\bso,\bsA \bsSigma \bsA')
      \qquad \text{where}\quad
      \bsA := \frac{\partial}{\partial \bsbeta'}[\bsa(\bsbeta)]
    \end{align*}
    Again, $\bsA$ is the first derivative of $\bsa$ evaluated at true
    parameter vector $\bsbeta$. Therefore, we can form the Wald
    statistic by squaring this MVN RV about its inverse variance (using
    the fact that $\bsa(\bsbeta)=0$ under the null) to get
    \begin{align*}
      \sqrt{N}
      \bsa(\bshatbeta_{OLS})'
      \big(
      \bsA \bshatSigma \bsA'
      \big)^{-1}
      \sqrt{N}
      \bsa(\bshatbeta_{OLS})
      \quad \dto\quad \Chi^2_C
    \end{align*}
    But this is precisely the statistic we defined above once you
    combine the $\sqrt{N}$ terms.
\end{enumerate}
\end{proof}

\clearpage
\subsection{Estimation of ARMA Models, AIC, BIC}

\begin{ex}(AR($p$) Model)
Suppose that we have AR($p$) model
\begin{align*}
  y_t &= \phi_1y_{t-1} + \cdots + \phi_py_{t-p}
  + \varepsilon_t
  \qquad \varepsilon_t \sim iid(0,\sigma^2)
\end{align*}
If all roots of the AR polynomial
$\phi(z) = 1-\phi_1z - \cdots - \phi_pz^p$ are outside the unit circle,
then $y_t$ is stationary and ergodic.
Therefore, letting $\bsx_t=(y_{t-1} \; \cdots \; y_{t-p})'$ and
$\bsbeta=(\phi_1\; \cdots \phi_p)'$, we can construct the following
regression,
\begin{align*}
  y_t = \bsx_t'\bsbeta + \varepsilon_t
\end{align*}
Even though $y_t$ is correlated over time and $\bsx_t$ is also
correlated over time, this regression satisfies all of the assumptions
of this section---stationary and ergodic $(y_t,\bsx_t)$, mds
$\bsx_t\varepsilon_t$), etc. So all of the results above apply equally
well to this regression. But we also happen to get a nice simplification
in the asymptotic distribution of our OLS estimator. In particular,
$\varepsilon_t$ is independent of all past $y_t$, so $\varepsilon_t$ is
independent of $\bsx_t$. Therefore, we can break up the middle term in
the asymptotic variance formula:
\begin{align*}
  \E[\bsx_t\bsx_t']^{-1}\;\E[\varepsilon^2_t\bsx_t\bsx_t']\;\E[\bsx_t\bsx_t']^{-1}
  &=
  \E[\bsx_t\bsx_t']^{-1}\;
  \E[\varepsilon^2_t]
  \E[\bsx_t\bsx_t']
  \;\E[\bsx_t\bsx_t']^{-1} \\
  &=
  \sigma^2\E[\bsx_t\bsx_t']^{-1}\;
\end{align*}
which implies that
\begin{align*}
  \sqrt{T}(\bshatbeta_{OLS}-\bsbeta)
  \dto N\big(0,\sigma^2\E[\bsx_t\bsx_t']^{-1}\big)
\end{align*}
\end{ex}


\begin{defn}(AIC and BIC)
We typically select the number of lags $p$ in an AR($p$) model by the
Akaike Information Criterion (AIC) or the Bayesian Information Criterion
(BIC).

Suppose that we want to choose the number of lags $p\in\{1,\ldots,P\}$
given $\{y_t\}_{t=1}^T$.
Then for each $\tilde{p}\in\{1,\ldots,P\}$, we estimate
\begin{align*}
  y_t &= \bsx_t^{(\tilde{p})} {}' \bsbeta^{(\tilde{p})}
  + \varepsilon_t^{(\tilde{p})}
  \qquad t=P+1,\ldots,T
  \\
  \text{where}\quad
  \bsx_t^{(\tilde{p})}
  &= (y_{t-1},\;\ldots,\;y_{t-\tilde{p}})
\end{align*}
as in the above example. By using only observations $t=P+1,\ldots,T$ on
the LHS for each $\tilde{p}$, we ensure that we can estimate and compare
the results across different lag sizes.
Next, compute the sum of squared residuals
\begin{align*}
  SSR(\tilde{p})
  &= \sum_{t=P+1}^T
  \left(y_t - \bsx_t^{(\tilde{p})}{}'\bshatbeta^{(\tilde{p})}\right)^2
\end{align*}
We then choose $p$ the lag size that minimizes either
\begin{align*}
  AIC(\tilde{p})
  &= \ln\left(\frac{SSR(\tilde{p})}{T-P}\right) + \frac{2\tilde{p}}{T-P}
  \\
  BIC(\tilde{p})
  &= \ln\left(\frac{SSR(\tilde{p})}{T-P}\right)
  + \tilde{p}\frac{\ln(T-P)}{T-P}
\end{align*}
As $T\ra\infty$, with probability converging to one, the AIC will
\emph{not} choose too small of a model and the BIC will choose the
\emph{correct} model.
\end{defn}



\clearpage
\subsection{Clustering}

In this section, the total $N$ observations are divided into $M$
clusters/groups, where the $m$th cluster/group has $N_m$ observations so
that
\begin{align*}
  N = \sum_{m=1}^N N_m
\end{align*}
Therefore, any sum over the $N$ individual observations can be rewritten
as a double-sum over clusters and individuals within a cluster. For
example
\begin{align}
  \sum\nN \bsx_n
  = \sum_{m=1}^M \sum_{n=1}^{N_m} \bsx_{mn}
  \label{clusterSums}
\end{align}
where $\bsx_{mn}$ is the $n$ observation within the $m$th cluster.
\\
\\
Lastly, clustering is obviously important when the observations really
do fall into clusters or bins or groups, so that there's good reason to
believe that the error terms are correlated within those clusters. But
it \emph{also} matters in another important and common case.

Let $x_{ni}$ denote the $i$th regressor on the RHS of some plain-vanilla
linear regression
\begin{align*}
  y_n = \bsx_n'\bsbeta + \varepsilon_n
\end{align*}
where the errors $\varepsilon_n$ are definitely \emph{not} correlated
within groups in the data-generating process. To fix concepts, suppose
$y_n$ is an individual's wages, and we include as RHS regressors
in $\bsx_n$ a slew of controls, including occupation and $i$th regressor
$x_{ni}$---average wage for someone who has individual $n$'s occupation.
Many occupations are in the sample, and there are many individuals per
occupation. So we \emph{could} cluster on occuupation if we wanted, but
again, the data-generating process for $\varepsilon_n$ has no
correlation by occupation. Therefore, it's not immediately clear that we
\emph{should} cluster.

However, suppose that in the data-generating process, wages $y_n$
actually depend upon upon $\log(x_{ni})$---the \emph{log} of the
occupation's average wage---rather than just $x_{ni}$. In other words,
we have misspecified the functional form of a regressor that is
\emph{occupation-specific}. Well even if there weren't any
within-occupation correlation for the true errors $\varepsilon_n$, this
misspecification \emph{would} induce within-occupation correlation in
the estimated errors $e_n$. Therefore, we would \emph{want} to cluster
on occupation.  Or more practically, if we're worried about
misspecification of the functional form of some RHS regressor that
varies only at a cluster/group level (not at an individual level), then
we should check whether the usual standard errors and the standard
errors clustered on those groups are wildly different.

Summarizing, we want to cluster (or at the very least, compare regular
and clustered standard errors) if \emph{either}
\begin{enumerate}
  \item We have good reason to believe the true errors $\varepsilon_n$
    are correlated within clusters/groups.
  \item We are worried that we have misspecified the functional form of
    some regressor that varies only at a cluster/group level---not an
    individual level.
\end{enumerate}

\subsubsection{Model with Disjoint Clusters}

The regression model we have in mind is
\begin{align}
  y_{mn} = \bsx_{mn}'\bsbeta + \varepsilon_{mn}
  \qquad \E[\bsx_{mn}\varepsilon_{mn}] = 0
  \label{clusterModel}
\end{align}
where $(y_{mn},\bsx_{mn})$ is the $n$th observation within cluster $m$,
and $\varepsilon_{mn}$ is the corresponding error term.
Now however, we want to allow for correlation of the error terms
\emph{within} a cluster:
\begin{align*}
  \E[\varepsilon_{mn_i}\varepsilon_{mn_j}]\neq 0
  \qquad n_i,n_j \in \{1,\ldots,N_m\}
\end{align*}
However, we will still assume that errors are uncorrelated \emph{across}
independently sampled clusters
\begin{align*}
  \E[\varepsilon_{m_in_k}\varepsilon_{m_jn_\ell}]= 0
  \qquad \forall m_i \neq m_j
  \quad\text{and}
  \quad
  \begin{cases}
    n_k \in \{1,\ldots,N_{m_i}\} \\
    n_\ell \in \{1,\ldots,N_{m_j}\} \\
  \end{cases}
\end{align*}
Therefore, we need to modify the asymptotic results for the OLS
estimator accordingly. This is accomplished by changing the unit of
observation. In particular, the basic unit of observation is no longer
the $N$ total \emph{observations} that we send to infinity to derive the
asymptotics, but instead the $M$ independently sampled \emph{clusters}
that we will send to infinity to derive the asymptotics.

Given this, we rewrite the OLS estimator as
\begin{align}
  \bshatbeta_{OLS}
  = (\bsX'\bsX)^{-1}\bsX'\bsy
  &=
  \left[
    \sumnN \bsx_n\bsx_n'
  \right]^{-1}
  \left[
    \sumnN
    \bsx_ny_n
  \right] \notag\\
  \text{Using~\ref{clusterSums}:}\qquad
  &=
  \left[
    \frac{1}{M}
    \sum_{m=1}^M \left(\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'\right)
  \right]^{-1}
  \left[
    \frac{1}{M}
    \sum_{m=1}^M \left(\sum_{n=1}^{N_m} \bsx_{mn}y_{mn} \right)
  \right] \notag\\
  \implies\quad
  \bshatbeta_{OLS} - \bsbeta
  &=
  \left[
    \frac{1}{M}
    \sum_{m=1}^M \left(\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'\right)
  \right]^{-1}
  \left[
    \frac{1}{M}
    \sum_{m=1}^M \left(\sum_{n=1}^{N_m} \bsx_{mn}\varepsilon_{mn} \right)
  \right]
  \label{clusterSamplingError}
\end{align}
Sending the number of clusters $M\ra\infty$ gives consistency
\begin{align*}
  \bshatbeta_{OLS} - \bsbeta
  \quad\pto\quad
    \E\left[\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'\right]^{-1}
    \E\left[\sum_{n=1}^{N_m} \bsx_{mn}\varepsilon_{mn} \right]
  = 0
\end{align*}
using the fact that $\E[\bsx_{mn}\varepsilon_{mn}]=0$ for all $m,n$.
We can then derive an asymptotic distribution from the LLN, CLT,
Slutsky, and Expression~\ref{clusterSamplingError} for the sampling
error:
\begin{align*}
  \sqrt{M}\left(\bshatbeta_{OLS} - \bsbeta\right)
  &=
  \left[
    \frac{1}{M}
    \sum_{m=1}^M \left(\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'\right)
  \right]^{-1}
  \left[
    \frac{1}{\sqrt{M}}
    \sum_{m=1}^M \left(\sum_{n=1}^{N_m} \bsx_{mn}\varepsilon_{mn} \right)
  \right] \\
  \text{as $M\ra\infty$}\qquad
  &\dto
  \E\left[\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'\right]^{-1}
  \cdot
  N\left(0,\;
    \E\left[\,
      \left(\sum_{n=1}^{N_m} \bsx_{mn}\varepsilon_{mn}\right)
      \left(\sum_{n=1}^{N_m} \bsx_{mn}\varepsilon_{mn}\right)'
    \, \right]
  \right)
\end{align*}
%If $\bsSigma$ is the asymptotic variance, then we form the natural
%estimator $\bshatSigma$ given the sample using the estimated errors
%$e_{mn}$ rather than the unobserved true errors $\varepsilon_{mn}$:
%\begin{align*}
  %\bshatSigma
  %&=
  %\left(
  %\frac{1}{M}
  %\sum_{m=1}^M
  %\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'
  %\right)^{-1}
  %\left(
  %\frac{1}{M}
  %\sum_{m=1}^M
  %\left(\sum_{n=1}^{N_m} \bsx_{mn}e_{mn}\right)
  %\left(\sum_{n=1}^{N_m} \bsx_{mn}e_{mn}\right)'
  %\right)
  %\left(
  %\frac{1}{M}
  %\sum_{m=1}^M
  %\sum_{n=1}^{N_m} \bsx_{mn}\bsx_{mn}'
  %\right)^{-1}
  %\\
  %&=
  %M\cdot
  %\left( \bsX'\bsX \right)^{-1}
  %\left(
  %\sum_{m=1}^M
  %\left(\sum_{i=1}^{N_m} \bsx_{mi}e_{mi}\right)
  %\left(\sum_{j=1}^{N_m} \bsx_{mj}e_{mj}\right)'
  %\right)
  %\left( \bsX'\bsX \right)^{-1}
  %\\
  %&=
  %M\cdot
  %\left( \bsX'\bsX \right)^{-1}
  %\left(
  %\sum_{m=1}^M
  %\sum_{i=1}^{N_m}
  %\sum_{j=1}^{N_m}
  %e_{mi}e_{mj}\bsx_{mi}\bsx_{mj}'
  %\right)
  %\left( \bsX'\bsX \right)^{-1}
%\end{align*}
%Since



\clearpage
\subsubsection{Model with Nested Clusters}

Suppose that we have nested clusters, e.g.\ counties within states. You
should cluster on the \emph{aggregate} or \emph{coarsest} group, e.g.\
states.

First, recall that the asymptotics of the last subsection hold only if
the clusters are \emph{independently} sampled. Almost certainly,
counties within states are correlated, so clustering on counties
wouldn't satisfy the independent sampling of clusters. Second, we made
no assumption on the error structure \emph{within} a cluster. It can be
correlated in any arbitrary way, so this really can capture whatever
correlation structure we need at the finer level.


\subsubsection{Model with Non-Nested Overlapping Clusters}

The regression model we have in mind is
\begin{align*}
  y_n = \bsx_n'\bsbeta + \varepsilon_n
  \qquad \E[\bsx_n\varepsilon_n] = 0
\end{align*}
However, there are $M$ possibly-overlapping clusters such that
\begin{align*}
  \text{Observations $n_i,n_j$ share some (any) cluster}
  \quad\implies\quad
  \E[\varepsilon_{n_i}\varepsilon_{n_j}] \neq 0
\end{align*}
We can derive an expression for the finite-sample covariance matrix,
starting from the standard expression:
\begin{align*}
  \bshatbeta_{OLS} - \bsbeta
  &= (\bsX'\bsX)^{-1}\bsX'\varepsilon
  %=
  %\left[
    %\sumnN \bsx_n\bsx_n'
  %\right]^{-1}
  %\left[
    %\sumnN \varepsilon_n\bsx_n
  %\right]
  \\
  \implies\quad
  \Var(\bshatbeta_{OLS} - \bsbeta|\bsX)
  &=
  (\bsX'\bsX)^{-1}
  \,
  \E[\bsX'\bsvarepsilon\bsvarepsilon'\bsX|\bsX]
  \,
  (\bsX'\bsX)^{-1} \\
  &=
  (\bsX'\bsX)^{-1}
  \E\left[
  \left(
    \sumnN \varepsilon_n\bsx_n
  \right)
  \left(
    \sumnN \varepsilon_n\bsx_n'
  \right)
  \big|\bsX
  \right]
  (\bsX'\bsX)^{-1}
  \\
  &=
  (\bsX'\bsX)^{-1}
  \E\left[
  \sumiN
  \sum_{j=1}^N
  \varepsilon_i\varepsilon_j\bsx_i\bsx_j'
  \cdot
  \mathbf{1}_{\{\text{$i,j$ share some cluster}\}}
  \big| \bsX
  \right]
  (\bsX'\bsX)^{-1}
\end{align*}
We can form an estimator of this object by using the estimated errors
$e_n$ (since the true errors $\varepsilon_n$ are unobserved) in the
middle term:
\begin{align*}
  \widehat{\Var}(\bshatbeta_{OLS} - \bsbeta|\bsX)
  &=
  (\bsX'\bsX)^{-1}
  \left(
  \sumiN
  \sum_{j=1}^N
  e_ie_j\bsx_i\bsx_j'
  \cdot
  \mathbf{1}_{\{\text{$i,j$ share some cluster}\}}
  \right)
  (\bsX'\bsX)^{-1}
\end{align*}


\clearpage
\section{Single Equation Endogenous Regressor Model}

\subsection{Model Assumptions}

\begin{defn}(Predetermined vs. Endogenous)
Suppose we have model $y_n = \bsx_n'\bsbeta + \varepsilon_n$.
Let $\bsw_n$ be any set of covariates associated with obs.
$n$ (including potentially elements of $\bsx_n$).
Then we call random vector $\bsw_n$ \emph{predetermined} if
$\E[\varepsilon_n\bsw_n]=\bso$, \emph{endogenous} if
$\E[\varepsilon_n\bsw_n]\neq \bso$.
\end{defn}

\begin{defn}(Model Assumptions)
We have dependent variable $y_n$ and vector of regressors
$\bsx_n\in\RK$. We also introduce vector $\bsz_n\in\RL$ of
instruments. Vectors $\bsx_n$ and $\bsz_n$ might have elements in
common.\footnote{%
  The extreme case $\bsx_n=\bsz_n$ is just OLS, though we obviously
  really care about the case where $\bsx_n\neq \bsz_n$.
}
Assumptions:
\begin{enumerate}
  \item \emph{Linearity}: $y_n = \bsx_n' \bsbeta + \varepsilon_n$
    where $\bsx_n,\bsbeta\in\RK$.

  \item \emph{Ergodic Stationarity}:
    The unique and nonconstant elelements of $(y_n, \bsx_n, \bsz_n)$ are
    jointly stationary and ergodic. This is implied by iid, but weaker.

  \item
    \emph{Orthogonality Conditions and Predetermined Instruments}:
    While we allow for endogenous regressors, i.e.
    $\E[\varepsilon_n\bsx_n]\neq\bso$, we assume the vector of
    instruments is \emph{predetermined}:
    \begin{align*}
      \E[\bsz_n\varepsilon_n]
      =\E[\bsz_n (y_n - \bsx'_n \bsbeta)]
      = \bso
    \end{align*}
    Thes are the moment conditions we will exploit in GMM to
    estimate $\bsbeta$.
    As a result, any regressor in $\bsx_n$ uncorrelated with
    $\varepsilon_n$ (i.e.\ predetermined, hence a valid instrument)
    should also be included in $\bsz_n$ so that we use as many valid
    moment conditions as possible in estimation.

  \item \emph{Rank \& Order Conditions for Identification}:
    $\E[\bsz_n\bsx_n']$ is finite with full column rank $K$.

    First, \emph{rank condition} $\rank(\E[\bsz_n\bsx_n'])=K$ ensures
    that the instruments and regressors are correlated and the IV
    estimator well-defined. Second, Assumptions 1 and 3 imply
    \begin{align}
      %\E[\bsz_n\varepsilon_n]=
      %\E[\bsz_n (y_n - \bsx'_n \bsbeta)]&=\bso
      %\quad \iff \quad
      \E[\bsz_n y_n] =
      \E[\bsz_n\bsx'_n] \bsbeta
      \label{rankandorder}
    \end{align}
    i.e.\ true $\bsbeta$ is \emph{a particular} solution of the $L$
    equations of (\ref{rankandorder}) in $K$ unknowns. The rank
    condition then also ensures true $\bsbeta$ is the \emph{only}
    solution, i.e.\ $\bsbeta$ is identified.

    A necessary condition for the rank condition is the
    \emph{order condition for identification}:
    \begin{align*}
      \dim(\bsx_n) = K \leq L = \dim(\bsz_n)
    \end{align*}
    i.e. more instruments than regressors. Each element of $\bsx_n$ must
    either be predetermined/non-endogenous (in both $\bsx_n$ and
    $\bsz_n$) or have an instrument to circumvent the endogeneity
    problem and estimate the corresponding element of $\bsbeta$
    correctly.

    If $K=L$, the model is \emph{exactly} identified. There are as many
    instruments as regressors, and we use Method of Moments to construct
    an IV estimator. If $K< L$, the model is \emph{overidentified},
    and we use GMM to estimate parameters. If $K>L$, the model is
    \emph{underidentified}; you are shit out of luck until you find more
    instruments.

  \item \emph{Martingale Difference Sequence $\bsz_n\varepsilon_n$}:
    We strengthen Assumption 3, supposing that
    $\bsz_n\varepsilon_n$ has finite second moment,
    $\E[\varepsilon_n^2\bsz_n\bsz_n']<\infty$, and is a martingale
    difference sequence,
    \begin{align*}
      \E[\bsz_n\varepsilon_n\;|\;
      \bsz_{n-1}\varepsilon_{n-1},\bsz_{n-2}\varepsilon_{n-2},\ldots]
      = \bso
    \end{align*}
\end{enumerate}
\end{defn}

\subsection{Method of Moments IV Estimator, Exactly Identified Case}

\begin{defn}(Method of Moments IV Estimator, Exactly Identified Case)
Take Assumptions 1-5 above and assume additionally that $K=L$ so there
are as many regressors as instruments. Then, as mentioned for
Expression~\ref{rankandorder}, Assumptions 1 and 3 imply
\begin{align*}
  \E[\bsz_n\varepsilon_n]=
  \E[\bsz_n (y_n - \bsx'_n \bsbeta)]&=\bso
  \quad \iff \quad
  \E[\bsz_n\bsx'_n] \bsbeta = \E[\bsz_n y_n]
\end{align*}
Assumption 3, the Rank Condition for Identification, the ensures that we
can solve for $\bsbeta$:
\begin{align*}
  \bsbeta = \E[\bsz_n\bsx'_n]^{-1}\E[\bsz_n y_n]
\end{align*}
Therefore, we can construct a method of moments estimator
$\bshatbeta_{IV}$, also called the
\emph{instrumental variables estimator}, by constructing the sample
analog:
\begin{align*}
  \bshatbeta_{IV} =
  \left(\frac{1}{N}\sumnN\bsz_n\bsx'_n\right)^{-1}
  \left(\frac{1}{N}\sumnN \bsz_n y_n\right)
  =
  (\bsZ'\bsX)^{-1}\bsZ'\bsy
\end{align*}
Substituting in $y_n=\bsx_n'\bsbeta + \varepsilon_n$, we can derive
\begin{align*}
  %\bshatbeta_{IV}
  %&=
  %\left(\frac{1}{N}\sumnN\bsz_n\bsx'_n\right)^{-1}
  %\left(\frac{1}{N}\sumnN \bsz_n (\bsx_n'\bsbeta + \varepsilon_n)\right)
  %\\
  %&=
  %\left(\frac{1}{N}\sumnN\bsz_n\bsx'_n\right)^{-1}
  %\left[
  %\left(
  %\frac{1}{N}
  %\sumnN \bsz_n\bsx_n'\right)\bsbeta
  %+
  %\left(
  %\frac{1}{N}
  %\sumnN \bsz_n \varepsilon_n\right)
  %\right]
  %\\
  %\implies\quad
  \bshatbeta_{IV} - \bsbeta
  &=
  \left(\frac{1}{N}\sumnN\bsz_n\bsx'_n\right)^{-1}
  \left( \frac{1}{N} \sumnN \bsz_n \varepsilon_n\right)
\end{align*}
By orthogonality Assumption 3, the estimator is unbiased. We can also
derive an asymptotic distribution through LLN, CLT, and Slutsky:
\begin{align*}
  \sqrt{N}\left(\bshatbeta_{IV} - \bsbeta\right)
  &\quad\dto\quad
  \E[\bsz_n\bsx'_n]^{-1}
  \cdot
  \calN\left(0,\;\E[\varepsilon_n^2\bsz_n\bsz_n']\right)
\end{align*}
Note that if $\bsz_n=\bsx_n$ so that all of the regressors are
predetermined (no endogeneity problems), this reduces to the OLS
estimator.
\end{defn}


\clearpage
\subsection{GMM Estimator, Overidentified Case}
\label{subsec:EndogenousRegressorGMM}

Take Assumptions 1-5 above and assume additionally that $K<L$ so there
are more instruments than regressors.
Next, by casting things in standard GMM notation, we can rewrite the
moment conditions as
\begin{align*}
  \bsg(\bsbeta) :=
  \E[\bsg_n(\bsbeta)]=\bso
  \qquad\text{where}\quad
  \bsg_n(\bsb) :=
  \bsz_n(y_n-\bsx_n'\bsb)
  = \bsz_n\varepsilon_n
\end{align*}
To construct the GMM estimator, we construct the sample analog of
$\bsg(\bsb)$:
\begin{align*}
  \bsbarg_N(\bsb)
  :=&\; \frac{1}{N}\sumnN \bsg_n(\bsb)
  =
  \frac{1}{N}\left[
  \left(\sumnN \bsz_n\bsx'_n \right)\bsb
  - \sumnN \bsz_n y_n
  \right]
  %\\
  %=&\;
  =
  \frac{1}{N}\left(\bsZ'\bsX\bsb - \bsZ'\bsy\right)
\end{align*}

\begin{prop}\emph{(GMM Estimator, Overidentified Case)}
Given weighting matrix $\bshatW$, the GMM estimator is given by
\begin{align*}
  \bshatbeta(\bshatW)
  &= \argmin_{\bsb} \; \bsbarg_N(\bsb)' \, \bshatW \, \bsbarg_N(\bsb)
  = \argmin_{\bsb} \;
    \left(\bsZ'\bsX\bsb - \bsZ'\bsy\right)'
    \bshatW
    \left(\bsZ'\bsX\bsb - \bsZ'\bsy\right)
\end{align*}
%Simplifying and removing constant terms that don't depend upon $\bsb$,
%we get
%\begin{align*}
  %\bshatbeta(\bshatW)
  %&= \argmin_{\bsb} \;
    %\bsb'\bsX'\bsZ\bshatW \bsZ'\bsX\bsb
    %- \bsb'\bsX'\bsZ\bshatW\bsZ'\bsy
    %- \bsy'\bsZ\bshatW \bsZ'\bsX\bsb
%\end{align*}
Since everything is linear, we can take the first order conditions to
the maximization problem and solve for $\bshatbeta(\bshatW)$ to get the
following closed-form expression for the GMM estimator:
\begin{align}
  %\bso &= 2\bsX'\bsZ\bshatW \bsZ'\bsX\bsb - 2\bsX'\bsZ\bshatW\bsZ'\bsy
  %\\
  %\implies\quad
  \bshatbeta(\bshatW) &=
  \left(\bsX'\bsZ\bshatW \bsZ'\bsX\right)^{-1}\bsX'\bsZ\bshatW\bsZ'\bsy
  \label{sendog:gmm}
\end{align}
\end{prop}

%\begin{prop}\emph{(Sampling Error)}
%Sub $\bsy = \bsX\bsbeta +\bsvarepsilon$ into the expression for
%$\bshatbeta(\bshatW)$ to get
%\begin{align*}
  %%\bshatbeta(\bshatW) &=
  %%\left(\bsX'\bsZ\bshatW \bsZ'\bsX\right)^{-1}\bsX'\bsZ\bshatW\bsZ'
  %%\left(\bsX\bsbeta +\bsvarepsilon\right) \\
  %%\implies\quad
  %\bshatbeta(\bshatW) - \bsbeta
  %&=
  %\left(\bsX'\bsZ\bshatW
  %\bsZ'\bsX\right)^{-1}\bsX'\bsZ\bshatW\bsZ'\bsvarepsilon \\
  %&=
  %\left[
    %\left(\frac{1}{N} \sumnN \bsx_n \bsz_n'\right)
    %\bshatW
    %\left(\frac{1}{N} \sumnN \bsz_n\bsx_n' \right)
  %\right]^{-1}
  %\left[
    %\left(\frac{1}{N} \sumnN \bsx_n \bsz_n'\right)
    %\bshatW
    %\left(\frac{1}{N} \sumnN \bsz_n \varepsilon_n \right)
  %\right]
%\end{align*}
%\end{prop}

\begin{prop}\emph{(Asymptotic Distribution)}
\label{prop:gmmasymptotics}
Since this is a GMM estimator, it is consistent. Assuming
$\bshatW\pto\bsW$, we can also apply the standard formulas to get the
asymptotic distribution:
\begin{align*}
  \sqrt{N}\left(\bshatbeta(\bshatW) - \bsbeta\right)
  \quad &\dto \quad
  (\bsG'\bsW\bsG)^{-1}\bsG\bsW\cdot
  \calN(\bso,\bsOmega) \\
  \text{where}\quad
  \bsOmega &=
    \Var[\bsg_n(\bsbeta)] =
    \E[\bsg_n(\bsbeta)\bsg_n(\bsbeta)']
    = \E[\varepsilon_n^2 \bsz_n\bsz_n']
  \\
  \bsG &=
  \E\left[
    \frac{\partial \bsg_n(\bsbeta)}{\partial \bsbp}
  \right]
  =
  \E\left[
    \bsz_n\bsx_n'
  \right]
  %\\
  %\cdot
  %\big[
  %N\big(0,\;
  %\underbrace{%
    %\E[\varepsilon_n^2 \bsz_n\bsz_n']
  %}_{\bsS}
  %\big)
  %\big]
\end{align*}
Given our GMM estimate $\bshatbeta(\bshatW)$ from
Expression~\ref{sendog:gmm}, we can construct a consistent estimator for
the asymptotic variance by taking sample analogs of the expectaions:
\begin{align*}
  \bshatOmega &=
  \frac{1}{N}\sumnN e_n^2\bsz_n\bsz_n'
  =
  \frac{1}{N}\sumnN \big(y_n-\bsx_n'\bshatbeta(\bshatW)\big)^2\bsz_n\bsz_n'
  \\
  \bshatG
  &=
  \frac{1}{N}\sumnN
  \bsz_n\bsx_n'
  = \frac{1}{N}\bsZ'\bsX
\end{align*}
Invoke the LLN and CMT to conclude that
$\big(\bshatG'\bshatW\bshatG\big)^{-1}
\bshatG\bshatW\bshatOmega
\bshatW\bshatG'
\big(\bshatG'\bshatW\bshatG\big)^{-1}$
%\quad\pto\quad
%\big(\bsG'\bsW\bsG\big)^{-1}
%\bsG\bsW\bsOmega
%\bsW\bsG'
%\big(\bsG'\bsW\bsG\big)^{-1}
is a consistent estimator for the corresponding ``hat-less'' the
asymptotic variance.
\end{prop}

\begin{prop}\emph{(Consistent Estimation of Error Variance)}
Using $\bshatbeta(\bshatW)$, a consistent estimator of $\bsbeta$, we can
obtain a consistent estimator of the error variance
\begin{align*}
  \frac{1}{N} \sumnN e_n^2
  =
  \frac{1}{N} \sumnN \big(y_n - \bsx_n'\bshatbeta(\bshatW)\big)^2
  \quad\pto\quad
  \E[\varepsilon_n^2]
\end{align*}
\end{prop}


\clearpage
\subsection{Efficient GMM Estimator}

\begin{defn}(Two-Stage Efficient GMM Estimator, Overidentified Case)
As is always the case with GMM, the estimate $\bshatbeta(\bshatW)$
depends crucially on our choice of $\bshatW$, and there exists an
efficient (lowest asymptotic variance) choice of weighting matrix equal
to $\bsOmega^{-1}$, where $\bsOmega=\Var[\bsg_n(\bsbeta)]$. In this
section's particular case of single-equation linear regression with
endogenous regressors, recall that $\bsg_n(\bsbeta)=\varepsilon_n\bsz_n$
so that $\bsOmega=\Var[\bsg_n(\bsbeta)]
=\E[\varepsilon^2_n\bsz_n\bsz_n']$.
\\
\\
Of course, we can't just set $\bshatW=\bsOmega^{-1}$ since we don't know
$\bsOmega$, which depends upon unknown $\bsbeta$ (through
$\varepsilon_n$). Therefore, we have to employ the standard two-step
procedure to estimate a consistent $\bshatOmega\pto\bsOmega$ so that we
can use $\bshatOmega^{-1}$ as our weighting matrix. Proceed as follows:
\begin{enumerate}[label=(\roman*)]
  \item First, pick any arbitrary weighting matrix $\bshatW_1$ whose
    probability limit is a symmetric positive definite matrix and
    construct the first-stage GMM estimate:
    \begin{align*}
      \bshatbeta(\bshatW_1) &=
      \argmin_{\bsb}
      \bsbarg_N(\bsb)' \, \bshatW_1 \, \bsbarg_N(\bsb)
      \qquad\quad
      \text{where}\quad
      \bsbarg_N(\bsb)
      = \frac{1}{N} \sumnN (y_n-\bsx_n'\bsb) \bsz_n
    \end{align*}
    Common choices for $\bshatW_1$ include $\bsI$ or $N\cdot
    (\bsX'\bsX)^{-1}$ (the latter being the efficient choice under
    the strong assumption of homoskedasticity, as we will see later).

  \item Use $\bshatbeta(\bsW_1)$ to compute residuals
    $e_n = y_n - \bsx_n'\bshatbeta(\bsW_1)$, and compute a consistent
    estimator of the efficient weighting matrix
    \begin{align*}
      \bshatW_2 = \left(\frac{1}{N}\sumnN e_n^2\bsz_n\bsz_n'\right)^{-1}
      \quad\pto\quad \E[\varepsilon_n\bsz_n\bsz_n']^{-1}
      = \bsOmega^{-1}
    \end{align*}
    Then construct the second stage efficient GMM estimate
    \begin{alignat*}{3}
      \bshatbeta(\bshatW_2) &=
      \argmin_{\bsb}\;
      \bsbarg_N(\bsb)' \, \bshatW_2 \, \bsbarg_N(\bsb)
      %\qquad\quad
      %&\bsbarg_N(\bsb)
        %&:= \frac{1}{N} \sumnN (y_n-\bsx_n'\bsb) \bsz_n \\
      %\text{where}\quad
      %\bshatW_2 &=
      %\left(\frac{1}{N}\sumnN e_n^2\bsz_n\bsz_n'\right)^{-1}
      %&e_n &:= y_n - \bsx_n'\bshatbeta(\bsW_1)
    \end{alignat*}
\end{enumerate}
While \emph{both} $\bshatbeta(\bshatW_1)$ and $\bshatbeta(\bshatW_2)$
consistently estimate $\bsbeta$, $\bshatbeta(\bshatW_2)$ has lower
asymptotic variance.
Using $\bshatW_2\pto \E[\varepsilon_n^2\bsz_n\bsz_n']^{-1}=\bsOmega^{-1}$,
if we sub this into the formulas in the previous subsection, the
asymptotic distribution of our second-stage estimator dramatically
simplifies to
\begin{align*}
  \sqrt{N}\left(\bshatbeta(\bshatW_2) - \bsbeta\right)
  \quad \dto \quad
  \calN\left(
    \bso,\;
    \big(
    \bsGp
    \bsOmega^{-1}
    \bsG
    \big)^{-1}
  \right)
\end{align*}
To form a consistent estimator for the asymptotice variance, simply
take $\big(\bshatG'\bshatOmega^{-1}\bshatG\big)^{-1}$ where
\begin{align*}
  \bshatOmega &=
  \frac{1}{N}\sumnN e_n^2\bsz_n\bsz_n'
  =
  \frac{1}{N}\sumnN \big(y_n-\bsx_n'\bshatbeta(\bshatW_2)\big)^2\bsz_n\bsz_n'
  \\
  \bshatG
  &=
  \frac{1}{N}\sumnN
  \bsz_n\bsx_n'
  = \frac{1}{N}\bsZ'\bsX
\end{align*}
\end{defn}

\clearpage
\subsection{2SLS and Efficient GMM Estimation under Conditional Homoskedasticity}


\begin{defn}(2SLS Estimator)
If we take weighting matrix $\bshatW=(\bsZ'\bsZ)^{-1}$, the resulting
GMM estimator is called the
\emph{two-stage least squares (2SLS) estimator}:
\begin{align*}
  \bshatbeta_{2SLS}
  := \bshatbeta\left((\bsZ'\bsZ)^{-1}\right)
  =
  \left(
  \bsX'\bsZ
  \left(\bsZ'\bsZ\right)^{-1}
  \bsZ'\bsX
  \right)^{-1}
  \bsX'\bsZ\left(\bsZ'\bsZ\right)^{-1}\bsZ'\bsy
\end{align*}
But there are many different ways we could write this, each with a
different interpretation. So first define
$\bsP = \bsZ \left(\bsZ'\bsZ\right)^{-1} \bsZ'$,
the projection matrix for a regression of some variable on the
instruments $\bsz_n$.
First, recall that this matrix is idempotent and symmetric.
Second, use it to define $\bshatX = \bsP\bsX$, the fitted values from
regressing $\bsx_n$ on $\bsz_n$.
Then we can write
\begin{align*}
  \bshatbeta_{2SLS}
  := \bshatbeta\left((\bsZ'\bsZ)^{-1}\right)
  &\underset{(a)}{=}
  \left(
  \bsX'\bsZ
  \left(\bsZ'\bsZ\right)^{-1}
  \bsZ'\bsX
  \right)^{-1}
  \bsX'\bsZ\left(\bsZ'\bsZ\right)^{-1}\bsZ'\bsy
  \\
  &\underset{(b)}{=}
  \left(
  \bsX'\bsP\bsX
  \right)^{-1}
  \bsX'\bsP\bsy
  =
  \left(
  \bshatX'\bsX
  \right)^{-1}
  \bshatX'\bsy
  \\
  &\underset{(c)}{=}
  \left(
  \bsX'\bsP'\bsP\bsX
  \right)^{-1}
  \bsX'\bsP'\bsy
  =
  \left(
  \bshatX'\bshatX
  \right)^{-1}
  \bshatX'\bsy
\end{align*}
In words, these are
\begin{enumerate}[label=(\alph*)]
  \item Here, $\bshatbeta_{2SLS}$ is simply the GMM estimate with
    $\bshatW = (\bsZ'\bsZ)^{-1}$
  \item The final expression in this line writes $\bshatbeta_{2SLS}$ as
    the exactly-identified IV estimator taking $\bsx_n$ as regressors
    and the fitted values $\bshatx_n$ (from regressing $\bsx_n$ on
    $\bsz_n$) as instruments.
  \item The final expression in this line writes $\bshatbeta_{2SLS}$ as
    a regression of $\bsy$ on the fitted values of $\bshatx_n$ (from a
    regression of $\bsx_n$ on $\bsz_n$)---hence the
    ``two-stage'' name.\footnote{%
      The two stages are the two regressions: (1) regressors on instruments,
      then (2) $y_n$ on fitted values from (1). This does not correspond to
      two-stages of GMM, since we can construct the 2SLS estimator
      straightaway in one step, using the matrices $\bsX$ and $\bsZ$ in
      our dataset.
    }
\end{enumerate}
\end{defn}

\begin{rmk}
We can \emph{always} form the 2SLS estimator because we can always just
take $\bshatW=(\bsZ'\bsZ)^{-1}$ as our GMM weighting matrix, and appeal
to Subsection~\ref{subsec:EndogenousRegressorGMM} to get the asymptotic
distribution.  But that then raises the question ``When is it a
\emph{good} idea to use that weighting matrix and thus use the 2SLS
estimator?'' That gives us our next proposition.
\end{rmk}


\begin{prop}
\emph{%
  (Efficient GMM Estimator under Conditional Homoeskedasticy is 2SLS)}
No part of the efficient GMM estimation section assumed conditional
homoskedasticity of the error term. However, if we are willing to assume
that
\begin{align}
  \text{\emph{Cond. homosked.:}}\quad
  \E[\varepsilon_n^2|\bsz_n] = \sigma^2
  \;\implies\;
  \Var(\bsg_n(\bsbeta))=
  \E[\varepsilon_n^2\bsz_n\bsz_n']
  = \sigma^2 \E[\bsz_n\bsz_n']
  \label{2SLSerror}
\end{align}
then the efficient GMM estimator is precisely the 2SLS estimator.
Moreover, this is an efficient GMM estimator that we can construct in
one step, rather than two.
\end{prop}

\begin{proof}
Expression~\ref{2SLSerror} implies that the efficient weighting matrix
$\bsOmega^{-1}$ under homoeskedasticy simplifies to
$\bsOmega^{-1}=\left(\sigma^2 \E[\bsz_n\bsz_n']\right)^{-1}$.
Now suppose just for a second that we knew $\sigma^2$. Then we could
form the following consistent estimator for $\bsOmega$:
\begin{align*}
  \bshatOmega
  = \frac{\sigma^2}{N} \sumnN \bsz_n\bsz_n'
  = \frac{\sigma^2\bsZ'\bsZ}{N}
  \quad\pto\quad
  \sigma^2 \E[\bsz_n\bsz_n']
  = \bsOmega
\end{align*}
Then, by Expression~\ref{sendog:gmm} for the GMM estimator, we have
\begin{align*}
  \bshatbeta\big(
  \bshatOmega^{-1}
  \big)
  &=
  \left[
  \bsX'\bsZ
  \bshatOmega^{-1}
  \bsZ'\bsX
  \right]^{-1}
  \bsX'\bsZ
  \bshatOmega^{-1}
  \bsZ'\bsy
  \\
  &=
  \left[
  \bsX'\bsZ
    \left(N\left(\sigma^2 \bsZ'\bsZ\right)^{-1}\right)
  \bsZ'\bsX
  \right]^{-1}
  \bsX'\bsZ\left(N\left(\sigma^2 \bsZ'\bsZ\right)^{-1}\right)\bsZ'\bsy
\end{align*}
But notice that the scalars $\sigma^2$ and $N$ cancel out. Neither
matters for the estimator, so we could have just used $(\bsZ'\bsZ)^{-1}$
as the weighting matrix. But then that exactly matches the definition of
the \emph{2SLS estimator} above.
\end{proof}


\clearpage
\subsection{Continuously Updated GMM Estimator (CUE)}


\begin{defn}(CUE Estimator)
As is always the case with CUE GMM, we want to use a weighting matrix
that is an automatically-updating sample approximation of the efficient
choice of weighting matrix. Therefore, under the assumptions of this
section with
$\bsg_n(\bsb) = (y_n-\bsx_n'\bsb)\bsz_n=\varepsilon_n\bsz_n$,
the \emph{continuously updated GMM estimator} (CUE) is given by
\begin{align*}
  \bshatbeta_{CUE} :=&\;
  \argmin_{\bsb}\;
  \bsbarg_N(\bsb)'
  \left(
    \frac{1}{N}\sumnN \bsg_n(\bsb)\bsg_n(\bsb)'
  \right)^{-1}
  \bsbarg_N(\bsb) \\
  \text{where} \quad
  \bsbarg_N(\bsb) :=&\;
  \frac{1}{N} \sumnN (y_n-\bsx_n'\bsb)\bsz_n
\end{align*}
We see that if $\bsb=\bsbeta$, the weighting matrix in the middle is a
sample estimate of
$\bsOmega^{-1}=\Var(\bsg_n(\bsbeta))=\E[\bsg_n(\bsbeta)\bsg_n(\bsbeta)']$,
the efficient GMM weighting matrix.
\end{defn}

\begin{rmk}(CUE Estimator under Homoeskedasticy is LIML)
If we're willing to assume homoskedasticity, then
$\E[\bsg_n(\bsbeta)\bsg_n(\bsbeta)']=\E[\varepsilon_n^2\bsz_n\bsz_n']=\sigma^2
\E[\bsz_n\bsz_n']$. Given this,
%if we set $\bsb=\bsbeta$ and let $N\ra\infty$,
the weighting matrix in the middle for CUE has probability limit
\begin{align}
  \left(
  \frac{1}{N}\sumnN
  \bsg_n(\bsb)\bsg_n(\bsb)'
  \right)^{-1}
  \quad \pto\quad
  \E\big[
  \bsg_n(\bsbeta)\bsg_n(\bsbeta)'
  \big]^{-1} =
  \left(\sigma^2 \E[\bsz_n\bsz_n']\right)^{-1}
  \label{sendog:cue}
\end{align}
But rather than using a consistent sample estimate for
$\E[\bsg_n(\bsbeta)\bsg_n(\bsbeta)']^{-1}$ as our weighting matrix (as
we do in the CUE maximization problem above), we could instead use the
following weighting matrix, which is asymptotically equivalent:
\begin{align*}
  \left[
    \left(
      \frac{1}{N}\sumnN (y_n-\bsx_n'\bsb)^2
    \right)
    \left(
      \frac{1}{N}\sumnN \bsz_n\bsz_n'
    \right)
  \right]^{-1}
  \quad\pto\quad
  \left(\sigma^2 \E[\bsz_n\bsz_n']\right)^{-1}
\end{align*}
where convergence is by the LLN and CMT.
Using instead the above expression as our weighting matrix, we can
rewrite the CUE maximization problem into something asymptotically
equivalent:
\begin{align*}
  \bshatbeta_{CUE} &=
  \argmin_{\bsb}\;
  \frac{%
  \left[
  \frac{1}{N} \sumnN (y_n-\bsx_n'\bsb)\bsz_n
  \right]'
  \left[
    \frac{1}{N}\sumnN \bsz_n\bsz_n'
  \right]^{-1}
  \left[
  \frac{1}{N} \sumnN (y_n-\bsx_n'\bsb)\bsz_n
  \right]
  }{\frac{1}{N}\sumnN (y_n-\bsx_n'\bsb)^2}
\end{align*}
But this is precisely the maximization problem that defines the LIML
estimator.
\end{rmk}



\clearpage
\subsection{Inference}

\begin{prop}
Suppose we have GMM estimator $\bshatbeta(\bshatW)$, where $\bshatW$ is
any weighting matrix---not necessarily the efficient one.
For simplicitly, we will denote the estimator  more simply as
$\bshatbeta$, though it is understood that everything below depends upon
(but still holds for) any choice of $\bshatW$.

Let $\bsSigma$ denote the asymptotic variance derived above in
Proposition~\ref{prop:gmmasymptotics}, and let $\bshatSigma$ represent
the corresponding consistent estimator of that asymptotic variance. Then
these estimators can be used to conduct the following tests:
\begin{enumerate}
  \item \emph{Robust $t$-Ratio for Testing Coefficients}: We can
    test the null hypothesis $H_0:\beta_k = \beta_k^0$ (where $\beta_k$
    is the $k$th element of $\bsbeta$) by constructing the
    \emph{robust $t$-ratio}:
    \begin{align*}
      t_k =
      \frac{\sqrt{N}(\hat{\beta}_k-\beta_k^0)}{\sqrt{\hat{\Sigma}_{kk}}}
      \quad\dto \quad \calN(0,1)
    \end{align*}
    where $\hat{\Sigma}_{kk}$ is the $k,k$ element of $\bshatSigma$.

  \item \emph{Wald Statistic for Testing Linear Hypotheses}:
    Suppose we want to test null hypothesis
    \begin{align*}
      H_0: \bsR'\bsbeta = \bsr
      \qquad \text{where} \quad \bsR \in \R^{K \times C}
    \end{align*}
    and where $\bsR$ has full column rank,
    $\rank(\bsR) = C$, i.e.\ no redundant constraints.
    Then
    \begin{align*}
      W = N\cdot
      (\bsR'\bshatbeta-\bsr)'
      \big(
      \bsR'\bshatSigma\bsR
      \big)^{-1}
      (\bsR'\bshatbeta-\bsr)
      \quad\dto\quad
      \Chi^2_C
    \end{align*}

  \item \emph{Wald Statistic for Testing Non-Linear Hypotheses}:
    Any nonlinear hypothesis can be written (w.l.o.g.) as
    $H_0: \bsa(\bsbeta) = \bso$ for some function $\bsa:\R^K\ra \R^C$.
    Then
    \begin{align*}
      W :=
      N \cdot \bsa(\bshatbeta)'
      \big(
      \bsA \bshatSigma \bsA'
      \big)^{-1}
      \bsa(\bshatbeta)
      \quad&\dto\quad
      \Chi^2_C
      \qquad
      \text{where} \quad
      \bsA := \; \frac{\partial}{\partial \bsbeta'}[\bsa(\bsbeta)]
    \end{align*}
    i.e. $\bsA$ is the derivative of $\bsa$ evaluated at true parameter
    vector $\bsbeta$.

  \item \emph{``LR'' Test for Non-Linear Hypotheses}:
    Given null hypothesis $H_0: \bsa(\bsbeta) = \bso$ for some function
    $\bsa:\R^K\ra \R^C$ above,  we consider restricted and unrestricted
    efficient GMM estimates
    \begin{align}
      \bshatbeta
      :=&\; \argmin_{\bsb}\; J(\bsb, \bshatOmega^{-1})
      \label{gmm:lrunrestricted}\\
      \bsbarbeta
      :=&\; \argmin_{\bsb}\; J(\bsb, \bshatOmega^{-1})
      \qquad\text{s.t.} \quad \bsa(\bsb) = \bso
      \label{gmm:lrrestricted}
    \end{align}
    where
    $J(\bsb, \bshatW) := N\cdot \bsbarg_N(\bsb)' \bshatW \bsbarg_N(\bsb)$
    and $\bshatOmega$ is a consistent estimator of
    $\bsOmega=\Var(\bsg_n(\bsbeta))=\E[\varepsilon_n^2\bsz_n\bsz_n']$,
    the inverse of the efficient weighting matrix. Then
    \begin{align*}
      LR :=
      J(\bsbarbeta, \bshatOmega^{-1})
      - J(\bshatbeta, \bshatOmega^{-1})
      \quad \dto \quad \Chi^2_C
    \end{align*}
    i.e.\ it has the same asymptotic distribution as the Wald Statistic
    $W$ in Test 3 above.
\end{enumerate}
\end{prop}

\begin{rmk}
The above should look very familiar. All of the formulas are essentially
the same as those from large-sample OLS. Only the estimator $\bshatbeta$
has changed from $\bshatbeta_{OLS}$ to $\bshatbeta(\bshatW)$ (the GMM
estimator) and of course, the underlying $\bshatSigma$ has a more
complicated structure.
\end{rmk}

\begin{rmk}
Comparing (3) and (4), both test the nonlinear null hypothesis
$H_0:\bsa(\bsbeta)=\bso$. However, Test 3 is \emph{not} invariant to
different (but equivalent) ways of writing the same null hypothesis.
Test 4 \emph{is} invariant to such rewriting.
\\
\\
\emph{However}, a few caveats for Test (4):
\begin{itemize}
  \item This test \emph{only} works with an efficient GMM weighting
    matrix such that
    $\bshatOmega^{-1}\pto \bsOmega=\E[\varepsilon_n^2\bsz_n\bsz_n']^{-1}$, otherwise,
    the LR is not asymptotically $\Chi^2$. In contrast, Wald test (3)
    works for any weighting matrix $\bshatW$, including inefficient
    ones.

  \item The same consistent estimate $\bshatOmega^{-1}$ for
    $\E[\varepsilon_n^2\bsz_n\bsz_n']$ should be used for both
    estimation Problems~\ref{gmm:lrrestricted} and
    \ref{gmm:lrunrestricted}. The typical procedure is
    \begin{enumerate}[label=(\roman*)]
      \item
        Choose initial weighting matrix $\bshatW$ and compute
        \begin{align*}
          \bshatbeta(\bshatW) = \argmin_{\bsb} J(\bsb,\bshatW)
        \end{align*}

      \item Use consistent $\bshatbeta(\bshatW)$ to compute the
        residuals and obtain a consistent estimate of
        $\bsOmega=\E[\varepsilon_n^2\bsz_n\bsz_n']^{-1}$, which we denote by
        $\bshatOmega^{-1}$.

      \item Use $\bshatOmega^{-1}$ to solve
        Problem~\ref{gmm:lrunrestricted} for the efficient unrestricted
        GMM estimator $\bshatbeta$.

      \item Use that same $\bshatOmega^{-1}$ to solve
        Problem~\ref{gmm:lrrestricted} for efficient restricted
        GMM estimator $\bsbarbeta$.
    \end{enumerate}
    Notice that we didn't resolve for a new $\bshatOmega^{-1}$ before Step
    (iv), running a restricted version of Step (i) to get a consistent
    $\bsbarbeta(\bshatW)$, then estimating the residuals again. We used
    the same consistent $\bshatOmega^{-1}$ for both minimization problems.

    While not a problem asymptotically, using different consistent
    estimates of $\bshatOmega^{-1}$ in solving
    Problems~\ref{gmm:lrunrestricted} and \ref{gmm:lrrestricted} is bad
    in finite samples because the LR statistic can be negative. Using
    the same $\bshatOmega^{-1}$ in both avoids this problem.
\end{itemize}
\end{rmk}





\clearpage
\section{Multiple Equation Endogenous Regressor Model}

\subsection{Model Assumptions}

\begin{defn}(Model Assumptions)
Each of the $N$ observations shows up in $M$ equations.
For each equation, we have a dependent variable $y_{mn}$, a vector of
potentially endogenous regressors $\bsx_{mn}\in K_m$, and a vector of
predetermined instruments $\bsz_{mn}\in L_m$.
For a given equation $m$, $\bsx_{mn}$
and $\bsz_{mn}$ might have elements in common. Also
\emph{across} equations, elements might be shared by the different
regressor and instrument vectors. We use $K$ and $L$ to denote
$\sum_{m=1}^M K_m$ and $\sum_{m=1}^M L_m$, respectively.
\\
\\
Model assumptions:
\begin{enumerate}
  \item \emph{Linearity}:
    For equation equation,
    $y_{mn} = \bsx_{mn}' \bsbeta_m + \varepsilon_{mn}$
    where $\bsx_{mn},\bsbeta_m\in\R^{K_m}$.

    For a given $n$, we allow \emph{interequation} correlation
    $\E[\varepsilon_{m_1,n}\varepsilon_{m_2,n}]>0$ (also called
    \emph{contemporaneous}, when $n$ is a time index).
    We assume no \emph{cross-equation restrictions} on coefficients,
    i.e. we do not assume $\beta_{m_1,k_1} = \beta_{m_2,k_2}$ for
    two equations $m_1\neq m_2$ and entries $k_1$ and $k_2$ in the
    ceofficient vectors, respectively.

  \item \emph{Ergodic Stationarity}:
    Joint stationarity and ergodicity for the unique, nonconstant
    elelements of
    $(y_{1n},\ldots,y_{Mn},\bsz_{1n},\ldots,\bsz_{Mn},\bsx_{1n}\ldots,\bsx_{Mn})$.
    %\begin{align*}
    %\end{align*}
    Implied by iid, but weaker.

  \item \emph{Orthogonality Conditions/Predetermined Instruments}:
    For each equation, $L_m\times 1$ vector of instruments $\bsz_{mn}$
    is predetermined (uncorrelated with $\varepsilon_{mn}$):
    \begin{align*}
      \E[\bsz_{mn} (y_{mn} - \bsx'_{mn} \bsbeta_m)]
        =\E[\bsz_{mn}\varepsilon_{mn}]= \bso
      \qquad m = 1,\ldots,M
    \end{align*}
    This can also be expressed compactly by stacking the moment
    conditions and parameters:
    \begin{align}
      \bso &=
      \begin{bmatrix}
        \E[\bsz_{1n}\varepsilon_{1n}] \\
        \vdots\\
        \E[\bsz_{Mn}\varepsilon_{Mn}] \\
      \end{bmatrix}
      =
      \begin{bmatrix}
        \E\big[\bsz_{1n}(y_{1n}-\bsx_{1n}'\bsbeta_1)\big] \\
        \vdots\\
        \E\big[\bsz_{Mn}(y_{Mn}-\bsx_{Mn}'\bsbeta_M)\big] \\
      \end{bmatrix}
      \in \R^{L}
      \label{multipleortho}
    \end{align}
    Note that we only assume \emph{within}-equation orthogonality, not
    \emph{cross}-equation orthogonality, so it is indeed possible that
    $\E[\bsz_{m_1,n}\varepsilon_{m_2,n}]\neq\bso$ for
    $m_1\neq m_2$.\footnote{%
      Of course, if a variable is \emph{common} to both $m_1$ and $m_2$,
      we will trivially have cross-equation orthogonality.
    }

  \item \emph{Rank Condition for Identification}:
    For each equation $m$,
    $\E[\bsz_{mn}\bsx_{mn}']\in\R^{L_m \times K_m}$ is finite with full
    column rank $K_m$

  \item \emph{Martingale Difference Sequence}:
    The stacked vector
    \begin{align*}
      \begin{bmatrix}
        z_{1n}\varepsilon_{1n} & \cdots &
        z_{Mn}\varepsilon_{Mn}
      \end{bmatrix}'
        \in \R^L
    \end{align*}
    is a joint martingale difference sequence with finite, nonsigular
    second moment matrix.
\end{enumerate}
\end{defn}

\clearpage
\subsection{Multiple Equation GMM Estimator}
\label{subsec:EndogenousRegressorMultipleGMM}

\begin{defn}(GMM Estimator, Overidentified Case)
Take Assumptions 1-5 above. The orthogonality conditions state that
\begin{align*}
  \begin{bmatrix}
    \E[\bsz_{1n}\varepsilon_{1n}] \\
    \vdots\\
    \E[\bsz_{Mn}\varepsilon_{Mn}]
  \end{bmatrix}
  =
  \begin{bmatrix}
    \E[\bsz_{1n}(y_{1n}-\bsx_{1n}'\bsbeta_1)] \\
    \vdots\\
    \E[\bsz_{Mn}(y_{Mn}-\bsx_{Mn}'\bsbeta_M)] \\
  \end{bmatrix}
  &=
  \bso
\end{align*}
This can also be equivalently written as
\begin{align*}
  \bsg(\bsbeta) &= \bso \\
  \text{where}\quad
  \bsg(\bsbeta) &=
  \begin{bmatrix}
    \E[\bsz_{1n}y_{1n}] \\
    \vdots\\
    \E[\bsz_{Mn}y_{Mn}] \\
  \end{bmatrix}
  -
  \begin{bmatrix}
    \E[\bsz_{1n}\bsx_{1n}'] &  &\\
    & \ddots & \\
    && \E[\bsz_{Mn}\bsx_{Mn}'] \\
  \end{bmatrix}
  \begin{bmatrix}
    \bsbeta_1 \\
    \vdots \\
    \bsbeta_M \\
  \end{bmatrix} \in \R^L
\end{align*}
So then construct the sample analog of $\bsg$, which we will try to get
close to zero:
\begin{align*}
  \bsbarg_N(\bsb) &=
  \begin{bmatrix}
    \frac{1}{N}\sumnN \bsz_{1n}y_{1n} \\
    \vdots\\
    \frac{1}{N}\sumnN\bsz_{Mn}y_{Mn} \\
  \end{bmatrix}
  -
  \begin{bmatrix}
    \frac{1}{N}\sumnN\bsz_{1n}\bsx_{1n}' &  &\\
    & \ddots & \\
    && \frac{1}{N}\sumnN\bsz_{Mn}\bsx_{Mn}' \\
  \end{bmatrix}
  \begin{bmatrix}
    \bsb_1 \\
    \vdots \\
    \bsb_M \\
  \end{bmatrix} \\
  &= \bss_{\bsz\bsy} - \bsS_{\bsz\bsx}\bsb
\end{align*}
We want to construct the GMM estimator (ignoring scalar constants like
$N$ and $\frac{1}{N}$) for some weighting matrix
$\bshatW\in \R^{L\times L}$:
\begin{align*}
  \bshatbeta(\bshatW)
  &= \argmin_{\bsb} \; \bsbarg_N(\bsb)' \, \bshatW \, \bsbarg_N(\bsb) \\
  &= \argmin_{\bsb} \;
    \big(\bss_{\bsz\bsy} - \bsS_{\bsz\bsx}\bsb\big)'
    \bshatW
    \big(\bss_{\bsz\bsy} - \bsS_{\bsz\bsx}\bsb\big)
\end{align*}
%Simplifying and removing constant terms that don't depend upon $\bsb$,
%we get
%\begin{align*}
  %\bshatbeta(\bshatW)
  %&= \argmin_{\bsb} \;
    %-
    %\bss_{\bsz\bsy}'
    %\bshatW
    %\bsS_{\bsz\bsx}\bsb
    %- \bsb'\bsS_{\bsz\bsx}'
    %\bshatW
    %\bss_{\bsz\bsy}
    %+ \bsb'\bsS_{\bsz\bsx}'
    %\bshatW
    %\bsS_{\bsz\bsx}\bsb
%\end{align*}
From the first order conditions, we get
\begin{align*}
  \bshatbeta(\bshatW) &=
  \big(\bsS_{\bsz\bsx}' \bshatW \bsS_{\bsz\bsx}\big)^{-1}
    \bsS_{\bsz\bsx}' \bshatW \bss_{\bsz\bsy}
\end{align*}
\end{defn}

\begin{prop}\emph{(Sampling Error)}
Sub $\bsy = \bsX\bsbeta +\bsvarepsilon$ into the expression for
$\bshatbeta(\bshatW)$ to get
\begin{align*}
  %\bshatbeta(\bshatW) &=
  %\left(\bsX'\bsZ\bshatW \bsZ'\bsX\right)^{-1}\bsX'\bsZ\bshatW\bsZ'
  %\left(\bsX\bsbeta +\bsvarepsilon\right) \\
  %\implies\quad
  \bshatbeta(\bshatW) - \bsbeta
  &=
  \left(\bsX'\bsZ\bshatW
  \bsZ'\bsX\right)^{-1}\bsX'\bsZ\bshatW\bsZ'\bsvarepsilon \\
  &=
  \left[
    \left(\frac{1}{N} \sumnN \bsx_n \bsz_n'\right)
    \bshatW
    \left(\frac{1}{N} \sumnN \bsz_n\bsx_n' \right)
  \right]^{-1}
  \left[
    \left(\frac{1}{N} \sumnN \bsx_n \bsz_n'\right)
    \bshatW
    \left(\frac{1}{N} \sumnN \bsz_n \varepsilon_n \right)
  \right]
\end{align*}
\end{prop}


\begin{prop}\emph{(Asymptotic Properties)}
\label{prop:gmmasymptoticsmultiple}
From the sampling error expression, we get
\begin{enumerate}[label=(\roman*)]
  \item Consistency: $\bshatbeta(\bshatW)\pto \bsbeta$
  \item Asymptotic Normality: Recalling that $\bshatW\pto \bsW$
    applying LLN, CLT, and Slutsky gives
    \begin{align*}
      \sqrt{N}\left(\bshatbeta(\bshatW) - \bsbeta\right)
      \quad &\dto \quad
      N(0,\bsP\bsS\bsP') \\
      \text{where}\quad
      \bsP &=
      %\bigg(
      \big(
        \E[\bsx_n\bsz_n']
        \bsW
        \E[\bsz_n\bsx_n']
      \big)^{-1}
      \E[\bsx_n\bsz_n']
      \bsW
      %\bigg)
      \\
      \bsS &=
        \E[\varepsilon_n^2 \bsz_n\bsz_n']
      %\cdot
      %\big[
      %N\big(0,\;
      %\underbrace{%
        %\E[\varepsilon_n^2 \bsz_n\bsz_n']
      %}_{\bsS}
      %\big)
      %\big]
    \end{align*}
    We can construct a consistent estimator of the asymptotic variance:
    \begin{align*}
      \bshatP\bshatS\bshatP' \pto \bsP\bsS\bsP'
      \quad\text{where}\quad
      \bshatP &=
      \left[
      \left(
      \frac{1}{N}
        \sumnN \bsx_n\bsz_n'
      \right)
      \bshatW
      \left(
      \frac{1}{N}
        \sumnN \bsz_n\bsx_n'
      \right)
      \right]^{-1}
      \left(
      \frac{1}{N}
        \sumnN \bsx_n\bsz_n'
      \right)
      \bshatW \\
      &= N \cdot [(\bsX'\bsZ)\bshatW(\bsZ'\bsX)]^{-1}(\bsX'\bsZ)\bshatW
      \\
      \bshatS &=
      \frac{1}{N}\sumnN e_n \bsz_n\bsz_n' \\
      \text{where}\quad
      e_n &= y_n - \bsx_n'\bshatbeta(\bshatW)
    \end{align*}
\end{enumerate}
\end{prop}

\begin{prop}\emph{(Consistent Estimation of Error Variance)}
Using $\bshatbeta(\bshatW)$, a consistent estimator of $\bsbeta$, we can
obtain a consistent estimator of the error variance
\begin{align*}
  \frac{1}{N} \sumnN e_n^2
  =
  \frac{1}{N} \sumnN \big(y_n - \bsx_n'\bshatbeta(\bshatW)\big)^2
  \quad\pto\quad
  \E[\varepsilon_n^2]
\end{align*}
\end{prop}


\clearpage
\section{Miscellaneous}

\subsection{Testing for Structural Breaks}

Suppose that
\begin{align*}
  y_t = \mu + \delta \cdot \mathbf{1}\{t>\tau\} + \varepsilon_t
  \qquad \varepsilon_t \iid(0,1)
\end{align*}
Alternatively, this can be written
\begin{align*}
  y_t = \mu_1\cdot \mathbf{1}\{t\leq\tau\}
  + \mu_2 \cdot \mathbf{1}\{t>\tau\} + \varepsilon_t
  \qquad \varepsilon_t \iid(0,1)
\end{align*}
The regressor matrix $X$ looks like
\begin{align*}
  X' =
  \begin{pmatrix}
    1 & \cdots & 1 & 0 & \cdots & 0 \\
    0 & \cdots & 0 & 1 & \cdots & 1 \\
  \end{pmatrix}
  \quad\implies\quad
  X'X =
  \begin{pmatrix}
    \tau & 0 \\
    0 & T-\tau\\
  \end{pmatrix}
\end{align*}
Clearly, the regression estimates will be $\hat{\mu}_i$ for $\mu_i$.
We want to test the hypothesis that $\mu_1=\mu_2$, i.e.
\begin{align*}
  H_0: r &= R'\beta \\
  H_0: 0 &=
  \begin{pmatrix}
    1 & -1
  \end{pmatrix}
  \begin{pmatrix}
    \mu_1 \\ \mu_2
  \end{pmatrix}
\end{align*}
With $R$ and $X$ as defined above
\begin{align*}
  R'(X'X)^{-1}R
  &=
  \begin{pmatrix}
    1 & -1
  \end{pmatrix}
  \begin{pmatrix}
    \tau^{-1} & 0 \\
    0 & (T-\tau)^{-1}\\
  \end{pmatrix}
  \begin{pmatrix}
    1 \\ -1
  \end{pmatrix}
  = \tau^{-1} + (T-\tau)^{-1}
\end{align*}
So form the $F$-statistic (though this is really a $\Chi^2$ since I'm
using the true error variance $\sigma^2=1$, rather than $s^2$)
\begin{align*}
  F_T(\tau)
  &=
  (R'\hat{\beta}-r)'
  [\sigma^2 R'(X'X)^{-1}R]^{-1}
  (R'\hat{\beta}-r)
  \\
  &=
  (\hat{\mu}_1-\hat{\mu}_2-0)'
  [\sigma^2 R'(X'X)^{-1}R]^{-1}
  (\hat{\mu}_1-\hat{\mu}_2-0)
  \\
  &=
  \frac{%
  \left(
  \frac{1}{\tau}\sum_{t=1}^\tau y_t
  -
  \frac{1}{T-\tau}\sum_{t=\tau+1}^T y_t
  \right)^2
  }{\tau^{-1} + (T-\tau)^{-1}}
\end{align*}



\clearpage
\section{Time Series GMM}

\subsection{Distribution of Sample Mean}
\label{sec:mean}

This is a rather straightfoward exercise, but it allows us to derive a
few important concepts in GMM notation. So let's go ahead.

Given a sample $\{\bsy_t\}^T_{t=1}$ of stationary (typically
autocorrelated) random variable $\bsy_t$, we defined an
estimator of the sample mean, $\mathbb{E}_T(\bsy_t)$. Now
let's characterize that estimator's distribution, since it too is a
random variable.

First, the mean:
\begin{align*}
  \mathbb{E}\left(\mathbb{E}_T(\bsy_t)\right)
  &= \mathbb{E}\left(\frac{1}{T}\sum^T_{t=1} \bsy_t\right)
  = \frac{1}{T}\sum^T_{t=1} \mathbb{E}\left(\bsy_t\right)
  = \frac{1}{T} \times T \; \mathbb{E}\left(\bsy_t\right)\\
  &= \mathbb{E}\left(\bsy_t\right)
\end{align*}
So $\mathbb{E}_T(\bsy_t)$ is an unbiased estimator of the
unconditional mean of $\bsy_t$.

Now we get the variance of the estimator, accounting for the fact that
the sample $\{\bsy_t\}_{t=1}^T$ is not necessarily iid (though
assumed stationary):
\begin{align}
  \text{Var}(\mathbb{E}_T(\bsy_t))
  &=
  \text{Var}\left(
  \frac{1}{T}\sum^T_{t=1} \bsy_t
  \right)
  =
  \frac{1}{T^2}\text{Var}\left(
  \sum^T_{t=1} \bsy_t
  \right)\notag\\
  &=
  \frac{1}{T^2}
  \sum^T_{j=-T} (T-|j|)\text{Cov}\left(
    \bsy_t, \bsy_{t-j}
  \right) \notag\\
  &=
  \frac{1}{T^2}
  \left[
    \sum^T_{t=1} \text{Var}(\boldsymbol{y_t})
    +
    2\sum^T_{j=1} (T-|j|)\text{Cov}(\bsy_t,\bsy_{t-j})
  \right]\notag\\
  \Rightarrow\quad
  \text{Var}(\mathbb{E}_T(\bsy_t))
  &=
  \frac{1}{T}
  \left[
    \text{Var}(\boldsymbol{y_t})
    +
    2\sum^T_{j=1} \frac{T-|j|}{T}
    \text{Cov}(\bsy_t,\bsy_{t-j})
  \right]
  \label{eq:estvar}
\end{align}
A few things to note about this last result:
\begin{enumerate}
  \item In the case of iid observations,
    $\text{Cov}(\bsy_t,\bsy_{t-j})$ will be zero for
    all $j\neq 0$, in which case we get the classic ``$\sqrt{T}$
    result'' for the standard deviation:
    \begin{align*}
      \text{Var}(\mathbb{E}_T(\bsy_t))
      &=
      \frac{\text{Var}(\boldsymbol{y_t})}{T}\\
      \Rightarrow\quad
      \sigma(\mathbb{E}_T(\bsy_t))
      &=
      \frac{\sigma(\boldsymbol{y_t})}{\sqrt{T}}
    \end{align*}
    In words, ``The average of any sample always gives a unbiased
    estimate of the mean. And when the data is uncorrelated, the
    precision of that estimate increases at a rate of $\sqrt{T}$.''

  \item When the observations are \emph{not} iid (when there is serial
    correlation in the sample), Equation~\ref{eq:estvar} says that you
    don't really have $T$ observations. The serial correlation decreases
    the number of \emph{effective} observations you have. So crank up
    the variance of your estimate, depending upon how strong that serial
    correlation is.

  \item
    You can see from Equation~\ref{eq:estvar} that when $T$ get's large,
    $\frac{T-|j|}{T}$ approaches unity, which gives
    \begin{align*}
      \lim_{T\rightarrow\infty}
      \text{Var}(\mathbb{E}_T(\bsy_t))
      &=
      \frac{1}{T}
      \left[
        \text{Var}(\boldsymbol{y_t})
        +
        2\sum^\infty_{j=1}
        \text{Cov}(\bsy_t,\bsy_{t-j})
      \right]
      =
      \frac{\bsS}{T}
    \end{align*}
    where $\bsS$ is the spectral density of $\bsy_t$
    at frequency at zero.
\end{enumerate}
From there, we can use the Central Limit Theorem to state the following
result.

\begin{thm}
Given a sample $\{\bsy_t\}_{t=1}^T$ for random variable
$\bsy_t$
\begin{align*}
  \sqrt{T}\left(
    \mathbb{E}_T(\bsy_t) -\mathbb{E}(\bsy_t)
  \right)
  \xrightarrow{d} N(0,\bsS)
\end{align*}
where $\bsS$ is the spectral density at frequency zero defined
\begin{align*}
  \bsS :=
  \sum^\infty_{j=-\infty}
    \text{\normalfont Cov}(\bsy_t, \bsy_{t-j})
  =
  \text{\normalfont Var}(\bsy_t)
  + 2 \sum^\infty_{j=1}
  \text{\normalfont Cov}(\bsy_t,\bsy_{t-j})
\end{align*}
This holds regardless of whether or not the data are iid or serially
correlated (since you're using the correct covariance matrix,
$\bsS$).
\end{thm}

Suppose that we have random variable
$\bsy\in \mathbb{R}^m$, with mean and variance-covariance matrix as
follows:
\begin{align*}
  \bsmu &:= \mathbb{E}(\bsy) =
  \begin{pmatrix}
    \mathbb{E}(y_1)\\\mathbb{E}(y_2)\\\vdots\\\mathbb{E}(y_m)
  \end{pmatrix}
  \in \mathbb{R}^m
  \\
  \bsSigma
  &:=
  \mathbb{E}\left[ (\bsy-\bsmu) (\bsy-\bsmu)' \right]
  \in \mathbb{R}^{m\times m}
\end{align*}
Now suppose that we have an iid sample $\left\{\bsy_i\right\}_{i=1}^N$.
We can construct an estimator of the first moment vector
\begin{align*}
  \bshatmu_N
  &=
  \frac{1}{N}
  \sum^N_{i=1}
  \bsy_i\\
  \sqrt{N}(\bshatmu_N - \bsmu)
  &\dto N(0,\bsSigma)
\end{align*}
where the convergence of this estimator follows from the Central Limit
Theorem.

\begin{rmk}
The fact that $\bsg(\bsmu)$ is only a function
of the first moment of $\bsy$ isn't that restrictive if we're willing to
generalize our notion of $\bsy$. We can easily accommodate more
complicated moments with a minor tweak.

To see this, suppose that we have some base random variable
$\bsx\in\mathbb{R}^n$
\begin{align*}
  \bsx =
  \begin{pmatrix}
    x_1\\x_2\\\vdots\\x_n
  \end{pmatrix}
  \in \mathbb{R}^n
\end{align*}
along with sample $\{\bsx_i\}_{i=1}^N$. If we want second
moments or cross-correlations of elements in $\bsx$, simply
define a function $\bsh: \mathbb{R}^n \rightarrow
\mathbb{R}^m$ to do the transformations you want and set
\begin{align*}
  \bsy
  :=\bsh(\bsx)
  =
  \begin{pmatrix}
    h_1(\bsx)\\h_2(\bsx)\\\vdots\\h_m(\bsx)
  \end{pmatrix}
  \in\mathbb{R}^m
\end{align*}
Then, to generate the sample for $\bsy$, just take
\begin{align*}
  \{\bsy_i\}_{i=1}^N &:=
  \left\{\bsh\left(\bsx_i\right)\right\}_{i=1}^N
\end{align*}
From there, proceed exactly as before. In this way, we can always use
the ``first moment representation'' above, leaving open the possibility
that $\bsy$ is actually a transformation or function of some
base random variable.
\end{rmk}


\subsection{Standard Errors}

Next, suppose we have a sample $\{\bsy_t\}^T_{t=1}$. To get
standard errors for the various objects we will estimate, we proceed in
a sequence of steps
\begin{enumerate}
  \item Define $\bsS$, the spectral density matrix of
    population moment vector
    $\bsg(
    \bsb)=\mathbb{E}[\boldsymbol{f}(\bsy,
    \bsb)]$ for fixed $\bsb$.

    Given fixed $\bsb$, the vector
    $\bsg(\bsb)
    =\mathbb{E}[\boldsymbol{f}(\bsy_t,\bsb)]$
    is simply the mean of object
    $\boldsymbol{f}(\bsy_t,\bsb)$.
    We recall the definition from Section~\ref{sec:mean} for
    the spectral density at frequency zero:
    \begin{align*}
      \bsS = \sum^\infty_{j={-\infty}}
        \text{Cov}(\boldsymbol{f}(\bsy_t,\bsb),
          \boldsymbol{f}(\bsy_t,\bsb))
        =
        \mathbb{E}\left[
        \boldsymbol{f}(\bsy_t,\bsb)
        \boldsymbol{f}(\bsy_t,\bsb)'
        \right]
    \end{align*}
    Nothing deep there, just applying our definition from earlier. Of
    course, eventually, we'll have to talk about estimating this beast
    given a sample (since we usually won't know $\bsS$), but
    for now, let's just take it as given and work through the math for
    everything else. We'll return to it later.

  \item Use $\bsS$ to derive the standard error of sample mean
    $\boldsymbol{a}_T\bsg_T(\bsb)$ for fixed
    $\bsb$.\footnote{We compute standard errors for
      $\boldsymbol{a}_T\bsg_T(\bsb)$, not just
      $\bsg_T(\bsb)$ because we impose that the
      former equals zero, but we don't know anything about the latter.
      And we will need to use the equality
      $\boldsymbol{a}_T\bsg_T(\bsb)=0$,
      Taylor-expand the lefthand side, shuffle things around, and get
      results in the next step.}

    Just like in Section~\ref{sec:mean}, we have a sample estimate
    $\bsg_T(\bsb)$ for a population mean vector
    $\bsg(\bsb)$. We also have $\bsS$, the
    spectral density at frequency zero. Put this together to get
    \begin{align*}
      \sqrt{T}\left(\bsg_T(\bsb)
      - \bsg(\bsb)\right)
      &\xrightarrow{d} N(0,\bsS)\\
      \Rightarrow \quad
      \text{Var}\left(
        \boldsymbol{a}
        \bsg_T(\bsb)
      \right)
      &=
      \frac{\boldsymbol{a}\bsS\boldsymbol{a}'}{T}
    \end{align*}
    where $\boldsymbol{a}= \text{plim}\;\boldsymbol{a}_T$.

  \item Use the delta method to derive standard errors for
    $\bshatb$.

    We know the distribution of $\bsg_T(\bsb)$
    for any fixed $\bsb$,
    so we can do the usual delta function math,\footnote{%
    Note, I could have used the delta method formula above, but the
    notation makes it
    tricky to apply here since this derivation treats
    $\bsg_T(\bsb)$ as the sampling distribution we
    \emph{know}, and $\bsb$ as the transformation of
    $\bsg_T(\bsb)$ whose sampling distribution we
    \emph{want to know}.  Now that's fine since $\bsb$ really
    is a function of $\bsg_T(\bsb)$ (it's the
    inverse), but it's not conducive to the notation in the delta method
    section. So I just opt for the more direct approach this one time.%
    }
    writing out a Taylor Series expansion of $\boldsymbol{a}
    \bsg_T(\bshatb)$ about the true parameter
    vector $\bsb_0$ to get
    \begin{align}
      \boldsymbol{a} \bsg_T(\bshatb)
      &= 0\notag\\
      \boldsymbol{a} \bsg_T(\bsb_0)
      + \boldsymbol{a}
      \frac{\partial \bsg_T}{\partial \bsb'}
      (\bshatb-\bsb_0)
      &= 0\notag\\
      (\bshatb-\bsb_0)
      &=
      -(\boldsymbol{a} \bsd)^{-1}
      \boldsymbol{a} \bsg_T(\bsb_0)
      \label{eq:bhat_b0}\\
      \text{Var}\left(\bshatb-\bsb_0\right)
      &=
      (\boldsymbol{a} \bsd)^{-1}
      \; \left[\text{Var}\left(
        \boldsymbol{a} \bsg_T(\bsb_0)
      \right)
      \right]
      {(\boldsymbol{a} \bsd)^{-1}}'\notag\\
      \text{Var}\left(\bshatb-\bsb_0\right)
      &=
      \frac{1}{T}
      (\boldsymbol{a} \bsd)^{-1}
      \boldsymbol{a} \bsS \boldsymbol{a}'
      {(\boldsymbol{a} \bsd)^{-1}}'\notag
    \end{align}
    where we defined
    \begin{align*}
      \bsd &:=
      \mathbb{E}\left[
        \frac{\partial \boldsymbol{f}}{\partial \bsb'}
      \right]
      =
      \frac{\partial \bsg_T(\bsb)}{%
        \partial \bsb'}
    \end{align*}

  \item Use the delta method to derive standard errors for
    $\bsg_T(\bshatb)$.

    We just got the distribution of $\bshatb$, now let's
    get the distribution of $\bsg_T(\bshatb$,
    which incorporates both variation in our sample \emph{and} in our
    parameter estimates). Again, delta method
    \begin{align}
      \bsg_T(\bshatb)
      &\approx
      \bsg_T(\bsb_0)
      + \bsd
      \left(\bshatb-\bsb_0\right)\notag\\
      \text{From (\ref{eq:bhat_b0})}
      \qquad\qquad
      &=
      \bsg_T(\bsb_0)
      -\bsd(\boldsymbol{a} \bsd)^{-1}
      \boldsymbol{a} \bsg_T(\bsb_0)\notag \\
      \bsg_T(\bshatb)
      &\approx
      \left(\boldsymbol{I}
      -\bsd(\boldsymbol{a} \bsd)^{-1}
      \boldsymbol{a} \right)
      \bsg_T(\bsb_0)\notag\\
      \Rightarrow\quad
      \text{Var}\left(
      \bsg_T(\bshatb)
      \right)
      &\approx
      \frac{1}{T}
      \left(\boldsymbol{I}
      -\bsd(\boldsymbol{a} \bsd)^{-1}
      \boldsymbol{a} \right)
      \bsS
      \left(\boldsymbol{I}
      -\bsd(\boldsymbol{a} \bsd)^{-1}
      \boldsymbol{a} \right)'
      \label{eq:vargtbhat}
    \end{align}
    And lastly, just a quick word about this variance.

    Recall that there are $m$ moments, and we set $p$ linear
    combinations of them to zero. Hence, $p$ elements of
    $\bsg_T(\bsb)$ will have zero variance
    for \emph{all} choices of $b$, by construction. That means the
    variance-covariance matrix for
    $\bsg_T(\bshatb)$ given by
    Equation~\ref{eq:vargtbhat} will be \emph{singular}.  Since some
    formulas require us to invert this object, we will need to use a
    pseudo-inverse.\footnote{Here's one quick way to do that given
      variance-covariance matrix $\Sigma$. Do an eigenvalue
      decomposition $\Sigma = Q\Lambda Q'$. Construct $\Lambda_*$, which
      simply inverts all non-zero eigenvalues on the diagonal of
      $\Lambda$, and set $\Sigma^{-1}=Q\Lambda_*Q'$.}

\end{enumerate}

\subsection{Test of Overidentifying Restrictions}

We're trying to set $\bsg_T(\bsb)$ to zero, but
again, we have to make some tradeoffs and set only a few moments (or
linear combinations of moments) to zero. We might then run tests to see
if the remaining moments we didn't force to zero are statistically
large---i.e.\ far from zero, so bad news for the model.

To that end, form the $J_T$ statistic
\begin{align}
  J_T =
  T
  \bsg_T(\bshatb)'
  \left[
  \left(
    \boldsymbol{I}-\bsd(\boldsymbol{a}\bsd)^{-1}
    \boldsymbol{a}
  \right)
  \bsS
  \left(
    \boldsymbol{I}-\bsd(\boldsymbol{a}\bsd)^{-1}
    \boldsymbol{a}
  \right)'
  \right]^{-1}
  \bsg_T(\bshatb)
  \label{eq:jtest}
\end{align}
Ignore the details and focus on the intution of the line above: We're
just squaring and adding up each elements of
$\bsg_T(\bshatb)$, but weighting those elements
by the object in brackets, which is precisely the inverse
variance-covaraince matrix of $\bsg_T(\bshatb)$
from Equation~\ref{eq:vargtbhat} above.  In other words, we're dividing
the size of each squared ``deviation from zero'' by its variance (to
standardize), and then adding all of them up to evaluate the deviations
\emph{jointly}.

Finally, we just need to specify what distribution $J_T$ follows. By the
central limit theorem, $\bsg_T(\bshatb)$ should
be multivariate normal. We standardized all the elements by their
variance, squared them, and added them up, so we have a sum of squared
standard normals: $\Chi^2$ distribution
\begin{align*}
  J_T \sim \Chi^2_{m-p}
\end{align*}
where $m$ is the number of moments and $p$ is the number of parameters
we estimated. We're used to this kind of degrees of freedom calculation,
but in the GMM context, it has additional meaning. In particular, we can
think of the $m-p$ degrees of freedom as a consequence of setting $p$
elements of $\bsg_T(\bshatb)$ to zero
\emph{always}. Recall that this meant zero variance for those elements,
hence a singular variance-covariance matrix for
$\bsg_T(\bshatb)$ of rank $m-p$. So there are
only $m-p$ squared standard normals to add up for our $\Chi^2$
distribution.



%%%% APPPENDIX %%%%%%%%%%%

\clearpage
\appendix

\section{Matrix Results and Conventions}

\subsection{Matrix Calculus}

Here, I lay down a few conventions and (light) results for derivatives
with respect to vectors and of vector-valued functions.

\paragraph{Scalar-Valued}
For scalar-valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$
which takes column vector $\bsx$ as an argument, we have
\begin{align*}
  \frac{\partial f}{\partial \bsx}
  &=
  \begin{pmatrix}
    \frac{\partial f}{\partial x_1} \\
    \vdots\\
    \frac{\partial f}{\partial x_n} \\
    \end{pmatrix} \in \mathbb{R}^n\\
  \frac{\partial f}{\partial \bsx'}
  &=
  \begin{pmatrix}
    \frac{\partial f}{\partial x_1} &
    \cdots&
    \frac{\partial f}{\partial x_n}
  \end{pmatrix}\in \mathbb{R}^n
  %\\\\
  %\frac{\partial^2 f}{\partial \bsx\partial \bsx'}
  %&=
  %\begin{pmatrix}
    %\frac{\partial^2 f}{\partial \theta_1^2} &
    %\frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} &
    %\cdots&
    %\frac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \\
    %\frac{\partial^2 f}{\partial \theta_2 \partial \theta_1} &
    %\frac{\partial^2 f}{\partial \theta_2^2} &
    %\cdots&
    %\frac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \\
  %\end{pmatrix}\in \mathbb{R}^n
\end{align*}
As a result of the definitions, for column vectors $\bsa,\bsx\in \Rn$,
\begin{align*}
  \frac{\partial}{\partial \bsx}
  [\bsa'\bsx]
  &= \bsa \\
  \frac{\partial}{\partial \bsx}
  [\bsx'\bsa]
  &= \bsa
\end{align*}
For column vector $\bsx\in \Rn$ and $\bsA\in\Rnn$,
\begin{align*}
  \frac{\partial}{\partial \bsx}
  [\bsx'\bsA\bsx]
  =
  \frac{\partial}{\partial \bsx}
  \left[
    \sumin
    \sumjn
    a_{ij} x_i x_j
  \right]
  &=
  \begin{pmatrix}
    \sumjn a_{1j} x_j
    + \sumin a_{i1} x_i \\
    %\sumjn a_{2j} x_j
    %+ \sumin a_{i2} x_i \\
    \vdots \\
    \sumjn a_{nj} x_j
    + \sumin a_{in} x_i \\
  \end{pmatrix}\\
  &=
  \begin{pmatrix}
    \sumjn a_{1j} x_j \\
    \vdots \\
    \sumjn a_{nj} x_j
  \end{pmatrix}
  +
  \begin{pmatrix}
    \sumin a_{i1} x_i \\
    \vdots \\
    \sumin a_{in} x_i \\
  \end{pmatrix}\\
  &= \bsA\bsx + \bsA'\bsx\\
  \implies\quad
  \frac{\partial}{\partial \bsx}
  [\bsx'\bsA\bsx]
  &= (\bsA+\bsA')\bsx
\end{align*}
From there, we can get
\begin{align*}
  \frac{\partial^2[\bsx'\bsA\bsx]}{\partial \bsx \partial \bsx'}
  =
  (\bsA + \bsA')
\end{align*}


\paragraph{Vector-Valued}

For vector-valued function $\bsg:
\mathbb{R}^n\rightarrow\mathbb{R}^m$,
\begin{align*}
  \bsg(\bstheta)
  &=
  \begin{pmatrix}
    g_1(\bstheta) \\
    \vdots \\
    g_m(\bstheta) \\
  \end{pmatrix}\in \mathbb{R}^m\\
  \frac{\partial \bsg}{\partial \boldsymbol{\theta'}}
  &=
  \begin{pmatrix}
    \frac{\partial g_1}{\partial \theta_1}
      & \cdots & \frac{\partial g_1}{\partial \theta_n}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial g_m}{\partial \theta_1}
      & \cdots & \frac{\partial g_m}{\partial \theta_n}\\
  \end{pmatrix}
  \in \mathbb{R}^{m\times n}
\end{align*}
We also have, in general,
\begin{align*}
  \frac{\partial\boldsymbol{g'}}{\partial \bstheta}
  =
  \left(
  \frac{\partial\bsg}{\partial \boldsymbol{\theta'}}
  \right)'
\end{align*}


\clearpage
\subsection{Matrix Manipulations}

\begin{defn}(Trace)
If $A$ is an $n\times n$ matrix, then
\begin{equation}
    \text{tr}[A] = \sum^n_{i=1} a_{ii}
\end{equation}
which is the sum of diagonal elements.
\end{defn}

\begin{defn}(Vec Operator)
Next, the \emph{vec operator} takes any matrix $A\in\Rmn$
and stacks to columns on top of each other (left to right) to form a
column vector of length $mn$.  Supposing that $a_i$ are column vectors
to simplify notation:
\begin{align*}
  A &= \begin{pmatrix} a_1 & \cdots & a_n \end{pmatrix}
	\qquad a_i \in \mathbb{R}^{n\times 1}
  \quad\implies\quad
  \vc\; A =
	\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
\end{align*}
\end{defn}

\begin{defn}(Kronecker Product)
Suppose we have two matrices, $A$ which is $m \times n$ and $B$
which is $p\times q$. Then the {\sl Kronecker Product} of $A$ and $B$ is
\begin{align*}
  A \otimes B = \begin{pmatrix} a_{11} B & \cdots & a_{1n} B \\
      \vdots & \ddots & \vdots \\
      a_{m1} B & \cdots & a_{mn}B \end{pmatrix}
  \in \R^{(mp)\times(nq)}
\end{align*}
\end{defn}


\begin{prop}\emph{(Matrix Identities)}
First, consider $P\in\Rmn$ and $Q\in\R^{n\times m}$. Then
\begin{enumerate}
  \item[\emph{(i)}]
    $\text{tr}[PQ]=\text{tr}[QP]$
\end{enumerate}
Next, consider $a\in\Rn$ and $M\in\R^{n\times n}$ where $M$ is a
symmetric positive definite matrix. Then
\begin{enumerate}
  \item[\emph{(ii)}]
    $a'Ma = \text{tr}[Maa']$
\end{enumerate}
Next, consider $A\in\Rmn$, $B\in\R^{p\times q}$, $C\in\R^{n\times r}$,
and $D\in\R^{q\times s}$. Then
\begin{enumerate}
  \item[\emph{(iii)}] $(A \otimes B)(C \otimes D) = AC \otimes BD$
  \item[\emph{(iv)}] $(A\otimes B)' = (A' \otimes B')$.
  \item[\emph{(v)}] $(A\otimes B)^{-1} = (A^{-1} \otimes B^{-1})$.
  \item[\emph{(vi)}]
    tr$\left[A'BCD'\right] = \text{vec}(A)' (D \otimes B) \text{vec}(C)$.
\end{enumerate}
\end{prop}
\clearpage
\begin{proof}
We prove the non-obvious or non-tedious proofs here:
\begin{enumerate}[label=(\roman*)]
  \item This isn't dificult to prove, just tedious.
  \item First, note that $a'Ma$ is just a scalar,  and the trace of a
    scalar is just the scalar itself, i.e.\ $a'Ma=\text{tr}[a'Ma]$. Now
    use Part (i) of the proposition to write
    \begin{align*}
        a' M a &= tr\left[a' (M a) \right]
      = tr\left[(M a) a' \right]  \\
      &= tr\left[M a a' \right]
    \end{align*}

  \item[(vi)] This one is easily the most ``What the hell?'' with no
    intuition, so let's prove it.

    We know that the matrix that results from $A'BCD'$ must be square to
    apply the trace operator to it.  So let's call that the beast inside
    the trace operator $E$ and suppose that it is $q \times q$. Now what
    does that imply about the size of our other matrices?

    Well to allow the matrix multiplication to be carried out (i.e.\ we
    can only multiply matrix $X$ and matrix $Y$ if the columns of $X$
    equal the rows of $Y$), and to have $E$ be $q\times q$, this forces
    \begin{align*}
        A\;\;  \text{is}\;\;  m \times q \qquad
      \qquad B \text{is} \;\; m \times n \qquad
        C\;\;  \text{is}\;\; n \times p \qquad
      & \qquad
        D\;\;  \text{is}\;\;  q \times p
    \end{align*}
    where $m$, $n$, $p$, and $q$ are all left unspecified.

    Okay then, let's get a move on
    \begin{align*}
        \text{tr}[A'BCD'] = \text{tr}[E] &=
      \sum^q_{i=1} e_{ii} \\
    \end{align*}
    Now suppose define $X = A'B$ and $Y = CD'$, implying
    that $X$ is $q\times n$ and $Y$ is $n\times q$.
    We can rewrite the diagonal components of $E$ in
    terms of the rows of $X$, the $x_{i\cdot}$, and
    the colums of $Y$, $y_{\cdot i}$.
    \begin{align*}
        \text{tr}[E] &=
      \sum^q_{i=1} e_{ii}
      = \sum^q_{i=1} x_{i\cdot} y_{\cdot i}
    \end{align*}
    Now let's not forget where the rows and columns of
    $X$ and $Y$ come from. The $i$th row of $X$ is
    the product of the $i$th row of $A'$ (or the $i$th
    column of $A$) with
    each subsequent column of $B$. Similarly, each
    column of $Y$ is the dot product of each
    successive row of $C$ with the $i$th column of $D'$
    (or the $i$th row of $D$).
    Mathematically,
    \begin{align}
        \text{tr}[E] &=
      \sum^q_{i=1} e_{ii}
      = \sum^q_{i=1} x_{i\cdot} y_{\cdot i} \notag \\
        &= \sum^q_{i=1} a'_{\cdot i}
      \begin{pmatrix} b_{\cdot 1} & \cdots & b_{\cdot n}
      \end{pmatrix}
      \begin{pmatrix} c_{1\cdot} \\ \vdots \\ c_{n \cdot}
      \end{pmatrix}
      d'_{i \cdot} \label{fullrep} \\
        &= \sum^q_{i=1} a'_{\cdot i}
      BC
      d'_{i \cdot} \label{simprep}
    \end{align}
    where $a'_{\cdot i}$ is the $i$th column of $A$,
    transposed, and $d'_{i \cdot}$ is the $i$th row of $D$,
    transposed.

    Alright, where did that get us? Well, let's look at
    that last expression. Let's suppress the three
    components to the write of $a'_{\cdot i}$---those nasty
    vectors and matrices with $b$'s, $c$'s, and $d$'s in
    them. We'll do so by just grouping those three
    terms into a ``back-box'' term called ``$z_i$,'' which
    we'll postpone computing for now.  This let's us
    write
    \begin{align}
        \text{tr}[E]
        &= \sum^q_{i=1} a'_{\cdot i}  z_i \label{goback}
    \end{align}
    where $z_i$ is of size $m\times 1$.
    \\
    \\
    But hey, that's a regular old dot product, so let's
    rewrite it as such with simple matrix notation:
    \begin{align}
        \text{tr}[E]
      &= \sum^q_{i=1} a'_{\cdot i}  z_i
      = \begin{pmatrix} a'_{\cdot 1}
          & \cdots & a'_{\cdot q} \end{pmatrix}
          \begin{pmatrix} z_{1}
          \\ \vdots \\ z_{q} \end{pmatrix}
      = \text{vec}(A)' \; Z
        \label{zrep}
    \end{align}
    Now hopefully you were able to make the jump to
    Equation \ref{zrep} yourself once you saw what
    we were doing.
    And now we at least have \emph{something}
    %from Equation \ref{vecder}.
    Now let's try to pin down those $z_i$ terms that
    helped us get here.
    \\
    \\
    Now, let's pin down that $Z$ matrix and show
    that, in fact,
    \begin{equation}
        Z = (D\otimes B) \text{vec}(C)
    \end{equation}
    We do so by first using Equations
    \ref{simprep}, \ref{goback}, and \ref{zrep} to infer the
    following:
    \begin{align}
        Z &= \begin{pmatrix} z_1 \\ \vdots \\ z_q
      \end{pmatrix} =
      \begin{pmatrix} BCd'_{1\cdot} \\ \vdots \\ BCd'_{q\cdot}
      \end{pmatrix} \label{bigz}
    \end{align}
    Next, let's consider the $j$th row of the last vector, then
    rewrite $C$, expanding it into a different but equivalent
    representation---a row of column vectors all lined up:
    \begin{align*}
        BCd'_{j\cdot} &= B
      \begin{pmatrix} c_{\cdot 1} & \cdots & c_{\cdot p}
      \end{pmatrix} d'_{j \cdot} \notag \\
        \text{expanding $d'_{j \cdot}$} \quad
      &= B
      \begin{pmatrix} c_{\cdot 1} & \cdots & c_{\cdot p}
      \end{pmatrix}
      \begin{pmatrix} d_{j 1} \\ \vdots \\ d_{j p}
      \end{pmatrix}
    \end{align*}
    Now distribute $B$ and dot product with the $d'_{j\cdot}$:
    \begin{align}
        BCd'_{j\cdot} &=
      \begin{pmatrix} Bc_{\cdot 1} d_{j 1} + \cdots
          + Bc_{\cdot p} d_{j p}
      \end{pmatrix}   \notag \\
        \text{Since the $d_{jm}$ are scalars} \quad &=
      \begin{pmatrix}  d_{j 1} Bc_{\cdot 1} + \cdots
          +  d_{j p}Bc_{\cdot p}
      \end{pmatrix}   \notag
    \end{align}
    Amost there, I promise.

    Now stack these $BCd'_{j\cdot}$ terms
    to get $Z$ as in Equation \ref{bigz}:
    \begin{align}
        Z &= \begin{pmatrix} z_1 \\ \vdots \\ z_q
      \end{pmatrix} =
      \begin{pmatrix} BCd'_{1\cdot} \\ \vdots \\ BCd'_{q\cdot}
      \end{pmatrix}
        = \begin{pmatrix}
          d_{1 1} Bc_{\cdot 1} + \cdots
          +  d_{1 p}Bc_{\cdot p} \\
          \vdots \\
          d_{q 1} Bc_{\cdot 1} + \cdots
          +  d_{q p}Bc_{\cdot p}
      \end{pmatrix}
    \end{align}
    Finally, take out the expanded $C$ term from
    each row and rewrite the sums in each row
    as a result of matrix multiplication
    \begin{align}
        Z &=\begin{pmatrix}
          d_{1 1} Bc_{\cdot 1} + \cdots
          +  d_{1 p}Bc_{\cdot p} \\
          \vdots \\
          d_{q 1} Bc_{\cdot 1} + \cdots
          +  d_{q p}Bc_{\cdot p}
      \end{pmatrix}
        = \begin{pmatrix}
          d_{1 1} B & \cdots
          &  d_{1 p}B \\
          \vdots & \ddots & \vdots \\
          d_{q 1} B & \cdots
          &  d_{q p}B
      \end{pmatrix}
        \begin{pmatrix}
      c_{\cdot 1} \\ \vdots \\ c_{\cdot p}
        \end{pmatrix}
    \end{align}
    Or in other words, if you look at the
    last line closely
    \begin{align*}
        Z = (D\otimes B) \text{vec}(C)
    \end{align*}
    QED (drops mic).
    \end{enumerate}
\end{proof}


\clearpage
\section{Hayashi Reference Sheet}

Hayashi makes the following definitions:
\begin{align*}
  \bsg_n &= \bsx_n\varepsilon_n \\
  \bsS &= \E\bsg_n\bsg_n'] = \E[\varepsilon_n^2\bsx_n\bsx_n'] \\
  \bsS_{\bsx\bsx} &= \frac{1}{N}\sumnN \bsx_n\bsx_n'\\
\end{align*}

%We have two equations, each estimated separately. Let $\beta_i$ denote
%the OLS estimate from the $i$th regression, with $\beta^0_i$ the true
%value, and ${se}_i$ the corresponding standard error. Whether in Chapter
%1 or 2, we will have normality of the coefficient estimates about the
%true value, i.e.
%\begin{align*}
  %\sqrt{N}
  %\left(
  %\begin{pmatrix}
    %\beta_1 \\ \beta_2
  %\end{pmatrix}
  %-
  %\begin{pmatrix}
    %\beta_1^0 \\ \beta_2^0
  %\end{pmatrix}
  %\right)
  %\sim
  %N\left(
  %\begin{pmatrix}
    %0 \\ 0
  %\end{pmatrix}
  %,\;
  %\begin{pmatrix}
    %{se}_1 & 0 \\
    %0 & {se}_2 \\
  %\end{pmatrix}
  %\right)
%\end{align*}
%Realistically, the coefficients could be correlated, but we have no
%information in the problem to pin that down. So I just assume zero
%correlation between errors/coefficients in the two equations, hence the
%zeros in the covariance matrix. Then we have some functions
%$g(\beta_1,\beta_2) = \beta_1 - \beta_2$ and $h(\beta_1,\beta_2) =
%\beta_1\beta_2$. So then just apply the delta-method with these
%functions to get
%\begin{align*}
  %\sqrt{N}
  %\left(
  %g(\beta_1,\beta_2)
  %-
  %g(\beta_1^0,\beta_2^0)
  %\right)
  %\sim
  %N\left(
  %0
  %,\;
  %G
  %\begin{pmatrix}
    %{se}_1 & 0 \\
    %0 & {se}_2 \\
  %\end{pmatrix}
  %G'
  %\right)
%\end{align*}
%where $G=\nabla g$, the row vector of first derivatives of $g$. Then,
%just use that distribution to get a confidence interval:
%\begin{align*}
  %g(\beta_1,\beta_2)
  %\pm
  %1.96\times
  %\sqrt{
  %\frac{1}{N}
  %G
  %\begin{pmatrix}
    %{se}_1 & 0 \\
    %0 & {se}_2 \\
  %\end{pmatrix}
  %G'
  %}
%\end{align*}





\section{Linear Gaussian Recurrences}

So far, we've only looked at the properties of a multivariate
normal RV at one point in time. This is already useful for
cross-section regressions, but it would
it would also be relevant to explore
the MVN RV in in the context of time series and stochastic
processes. For that, we turn
to \emph{recurrence relations}, which will conveniently characterize
a discrete-time, continuous space stochastic process,
$\{{X}_t\}$.

\subsection{Single Lag, Order 1 Markov Chains}

Suppose we know that the stochastic process $\{{X}_t\}$
evolves according to the following recurrence relation,
where ${Z}_t$ is independent of ${X}_t$:
\begin{equation}
    {X}_{t+1} = A {X}_t + B {Z}_t
    \qquad {Z}_t \sim \text{N}_d(0, \; I_d)
\end{equation}
Because ${Z}_t$ is Gaussian, this implies that
${X}_{t+1}$ will \emph{also} be Gaussian for reasons we saw
in Section 3 above. So let's define
\begin{equation}
    {X}_t \sim \text{N}_d(\mu_t, \; \Sigma_t)
\end{equation}
{\sl Forward Mean}:
Now our task is to compute $\mu_{t+1}$ and $\Sigma_{t+1}$
given the parameters $\mu_t$ and $\Sigma_t$.  Let's start with the
mean:
\begin{align*}
    \mu_{t+1} = E{X}_{t+1} &= E\left[A{X}_t +
	B {Z}_t\right] \\
    &= AE[{X}_t] + BE[{Z}_t] \\
    \Rightarrow \quad \mu_{t+1} &= A\mu_t
\end{align*}
{\sl Forward Covariance Matrix}:
Now, let's compute the covariance matrix, rewriting the recurrence
relation somewhat suggestively:
\begin{align*}
    {X}_{t+1} &= A {X}_t + B {Z}_t \\
    \Leftrightarrow \qquad
	{X}_{t+1} - \mu_{t+1} &=
	A ({X}_t - \mu_t) + B {Z}_t \\
\end{align*}
Now, we move to the computation of $\Sigma_{t+1}$, which simplifies
nicely (omitting the explicit simplification and derivation process):
\begin{align*}
    \Sigma_{t+1} &= E\left\{ \left[{X}_{t+1} - \mu_{t+1} \right]
    \left[{X}_{t+1} - \mu_{t+1} \right]^T \right\} \\
    &= E\left\{ \left[A ({X}_t - \mu_t) + B {Z}_t\right]
	\left[A ({X}_t - \mu_t) + B {Z}_t\right]^T
	\right\} \\
    &= A\Sigma_t A^T + BB^T
\end{align*}
This is an example of a \emph{forward equation} because the
distribution of ${X}_{t+1}$ is determined entirely by
the distribution of ${X}_t$ and the recurrence relation.
\\
\\
Finally, note that in some cases, the noise vector won't necessarily
be of dimension $d$.  It might have dimension $m<d$, in which
case $B$ will have dimension $d\times m$. You can think of
$m$ correlated shocks being distributed over $d$ separate variables.


\newpage
\subsection{Multiple Lag, Higher Order Markov Chains}

Let's generalize a bit.  Suppose we want a particular RV,
${X}_{t+1}$, in a stochastic process, $\{{X}_t\}$, to
depend upon more than one lag, say ``$k$'' lags. Namely, suppose that
\begin{equation}
    \label{recgen}
    {X}_{t+1} = A_0 {X}_t + A_1 {X}_{t-1}
    + \cdots + A_{k-1}{X}_{t-k+1} + B {Z}_t
\end{equation}
We can use a \emph{state space expansion} to augment the matrices
and write Equation \ref{recgen}'s more general recurrence relation
as follows:
\begin{align*}
    \tilde{{X}}_{t+1} &= \tilde{A}_t
	\tilde{{X}}_t + \tilde{B}_t {Z}_t \\
    \Leftrightarrow \qquad
    \begin{pmatrix} {X}_{t+1} \\
	{X}_{t} \\ \vdots \\ {X}_{t-k+2} \end{pmatrix}
	&=
	\begin{pmatrix} A_0 & A_1 & \cdots & & A_{k-1} \\
			1 & 0 & \cdots & & 0 \\
			0 & 1 & \cdots & & 0 \\
			\vdots & & \ddots & & \vdots \\
			0 & \cdots & & 1 & 0
	\end{pmatrix}
	\begin{pmatrix} {X}_{t} \\
	{X}_{t-1} \\ \vdots \\ {X}_{t-k+1} \end{pmatrix}
	+ \begin{pmatrix} B \\ 0 \\ \vdots \\ 0 \end{pmatrix}
	{Z}_t
\end{align*}
where $\tilde{A}$ is the \emph{companion matrix} of the
recurrence relation.
Note that ${Z}_t$ needs no augmenting because it is memoryless.
\\
\\
{\sl Forward Moments}: Using the results from the previous
subsection, we immediately know that if the stochastic process
$\{{X}_t\}$ satisifes Equation \ref{recgen},
$E\tilde{{X}}_t = \tilde{\mu}_t$, and
Cov$(\tilde{{X}}_t) = \tilde{\Sigma}_t$, then
\begin{align*}
    \tilde{\mu}_{t+1} &= \tilde{A}\tilde{\mu}_t \\
    \tilde{\Sigma}_{t+1} &= \tilde{A} \tilde{\Sigma}_t\tilde{A}^T +
	\tilde{B} \tilde{B}^T
\end{align*}
And if we want to find the covariance matrix $\Sigma_{t+1}$ for
${X}_{t+1}$, then we can just find the $kd \times kd$
covariance matrix $\tilde{\Sigma}_{t+1}$ and look at the top left
$d\times d$ block.
\\
\\
{\sl State Space Expansion}: Note what we did here.  We had a
stochastic process $\{{X}_{t}\}$ that was not a Markov
Chain---future values depended upon more than one lag of
${X}_t$, violating the Markov Property.  However,
\emph{state space expansion} allowed us to form a new stochastic
process $\{\tilde{{X}}_t\}$ that was a Markov Chain, obeying
the Markov property.  Therefore, anytime a stochastic process
does not display the Markov Property, blame that on the size of
the state space.
\\
\\
Also note, state space expansion brings us to the sitaution we
mentioned in the previous subsection where the noise matrix,
$\tilde{B}$, has fewer sources of noise than state variables.


\clearpage
\bibliographystyle{apalike}  % Or some other style
\bibliography{biblio} % where sources.bib has all the citation info


\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf}
%	 }
%      }
%   \end{figure}


%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
