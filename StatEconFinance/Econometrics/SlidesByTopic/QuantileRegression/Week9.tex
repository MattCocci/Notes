%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usepackage{cmbright}
%\usetheme{metropolis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis with Sidebar %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[handout]{beamer}
%\documentclass{beamer}
\usepackage{cmbright}

\useoutertheme[right, width=0.20\paperwidth]{sidebar}
\usecolortheme{metropolis}
\useinnertheme[sectionpage=none]{metropolis}
\usefonttheme{metropolis}

\makeatletter
\beamer@headheight=1.75\baselineskip     %controls the height of the headline, default is 2.5
\makeatother

%\setbeamertemplate{sidebar canvas right}{}
%\setbeamertemplate{sidebar right}{BOB}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Simple Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usetheme{Boadilla}
%\usepackage{lmodern}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sidebar Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[serif]{beamer}  % For serif latex font
%\usepackage{pxfonts}
%\usepackage{eulervm}
%\usepackage{mathpazo}

%\usetheme{Goettingen}
%\usecolortheme{lily}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Common Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[]{ECO-466/FIN-521: Precept, Week 9 \\ Matt Cocci}
\author[]{}
\date{\today}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
%\setbeamertemplate{section in toc}[square unnumbered]
%\setbeamertemplate{subsection in toc}[square unnumbered]

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\usepackage{bbm} % For bold numbers

%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% For easier looping
\usepackage{pgffor}

\usepackage{pdfpages}

%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\widehat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}


% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsphi}{\boldsymbol{\phi}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bshatOmega}{\boldsymbol{\widehat{\Omega}}}
\newcommand{\bshatG}{\boldsymbol{\widehat{G}}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}

% Special cases like \bshatb for \boldsymbol{\widehat{b}}
\generate[\hat]{bshat}{\boldsymbol}{b,y,x,X,V,S,W}
\newcommand{\bshatphi}{\boldsymbol{\widehat{\phi}}}
\newcommand{\bshatmu}{\boldsymbol{\widehat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\widehat{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\widehat{\Sigma}}}
\newcommand{\bstildephi}{\boldsymbol{\tilde{\phi}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bsbarphi}{\boldsymbol{\overline{\phi}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}

% Redefine \bso to be the zero vector
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\widehat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\widehat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\widehat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Rnk}{\mathbb{R}^{n\times k}}
\newcommand{\Rkk}{\mathbb{R}^{k\times k}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}


% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}


% Various probability and statistics commands
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}


% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\nNz}{_{n=0}^N}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}


\begin{frame}{Outline}
\tableofcontents[hideallsubsections]
%\tableofcontents
\end{frame}


\section{Binary Choice Model}

\subsection{Marginal and Partial Effects}

\begin{frame}[shrink]{Binary Choice Model}
Binary choice model
\begin{align*}
  P[Y=1|X=x]
  =
  G(x'\beta_0)
  =
  G(x_1\beta_{0,1} + x_{-1}'\beta_{0,-1})
\end{align*}
\alert{Marginal Effects}
\begin{itemize}
  \item \alert{Continuous $x_1$}:
    The marginal effect with respect to the 1st regressor at a
    particular value $x=(x_1,\,x_{-1}')'$ of the regressors is
    \begin{align*}
      \frac{\partial}{\partial x_1}
      P[Y=1|X=x]
      =
      g(x'\beta_0)\beta_{0,1}
    \end{align*}

  \item \alert{Binary $x_1$}:
    When the 1st regressor is binary, use instead
    \begin{align*}
      &
      P[Y=1|X=(1,x_{-1})']
      -
      P[Y=1|X=(0,x_{-1})']
      \\
      &=
      G(\beta_{0,1} + x_{-1}'\beta_{0,-1})
      -
      G(x_{-1}'\beta_{0,-1})
    \end{align*}

\end{itemize}
Plug-in estimator, s.e.'s from delta method or bootstrap.
\end{frame}


\begin{frame}[shrink]{Binary Choice Model}
Binary choice model
\begin{align*}
  P[Y=1|X=x]
  =
  G(x'\beta_0)
  =
  G(x_1\beta_{0,1} + x_{-1}'\beta_{0,-1})
\end{align*}
\alert{Average Partial Derivative and Effect}
\begin{itemize}
  \item \alert{Continuous $x_1$}:
    \begin{align*}
      \E\bigg[
      \frac{\partial }{\partial x_1}
      P[Y=1|X=x]
      \bigg|_{x=X}
      \bigg]
      =
      \E[g(x'\beta_0)\beta_{0,1}]
    \end{align*}
    Estimator is sample counterpart:
    \begin{align*}
      \left[
        \frac{1}{N}
        \sumin
        g(X_i'\hat{\beta})\hat{\beta}_1
      \right]
    \end{align*}
  \item \alert{Binary $x_1$}:
    When the 1st regressor is binary,
    \begin{align*}
      &
      \E\bigg[
      P[Y=1|X=(1,x_{-1})']
      -
      P[Y=1|X=(0,x_{-1})']
      \bigg]
      \\
      &=
      \E\bigg[
      G(\beta_{0,1} + X_{-1}'\beta_{0,-1})
      -
      G(X_{-1}'\beta_{0,-1})
      \bigg]
    \end{align*}
    Estimator is sample counterpart:
    \begin{align*}
      \frac{1}{N}
      \sumiN
      \bigg[
      G(\hat{\beta}_{1} + X_{-1}'\hat{\beta}_{-1})
      -
      G(X_{-1}'\hat{\beta}_{-1})
      \bigg]
    \end{align*}

\end{itemize}
Standard errors by 2-step or bootstrap.
\end{frame}




\begin{frame}[shrink]{Marginal Effects: Continuous $x_1$}
Marginal effect at $x$ with respect to 1st covariate:
\begin{align*}
  \frac{\partial}{\partial x_1}
  P[Y=1|X=x]
  =
  g(x'\beta_0)\beta_{0,1}
\end{align*}
Let $e_1$ denote the 1st column of $I_k$.
Define
\begin{align*}
  h(b)&= g(x'b)b_1=g(x'b)e_1'b \\
  \implies\quad
  H
  &=
  \frac{\partial}{\partial b'}
  h(b)
  \big|_{b=\beta_0}
  =
  \big[
  g'(x'b)b_1x'
  +
  g(x'b)e_1'
  \big]_{b=\beta_0}
\end{align*}
Suppose
\begin{align*}
  \sqrt{N}(\hat{\beta}-\beta_0)
  \quad\dto\quad
  \calN(0,V)
\end{align*}
Then by the delta method
\begin{align*}
  &\sqrt{N}(h(\hat{\beta})-h(\beta_0))
  \\
  &=
  \sqrt{N}\big(
    g(x'\hat{\beta})\hat{\beta}_1-g(x'\beta_0)\beta_{0,1}
  \big)
  \quad\dto\quad
  \calN(0,HVH')
\end{align*}
\end{frame}

\begin{frame}[shrink]{Marginal Effects: Binary $x_1$}

Marginal effect at $x$ with respect to 1st covariate:
\begin{align*}
  &
  P[Y=1|X=(1,x_{-1}')']
  -
  P[Y=1|X=(0,x_{-1}')']
  \\
  &=
  G(\beta_{0,1} + x_{-1}'\beta_{0,-1})
  -
  G(x_{-1}'\beta_{0,-1})
\end{align*}
Define
\begin{align*}
  h(b)
  &=
  G(b_1 + x_{-1}'b_{-1})
  -
  G(x_{-1}'b_{-1})
  \\
  &=
  G\big(
  (1,\,x_{-1})'b
  \big)
  -
  G\big(
  (0,\,x_{-1})'b
  \big)
  \\
  \implies\quad
  H=
  \frac{\partial}{\partial b'}
  h(b)
  \bigg|_{b=\beta_0}
  &=
  g\big(
  (1,\,x_{-1})'b
  \big)
  \begin{pmatrix}
    1 & x_{-1}'
  \end{pmatrix}
  -
  g\big(
  (0,\,x_{-1})'b
  \big)
  \begin{pmatrix}
    0 & x_{-1}'
  \end{pmatrix}
  \bigg|_{b=\beta_0}
  %\\
  %&=
  %\begin{pmatrix}
    %g\big(
    %(1,\,x_{-1})'b
    %\big)
    %&
    %x_{-1}
    %\big[
    %g\big(
    %(1,\,x_{-1})'b
    %\big)
    %-
    %g\big(
    %(0,\,x_{-1})'b
    %\big)
    %\big]
  %\end{pmatrix}
  %\bigg|_{b=\beta_0}
\end{align*}
Suppose
\begin{align*}
  \sqrt{N}(\hat{\beta}-\beta_0)
  \quad\dto\quad
  \calN(0,V)
\end{align*}
Then by the delta method
\begin{align*}
  &\sqrt{N}(h(\hat{\beta})-h(\beta_0))
  \\
  &=
  \sqrt{N}\big(
    [
    G(\hat{\beta}_{1} + x_{-1}'\hat{\beta}_{-1})
    -
    G(x_{-1}'\hat{\beta}_{-1})
    ]
    -
    [
    G(\beta_{0,1} + x_{-1}'\beta_{0,-1})
    -
    G(x_{-1}'\beta_{0,-1})
    ]
  \big)
  \quad\dto\quad
  \calN(0,HVH')
\end{align*}
\end{frame}



\begin{frame}[shrink]{Average Partial Derivative}
For continuous $x_1$,
\begin{align*}
  \E\bigg[
  \frac{\partial }{\partial x_1}
  P[Y=1|X=x]
  \bigg|_{x=X}
  \bigg]
  =
  \E[g(x'\beta_0)\beta_{0,1}]
\end{align*}
Estimator is sample counterpart:
\begin{align*}
  \left[
    \frac{1}{N}
    \sumin
    g(X_i'\hat{\beta})\hat{\beta}_1
  \right]
\end{align*}
Can write as 2-step GMM estimator.
\begin{itemize}
  \item Recall that $\hat{\beta}$ is MLE:
    \begin{align*}
      \hat{\beta}
      &=
      \argmin
      \frac{1}{N}
      \sumiN
      Y_i\log G(X_i'\beta)
      +
      (1-Y_i)\log\big(1-G(X_i'\beta)\big)
    \end{align*}
    Hence $\beta_0$ is defined by associated score function
    \begin{align*}
      0 = \E[s(Y_i,X_i,\beta_0)]
      =
      \E\left[
        \left(
        \frac{Y_ig(X_i'\beta_0)}{G(X_i'\beta_0)}
        -
        \frac{(1-Y_i)g(X_i'\beta_0)}{1-G(X_i'\beta_0)}
        \right)
        X_i
      \right]
    \end{align*}

  \item
    Can also let $\gamma_0$ denote the average partial derivative, which
    is defined by moment condition
    \begin{align*}
      0 = \E[\gamma_0-g(X_i'\beta_0)\beta_{0,1}]
    \end{align*}

  \item
    Can write estimation as just-identified GMM:
    \begin{align*}
      0
      &=
      \E
        \begin{bmatrix}
          s(Y_i,X_i,\beta_0)
          \\
          \gamma_0-g(X_i'\beta_0)\beta_{0,1}
        \end{bmatrix}
    \end{align*}
\end{itemize}
\end{frame}



\begin{frame}[shrink]{Average Partial Derivative}
Moment condition:
\begin{align*}
    0
    &=
    \E
      \begin{bmatrix}
        s(Y_i,X_i,\beta_0)
        \\
        \gamma_0-g(X_i'\beta_0)\beta_{0,1}
      \end{bmatrix}
\end{align*}
Then we have
\begin{align*}
  \sqrt{N}(\hat{\theta}-\theta_0)
  \quad\dto\quad
  \calN(0,(G_{gmm}'\Omega^{-1}G_{gmm})^{-1})
\end{align*}
where we have expected derivative of moment condition with respect to
parameter $\theta:=(\beta',\gamma)'$:
\begin{align*}
  &
  G_{gmm}
  :=
  \E\left[
    \frac{\partial}{\partial \theta'}
    \begin{pmatrix}
      s(Y_i,X_i,\beta)
      \\
      \gamma-g(X_i'\beta)e_1'\beta
    \end{pmatrix}_{\theta=\theta_0}
  \right]
  \\
  &=
  \E
  \begin{bmatrix}
    \frac{\partial }{\partial \beta'}
    \big(s(Y_i,X_i,\beta)\big)\big|_{\theta=\theta_0}
    &
    \frac{\partial}{\partial \gamma}
    \big( s(Y_i,X_i,\beta)\big)\big|_{\theta=\theta_0}
    \\
    \frac{\partial}{\partial \beta'}
    \big(
    \gamma-g(X_i'\beta)e_1'\beta
    \big)
    \big|_{\theta=\theta_0}
    &
    \frac{\partial}{\partial \gamma}
    \big(
    \gamma-g(X_i'\beta)e_1'\beta
    \big)
    \big|_{\theta=\theta_0}
  \end{bmatrix}
  \\
  &=
  \E
  \begin{bmatrix}
    \frac{\partial s(Y_i,X_i,\beta_0)}{\partial \beta'}
    &
    0
    \\
    -g'(X_i'\beta_0)e_1'\beta_{0}X_i'
    -g(X_i'\beta_0)e_1'
    &
    1
  \end{bmatrix}
\end{align*}
and variance of the moment condition
\begin{align*}
  \Omega
  &=
  \Var
  \begin{pmatrix}
    s(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{pmatrix}
  =
  \E
  \left[
  \begin{pmatrix}
    s(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{pmatrix}
  \begin{pmatrix}
    s(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{pmatrix}'
  \right]
\end{align*}
Both of which we can estimate via their sample counterparts.
\end{frame}


%\begin{frame}[shrink]{Exponential Family}
%Family of distributions for $\{X_i\}_{i=1}^n$ belongs to exponential
%family if it can be written
%\begin{align*}
  %f(x|\theta)
  %=
  %h(x)\exp\left(
  %\sum_{i=1}^s \eta_i(\theta)T_i(x)
  %- A(\theta)
  %\right)
%\end{align*}
%Note that $T_i(x)$ is sufficient for $\eta_i(\theta)$.
%\end{frame}


\subsection{FE in Logit}


\begin{frame}[shrink]{Binary Choice: Panel Data}
Suppose we have a panel $\{Y_{it},X_{it}\}$ where $Y_{it}$ is binary.
Let $\pi_{it} := P[Y_{it}=1|x_{it}]$, for some model or parametric
functional form for the conditional success probability.

Write the joint likelihood for a single entity:
\begin{align*}
  \calL(Y_{i1},\ldots,Y_{iT})
  &=
  \exp\left(
  \sumtT
  \big[
  Y_{it}\log \pi_{it}
  +
  (1-Y_{it})\log\big(1-\pi_{it}\big)
  \big]
  \right)
  \\
  &=
  \exp\left(
  \sumtT
  Y_{it}
  \log\left(\frac{\pi_{it}}{1-\pi_{it}}\right)
  +
  \sumtT
  \log\big(1-\pi_{it}\big)
  \right)
\end{align*}
Suppose we choose a model for $\pi_{it}$ so that
\begin{align*}
  \log\left(\frac{\pi_{it}}{1-\pi_{it}}\right)
  =
  \alpha_i + X_{it}'\beta
\end{align*}
Then, substituting in,
\begin{align*}
  \calL(Y_{i1},\ldots,Y_{iT})
  =
  \exp\left(
  \alpha_i
  \sumtT
  Y_{it}
  +
  \beta'
  \sumtT
  Y_{it} X_{it}
  +
  \sumtT
  \log\big(1-\pi_{it}\big)
  \right)
\end{align*}
This is a distribution in the exponential family with
$\sumtT Y_{it}$ sufficient for $\alpha_i$.
\end{frame}


\begin{frame}[shrink]{Binary Choice: Panel Data}
Summarizing:
If we choose
\begin{align*}
  \log\left(\frac{\pi_{it}}{1-\pi_{it}}\right)
  &=
  \alpha_i + X_{it}'\beta
  \\
  \iff\qquad
  \pi_{it}
  =
  P[Y_{it}=1|X_{it}]
  &=
  \frac{%
    \exp(\alpha_i+X_{it}'\beta)
  }{%
    1+\exp(\alpha_i+X_{it}'\beta)
  }
  =
  G(\alpha_i+X_{it}'\beta)
\end{align*}
then a sufficient statistic for $\alpha_i$ is
the total number of successes
\begin{align*}
  \sumtT
  Y_{it}
\end{align*}
This is useful because then we can form the conditional likelihood
(conditional on the sufficient stat above), in which the parameter
$\alpha_i$ will drop out.
So just do ML using the conditional likelihood.
\end{frame}


\begin{frame}[shrink]{Binary Choice: Panel Data}
To implement, first recall the likelihood:
\begin{align*}
  &\calL(Y_{i1},\ldots,Y_{iT})
  =
  \exp\left(
  \alpha_i
  \sumtT
  Y_{it}
  +
  \beta'
  \sumtT
  Y_{it} X_{it}
  -
  \sumtT
  \log\big(1+\exp(\alpha_i+X_{it}'\beta)\big)
  \right)
\end{align*}
We can then compute the distribution of the sufficient stat by computing
the probabilities that the sufficient stat equals some total $\tau_i$.
This is done by adding up the probabilities of all configurations that
induce total $\tau_i$:
\begin{align*}
  &
  P\left[
  \sumtT
  Y_{it}
  =
  \tau_i
  \right]
  =
  \sum_{%
    {Y}_i\,:\,\sum_{t=1}^TY_{it}=\tau_i
  }
  \exp\left(
  \alpha_i
  \tau_i
  +
  \beta'
  \sumtT
  {Y}_{it} X_{it}
  -
  \sumtT
  \log\big(1+\exp(\alpha_i+X_{it}'\beta)\big)
  \right)
\end{align*}
Compute the likelihood conditional on the sufficient statistic by
dividing:
\begin{align*}
  P\left[
  Y_{i1},\ldots,Y_{iT}
  \;\bigg|\;
  \sumtT
  Y_{it}
  =
  \tau_i
  \right]
  &=
  \frac{%
    \calL(Y_{i1},\ldots,Y_{iT})
  }{%
    P\left[
    \sumtT
    Y_{it}
    =
    \tau_i
    \right]
  }
  =
  \frac{%
    \exp\left(
    \beta'
    \sumtT
    Y_{it} X_{it}
    \right)
  }{%
    \sum_{%
      \tilde{Y}_i\,:\,\sum_{t=1}^T\tilde{Y}_{it}=\tau_i
    }
    \exp\left(
    \beta'
    \sumtT
    \tilde{Y}_{it} X_{it}
    \right)
  }
\end{align*}
Notice that this is a distribution characterizing $\beta$ without
$\alpha_i$ terms.
We've ``conditioned them out'' similarly to how we difference out the FE
in linear FE models.
\end{frame}


\begin{frame}[shrink]{Remarks}
Remarks
\begin{itemize}
  \item Clear from this how the logit assumption was crucial.
    It was just the right assumption for $\pi_{it}=P[Y_{it}=1|X_{it}]$
    so that we could get a sufficient statistic for the $\alpha_i$ and
    condition them out.
    Other choices of $G(\,\cdot\,)$ do not deliver that.
  \item
    If $\tau_i=0$ or $\tau_i=T$, you don't use those observations
    because the conditional likelihood just equals 1 for all values of
    the parameters.
    So if many individuals don't change states, you throw away a lot of
    observations.
\end{itemize}
\end{frame}


\begin{frame}[shrink]{Binary Choice: Panel Data}
Suppose $T=2$.
As stated, $\tau_i=0$ and $\tau_i=2$ are trivial.
So consider $\tau_i=1$.
Compute
\begin{align*}
  P\left[
  Y_{i1}=0,Y_{i2}=1
  \;\bigg|\;
  \sumtT
  Y_{it}
  =
  \tau_i
  \right]
  &=
  \frac{%
    \exp\left(
    \beta' X_{i2}
    \right)
  }{%
    \exp\left(
    \beta'X_{i1}
    \right)
    +
    \exp\left(
    \beta'X_{i2}
    \right)
  }
  \\
  &=
  \frac{%
    1
  }{%
    1+
    \exp\left(
    -\beta'(X_{i2}-X_{i1})
    \right)
  }
\end{align*}
Parallels with FE here:
\begin{itemize}
  \item Using changes in $X$'s to identify $\beta$'s.
  \item Can't identify parameters for covariates fixed across time.
\end{itemize}
\end{frame}



\section{Quantile Regression}



\begin{frame}[shrink]{Linearity of Conditional Quantile Function}
When is QR assumption valid?
\begin{align*}
  Q_\tau(Y|X) = X'\beta_\tau
\end{align*}
Examples
\begin{itemize}
  \item \alert{Location-Scale Model}:
    \begin{align*}
      Y = X'\beta + (X'\delta)\varepsilon
      \qquad
      \varepsilon\perp X,
      \quad
      P[X'\delta>0]=1
    \end{align*}

  \item
    \alert{Random Coefficients Model}:
    \begin{align*}
      Y = X'\beta(U)
      \qquad
      U\perp X,
    \end{align*}
    where $U$ is a scalar RV and $u\mapsto X'\beta(u)$ is a strictly
    increasing function with probability 1.
\end{itemize}
Homework:
Independence between $X$ and errors $\varepsilon$ implies $\beta_\tau$
constant.
%So varying $\beta_\tau$ implies heteroskedasticity.
\end{frame}



\begin{frame}[shrink]{Random Coefficients Model}
Random Coefficients Model:
$Y = X'\beta(U)$
\begin{itemize}
  \item Writing out $Y$ in conditional quantile function:
    \begin{align*}
      Q_\tau(Y|X)
      =
      Q_\tau(X'\beta(U)\,|\,X)
    \end{align*}

  \item By assumption, $h(u)= X'\beta(u)$ is a strictly increasing
    function with probability 1.
    Therefore,
    \begin{align*}
      Q_\tau(X'\beta(U)\,|\,X)
      =
      Q_\tau(h(U)\,|\,X)
      =
      h\big(
      Q_\tau(U|X)
      \big)
      =
      X'\beta(Q_\tau(U|X))
    \end{align*}

  \item
    Therefore, we have
    \begin{align*}
      Q_\tau(Y|X)
      =
      X'
      \underbrace{%
        \beta(Q_\tau(U|X))
      }_{\beta_\tau}
    \end{align*}

  \item
    Natural interpretation.
    The $\beta_\tau$ coefficient is just the $\tau$th quantile of
    generated $\beta(U)$'s.
\end{itemize}
\end{frame}


\begin{frame}[shrink]{Location Scale Model}
Location Scale Model:
$Y = X'\beta + (X'\delta)\varepsilon$.
\begin{itemize}
  \item
    Writing out $Y$ in conditional quantile function:
    \begin{align*}
      Q_\tau(Y|X)
      =
      Q_\tau(X'\beta + (X'\delta)\varepsilon\,|\,X)
    \end{align*}

  \item
    When we condition on $X$, $X'\beta$ is a constant, hence
    \begin{align*}
      Q_\tau(Y|X)
      &=
      Q_\tau(X'\beta + (X'\delta)\varepsilon\,|\,X)
      =
      X'\beta + Q_\tau((X'\delta)\varepsilon\,|\,X)
    \end{align*}

  \item
    Because $P[X'\delta > 0]=1$, the function $h(u)=(X'\delta)u$ is
    strictly increasing.
    Hence,
    \begin{align*}
      Q_\tau( (X'\delta)\varepsilon\,|\,X)
      =
      Q_\tau(h(\varepsilon)\,|\,X)
      =
      h\big(
      Q_\tau(\varepsilon\,|\,X)
      \big)
      =
      (X'\delta)
      Q_\tau(\varepsilon\,|\,X)
    \end{align*}
  \item
    Therefore, we have
    \begin{align*}
      Q_\tau(Y|X)
      &=
      X'\beta
      +
      (X'\delta)
      Q_\tau(\varepsilon\,|\,X)
      \\
      &=
      X'
      \underbrace{%
        \big(
        \beta
        +
        \delta
        Q_\tau(\varepsilon\,|\,X)
        \big)
      }_{\beta_\tau}
    \end{align*}
\end{itemize}
\end{frame}


\begin{frame}[shrink]{Asymptotic Distribution}
For iid data, assume the estimator
\begin{align*}
  \hat{\theta}
  =
  \argmax_{\theta\in\Theta}
  \frac{1}{N}
  \sumiN
  m(Z_i,\theta)
\end{align*}
is consistent for $\theta_0\in \text{Int}(\Theta)$.
Assume also
\begin{itemize}
  \item
    $|m(z,\theta)-m(z,\tilde{\theta})|\leq b(z)\lVert
    \theta-\tilde{\theta}\rVert$
    for all $z,\theta,\tilde{\theta}$ where
    $b(\,\cdot\,)\geq 0$
    satisfies $\E[b(Z_i)^2]<\infty$
  \item $\theta\mapsto m(z,\theta)$ is differentiable at $\theta_0$ with
    prob. 1.
  \item $\theta\mapsto \E[m(Z_i,\theta)]$ is twice differentiable at
    $\theta_0$ with nonsingular Hessian matrix $H$.
\end{itemize}
Then
\begin{align*}
  \sqrt{N}(\hat{\theta}-\theta_0)
  \quad\dto\quad
  N(0,H^{-1}\Sigma H^{-1})
  \quad
  \Sigma
  =
  \E
  \left[
    \frac{\partial m(Z_i,\theta_0)}{\partial \theta}
    \frac{\partial m(Z_i,\theta_0)}{\partial \theta'}
  \right]
\end{align*}
\end{frame}



\begin{frame}[shrink]{Asymptotic Distribution}
Estimator
\begin{align*}
  \hat{\beta}_\tau
  &=
  \argmin_b
  \frac{1}{N}
  \sumiN
  \rho_\tau(Y_i-X_i'b)
  \\
  \rho_\tau(u)
  &=
  (\tau-{1}\{u\leq 0\})u
  =
  \begin{cases}
    (1-\tau)|u| & u \leq 0 \\
    \tau|u| & u > 0
  \end{cases}
\end{align*}
Note
\begin{align*}
  \E[m(Z_i,\theta)]
  &=
  \E\left[
    \rho_\tau(Y_i-X_i'b)
  \right]
  \\
  &=
  \E\left[
    (\tau-{1}\{Y_i\leq X_i'b\})(Y_i-X_i'b)
  \right]
\end{align*}
\end{frame}

\begin{frame}[shrink]{Asymptotic Distribution}
Note
\begin{align*}
  \E[m(Z,\theta)]
  &=
  \E\left[
    (\tau-{1}\{Y\leq X'b\})(Y-X'b)
  \right]
\end{align*}
Differentiate, ignoring kink at $Y=X'b$:
\begin{align*}
  \frac{\partial}{\partial \theta}
  m(Z,\theta)
  &=
  \frac{\partial}{\partial \beta}
  (\tau-{1}\{Y\leq X'b\})(Y-X'b)
  \\
  &=
  -
  (\tau-{1}\{Y\leq X'b\})X
  \\
  &=
  ({1}\{Y\leq X'b\}-\tau)X
\end{align*}
To compute the expected second derivative, start with
\begin{align*}
  \frac{\partial}{\partial\theta}
  \E[m(Z,\theta)]
  &=
  \E\left[
    \frac{\partial}{\partial\theta}
    m(Z,\theta)
  \right]
  \\
  &=
  \E\left[
  \frac{\partial}{\partial\theta'}
  ({1}\{Y\leq X'b\}-\tau)X
  \right]
  \\
  \text{LIE}\quad
  &=
  \E\left[
    \E\big[
    ({1}\{Y\leq X'b\}-\tau)X
    \;|\;X
    \big]
  \right]
  \\
  &=
  \E\left[
    \big(
      \E\big[
      {1}\{Y\leq X'b\}
      \;|\;X
      \big]
      -\tau
    \big)X
  \right]
  \\
  &=
  \E\left[
    \big(
      P[Y\leq X'b |X]
      -\tau
    \big)X
  \right]
  \\
  &=
  \E\left[
    \big(
      F_{Y|X}(X'b)
      -\tau
    \big)X
  \right]
\end{align*}
Second derivative:
\begin{align*}
  \frac{\partial^2}{\partial \theta\partial\theta'}
  \E[m(Z,\theta)]
  &=
  \E\left[
    \frac{\partial^2}{\partial \theta\partial\theta'}
    m(Z,\theta)
  \right]
  =
  \E\left[
    f_{Y|X}(X'b)
    XX'
  \right]
\end{align*}

\end{frame}


\section{Quantile IV}


\begin{frame}[shrink]{Quantile IV: Motivation}
In the non-IV quantile regression case, we do quantile regression when
we believe the conditional $Q_\tau(Y|X)$ satisfies
\begin{align*}
  Q_\tau(Y|X)=X'\beta_\tau
\end{align*}
for some $\beta_\tau$.
This is a restriction on the conditional quantile function that we
exploit by estimating a quantile regression.

In Quantile IV, we also want a conditional quantile restriction, but one
that conditions on $Z$, i.e.
\begin{align}
  Q_\tau(g(Y,X)|Z)=\text{something}
  \label{qivrestriction}
\end{align}
for some function $g$ and some ``something'' that we will pin down in
the next few slides.
Then we can exploit this restriction and estimate some parameters.

The next few slides will pursue a conditional quantile restriction like
Expression~\ref{qivrestriction}.
\end{frame}

\begin{frame}[shrink]{Quantile IV: Setup}
Model
\begin{align*}
  Y = h(X,\varepsilon)
\end{align*}
with $X$ and $\varepsilon$ possibly correlated.
Running example
\begin{itemize}
  \item $Y$: Wages
  \item $X$: Education
  \item $\varepsilon$: Ability
\end{itemize}
Define for fixed $x$ the
\alert{structural quantile function}:
\begin{align*}
  S_\tau(x) = Q_\tau(h(x,\varepsilon))
  \qquad
  \forall x
\end{align*}
This is the $\tau$th quantile if we could reassign \alert{everyone} to
education level $x$, look at the resulting \alert{counterfactual}
distribution of weages (with randomness/differences only arising from
differences in ability $\varepsilon$), and pick out the $\tau$th
quantile.
\end{frame}


\begin{frame}[shrink]{Aside: $Q_\tau(\,\cdot\,)$ Operator}
$Q_\tau(\,\cdot\,)$ is the $\tau$th quantile operator which can be
applied (like the expectation operator $\E[\,\cdot\,]$) to any RV,
$Y$, $X$, $\varepsilon$.
It must be inferred from context what the random object is
whose $\tau$th quantile we want, e.g.
\begin{itemize}
  \item For fixed $x$, the only source of randomness in
    $h(x,\varepsilon)$ is $\varepsilon$, hence
    $Q_\tau(h(x,\varepsilon))$ gets the $\tau$th quantile of the RV
    $h(x,\varepsilon)$, $x$ fixed and $\varepsilon$ random.
  \item
    $Q_\tau(Y)=Q_\tau(h(X,\varepsilon))$ gets the $\tau$th quantile of
    the RV $Y$ or equivalently $h(X,\varepsilon)$, with $X$ and
    $\varepsilon$ both random implying that $h(X,\varepsilon)$ is an RV
    with some $\tau$th quantile.
\end{itemize}
\end{frame}



\begin{frame}[shrink]{Quantile IV: Setup}
Previous slide:
For fixed $x$
\begin{align*}
  S_\tau(x) = Q_\tau(h(x,\varepsilon))
  \qquad
  \forall x
\end{align*}
Can also consider
\begin{align*}
  Q_\tau(Y|X)
  =
  Q_\tau(h(X,\varepsilon)|X)
\end{align*}
This is the $\tau$th quantile if we look at the \alert{actual} (not
counterfactual) dist. of wages at covariate level $X$.
Because of selection, generally
\begin{align*}
  Q_\tau(Y|X=x)
  =
  Q_\tau(h(X,\varepsilon)|X=x)
  \neq
  S_\tau(x) = Q_\tau(h(x,\varepsilon))
\end{align*}
The RHS of the $\neq$ sign involves counterfactual distributions, the
LHS involves realized distributions.
\end{frame}


\begin{frame}[shrink]{Quantile IV: Picture}
To emphasize that
\begin{align*}
  Q_\tau(Y|X=x)
  =
  Q_\tau(h(X,\varepsilon)|X=x)
  \neq
  S_\tau(x) = Q_\tau(h(x,\varepsilon))
\end{align*}
Picture to have in mind
\begin{figure}
  \includegraphics[angle=270, origin=c, scale=0.15, trim={40cm, 5cm, 30cm, 0cm}, clip]{./picture.jpg}
\end{figure}
\end{frame}


\begin{frame}[shrink]{Quantile IV}
Suppose that $h(x,e)$ is strictly increasing in $e$ for any fixed $x$.
Then by monotonicity, for a fixed $x$, we have
\begin{align*}
  S_\tau(x)
  = Q_\tau(h(x,\varepsilon))
  = h(x,Q_\tau(\varepsilon))
\end{align*}
In words
\begin{itemize}
  \item $Q_\tau(h(x,\varepsilon))$:
    Suppose we counterfactually reassign everyone to education level
    $x$. The distribution of wages is then determined only by differences in
    ability $\varepsilon$, and we can compute the $\tau$th quantile in
    wages at that $x$, i.e. compute $Q_\tau(h(x,\varepsilon))$.
  \item
    $h(x,e)$ increasing in $e$ for each fixed $x$:
    Suppose wages are increasing in ability for any level of
    education. The high ability folks always earn more at each level of
    education $x$, no matter the counterfactual level $x$ assigned to
    everyone.
  \item
    $Q_\tau(h(x,\varepsilon)) = h(x,Q_\tau(\varepsilon))$:
    To find out the $\tau$th quantile of wages if everyone has education
    level $x$ (i.e. $Q_\tau(h(x,\varepsilon))$), we just need to find
    the ability of the guy who's always at at the $\tau$th quantile of
    ability (i.e. $Q_\tau(\varepsilon)$), plug in.
\end{itemize}
\end{frame}


\begin{frame}[shrink]{Deducing a Restriction for Estimation}
Next, because the function $h(X,e)$ is increasing in $e$ for any fixed
$X$
\begin{align*}
  \{\varepsilon\leq Q_\tau(\varepsilon)\}
  \quad&\iff\quad
  \{h(X,\varepsilon)\leq h(X,Q_\tau(\varepsilon))\}
  \\
  \text{Last slide}
  \quad&\iff\quad
  \{h(X,\varepsilon)\leq Q_\tau(h(X,\varepsilon))\}
  \\
  \text{Defintions}
  \quad&\iff\quad
  \{Y\leq S_\tau(X)\}
  \\
  \text{Defintions}
  \quad&\iff\quad
  \{Y-S_\tau(X)\leq 0\}
\end{align*}
This result relied strictly on the property that the function $h(X,e)$
is increasing in $e$ for fixed $X$ (regardless of whether $X$ and
$\varepsilon$ are correlated).


If $Z\perp \varepsilon$, then we can take a first step towards deducing
a conditional quantile restriction by using the above results to note
\begin{align*}
  P[Y-S_\tau(X)\leq 0\,|\,Z]
  =
  P[\varepsilon \leq Q_\tau(\varepsilon)\,|\,Z]
  =
  P[\varepsilon \leq Q_\tau(\varepsilon)]
  =
  \tau
\end{align*}
We conditioned on $Z$ in the first line so we can (below) deduce a
conditional quantile restriction.

If we have a linear SQF
\begin{align*}
  S_\tau(x) = x'\alpha_\tau
  \qquad
  \forall x
\end{align*}
the above implies the conditional quantile restriction
\begin{align*}
  Q_\tau(Y-S_\tau(X)|Z)
  =
  0
\end{align*}
\end{frame}

\begin{frame}[shrink]{Quantile IV: The Key Restriction}

Under a linear SQF
\begin{align*}
  S_\tau(x)
  = Q_\tau(h(x,\varepsilon))
  =
  x'\alpha_\tau
\end{align*}
and given an instrument $Z\perp \varepsilon$, we have deduced the
following conditional quantile restriction on the
\begin{align*}
  Q_\tau(Y-X'\alpha_\tau|Z)
  = 0
\end{align*}
This is a restriction that can be exploiting for estimation of
$\alpha_\tau$.
Simply choose $\alpha_\tau$ as the vector that gives teh \emph{smallest}
coefficients when running a quantile regression of $Y-X'\alpha_\tau$ on
$Z$.
\end{frame}






\end{document}





\begin{frame}[shrink]
Binary choice model, for some link function $G(\,\cdot\,)$ with the
properties of a CDF:
\begin{align*}
  P[Y=1|X=x]
  =
  G(x'\beta_0)
\end{align*}
For iid data, the MLE maximizes the log-likelihood function
\begin{align*}
  \hat{\calL}(\beta)
  &=
  \sumiN
  G(X_i'\beta)^{Y_i}
  [1-G(X_i'\beta)]^{1-Y_i}
  \\
  \log\hat{\calL}(\beta)
  &=
  \sumiN
  \bigg\{
  Y_i\log\big( G(X_i'\beta)\big)
  +
  (1-Y_i)
  \log\big(
  1-G(X_i'\beta)
  \big)
  \bigg\}
\end{align*}
Want to show that if $\eta \mapsto \log G(\eta)$ concave, then the
log-likelihood function is globally concave.

For any two points $b_1$ and $b_2$, consider for $\lambda\in[0,1]$:
\begin{align*}
  \log\hat{\calL}(\lambda b_1 + (1-\lambda)b_2)
  &=
  \sumiN
  \bigg\{
  Y_i\log\big( G(X_i'[\lambda b_1 + (1-\lambda)b_2])\big)
  +
  (1-Y_i)
  \log\big(
  1-G(X_i'[\lambda b_1 + (1-\lambda)b_2])
  \big)
  \bigg\}
\end{align*}
Note
\begin{align*}
  G(X_i'[\lambda b_1 + (1-\lambda)b_2])
  =
  G(\lambda X_i'b_1 + (1-\lambda)X_i'b_2)
\end{align*}
By concavity
\begin{align*}
  \log G(\lambda X_i'b_1 + (1-\lambda)X_i'b_2)
  \geq
  \lambda \log G(X_i'b_1)
  +
  (1-\lambda)\log G(X_i'b_2)
\end{align*}

\end{frame}


\begin{frame}[shrink]
Define
\begin{align*}
  \tilde{G}(\eta)
  =
  \log G(\eta)
\end{align*}
Note
\begin{align*}
  \tilde{G}'(\eta)
  =
  \frac{\partial}{\partial \eta}
  \log G(\eta)
  &=
  \frac{G'(\eta)}{G(\eta)}
  \\
  \tilde{G}''(\eta)
  =
  \frac{\partial}{\partial \eta^2}
  \log G(\eta)
  &=
  \frac{G''(\eta)}{G(\eta)}
  -
  \frac{G'(\eta)^2}{G(\eta)^2}
  =
  \frac{G''(\eta)}{G(\eta)}
  -
  \tilde{G}'(\eta)^2
\end{align*}
If $\tilde{G}(\eta)=\log G(\eta)$ concave, then
\begin{align*}
  \tilde{G}''(\eta)
  &\leq 0
  \\
  \implies\quad
  \frac{G''(\eta)}{G(\eta)}
  -
  \tilde{G}'(\eta)^2
  &\leq
  0
  \\
  \frac{G''(\eta)}{G(\eta)}
  &\leq
  \tilde{G}'(\eta)^2
  \\
  G''(\eta)
  &\leq
  \frac{G'(\eta)^2}{G(\eta)}
\end{align*}
\end{frame}


\begin{frame}[shrink]
\begin{align*}
  \log \calL_i(\beta)
  &=
  Y_i\log\big( G(X_i'\beta)\big)
  +
  (1-Y_i)
  \log\big(
  1-G(X_i'\beta)
  \big)
\end{align*}
Derivative
\begin{align*}
  &=
  \left[
  \frac{Y_i}{G(X_i'\beta)}
  -
  \frac{1-Y_i}{1-G(X_i'\beta)}
  \right]
  G'(X_i'\beta)X_i
  \\
  &=
  \left[
  \frac{%
    Y_i(1-G(X_i'\beta))
    -
    (1-Y_i)
    G(X_i'\beta)
  }{G(X_i'\beta)(1-G(X_i'\beta))}
  \right]
  G'(X_i'\beta)X_i
  \\
  &=
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{G(X_i'\beta)(1-G(X_i'\beta))}
  \right]
  G'(X_i'\beta)X_i
  \\
  &=
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{1-G(X_i'\beta)}
  \right]
  \frac{G'(X_i'\beta)}{G(X_i'\beta)}
  X_i
  \\
  &=
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{1-G(X_i'\beta)}
  \right]
  \tilde{G}'(X_i'\beta)
  X_i
  \\
  &=
  [Y_i - G(X_i'\beta)]
  [
  1-G(X_i'\beta)
  ]^{-1}
  \tilde{G}'(X_i'\beta)
  X_i
\end{align*}
\end{frame}


\begin{frame}[shrink]
Derivative again
\begin{align*}
  &=
  -
  G'(X_i'\beta)
  [
    1-G(X_i'\beta)
  ]^{-1}
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  [
    1-G(X_i'\beta)
  ]^{-2}
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  [
    1-G(X_i'\beta)
  ]^{-1}
  \tilde{G}''(X_i'\beta)
  X_iX_i'
\end{align*}
\begin{align*}
  &=
  -
  \frac{1}{%
    1-G(X_i'\beta)
  }
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  \frac{%
    1
  }{%
    (1-G(X_i'\beta))^2
  }
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  \frac{%
    1
  }{%
    (1-G(X_i'\beta))
  }
  \tilde{G}''(X_i'\beta)
  X_iX_i'
\end{align*}

Collect terms
\begin{align*}
  &=
  \bigg[
  Y_i
  \left(
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  +
  \tilde{G}''(X_i'\beta)
  \right)
  \\
  &
  -
  \left(
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  +
  G(X_i'\beta)
  \tilde{G}''(X_i'\beta)
  \right)
  \bigg]
  \frac{1}{%
    1-G(X_i'\beta)
  }
  X_iX_i'
\end{align*}


\end{frame}

\begin{frame}[shrink]
Simplify
\begin{align*}
  &=
  -
  \left[
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
    +
    G(X_i'\beta) \tilde{G}''(X_i'\beta)
    -
    G(X_i'\beta)^2\tilde{G}''(X_i'\beta)
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &
  +
  Y_i
  \left[
  \frac{%
    G'(X_i'\beta)
    \tilde{G}'(X_i'\beta)
  }{%
    (1-G(X_i'\beta))^2
  }
  +
  \frac{%
    \tilde{G}''(X_i'\beta)
    (1-G(X_i'\beta))
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &=
  -
  \left[
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
    +
    G(X_i'\beta) \tilde{G}''(X_i'\beta)
    -
    G(X_i'\beta)^2\tilde{G}''(X_i'\beta)
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &
  +
  Y_i
  \left[
  \frac{%
    G'(X_i'\beta)
    \tilde{G}'(X_i'\beta)
    +
    \tilde{G}''(X_i'\beta)
    (1-G(X_i'\beta))
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &=
  \left(
  \frac{%
    Y_i
    -
    1
  }{%
    (1-G(X_i'\beta))^2
  }
  \right)
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  \left(
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  \right)
  \tilde{G}''(X_i'\beta)
  X_iX_i'
\end{align*}

\end{frame}










\end{document}

















% Sample code
\pause to add breaks
\textcolor{red}{text to show}.
\vspace{20pt}

\usebeamercolor{dull text}
\alert<+>{Words that you want to alert}

