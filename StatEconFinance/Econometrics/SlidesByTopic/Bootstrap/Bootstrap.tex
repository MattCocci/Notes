%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usepackage{cmbright}
%\usetheme{metropolis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis with Sidebar %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aspectratio=169, handout]{beamer}
%\documentclass{beamer}
\usepackage{cmbright}
\usepackage{comment}

\useoutertheme[right, width=0.20\paperwidth]{sidebar}
\usecolortheme{metropolis}
\useinnertheme[sectionpage=none]{metropolis}
\usefonttheme{metropolis}

\makeatletter
\beamer@headheight=1.75\baselineskip     %controls the height of the headline, default is 2.5
\makeatother

\usepackage{etoolbox}
\patchcmd\insertverticalnavigation{\dohead}{\vskip-35pt\dohead}{}{}

%\setbeamertemplate{sidebar canvas right}{}
%\setbeamertemplate{sidebar right}{BOB}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Simple Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usetheme{Boadilla}
%\usepackage{lmodern}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sidebar Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[serif]{beamer}  % For serif latex font
%\usepackage{pxfonts}
%\usepackage{eulervm}
%\usepackage{mathpazo}

%\usetheme{Goettingen}
%\usecolortheme{lily}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Common Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[]{Bootstrap \\ Matt Cocci}
\author[]{}
\date{\today}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{section in toc}[sections numbered]
%\setbeamertemplate{section in toc}[square unnumbered]
%\setbeamertemplate{section in toc}[square unnumbered]

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\usepackage{bbm} % For bold numbers

%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% For easier looping
\usepackage{pgffor}

\usepackage{pdfpages}

%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\hat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}


% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bshatOmega}{\boldsymbol{\hat{\Omega}}}
\newcommand{\bshatG}{\boldsymbol{\hat{G}}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}

% Special cases like \bshatb for \boldsymbol{\hat{b}}
\generate[\hat]{bshat}{\boldsymbol}{b,y,x,X,V,S,W}
\newcommand{\bshatbeta}{\boldsymbol{\hat{\beta}}}
\newcommand{\bshatmu}{\boldsymbol{\hat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\hat{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\hat{\Sigma}}}
\newcommand{\bstildebeta}{\boldsymbol{\tilde{\beta}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bsbarbeta}{\boldsymbol{\overline{\beta}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}

% Redefine \bso to be the zero vector
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\hat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\hat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\hat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Rnk}{\mathbb{R}^{n\times k}}
\newcommand{\Rkk}{\mathbb{R}^{k\times k}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}


% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}


% Various probability and statistics commands
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}


% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\nNz}{_{n=0}^N}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}


\begin{frame}{Outline}
\tableofcontents[hideallsubsections]
%\tableofcontents
\end{frame}




\section{Empirical CDF and Distributions}


{\scriptsize
\begin{frame}{Empirical CDF and Distribution}

Fix a given sample $\{Y_i\}_{i=1}^n$ of scalar RVs.
The \alert{empirical CDF} is defined
\begin{align*}
  \hat{F}_n(y)
  =
  \frac{1}{n}
  \sum_{i=1}^n
  {1}\{Y_i \leq y\}
\end{align*}
We often think of this as an estimate of the true underlying CDF of the
data $Y_i\sim F_0$.
But we can also think of this as a CDF for a RV $X$ that has
distribution
\begin{align*}
  P_{\hat{F}_n}\big[
    X = x
  \big]
  &=
  \frac{1}{n}
  \quad\text{for}\quad
  x\in\{Y_1,\ldots,Y_n\}
  %\begin{cases}
    %%\frac{1}{n} & x=Y_2\\
    %\vdots & \quad \,\,\vdots \\
    %\frac{1}{n} & x=Y_n\\
  %\end{cases}
\end{align*}
This above distribution that puts mass $1/n$ on each observed $Y_i$ is
called the \alert{empirical distribution} associated with the
sample/dataset $\{Y_i\}_{i=1}^n$.
\begin{itemize}
  \item Histograms are a binned representation of the above
    distribution.
  \item
    We can compute the \alert{mean} of empirical distribution in the
    usual way
    \begin{align*}
      \E_{\hat{F}_n}[X]
      =
      \sum_{i=1}^n
      Y_i \cdot \frac{1}{n}
      =
      \frac{1}{n}
      \sum_{i=1}^n
      Y_i
    \end{align*}
    Can compute \alert{quantiles}, \alert{percentiles}, and other
    features of the distribution.

  \item \alert{Resampling} from our dataset/sample is drawing from the
    empirical distribution associated with the observed sample.
\end{itemize}

%%This distribution \alert{induces/implies} an associated sampling
%%distribution for our estimator:
%%\begin{align*}
  %%P_{\hat{F}_n}[T_n^{(b)} \leq \tau]
%%\end{align*}
\end{frame}
}


\section{Characterizing Sampling Distributions}

{\footnotesize
\begin{frame}{Setup}
\alert{Bootstrap}: Way to \alert{approximate} the \alert{sampling}
distribution of statistic/estimator $T_n$ via resampling, no analytical
derivations or strong assumptions.

The frequentist inference problem:
\begin{itemize}
  \item Observe \alert{sample} $\{W_i\}_{i=1}^n$
    drawn iid from \alert{population} distribution $F_0$
  \item Compute some \alert{statistic} $T_n=T_n(W_1,\ldots,W_n)$ based
    on sample.
  \item Want \alert{sampling distribution} of statistic $T_n$.
  \item
    For some given null $\theta_0$, can set
    $T_n={\sqrt{n}}(\hat{\theta}_n-\theta_0)$ (i.e. recentered and
    rescaled estimator of some parameter) and use sampling
    distribution of $T_n$ under some null $\theta_0$ to conduct
    (asymptotically correct) inference about $\theta_0$.
\end{itemize}
\alert{Key Question}:
How to get the sampling distribution of $T_n$, i.e.
\begin{align*}
  P_{F_0}[T_n \leq \tau]
\end{align*}
\end{frame}
}




{\footnotesize
\begin{frame}{Characterizing the Sampling Distribution}
How to get the \alert{sampling distribution} of some statistic, $T_n$,
\begin{align*}
  P_{F_0}[T_n \leq \tau]
\end{align*}
given that observed sample $\{W_i\}_{i=1}^n$ is just a \alert{single}
draw from sampling distribution.
\begin{enumerate}
  {\footnotesize
  \item If we knew/assume the distribution $F_0$, can \alert{deduce}
    exact sampling dist of $T_n=T_n(W_1,\ldots,W_n)$, e.g.
    \begin{align*}
      W_i\iid \calN(\mu,\sigma^2)
      \quad\implies\quad
      T_n
      &=
      \frac{1}{n}
      \sumin
      W_i
      \sim \calN(\mu,\sigma^2/n)
      \\
      P_{F_0}[T_n \leq \tau]
      &=
      \Phi\left(
      \frac{\tau-\mu}{\sigma/\sqrt{n}}
      \right)
    \end{align*}

  \item
    Suppose we can derive the \alert{asymptotic sampling distribution},
    e.g.
    \begin{align*}
      T_n = \sqrt{n}(\hat{\theta}-\theta_0)
      \quad\dto\quad
      \calN(0,V)
      \quad&\implies\quad
      T_n
      \overset{a}{\sim}
      \calN(0,V)
      \\
      \quad&\iff\quad
      \hat{\theta}
      \overset{a}{\sim}
      \calN(\theta_0,V/n)
    \end{align*}
  \item[3/4.]
    Simulation/resampling methods: Next slide
  }
\end{enumerate}
\end{frame}
}


{\footnotesize
\begin{frame}{Characterizing the Sampling Distribution}
How to get the \alert{sampling distribution} of some statistic, $T_n$?
\begin{align*}
  P_{F_0}[T_n \leq \tau]
\end{align*}
\vspace{-10pt}
Approaches
%\vspace{-10pt}
\begin{enumerate}
  {\footnotesize
  \item[3.]
    Suppose we could simulate from the distribution $F_0$.
    \begin{enumerate}
      {\footnotesize
      \item[(i)] Generate $b$th sample of size $n$
        by drawing iid from $F_0$:
        $\{W_i^{(b)}\}_{i=1}^n$
      \item[(ii)] Using $b$th sample, compute
        $T_n^{(b)}=T_n(W_1^{(b)},\ldots,W_n^{(b)})$
      \item[(iii)] Repeat for $B$ samples, where $B$ large to get
        $\{T_n^{(b)}\}_{b=1}^B$
      }
    \end{enumerate}
    The empirical distribution of the simulated $\{T_n^{(b)}\}_{b=1}^B$
    approximates the sampling distribution of the statistic $T_n$.
    \begin{align*}
      P_{F_0}[T_n \leq \tau]
      \approx
      \frac{1}{B}
      \sum_{b=1}^B
      1\{T_n^{(b)}\leq \tau\}
    \end{align*}


  \item[4.] \alert{Bootstrap}:
    We don't know $F_0$, but for large $n$, our dataset
    $\{W_i\}_{i=1}^n$ and associated empirical distribution $\hat{F}_n$
    will \alert{look like} the superpopulation distribution $F_0$.
    Therefore, if we resample from our dataset (from $\hat{F}_n$),
    eventually (for large $n$) it's like we're drawing from $F_0$.

    %\alert{Sample taken as known population}, simulate many samples,
    %get sampling distribution of statistic, use as estimate of true
    %unknown sampling distribution.
  }
\end{enumerate}
\end{frame}
}



{\scriptsize
\begin{frame}{Bootstrap approach to Characterizing Sampling Distribution}

Want sampling distribution of estimator
$T_n=T_n(W_1,\ldots,W_n)$, which is
\begin{align*}
  P_{F_0}[T_n \leq \tau]
\end{align*}
Bootstrap: Given original dataset $\{W_i\}_{i=1}^n$,
\begin{enumerate}
  {\footnotesize
  \item[(i)] Generate $b$th bootstrap by drawing with replacement from
    $\{W_i\}_{i=1}^n$:
    \begin{align*}
      \{W_i^{(b)}\}_{i=1}^n
    \end{align*}
    i.e. draw sample of size $n$ from empirical dist
    associated with $\{W_i\}_{i=1}^n$.
  \item[(ii)] Using $b$th bootstrap sample, compute
    \begin{align*}
      T_n^{(b)}=T_n(W_1^{(b)},\ldots,W_n^{(b)})
    \end{align*}
  \item[(iii)] Repeat for $B$ samples, where $B$ large to get
    $B$ draws from sampling dist:
    \begin{align*}
      \{T_n^{(b)}\}_{b=1}^B
    \end{align*}
  \item[(iv)]
    Use empirical dist of $\{T_n^{(b)}\}_{b=1}^B$ to
    approximate sampling distribution, i.e.
    \begin{align*}
      P_{F_0}[T_n \leq \tau]
      \approx
      P_{\hat{F}_n}
      \big[
      T_n^{(b)}\leq \tau
      \big]
      \approx
      \frac{1}{B}
      \sum_{b=1}^B
      1\{T_n^{(b)}\leq \tau\}
    \end{align*}
  }
\end{enumerate}
\end{frame}
}


{\scriptsize
\begin{frame}{Approximations}

We're relying on the fact that the empirical distribution corresponding
to our dataset $\hat{F}_n$ ``looks like'' (converges to) the true
population distribution for large enough $n$.
Therefore, we're approximating
\begin{align}
  P_{F_0}[T_n \leq \tau]
  \approx
  P_{\hat{F}_n}
  \big[
  T_n^{(b)}\leq \tau
  \big]
  \label{bootapproxasymptotic}
\end{align}
In words, the \alert{true} sampling distribution of
\begin{align*}
T_n=T_n(W_1,\ldots,W_n)
\qquad
W_i\iid F_0
\end{align*}
is approximately equal to the sampling distribution of
\begin{align*}
  T_n^{(b)}=T_n(W_1^{(b)},\ldots,W_n^{(b)})
  \qquad
  W_i^{(b)}
  \iid
  \hat{F}_n
\end{align*}
because $\hat{F}_n$ converges to $F_0$.

The bootstrap is valid by asymptotic arguments, just like the
traditional approach of deriving the asymptotic distribution of an
estimator and using for inference.
It therefore suffers from the same drawbacks as traditional
asymptotic approximations to finite sample distributions.
Moreover, $T_n$ should be something with a well-defined
asymptotic distribution.
Hence
\begin{align*}
  T_n=\sqrt{n}(\hat{\theta}-\theta_0)
  \qquad
  \text{not}
  \qquad
  T_n=\hat{\theta}
\end{align*}
\end{frame}
}





{\footnotesize
\begin{frame}{Approximations}
We approximated the true sampling distribution
\begin{align*}
  P_{F_0}[T_n\leq \tau]
\end{align*}
with the sampling distribution that corresponds to drawing observations
from the empirical distribution associated with our dataset:
$\hat{F}_n$
\begin{align*}
  P_{\hat{F}_n}
  \big[
  T_n^{(b)}\leq \tau
  \big]
\end{align*}
In principle, we could compute this analytically, enumerating all
possible size-$n$ sample draws.
But instead, we just pick a large number $B$, resample as described, and
approximate with the empirical CDF
\begin{align*}
  P_{\hat{F}_n}
  \big[
  T_n^{(b)}\leq \tau
  \big]
  \approx
  \frac{1}{B}
  \sum_{b=1}^B
  1\{T_n^{(b)}\leq \tau\}
\end{align*}
Even though we use the RHS in practice, I will often write the LHS of
the above expression for simplicity and because we can get arbitrarily
close to it for $B$ large enough.
\end{frame}
}








%\begin{frame}[shrink]{Bootstrap: Intuition}
%We have data $\{X_i\}_{i=1}^n$ with associated stat $T_n$.
%\begin{itemize}
  %\item Consider some RV $X^*$ whose \alert{population} dist has
    %support $\{X_i\}_{i=1}^n$ with associated probs $1/n$ (i.e.\
    %a discrete dist over the points $\{X_i\}_{i=1}^n$).

    %Note: Computed $T_n$ no longer ``sample stat'' but now the
    %\alert{true} value for this ``population'' since it's computed on
    %the full ``population'' $\{X_i\}_{i=1}^n$.

  %%\item Note: Be careful to carry around $\hat{\theta}$ (the original
    %%estimate) as the ``population'' parameter in bootstrap calculations.
    %%Example later.
%\end{itemize}
%\end{frame}


\section{Using Bootstrap Draws}
\begin{frame}[shrink]{Using Bootstrap Draws}
We now know how to generate $B$ bootstrap samples,
\begin{align*}
  \big\{W_i^{(b)}\big\}_{i=1}^n
  \qquad
  b = 1,\ldots,B
\end{align*}
from which we can compute statistic of interest on these samples:
\begin{align*}
  \big\{T^{(b)}_n\big\}_{b=1}^B
\end{align*}
which delivers an approximate sampling distribution
\begin{align*}
  P_{F_0}[T_n\leq \tau]
  \approx
  P_{\hat{F}_n}[T_n^{(b)}\leq \tau]
\end{align*}
We now discuss how to use the $\{T_n^{(b)}\}$ draws.
%\begin{itemize}
  %\item Approx sampling distribution
  %\item Standard errrors
  %\item Confidence interval (Efron's percentile interval)
  %\item Percentile-$t$ interval
  %\item Critical values
  %\item $p$-values
  %\item Bias correction
%\end{itemize}
\end{frame}


\subsection{Standard Errors}

{\scriptsize
\begin{frame}{Using Bootstrap Draws: Standard Errors}
\begin{itemize}
  {\scriptsize
  \item Suppose we wish to construct standard errors for our estimate
    $\hat{\theta}$. So take
    \begin{align*}
      T_n=\sqrt{n}(\hat{\theta}-\theta_0)
    \end{align*}
    Notice this is an estimator $\hat{\theta}$ with some
    dist induced by random sampling from true DGP $F_0$,
    minus the truth $\theta_0$ defined from the true DGP $F_0$, scaled
    by $\sqrt{n}$.


  \item
    We draw many bootstrap samples and compute
    \begin{align*}
      \big\{T^{(b)}_n\big\}_{b=1}^B
      =
      \big\{\sqrt{n}(\hat{\theta}^{(b)}-\hat{\theta})\}_{b=1}^B
    \end{align*}
    Notice this is an estimator $\hat{\theta}^{(b)}$ with some
    dist induced by sampling from $\hat{F}_n$,
    minus the truth $\hat{\theta}$ defined from the empirical
    dist $\hat{F}_n$, scaled by $\sqrt{n}$.
    Notice how $\hat{\theta}$ serves as the ``truth'' here.
    This is because resampling from $\hat{F}_n$ means that the
    original dataset \alert{is} the population.
    Since $\hat{\theta}$ is the estimate computed on this pop, it is the
    true value for $\hat{F}_n$.

  \item
    Taking the variance over bootstrap draws (over $b$)
    \begin{align*}
      \hat{V}
      =
      \sqrt{%
        \frac{1}{B-1}
        \sum_{b=1}^B
        \bigg(
        T_n^{(b)}-\overline{T_n^{(b)}}
        \bigg)^2
      }
      \;\implies\;
      \Var(\sqrt{n}(\hat{\theta}-\theta_0))
      &\approx
      \hat{V}
      \;\implies\;
      \Var(\hat{\theta})
      \approx
      \frac{\hat{V}}{n}
    \end{align*}

  \item
    Equivalent to just taking the variance over $\hat{\theta}^{(n)}$,
    which gives $\hat{V}/n$ directly.
  }
\end{itemize}
\end{frame}
}


\subsection{Confidence Intervals}


{\scriptsize
\begin{frame}{Using Bootstrap Draws: Efron's Percentile Interval}
\begin{itemize}
  \item
    Suppose we wish to construct a confidence interval for $\theta_0$.
    Again take
    \begin{align*}
      T_n = \sqrt{n}(\hat{\theta}-\theta_0)
    \end{align*}

  \item Draw many bootstrap samples and compute
    \begin{align*}
      \big\{T^{(b)}_n\big\}_{b=1}^B
      =
      \big\{\sqrt{n}(\hat{\theta}^{(b)}-\hat{\theta})\}_{b=1}^B
    \end{align*}
  \item Compute $\alpha/2$ and $1-\alpha/2$ percentiles
    $\hat{q}_{\alpha/2}$ and $\hat{q}_{1-\alpha/2}$ from bootstrap
    distribution, which satisfy
    \begin{align*}
      1-\alpha
      =
      P_{\hat{F}_n}
      \bigg[
        T_n^{(b)}\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
      \bigg]
    \end{align*}
    By bootstrap consistency, these approximate the corresponding
    percentiles of the sampling distribution of $T_n$:
    \begin{align*}
      1-\alpha
      =
      P_{\hat{F}_n}
      \bigg[
        T_n^{(b)}\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
      \bigg]
      &\approx
      P_{F_0}
      \bigg[
        T_n\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
      \bigg]
      \\
      &=
      P_{F_0}
      \bigg[
        \sqrt{n}(\hat{\theta}-\theta_0)
        \in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
      \bigg]
      \\
      &=
      P_{F_0}
      \bigg[
        \hat{\theta}
        -
        \hat{q}_{1-\alpha/2}/\sqrt{n}
        \leq
        \theta_0-
        \leq
        \hat{\theta}
        -
        \hat{q}_{\alpha/2}/\sqrt{n}
      \bigg]
    \end{align*}

\end{itemize}
\end{frame}
}




{\scriptsize
\begin{frame}{Percentile-$t$ Interval}
Special case when we choose $T_n$ to be an asymptotically pivotal
statistic, i.e. it's asymptotic distribution does not depend upon
unknown parameters:
\begin{align*}
  T_n
  =
  \frac{\sqrt{n}(\hat{\theta}-\theta_0)}{\hat{s}}
  \ra \calN(0,1)
\end{align*}
Given bootstrap draws $\{T_n^{(b)}\}_{b=1}^B$,
compute $\alpha/2$ and $1-\alpha/2$ percentiles
\begin{align*}
  1-\alpha
  &=
  P_{\hat{F}_n}
  \bigg[
    T_n^{(b)}\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
  \bigg]
\end{align*}
By bootstrap consistency, these approximate the corresponding
percentiles of sampling dist:
\begin{align*}
  1-\alpha
  =
  P_{\hat{F}}
  \bigg[
    T_n^{(b)}\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
  \bigg]
  &\approx
  P_{F_0}
  \bigg[
    T_n\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
  \bigg]
  %\\
  %&=
  %P_{F_0}
  %\bigg[
    %\frac{\sqrt{n}(\hat{\theta}_n-\theta_0)}{\hat{s}_n}
    %\in [\hat{q}_{\alpha/2},\hat{q}_{1-\alpha/2}]
  %\bigg]
  \\
  &=
  P_{F_0}
  \bigg[
    \hat{\theta}
    -\hat{q}_{1-\alpha/2}
    \frac{\hat{s}}{\sqrt{n}}
    \leq
    \theta_0
    \leq
    \hat{\theta}
    -\hat{q}_{\alpha/2}
    \frac{\hat{s}}{\sqrt{n}}
  \bigg]
\end{align*}
Hence, \alert{percentile-t interval}
\begin{align*}
  \bigg[
    \hat{\theta}
    -\hat{q}_{1-\alpha/2}
    \frac{\hat{s}}{\sqrt{n}}
    ,\;\;
    \hat{\theta}
    -\hat{q}_{\alpha/2}
    \frac{\hat{s}}{\sqrt{n}}
  \bigg]
\end{align*}
which is easily constructed given $\hat{q}_{1-\alpha/2}$ and
$\hat{q}_{\alpha/2}$ computed from the bootstrap distribution and
$(\hat{\theta},\hat{s})$ computed from the original sample.
\end{frame}
}



\subsection{Critical Values}

\begin{frame}{Critical Values}
Consider a two-sided test that rejects for large values of $|T_n|$.
Given bootstrapped $\{T_n^{(b)}\}_{b=1}^B$, find a critical value such
that
\begin{align*}
  \alpha
  =
  P_{\hat{F}_n}\big[\big|T_n^{(b)}\big|\geq cv\big]
\end{align*}
Reject if $T_n>cv$.
Under bootstrap consistency,
\begin{align*}
  P_{F_0}[T_n\geq cv]
  \approx
  P_{\hat{F}_n}[T_n^{(b)}\geq cv]
  = \alpha
\end{align*}
Recall that $\hat{T_n}$ should converge to a non-degenerate asymptotic
distribution, e.g.
\begin{align*}
  T_n
  &=
  \sqrt{n}|\hat{\theta}-\theta_0|
  %\\
  %T_n^{(b)}
  %&=
  %\sqrt{n}|\hat{\theta}^{(b)}-\hat{\theta}|
\end{align*}
\end{frame}


{\footnotesize
\begin{frame}{Critical Value: Two-Sided Test}
When doing a two-sided test, we have a choice of basing the test on
$|T_n|$ and choosing a single critical value, or defining a possibly
asymmetric critical region such that we reject if
$T_n\not\in [q_{\alpha/2},q_{1-\alpha/2}]$.

In general, we don't have good reasons to prefer one over the other from
a first-order asymptotic perspective.
In finite samples, the choice will generally matter for inference, but
because  everything is normal and symmetric asymptotically, we have no
reason to prefer one over the other from a first-order asymptotic
perspective.

However, if we bootstrap a pivotal statistic, we generally reap an
asymptotic refinement, in which case the critical values from the
bootstrap approximation might be better than those from the normal
approximation, even though asymptotically, the approaches are the same
from a first-order perspective.

%Both are fine for the homework.
See the Horowitz handbook chapter and its discussion on bootstrapping
critical values.
\end{frame}
}


\subsection{P-Values}

\begin{frame}[shrink]{Using Bootstrap Draws: $p$-Values}
Consider a two-sided test that rejects for large values of $T_n$.
For example $T_n=\sqrt{n}|\hat{\theta}-\theta_0|$.
The $p$-value is the probability under the null that we observe
something as extreme or more-extreme than realized $t_n$ computed on the
original dataset $\{W_i\}_{i=1}^n$:
\begin{align*}
  \text{p-value}
  =
  P_{F_0}[T_n \geq t_n]
\end{align*}
where $T_n$ is the RV (that varies across repeated samples) and $t_n$ is
the fixed realized value of the $t$-statistic computed in the original
dataset $\{W_i\}_{i=1}^n$.
Under bootstrap consistency,
\begin{align*}
  \text{p-value}
  =
  P_{F_0}[T_n \geq t_n]
  \approx
  P_{\hat{F}_n}[T_n^{(b)}\geq t_n]
\end{align*}
%Special case when
%\begin{align*}
  %T_n
  %&=
  %\sqrt{n}|\hat{\theta}_n-\theta_0|
  %\\
  %T_n^{(b)}
  %&=
  %\sqrt{n}|\hat{\theta}_n^{(b)}-\hat{\theta}_n|
%\end{align*}
%in which case we compute the share of bootstrap estimates $T_n^{(b)}$
%that are greater than $T_n$.
\end{frame}




\section{Alternative Ways to Obtain Bootstrap Draws}


{\footnotesize
\begin{frame}{Alternative Ways to Obtain Bootstrap Draws: Motivation}

Given an original dataset $\{W_i\}_{i=1}^n$ and associated value of
statistic $T_n=T_n(W_1,\ldots,W_n)$, we saw how to generate
multiple \alert{bootstrap samples}
\begin{align*}
  \{W_i^{(b)}\}_{i=1}^n
  \quad b=1,\ldots,B
\end{align*}
from which we can compute the statistic on each bootstrap sample
\begin{align*}
  T_n^{(b)}=T_n(W_1^{(b)},\ldots,W_n^{(b)})
\end{align*}
and then use the empirical distribution over the $B$ bootstrap samples
to approximate the unknown sampling distribution of the statistic:
\begin{align*}
  \frac{1}{B}
  \sum_{b=1}^B
  1\{T_n^{(b)}\leq \tau\}
  \approx
  P_{F_0}[T_n \leq \tau]
\end{align*}
We presented a single way to obtain bootstrap samples
$\{W_i^{(b)}\}_{i=1}^n$ from the original sample $\{W_i\}_{i=1}^n$,
called the \alert{nonparametric bootstrap}, but there are other ways to
do so, although less generally applicable.

You should default to nonparametric bootstrap, but understand the others
since people sometimes use them.
\end{frame}
}


\subsection{Nonparametric Bootstrap}

{\footnotesize
\begin{frame}{Nonparametric Bootstrap}

Setting
\begin{itemize}
  \item
    \alert{Sample $\{W_i\}_{i=1}^n$} where $W_i$ is a vector (entire row
    in Stata) with all the variables and covariates of interest for
    observation $i$
\end{itemize}
{Mechanics}
\begin{itemize}
  \item Draw randomly with replacement from the original sample
    $\{W_i\}_{i=1}^n$ to generate
    \alert{bootstrap sample $\{W_i^*\}_{i=1}^n$}?
\end{itemize}
\end{frame}
}


\subsection{Parametric Bootstrap}

{\footnotesize
\begin{frame}{Parametric Bootstrap}


Setting
\begin{itemize}
  \item
    \alert{Sample $\{W_i\}_{i=1}^n$} where $W_i$ is a vector (entire row
    in Stata) with all the variables and covariates of interest for
    observation $i$
  \item Fully specified likelihood $p(W_i|\theta)$.
    Given $\theta$, can draw $W_i$
\end{itemize}
{Mechanics}
\begin{itemize}
  \item Estimate $\hat{\theta}$ on original sample $\{W_i\}_{i=1}^n$.
  \item Simulate new data $\{W_i^*\}_{i=1}^n$ from model under
    $\hat{\theta}$, i.e. draw from $p(W_i^*|\hat{\theta})$
\end{itemize}
\end{frame}
}


\subsection{Residual and Wild Bootstrap}

{\footnotesize
\begin{frame}{Residual and Wild Bootstrap}
Setting
\vspace{-7pt}
\begin{itemize}
  \item \alert{Sample $\{W_i\}_{i=1}^n=\{Y_i,X_i\}_{i=1}^n$} where $Y_i$
    is a vector with all the variables and covariates of interest for
    observation $i$
  \item Model:
    $Y_i = h(X_i,\theta) + u_i$
\end{itemize}
\vspace{-7pt}
\alert{Residual Bootstrap}
\vspace{-7pt}
\begin{itemize}
  \item Estimate $\theta$ and construct $\hat{Y}_i$ and
    $\hat{u}_i$.
  \item Compute $\{\hat{Y}_i,\hat{u}_i\}_{i=1}^n$
  \item Randomly resample $\{\hat{u}_i^*\}_{i=1}^n$ with repl.
    from $\{\hat{u}_i\}_{i=1}^n$
  \item Construct
    $\{W_i^*\}_{i=1}^n=\{\hat{Y}_i+\hat{u}_i^*,X_i\}_{i=1}^n$
\end{itemize}
\vspace{-7pt}
\alert{Wild Bootstrap}
\vspace{-7pt}
\begin{itemize}
  \item Estimate $\theta$ and $u_i$ in sample.
  \item Compute $\{\hat{Y}_i,\hat{u}_i\}_{i=1}^n$
  \item Randomly sample $\{\omega_i^*\}_{i=1}^n$
    where $\omega_i^*$ equals $\pm 1$ with probability 1/2 each.
  \item Construct bootstrap sample
    $\{\hat{Y}_i+\omega_i^*\hat{u}_i,X_i\}_{i=1}^n$
\end{itemize}
\end{frame}
}

\subsection{Cluster Bootstrap}


{\footnotesize
\begin{frame}{Cluster Bootstrap}
Setting
\begin{itemize}
  %\item
    %\alert{Sample $\{W_i\}_{i=1}^n$} where $W_i$ is a vector (entire row
    %in Stata) with all the variables and covariates of interest for
    %observation $i$
  \item Suppose we can group individual observations into clusters
    $\{W_g\}_{g=1}^G$, where $W_g$ stacks all observations in cluster
    $g$.
  \item For Wild Cluster Bootstrap, we further suppose
    $\{W_g\}_{g=1}^G=\{Y_g,X_g\}_{g=1}^G$, where $Y_g$ and $X_g$ stack
    all observations in cluster $g$.
\end{itemize}
\alert{Cluster Bootstrap}
\begin{itemize}
  \item Nonpar bootstrap, but we randomly resample
    \alert{whole clusters} with replacement.
\end{itemize}
\alert{Wild Cluster Bootstrap}:
Suppose we can group the data into clusters
\begin{itemize}
  \item Estimate model and compute $\{\hat{Y}_g,\hat{u}_g\}_{g=1}^G$
  \item Randomly sample $\{\omega_g^*\}_{g=1}^G$ where
    $\omega_g^*$ equals $\pm$ with probability 1/2 each.
  \item Construct bootstrap sample
    $\{\hat{Y}_g^*+\omega_g^*\hat{u}_g,X_g\}_{g=1}^G$
\end{itemize}
So we're flipping signs of residuals cluster-by-cluster.
\end{frame}
}



\subsection{Block Bootstrap}

{\footnotesize
\begin{frame}{Obtaining Bootstrap Draws: Serial Corr}
Suppose we have data $\{W_t\}_{t=1}^T$ with unknown time-dependence,
i.e. we don't know or don't want to fit an ARMA model, but we suspect
autocorrelation.

Example: Regression model with serially correlated $\{W_t\}=\{y_t,x_t\}$
and we forgot everything about HAC/HAR inference.

\alert{Block Bootstrap}
\begin{itemize}
  \item Define blocks of $\ell$ total observations.
    Can be non-overlapping
    \begin{align*}
      \{W_1,\ldots,W_\ell\},
      \;
      \{W_{\ell+1},\ldots,W_{2\ell}\},
      \cdots
    \end{align*}
    or overlapping
    \begin{align*}
      \{W_1,\ldots,W_\ell\},
      \;
      \{W_{2},\ldots,W_{\ell+1}\},
      \cdots
    \end{align*}
    with $\ell=n^r$ for $r\in[1/3,1/5]$ (see Horowitz),
    depending on what you will use bootstrap dist for.
    Note: $\ell$ must increase with $n$ to work.

  \item Draw blocks randomly with replacement, keeping order within
    blocks, and laying them end-to-end.

\end{itemize}
\end{frame}
}



%\begin{frame}[shrink]{Definitions and Objects of Interest}
%\begin{itemize}
  %\item $Z_i \sim \alert{F_0}$
    %where $F_0\in \{F\}$, some relevant family.
  %\item $\alert{T_n}$: Statistic of interest, e.g.
    %$\alert{T_n=\sqrt{n}(\hat{\theta}-\theta_0)}$.

    %\alert{Note}:
    %Must have well-defined limiting dist withing family of dists, that's
    %why we bootstrap $\sqrt{n}(\hat{\theta}-\theta_0)$ for inference
    %about $\theta_0$ and not $\sqrt{n}\hat{\theta}$.

  %\item
    %Implied finite sample dist/CDF of $T_n$ when $Z_i\sim F$
    %\begin{align*}
      %G_n(\tau,F)=P_F[T_n\leq \tau]
    %\end{align*}
  %\item
    %True finite-sample CDF and target:
    %\begin{align*}
      %G_n(\tau,F_0)=P_{F_0}[T_n\leq \tau]
    %\end{align*}
  %\item
    %Limiting dist of $T_n$:
    %\begin{align*}
      %G_\infty(\tau,F_0)=\lim_{n\ra\infty} P_{F_0}[T_n\leq \tau]
    %\end{align*}

  %\item Estimator $\hat{F}$ of $F_0$
  %\item $G_n(\tau,\hat{F})$:
    %Dist of test statistic under estimator $\hat{F}$.
  %\item $T_n$ is pivotal if it's CDF $G_n(\tau,F)$ does not dependent
    %upon $F$.
%\end{itemize}
%\end{frame}


\section{Formalities: Bootstrap Consistency}

{\scriptsize
\begin{frame}{Bootstrap Consistency from Horowitz (2001)}

Let $\rho(\cdot,\cdot)$ be a metric on the space of probability
distributions for the data $W$.
Let the following denote the true finite-sample CDF of the statistic
$T_n$ when $W\sim F$:
\begin{align*}
  G_n(\tau,F) = P_F[T_n\leq \tau]
\end{align*}
Let the following denote the asymptotic CDF of the statistic $T_n$
(which must converge to a nondegenerate limiting distribution)
when $W\sim F$
\begin{align*}
  G_\infty(\tau,F)
  =
  \limn
  P_{F}[T_n\leq \tau]
\vspace{-8pt}
\end{align*}
Assume for any $\varepsilon>0$:
\vspace{-5pt}
\begin{itemize}
  \item[(i)] \alert{Consistency} of $\hat{F}_n$:
    $\limn P[\rho(\hat{F}_n,F_0)>\varepsilon]= 0$
  \item[(ii)] $G_\infty(\tau,F)$ is continuous in $\tau$ for each
    possible distribution $F$ of $W$
  \item[(iii)] $\forall \tau$ and any sequence $\{F_k\}$ of dists for
    $W$ satisfying $\limn \rho(F_k,F_0)=0$,
    \begin{align*}
      \lim_{k\ra\infty}
      G_n(\tau,F_k)
      =
      G_\infty(\tau,F_0)
\vspace{-7pt}
    \end{align*}
\end{itemize}
\vspace{-5pt}
Then for any $\varepsilon>0$
\vspace{-5pt}
\begin{align*}
  \limn
  P\left[
    \sup_{\tau\in \R}
    \big\lvert
    G_n(\tau,\hat{F}_n)-G_\infty(\tau,F_0)
    \big\rvert
    > \varepsilon
  \right]
  =
  0
\end{align*}
\vspace{-10pt}
We use this informally as
\vspace{-10pt}
\begin{align*}
  P_{\hat{F}_n}[T_n\leq \tau]
  \approx
  \limn
  P_{F_0}[T_n\leq \tau]
  \approx
  P_{F_0}[T_n\leq \tau]
\end{align*}

\end{frame}
}


\section{Failures of the Bootstrap}


{\footnotesize
\begin{frame}{Failures of the Bootstrap}

The bootstrap works well in problems where the parameter is scalar value
in some open set.

It does poorly with boundary parameters.

It also does poorly if we choose the wrong bootstrap and our sample
scheme does not reflect the DGP or the sampling scheme.
\begin{itemize}
  \item Usual the simple residual bootstrap in a situation with
    heteroskedasticity generates bootstrap samples that are unlike the
    DGP that generated the observed sample.
  \item If we use a nonparametric bootstrap, but the sample scheme was
    clustered, we will conduct incorrect inference.
\end{itemize}
\end{frame}
}



\end{document}








% Sample code
\pause to add breaks
\textcolor{red}{text to show}.
\vspace{20pt}

\usebeamercolor{dull text}
\alert<+>{Words that you want to alert}

