%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usepackage{cmbright}
%\usetheme{metropolis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis with Sidebar %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aspectratio=169, handout]{beamer}
%\documentclass[aspectratio=169]{beamer}
%\documentclass{beamer}
\usepackage{cmbright}
\usepackage{comment}

\useoutertheme[right, width=0.20\paperwidth]{sidebar}
\usecolortheme{metropolis}
\useinnertheme[sectionpage=none]{metropolis}
\usefonttheme{metropolis}

\makeatletter
\beamer@headheight=1.75\baselineskip     %controls the height of the headline, default is 2.5
\makeatother

\usepackage{etoolbox}
\patchcmd\insertverticalnavigation{\dohead}{\vskip-35pt\dohead}{}{}

%\setbeamertemplate{sidebar canvas right}{}
%\setbeamertemplate{sidebar right}{BOB}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Simple Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usetheme{Boadilla}
%\usepackage{lmodern}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sidebar Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[serif]{beamer}  % For serif latex font
%\usepackage{pxfonts}
%\usepackage{eulervm}
%\usepackage{mathpazo}

%\usetheme{Goettingen}
%\usecolortheme{lily}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Common Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[]{Estimation \\ Matt Cocci}
\author[]{}
\date{April 15, 2020}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
%\setbeamertemplate{section in toc}[square unnumbered]
%\setbeamertemplate{subsection in toc}[square unnumbered]

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\usepackage{bbm} % For bold numbers

%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% For easier looping
\usepackage{pgffor}

\usepackage{pdfpages}

%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\hat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}


% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bshatOmega}{\boldsymbol{\hat{\Omega}}}
\newcommand{\bshatG}{\boldsymbol{\hat{G}}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}

% Special cases like \bshatb for \boldsymbol{\hat{b}}
\generate[\hat]{bshat}{\boldsymbol}{b,y,x,X,V,S,W}
\newcommand{\bshatbeta}{\boldsymbol{\hat{\beta}}}
\newcommand{\bshatmu}{\boldsymbol{\hat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\hat{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\hat{\Sigma}}}
\newcommand{\bstildebeta}{\boldsymbol{\tilde{\beta}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bsbarbeta}{\boldsymbol{\overline{\beta}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}

% Redefine \bso to be the zero vector
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\hat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\hat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\hat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Rnk}{\mathbb{R}^{n\times k}}
\newcommand{\Rkk}{\mathbb{R}^{k\times k}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}


% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}


% Various probability and statistics commands
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}


% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\nNz}{_{n=0}^N}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}


\begin{frame}{Outline}
\tableofcontents[hideallsubsections]
%\tableofcontents
\end{frame}



\section{Maximum Likelihood}

\subsection{MLE and Misspecification}

\subsubsection{General Result}

{\scriptsize
\begin{frame}{General MLE under Possible Misspecification}
Suppose we have a sample $W_i\iid p(W_i|\theta)$ from which we define
the log-likelihood for a single observation
$\log p(W_i|\theta)$.

The MLE, by definition, maximizes the sample log-likelihood
\begin{align*}
  \hat{\theta}
  &=
  \argmax_\theta
  \calL(\theta)
  =
  \argmin_\theta
  -
  \sumin
  \log p(W_i|\theta)
\end{align*}
Hence it satisfies the FOC 
\begin{align*}
  0
  &=
  \frac{\partial \calL(\theta)}{\partial \theta}
  \bigg|_{\theta=\hat{\theta}}
\end{align*}
which we can Taylor expand about the true value $\theta_0$ (row-by-row):
\begin{align*}
  0
  \approx
  \frac{\partial \calL(\theta_0)}{\partial \theta}
  +
  \frac{\partial^2 \calL(\tilde{\theta})}{\partial \theta\partial\theta'}
  (\hat{\theta}-\theta_0)
\end{align*}
which we can rearrange
\begin{align*}
  \sqrt{n} (\hat{\theta}-\theta_0)
  &=
  -
  \left[
  \frac{\partial^2 \calL(\tilde{\theta})}{\partial \theta\partial\theta'}
  \right]^{-1}
  \sqrt{n}
  \frac{\partial \calL(\theta_0)}{\partial \theta}
  \\
  &=
  -
  \left[
  \frac{1}{n}
  \sumin
  \frac{\partial^2 \log p(W_i|\tilde{\theta})}{\partial \theta\partial\theta'}
  \right]^{-1}
  \frac{1}{\sqrt{n}}
  \sumin
  \frac{\partial \log p(W_i|\theta_0)}{\partial \theta}
\end{align*}
We now consider the probability limit of this expression.
\end{frame}
}


{\scriptsize
\begin{frame}{General MLE under Possible Misspecification}
Allowing the model to be \alert{misspecified},
estimator $\dto \calN$ with sandwich variance matrix:
\begin{align*}
  \sqrt{n} (\hat{\theta}-\theta_0)
  &=
  -
  \left[
  \frac{1}{n}
  \sumin
  \frac{\partial^2 \log p(W_i|\tilde{\theta})}{\partial \theta\partial\theta'}
  \right]^{-1}
  \frac{1}{\sqrt{n}}
  \sumin
  \frac{\partial \log p(W_i|\theta_0)}{\partial \theta}
  \\
  &\dto
  \calN\left(
  0,\;
  A^{-1}
  \Omega
  A^{-1}{}'
  \right)
\end{align*}
where, for pseudo-true parameter $\theta_0$ (which is a well-defined
object that minimizes the KL divergence between the true DGP and
the parametric family we apply MLE to),
\begin{align*}
  A
  &=
  -
  \E\left[
  \frac{\partial^2 \log p(W_i|\theta_0)}{\partial \theta\partial\theta'}
  \right]
  \qquad
  \Omega
  =
  \Var\left(
    \frac{\partial \log p(W_i|\theta_0)}{\partial \theta}
  \right)
  =
  \E
  \left[
    \frac{\partial \log p(W_i|\theta_0)}{\partial \theta}
    \frac{\partial \log p(W_i|\theta_0)}{\partial \theta'}
  \right]
\end{align*}
We also know that if the model is correctly \alert{specified},
we have the information equality,
\begin{align*}
  I_{\theta_0}
  =
  A
  =
  \Omega
  \quad\implies\quad
  \sqrt{n} (\hat{\theta}-\theta_0)
  &\dto
  \calN\left(
  0,\;
  I_{\theta_0}^{-1}
  \right)
\end{align*}
We can estimate the quantities $A$ and $\Omega$ by
\begin{align*}
  \widehat{A}
  &=
  -
  \frac{1}{n}
  \sumin
  \frac{\partial^2 \log p(W_i|\hat{\theta})}{\partial \theta\partial\theta'}
  \quad
  \widehat{\Omega}_1
  =
  \widehat{\Var}\left(
    \frac{\partial \log p(W_i|\hat{\theta})}{\partial \theta}
  \right)
  \quad
  \widehat{\Omega}_2
  =
  \frac{1}{n}
  \sumin
  \frac{\partial \log p(W_i|\hat{\theta})}{\partial \theta}
  \frac{\partial \log p(W_i|\hat{\theta})}{\partial \theta'}
\end{align*}
\end{frame}
}


\subsubsection{OLS Example}


\begin{frame}[shrink]{QMLE in Normal Linear Regression is OLS}
Normal linear regression model
\begin{align*}
  Y = X'\beta + \varepsilon
  \qquad
  (\varepsilon|X)
  \sim
  \calN(0,\sigma^2)
\end{align*}
This implies
\begin{align*}
  (Y|X)
  \sim
  \calN(X'\beta,\sigma^2)
\end{align*}
Log-Likelihood
\begin{align*}
  \hat{\calL}(\beta)
  &=
  \prodiN
  \frac{1}{\sqrt{2\pi\sigma^2}}
  \exp\left\{
    -\frac{1}{2\sigma^2}
    (Y_i-X_i'\beta)^2
  \right\}
  \\
  \log
  \hat{\calL}(\beta)
  &=
  - \frac{1}{2}
  \sumiN
  \log\left(
  2\pi\sigma^2
  \right)
  -
  \sumiN
  \frac{1}{2\sigma^2}
  (Y_i-X_i'\beta)^2
  \\
  &=
  - \frac{N}{2}
  \log\left(
  2\pi
  \right)
  - \frac{N}{2}
  \log\left(
  \sigma^2
  \right)
  -
  \frac{1}{2\sigma^2}
  \sumiN
  (Y_i-X_i'\beta)^2
\end{align*}
\end{frame}


\begin{frame}[shrink]{QMLE in Normal Linear Regression is OLS}
First order conditions
\begin{align*}
  \beta:\quad
  0
  &=
  -2
  \sumiN
  \frac{1}{2\sigma^2}
  (Y_i-X_i'\beta)
  X_i
  \\
  \sigma^2:\quad
  0
  &=
  - \frac{N}{2\sigma^2}
  +
  \frac{1}{2(\sigma^2)^2}
  \sumiN
  (Y_i-X_i'\beta)^2
\end{align*}
Notice that to get $\beta$, we minimize the sum of squared residuals.
Hence the QMLE coincides with OLS.

Solve these conditions to get
\begin{align*}
  \hat{\beta}
  &=
  \left[
  \sumiN
  X_iX_i'
  \right]^{-1}
  \left[
  \sumiN
  X_iY_i
  \right]
  \\
  \hat{\sigma}^2
  &=
  \frac{1}{N}
  \sumiN
  (Y_i-X_i'\hat{\beta})^2
  =
  \frac{1}{N}
  \sumiN
  \hat{\varepsilon}_i^2
\end{align*}

\end{frame}





\section{Optimal Instruments}

{\footnotesize
\begin{frame}{Optimal Instruments}

These next few slides are just for fun.

Because estimation of linear panel data model works by generating moment
conditions from assumptions of the form
\begin{align*}
  \E[\varepsilon_i|\alpha_i,x_{i1},\ldots,x_{iT}]
\end{align*}
one might wonder more generally how to optimally construct moment
conditions from such assumptions.

The next few slides state a very nice result that offers some intuition
and a starting point for thinking about that.
\end{frame}
}


{\footnotesize
\begin{frame}{Optimal Instruments: Motivation}
Given a model in which some function $g(W_i,\theta_0)$ of
observed data $W_i$ and true parameter vector $\theta_0$ is mean-zero,
conditional on observed $X_i$ (elements generally also in $W_i$):
\begin{align}
  0 =
  \E[
    \underbrace{%
      g(W_i,\underset{(k\times 1)}{\theta_0})
    }_{p\times 1}
    |
    X_i
  ]
  \label{momentmodel}
\end{align}
Think of $g(W_i,\theta)$ as a \alert{residual} function or set of
residuals.

\alert{Examples}:
\begin{itemize}
  \item OLS: $g(W_i,\theta)=Y_i-X_i'\theta$
  \item NLS: $g(W_i,\theta)=Y_i-f(X_i,\theta)$.
\end{itemize}
For any $(m\times p)$-matrix-valued function $a(X_i)$,
we can use the following $m$ moment conditions to estimate the
parameters $\theta$ if $m>k$.
\begin{align}
  0 &= \E[a(X_i)g(W_i,\theta_0)]
\end{align}
\alert{Question}:
Which $a(\,\cdot\,)$ to use to form moment conditions?
Optimal choice?
\end{frame}
}

{\footnotesize
\begin{frame}{Optimal Instruments: Chamberlain (1987)}
Asymptotically efficient estimator can be obtained by using
\begin{align*}
  \underset{(k \times p)}{%
    a_*(X_i)
  }
  &=
  \underset{(k \times p)}{%
  G(X_i)'
  }
  \underset{(p \times p)}{%
  V(X_i)^{-1}
  }
\end{align*}
where
\begin{align*}
  G(X_i)
  &:=
  \E\left[
    \frac{\partial g(W_i,\theta_0)}{\partial \theta}
    \;\bigg|\;
    X_i
  \right]
  \\
  V(X_i)
  &:=
  \E\left[
    g(W_i,\theta_0)
    g(W_i,\theta_0)'
    \;\big|\;
    X_i
  \right]
\end{align*}
In words, $V(X_i)$ is the conditional variance of the residual.
$G(X_i)$ gives the average sensitivity of the residual in response to
changes in the parameters.

Moreover, the GMM estimator using optimal moment conditions
$\E[a_*(X_i)g(W_i,\theta_0)]=0$
satisfies
\begin{align*}
  \sqrt{n}(\hat{\theta}-\theta_0)
  \quad\dto\quad
  \calN\big(
  0,\,
  \E\big[G(X_i)'V(X_i)^{-1}G(X_i)\big]^{-1}
  \big)
\end{align*}

\end{frame}
}


{\footnotesize
\begin{frame}{Optimal Instruments: Chamberlain (1987)}

Surprising result:
\begin{itemize}
  \item Notice the optimal estimator is a \alert{method of moments}
    estimator.
  \item Provided we can choose any function $a(X_i)$, we \alert{do not}
    need to generate infinitely many instruments to fully exploit the
    assumed independence condition $\E[g(W_i,\theta_0)|X_i]$.

  \item We can optimally estimate $\theta_0\in \R^k$ with only $k$
    well-chosen moments.
\end{itemize}
\end{frame}
}

{\footnotesize
\begin{frame}{Optimal Instruments: Example}
Suppose we have a linear model
\begin{align*}
  Y_i &= X_i'\theta_0 + \varepsilon_i
  \qquad
  \quad\text{where}\quad
  0 =
  \E[\varepsilon_i\,|\,X_i]
  =
  \E[
    \underbrace{%
      Y_i-X_i'\theta_0
    }_{g(W_i,\theta_0)}
    \,|\,X_i]
\end{align*}
Can compute
\begin{align*}
  V(X_i)
  &=
  \E\left[
    g(W_i,\theta_0)
    g(W_i,\theta_0)'
    \;\big|\;
    X_i
  \right]
  =
  \E\left[
    \varepsilon_i^2
    \,\big|\,
    X_i
  \right]
  =
  \Var(\varepsilon_i|X_i)
  \\
  G(X_i)
  &=
  \E\left[
    \frac{\partial g(W_i,\theta_0)}{\partial \theta}
    \;\bigg|\;
    X_i
  \right]
  =
  -X_i
  \\
  \implies\quad
  a_*(X_i)
  &=
  \frac{1}{\Var(\varepsilon_i|X_i)}
  X_i
\end{align*}
Therefore, the optimal choice of moment conditions for estimation of
$\theta_0$ in the linear model is
\begin{align*}
  0
  &=
  \E[a_*(X_i)g(W_i,\theta_0)]
  =
  \E\left[
    \frac{1}{\Var(\varepsilon_i|X_i)}
    X_i\varepsilon_i
  \right]
\end{align*}
Notice that these GMM moment conditions are precisely equivalent to the
FOCs \alert{GLS} objective function.
\end{frame}
}


\end{document}







% Sample code
\pause to add breaks
\textcolor{red}{text to show}.
\vspace{20pt}

\usebeamercolor{dull text}
\alert<+>{Words that you want to alert}

