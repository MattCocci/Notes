%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usepackage{cmbright}
%\usetheme{metropolis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis with Sidebar %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[handout]{beamer}
%\documentclass{beamer}
\usepackage{cmbright}

\useoutertheme[right, width=0.20\paperwidth]{sidebar}
\usecolortheme{metropolis}
\useinnertheme[sectionpage=none]{metropolis}
\usefonttheme{metropolis}

\makeatletter
\beamer@headheight=1.75\baselineskip     %controls the height of the headline, default is 2.5
\makeatother

%\setbeamertemplate{sidebar canvas right}{}
%\setbeamertemplate{sidebar right}{BOB}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Simple Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usetheme{Boadilla}
%\usepackage{lmodern}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sidebar Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[serif]{beamer}  % For serif latex font
%\usepackage{pxfonts}
%\usepackage{eulervm}
%\usepackage{mathpazo}

%\usetheme{Goettingen}
%\usecolortheme{lily}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Common Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[]{ECO-466/FIN-521: Precept, Week 1 \\ Matt Cocci}
\author[]{}
\date{\today}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
%\setbeamertemplate{section in toc}[square unnumbered]
%\setbeamertemplate{subsection in toc}[square unnumbered]

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\usepackage{bbm} % For bold numbers

%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% For easier looping
\usepackage{pgffor}

\usepackage{pdfpages}

%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\hat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}


% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bshatOmega}{\boldsymbol{\hat{\Omega}}}
\newcommand{\bshatG}{\boldsymbol{\hat{G}}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}

% Special cases like \bshatb for \boldsymbol{\hat{b}}
\generate[\hat]{bshat}{\boldsymbol}{b,y,x,X,V,S,W}
\newcommand{\bshatbeta}{\boldsymbol{\hat{\beta}}}
\newcommand{\bshatmu}{\boldsymbol{\hat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\hat{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\hat{\Sigma}}}
\newcommand{\bstildebeta}{\boldsymbol{\tilde{\beta}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bsbarbeta}{\boldsymbol{\overline{\beta}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}

% Redefine \bso to be the zero vector
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\hat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\hat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\hat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Rnk}{\mathbb{R}^{n\times k}}
\newcommand{\Rkk}{\mathbb{R}^{k\times k}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}


% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}


% Various probability and statistics commands
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}


% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\nNz}{_{n=0}^N}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}


\begin{frame}{Outline}
\tableofcontents[hideallsubsections]
%\tableofcontents
\end{frame}



\section{Housekeeping}

\begin{frame}{Precept \& Office Hours}
\alert{Precept}: Tuesday 11:00-12:00 in McCosh Hall 62.
\begin{itemize}
  \item Concept reviews, overviews, previews
  \item Practice problems
  \item Previous homework
  \item Upcoming homework
  \item Slides posted, often with more info and details.
\end{itemize}
\alert{Office hours}
\begin{itemize}
  \item Tuesday 4:30-5:30, JRRB A74
  \item Wednesday 4:00-5:00, A-Floor Tables
  \item Also by appt. in my office A95, or A-Floor Tables
\end{itemize}
\alert{Contact}: mcocci@princeton.edu
\end{frame}


\begin{frame}[shrink]{Homework}
Due Thursday by 6pm in my mailbox (A floor)
\begin{itemize}
  \item Write or type
  \item Answers in correct order
  \item Code at the end but main answer (plots, parameters estimates,
    discussion, etc.) in the body among the rest of the properly ordered
    answers
  \item Don't put your answer to coding questions buried in comments in
    the code appended at the end, or buried in messy copy-pasted
    plain-text output.
  \item Usual coding languages all fine.
  \item Goal is minimizing hunting for answers and flipping back and
    forth.
\end{itemize}
\end{frame}




\section{Lag Polynomials}


\begin{frame}[shrink]{Lag Polynomials}
Tasks
\begin{itemize}
  \item Review: Inverting a first order lag polynomial
  \item Inverting multiple-order lag polynomials
  \item Filtering Perspective
  \item Lag polynomials ignore constants
\end{itemize}
\end{frame}


\subsection{Inverses}



\begin{frame}[shrink]{Lag Polynomial Inverses}
Inverse of lag polynomial $h(z)$ satisfies
\begin{align*}
  h(z)^{-1}h(z) = 1
\end{align*}
Inverses of first-order polynomials:
\begin{align*}
  (1-\phi L)^{-1}
  &=
  1 + \phi L + \phi^2 L^2 + \cdots
  \qquad
  \qquad
  \qquad
  \qquad
  \quad
  |\phi|<1
  \\
  (1-\phi L)^{-1}
  &=
  \phi^{-1}L^{-1}
  (1+ \phi^{-1}L^{-1} + \phi^{-2}L^{-2} + \cdots)
  \qquad
  |\phi|>1
  \\
  &=
  \phi^{-1}L^{-1} + \phi^{-2}L^{-2} + \phi^{-3}L^{-3} + \cdots
\end{align*}
Inverse does not exist if $|\phi|=1$.
Can't take advantage of converging geometric sequence.
%Intuition:
%\begin{itemize}
  %\item $|\phi|<1$: Past shocks decay.
  %\item $|\phi|>1$: Process explodes so far in the future shocks
    %dominated by the process.
%\end{itemize}

\pause
\alert{Question}:
How can we invert multi-order lag polynomials?

\pause
\alert{Answer}: Factor the polynomial and invert one at a time
\end{frame}

\begin{frame}[shrink]{Lag Polynomial Inverses: Multiple Order}
Example I:
\begin{align*}
  x_t
  &=
  \frac{3}{4}x_{t-1} - \frac{1}{8}x_{t-2}
  +
  \varepsilon_t
\end{align*}
\pause
Form lag polynomial:
\begin{align*}
  \phi(z)
  &=
  1 - \frac{3}{4}z + \frac{1}{8}z^2
\end{align*}
\pause
Factor
\begin{align*}
  \phi(z)
  &=
  \left(
    1-\frac{1}{2}z
  \right)
  \left(
    1-\frac{1}{4}z
  \right)
\end{align*}
\pause
Invert each one at a time
\begin{align*}
  \phi(L)^{-1}
  &=
  \left(
    1-\frac{1}{2}L
  \right)^{-1}
  \left(
    1-\frac{1}{4}L
  \right)
  \\
  &=
  \left(
    1
    +\frac{1}{2}L
    +\frac{1}{4}L^2
    +\frac{1}{8}L^8
    +\cdots
  \right)
  \left(
    1
    +\frac{1}{4}L
    +\frac{1}{16}L^2
    +\cdots
  \right)
  \\
  &=
  1
  +
  \left[
    \frac{1}{2}
    +
    \frac{1}{4}
  \right]
  L
  +
  \left[
    \frac{1}{4}
    +
    \frac{1}{8}
    +
    \frac{1}{2}
    \cdot
    \frac{1}{4}
  \right]
  L^2
  +
  \cdots
\end{align*}
\end{frame}



\begin{frame}[shrink]{Lag Polynomial Inverses: Multiple Order}
Example II:
\begin{align*}
  x_t
  &= 0.5x_{t-1} + 0.5x_{t-2} + \varepsilon_t
\end{align*}
Form the lag polynomial
\begin{align*}
  \phi(z)
  &= 1 - 0.5z -0.5z^2
\end{align*}
\pause
Factor
\begin{align*}
  \phi(z)
  &= (1-z)(1+0.5z)
\end{align*}
Notice there is a unit root.

\pause
Of course, you can always invert parts of the polynomial
\begin{align*}
  \phi(L)x_t
  =
  (1-L)(1+0.5L)
  x_t
  &=
  \varepsilon_t
  \\
  (1-L)x_t &= (1+0.5L)^{-1}\varepsilon_t
  \\
  x_t &= x_{t-1} + (1+0.5L)^{-1}\varepsilon_t
\end{align*}


\pause
\alert{General Note}:
You quickly check for unit roots in AR models by summing the
coefficients. Equivalently, make sure $\phi(1)\neq 0$.
\end{frame}



\subsection{Filtering Perspective}


\begin{frame}[shrink]{Filtering Perspective}

Lag polynomials encourage a \alert{filtering} perspective of linear time
series.
%Taking inverses is crucial to reap the benefits of that
%perspective.
\begin{itemize}
  \item \alert{Idea}:
    Create output process $\{y_t\}$ from input $\{x_t\}$
  \pause
  \item \alert{Examples}
    \begin{itemize}
      \item MA process creates $\{y_t\}$ by filtering white
        noise
        \begin{align*}
          \{\varepsilon_t\} &\ra \{y_t\}
          \quad\text{s.t.}\quad
          y_t
          =
          \theta(L)\varepsilon_t
        \end{align*}
        $\varepsilon_t$ is a very simple random object.
        $\theta(L)$ is a very simple algebraic object.

      \pause
      \item
        Can filter one time series process to get another:
        \begin{align*}
          x_t &= \theta_x(L)\varepsilon_t
          \\
          y_t &= \theta_y(L)x_t
          = \theta_y(L)\theta_x(L)\varepsilon_t
          = \tilde{\theta}_y(L)\varepsilon_t
        \end{align*}

      \pause
      \item
        ARMA model: By inverting
        \begin{align*}
          \phi(L)y_t &= \theta(L)\varepsilon_t
          \quad\implies\quad
          y_t = \phi(L)^{-1}\theta(L)\varepsilon_t
        \end{align*}
        Back to filtered white noise.
    \end{itemize}
\end{itemize}
\pause
\alert{Note}: Many results in Hamilton are stated ``Suppose the filter
is absolutely summable'' or ``square summable.'' Treat all processes as
filtered white noise and place conditions on filters.
\end{frame}


\begin{frame}[shrink]{Filtering Perspective and ACGFs}
\alert{Recall}: Definition of ACGF:
\begin{align*}
  \lambda(z)
  =
  \sum_{j=-\infty}^\infty \lambda_j z^j
\end{align*}
\alert{Result}:
Suppose that $\{x_t\}$ has ACGF $\lambda_x(z)$ and
\begin{align*}
  y_t = h(L)x_t
\end{align*}
Then
\begin{align*}
  \lambda_y(z) = h(z)h(z^{-1})\lambda_x(z)
\end{align*}
\end{frame}

\begin{frame}[shrink]{Filtering Perspective and ACGFs}
\alert{Application}:
Suppose that $y_t$ is an MA process,i.e. filtered white noise
\begin{align*}
  y_t = \theta(L)\varepsilon_t
  \qquad
  \varepsilon_t
  \iid
  (0,\sigma^2)
\end{align*}
The ACGF of white noise process $\{\varepsilon_t\}$ is simply
\begin{align*}
  \lambda_\varepsilon(z) = \sigma^2
\end{align*}
By the above result,
\begin{align*}
  \lambda_y(z) = \sigma^2 \theta(z)\theta(z^{-1})
\end{align*}
\pause
\alert{Application}:
ARMA model
\begin{align*}
  \phi(L)y_t
  &=
  \theta(L)\varepsilon_t
\end{align*}
\pause
\alert{Punchline}:
If we can write $\{y_t\}$ as filtered white noise or the filtering of
another process with known ACGF, we can easily deduce the ACGF of $y_t$
and thus its covariance properties.
We might need to take inverses (and verify existence of the inverses) to
get there.
As we'll see, ACGF is central key for HAC inference.
\end{frame}




%\begin{frame}[shrink]
%Linear Time Series and Filtering
%- One way to think about linear time series is the study of ``filters''
  %on stochastic processes
%- Filtering: Given an input process, create a new output
%- Inputs form outputs, epsilons form y
%- Examples
  %- Moving average, the way most people outside of time series use it
  %- HP Filter
  %- Moving average process: Filtered white noise
  %- ARMA
%- We can also think about creating a new linear time series process from
  %filtering non-white noise process
%- Many nice results that we'll use, e.g. ACGF
  %- Defn of ACGF
%- Example: White noise, and statement in class
  %- ACGF of white noise: sigma^2
  %- So then then apply filter with z and z^{-1} on either side
%- Example: More complicated
  %- Has two polynomials
%- When we do inference, one object that will matter is spectral density,
  %which is related to the long run variance, which we'll need for
  %inference. Both of them are a direct derivative of the ACGF, which is
  %built from the filter.

%- One sided vs. two-sided filters
%- Conditions on infinite length filters: square and absolute summability
%- Absoltely summable implies square summable
%- Many results in Hamilton have this
%\end{frame}


\subsection{Lag Polynomials and Constants}


\begin{frame}[shrink]{Lag Polynomials and Constants}

How can we easily represent a stationary process with non-zero
conditional mean?
\begin{align*}
  \phi(L)u_t &=
  \theta(L)\varepsilon_t
  \\
  x_t
  &= \mu_x + u_t
\end{align*}
Define a zero mean process, shift the zero mean process by a constant.

Example: Deducing dynamics of the shifted process.
\begin{align*}
  u_t
  &=
  \phi_1 u_{t-1}
  +
  \cdots
  +
  \phi_p u_{t-p}
  + \varepsilon_t
  \\
  &
  \hphantom{%
  (1-\phi_1-\cdots-\phi_p)\mu_x
  + \phi_1 x_{t-1}
  + \cdots
  + \phi_p x_{t-p}
  + \varepsilon_t
  }
\end{align*}
\pause
\vspace{-50pt}
\begin{align*}
  x_t
  &= \mu_x + u_t
  \\
  &= \mu_x
  +
  \left(
  \phi_1 u_{t-1}
  +
  \cdots
  +
  \phi_p u_{t-p}
  + \varepsilon_t
  \right)
  \\
  &=
  \mu_x
  +
  \phi_1 (x_{t-1}-\mu_x)
  +
  \cdots
  +
  \phi_p (x_{t-p}-\mu_x)
  + \varepsilon_t
  \\
  &=
  (1-\phi_1-\cdots-\phi_p)\mu_x
  + \phi_1 x_{t-1}
  + \cdots
  + \phi_p x_{t-p}
  + \varepsilon_t
  \\
  &=
  \phi(1)\mu_x
  + \phi_1 x_{t-1}
  + \cdots
  + \phi_p x_{t-p}
  + \varepsilon_t
\end{align*}
\end{frame}



\begin{frame}[shrink]{Lag Polynomials Ignore Constants}

Consider a stationary process with a constant
\begin{align*}
  \phi(L)x_t
  =
  c + \varepsilon_t
\end{align*}
How does $c$ affect stationarity and the lag polynomial?
\pause
From the previous slide,
\begin{align*}
  \phi(L)x_t
  =
  \phi(1)\mu_x + \varepsilon_t
\end{align*}
\pause
Can take out the mean
\begin{align*}
  \phi(L)(x_t-\mu_x+\mu_x)
  &=
  \phi(1)\mu_x + \varepsilon_t
  \\
  \phi(L)(x_t-\mu_x)
  &=
  \varepsilon_t
  \\
  \phi(L)\tilde{x}_t
  &=
  \varepsilon_t
\end{align*}
\pause
Observations:
\begin{itemize}
  \item $\tilde{x}_t=x_t-\mu_x$ is zero mean.
  \item $\phi(z)$ governs stationarity of zero-mean
    $\tilde{x}_t$.
  \item If $\tilde{x}_t=x_t-\mu_x$ stationary, adding a constant to
    get $\tilde{x}_t+\mu_x=x_t$ doesn't change stationarity.
  \item Conclusion: Use the same lag polynomial.
\end{itemize}
\end{frame}



\section{Wold's Theorem}

\begin{frame}[shrink]{Linear Time Series Models}

Two perspectives on a given linear time series process:
\begin{itemize}
  \item \alert{Data Generating Process (DGP)}:
    Suppose we write
    \begin{align*}
      y_{t+1} &= \phi_1 y_{t} + \phi_2 y_{t-1} + \varepsilon_{t+1}
      \qquad
      \varepsilon_{t+1} \sim (0,\sigma^2_y)
      \\
      x_{t+1} &= \varepsilon_{t+1} + \theta_1\varepsilon_{t}
      \qquad
      \varepsilon_{t+1} \sim (0,\sigma^2_x)
    \end{align*}
    We can think of the above expressions specifying how the sequence
    $\{x_t\}$ or $\{y_t\}$ gets produced from inputs (shocks, initial
    condition).
    This is \emph{how the process is generated}.

    Note: Specified DGP can be nonstationary or noninvertible.
    We now have tools to check.
    Other DGPs might also lead to the same 2nd order properties.

  \pause
  \item \alert{Linear Representation}:
    Suppose we don't know or \emph{don't} know or specify the DGP for a
    given stationary process.
    There still exists a linear time series representation of that
    process, according to Wold's Theorem.

    That is, we might be able to write down a linear time series process
    with same 2nd-order properties as the original process (with
    unknown DGP)
\end{itemize}
\end{frame}


%\begin{frame}[shrink]{Linear Time Series Models}
%Linear time series models amount to \alert{specification} of the data
%generating process (\alert{DGP}).
%So when we write down an AR($p$), MA($q$), etc.
%In so doing, we also specify \alert{conditional moments}, e.g.
%\begin{align*}
  %\E_{t}[y_{t+1}]
  %&=
  %\phi_1 y_{t} + \phi_2 y_{t-1}
  %\\
  %\Var_t(y_{t+1}) &=
  %\sigma^2
%\end{align*}
%This is one justification for focusing on linear time series models:
%Simple specification of the DGP for stochastic processes.
%Wold's Theorem provides another justification.
%\end{frame}



%\begin{frame}[shrink]
%Picture to Have in Mind for Linear Time Series
%- Sample draw looks like an entire path
%- A data series looks like a slice
%- A forecast is standing at a point, looking forward.
%- Dynamics specify that
%- The conditional mean, the conditional variance
%- The unconditional mean and variance are an average over many paths
%- A persistent process makes it.
%\end{frame}


%Linear time series models
%- AR process: Errors in statinary AR process are already constructed from current and past values of the errors
%- Connection to Wold errors is what we're testing in the homework

%Big questions in Linear Time Series Models

%\begin{frame}[shrink]{Linear Time Series Models}

%Important Questions
%\begin{itemize}
  %\item What can we say about \alert{unconditional} moments?
    %Is the model \alert{stationary}?
%\end{itemize}
%- AR models: Is it stationary?
  %- Does the series explode or settle down?
  %- Are initial conditions important, or does their affect die out?
  %- Are shocks termporary or permanent?
%- MA models: Is it invertible?
  %- Don't have stationarity problems, always stationary
  %- "Can we construct epsilons from past y"
  %- Requres roots of polynomial greater than 1 in modulus
  %- Implies a unique choice of MA rep, among several that are always there
  %- Other MA reps of the same process, any MA process
  %- Invertibility implies a choice
%\end{frame}


\begin{frame}[shrink]{Wold's Theorem: Motivation}

Starting point for time series
\begin{itemize}
  \item In general, we have a
    \alert{covariance stationary stochastic processes}
    $\{x_t\}_{t=1}^T$
    \begin{itemize}
      \item Simple assumption about the distribution, doesn't require
        full information specification
      \item Nonetheless lets us say a lot
    \end{itemize}
\end{itemize}
\pause
Questions
\begin{itemize}
  \item What can we say given the assumption of covariance stationarity?
  \item How should we \alert{represent} covariance stationary processes?
    Why do we see linear processes like AR($p$)'s and MA($q$)'s?
    Why is that the dominant way of thinking about covariance stationary
    processes?
\end{itemize}
\alert{Answer}: Wold's Theorem
\end{frame}


\begin{frame}[shrink]{Wold's Theorem}
Wold's Theorem
\begin{itemize}
  \item
    Summary:
    If a stochastic process $\{x_t\}$ is \alert{covariance stationary},
    then \alert{$\exists$} an \alert{MA($\infty$)}
    \alert{representation}

    \pause
    i.e. $\exists$ representation as a \alert{linear time series
    process} with fixed (time-invariant) coefficients

  \pause
  \item
    Implications
    \begin{itemize}
      \item Linear time series are a \alert{workhorse representation}
        for covariance stationary processes.
      \item Simple convenient way to specify DGPs
      \item One such representation guaranteed to exist for any
        covariance stationary process by Wold's Theorem
      \item Because we've only specified covariance stationarity and
        haven't \emph{fully} specified the distribution, there are
        uniqueness issues and equivalent representations (e.g. flipping
        roots, AR, MA)
    \end{itemize}
  %\item
    %Implication:
    %If we're working with covariance stationary processes, restricting
    %to linear MA($\infty$) process ``good/rich enough'' for capturing
    %features of covariance stationary processes.


  %\item
    %Secondary Implication:
    %Because AR($p$) and MA($q$) processes are ``dense'' in the space of
    %covariance stationary processes, i.e. if we choose the order high
    %enough, we can get arbitrarily close to the MA($\infty$)
    %representation.
\end{itemize}
\end{frame}


\begin{frame}[shrink]{Wold's Theorems: Questions}
Prompts questions:
\begin{itemize}
  \item Why bother with AR, ARMA, MA($q$) (i.e. other linear time
    series) models when Wold's Theorem delivers an MA($\infty$) rep?
    \begin{itemize}
      \item Theorem needs an MA($\infty$) to work for \emph{all}
        covariance stationary processes generally.  But might be simpler
        representations for given process.

      \pause
      \item The covariance stationary \alert{process} $\{x_t\}$ itself
        and its \alert{DGP} are \emph{distinct} from a linear time
        series \alert{representation} of the process.
        That's why we can have multiple representations and uniqueness
        issues, i.e. different representations that have that the same
        distributional implications.

      \pause
      \item Some representations are more convenient, e.g. an
        MA($\infty$) might simplify to an AR(1).

      \pause
      \item The set of MA($q$) and AR($p$) processes are ``dense'' in
        the set of covariance stationary processes, so if we take large
        enough $q$ or $p$, we can get arbitrarily close to the truth.
    \end{itemize}

  \pause
  \item Relationship between Wold shocks and other shocks/innovations we
    write in our MA, AR, ARMA models?
\end{itemize}
\end{frame}



\begin{frame}[shrink]{Wold's Theorem: The Shocks}

When we write down a DGP, the shocks we write down
\alert{may or may not} coincide with the Wold shocks.

Given a covariance stationary process, think of the Wold shocks as
existing ``somewhere out there'', and then inspect your model to
determine whether the innovations in your DGP are the Wold shocks or
not.

This is Question (3) of this week's homework.
% Wold
%
%- The Wold errors exist out there for any stationary process, they may
%  or may not match the errors/shocks that you have in your
%  specification of the model. The task, and the point of one question
%  of this homework, is to get you to identify when they do match or
%  might not match.
%
%- What about the reverse: Is a linear time series process necessarily
%  stationary or connected to the Wold rep? No, and we have to study.
%
\end{frame}










\end{document}
















Bayes estimation and Bayes Estimation


- Checking lag polynomial for stationarity works because
  - In companion form, the eigenvalues matter
  - The eigenvalues are reciprocals of what's in the lag polynomial
- Side tip
  - Eigenvalue decomposition
    - Can always do it with a covariance matrix
    - PCA
  - Singular value decomposition
    - Always exist
  - Useful for terms where need easy inverses, iterations, multiplying the same thing against transposes of itself like projection matrix









% Sample code
\pause to add breaks
\textcolor{red}{text to show}.
\vspace{20pt}

\usebeamercolor{dull text}
\alert<+>{Words that you want to alert}

