%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usepackage{cmbright}
%\usetheme{metropolis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Metropolis with Sidebar %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aspectratio=169, handout]{beamer}
%\documentclass{beamer}
\usepackage{cmbright}
\usepackage{comment}

\useoutertheme[right, width=0.20\paperwidth]{sidebar}
\usecolortheme{metropolis}
\useinnertheme[sectionpage=none]{metropolis}
\usefonttheme{metropolis}

\makeatletter
\beamer@headheight=1.75\baselineskip     %controls the height of the headline, default is 2.5
\makeatother

\usepackage{etoolbox}
\patchcmd\insertverticalnavigation{\dohead}{\vskip-35pt\dohead}{}{}

%\setbeamertemplate{sidebar canvas right}{}
%\setbeamertemplate{sidebar right}{BOB}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Simple Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass{beamer}
%\usetheme{Boadilla}
%\usepackage{lmodern}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sidebar Template %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[serif]{beamer}  % For serif latex font
%\usepackage{pxfonts}
%\usepackage{eulervm}
%\usepackage{mathpazo}

%\usetheme{Goettingen}
%\usecolortheme{lily}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Common Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[]{ECO-518: Precept 4 \\ Matt Cocci}
\author[]{}
\date{\today}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
%\setbeamertemplate{section in toc}[square unnumbered]
%\setbeamertemplate{subsection in toc}[square unnumbered]

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\usepackage{bbm} % For bold numbers

%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% For easier looping
\usepackage{pgffor}

\usepackage{pdfpages}

%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Misc Math
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}[1]{\mathbf{1}_{#1}}


% Command to generate new math commands:
% - Suppose you want to refer to \boldsymbol{x} as just \bsx, where 'x'
%   is any letter. This commands lets you generate \bsa, \bsb, etc.
%   without copy pasting \newcommand{\bsa}{\boldsymbol{a}} for each
%   letter individually. Instead, just include
%
%     \generate{bs}{\boldsymbol}{a,...,z}
%
% - Uses pgffor package to loop
% - Example with optional argument. Will generate \bshatx to represent
%   \boldsymbol{\hat{x}} for all letters x
%
%     \generate[\hat]{bshat}{\boldsymbol}{a,...,z}

\newcommand{\generate}[4][]{%
  % Takes 3 arguments (maybe four):
  % - 1   wrapcmd (optional, defaults to nothing)
  % - 2   newname
  % - 3   mathmacro
  % - 4   Names to loop over
  %
  % Will produce
  %
  %   \newcommand{\newnameX}{mathmacro{wrapcmd{X}}}
  %
  % for each X in argument 4

  \foreach \x in {#4}{%
    \expandafter\xdef\csname%
      #2\x%
    \endcsname%
    {\noexpand\ensuremath{\noexpand#3{\noexpand#1{\x}}}}
  }
}


% MATHSCR: Gen \sX to stand for \mathscr{X} for all upper case letters
\generate{s}{\mathscr}{A,...,Z}


% BOLDSYMBOL: Generate \bsX to stand for \boldsymbol{X}, all upper and
% lower case.
%
% Letters and greek letters
\generate{bs}{\boldsymbol}{a,...,z}
\generate{bs}{\boldsymbol}{A,...,Z}
\newcommand{\bstheta}{\boldsymbol{\theta}}
\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsSigma}{\boldsymbol{\Sigma}}
\newcommand{\bsvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bsalpha}{\boldsymbol{\alpha}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsOmega}{\boldsymbol{\Omega}}
\newcommand{\bshatOmega}{\boldsymbol{\hat{\Omega}}}
\newcommand{\bshatG}{\boldsymbol{\hat{G}}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}

% Special cases like \bshatb for \boldsymbol{\hat{b}}
\generate[\hat]{bshat}{\boldsymbol}{b,y,x,X,V,S,W}
\newcommand{\bshatbeta}{\boldsymbol{\hat{\beta}}}
\newcommand{\bshatmu}{\boldsymbol{\hat{\mu}}}
\newcommand{\bshattheta}{\boldsymbol{\hat{\theta}}}
\newcommand{\bshatSigma}{\boldsymbol{\hat{\Sigma}}}
\newcommand{\bstildebeta}{\boldsymbol{\tilde{\beta}}}
\newcommand{\bstildetheta}{\boldsymbol{\tilde{\theta}}}
\newcommand{\bsbarbeta}{\boldsymbol{\overline{\beta}}}
\newcommand{\bsbarg}{\boldsymbol{\overline{g}}}

% Redefine \bso to be the zero vector
\renewcommand{\bso}{\boldsymbol{0}}

% Transposes of all the boldsymbol shit
\newcommand{\bsbp}{\boldsymbol{b'}}
\newcommand{\bshatbp}{\boldsymbol{\hat{b'}}}
\newcommand{\bsdp}{\boldsymbol{d'}}
\newcommand{\bsgp}{\boldsymbol{g'}}
\newcommand{\bsGp}{\boldsymbol{G'}}
\newcommand{\bshp}{\boldsymbol{h'}}
\newcommand{\bsSp}{\boldsymbol{S'}}
\newcommand{\bsup}{\boldsymbol{u'}}
\newcommand{\bsxp}{\boldsymbol{x'}}
\newcommand{\bsyp}{\boldsymbol{y'}}
\newcommand{\bsthetap}{\boldsymbol{\theta'}}
\newcommand{\bsmup}{\boldsymbol{\mu'}}
\newcommand{\bsSigmap}{\boldsymbol{\Sigma'}}
\newcommand{\bshatmup}{\boldsymbol{\hat{\mu'}}}
\newcommand{\bshatSigmap}{\boldsymbol{\hat{\Sigma'}}}

% MATHCAL: Gen \calX to stand for \mathcal{X}, all upper case
\generate{cal}{\mathcal}{A,...,Z}

% MATHBB: Gen \X to stand for \mathbb{X} for some upper case
\generate{}{\mathbb}{R,Q,C,Z,N,Z,E}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\RK}{\mathbb{R}^K}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\Rl}{\mathbb{R}^\ell}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rnm}{\mathbb{R}^{n\times m}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Rnk}{\mathbb{R}^{n\times k}}
\newcommand{\Rkk}{\mathbb{R}^{k\times k}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}

% First derivatives
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\dfdy}{\frac{df}{dy}}
\newcommand{\dfdz}{\frac{df}{dz}}

% Second derivatives
\newcommand{\ddyddx}{\frac{d^2y}{dx^2}}
\newcommand{\ddydxdy}{\frac{d^2y}{dx dy}}
\newcommand{\ddydydx}{\frac{d^2y}{dy dx}}
\newcommand{\ddfddx}{\frac{d^2f}{dx^2}}
\newcommand{\ddfddy}{\frac{d^2f}{dy^2}}
\newcommand{\ddfddz}{\frac{d^2f}{dz^2}}
\newcommand{\ddfdxdy}{\frac{d^2f}{dx dy}}
\newcommand{\ddfdydx}{\frac{d^2f}{dy dx}}


% First Partial Derivatives
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\pfpx}{\frac{\partial f}{\partial x}}
\newcommand{\pfpy}{\frac{\partial f}{\partial y}}
\newcommand{\pfpz}{\frac{\partial f}{\partial z}}


% argmin and argmax
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}


% Various probability and statistics commands
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\vc}{\operatorname{vec}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\Lqto}[1]{\xrightarrow{L_{#1}}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}


% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumjn}{\sum^n_{j=1}}
\newcommand{\sumim}{\sum^m_{i=1}}
\newcommand{\sumik}{\sum^k_{i=1}}
\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

\newcommand{\prodnN}{\prod^N_{n=1}}
\newcommand{\prodin}{\prod^n_{i=1}}
\newcommand{\prodiN}{\prod^N_{i=1}}
\newcommand{\prodkn}{\prod^n_{k=1}}
\newcommand{\prodtT}{\prod^T_{t=1}}
\newcommand{\prodnNz}{\prod^N_{n=0}}
\newcommand{\prodinz}{\prod^n_{i=0}}
\newcommand{\prodknz}{\prod^n_{k=0}}
\newcommand{\prodtTz}{\prod^T_{t=0}}

% Bounds
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\nN}{_{n=1}^N}
\newcommand{\tT}{_{t=1}^T}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}
\newcommand{\nNz}{_{n=0}^N}

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limk}{\lim_{k\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Shorter integrals: ``Integral from X to Y''
% - intXY is equivalent to \int^Y_X
\newcommand{\intab}{\int_a^b}
\newcommand{\intzN}{\int_0^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}


\begin{frame}{Outline}
\tableofcontents[hideallsubsections]
%\tableofcontents
\end{frame}


\section{Introduction}


{\footnotesize
\begin{frame}{Binary Choice}

Setting:
$Y_i\in\{0,1\}$ and we wish to predict given covariates $X_i$.

Topics
\begin{itemize}
  \item \alert{LPM} in this setting, and why you should always take care to use
    \alert{heteroskedasticity-robust standard errors}

  \item MLE estimates under \alert{general link function}, which
    includes standard errors that are \alert{robust} to misspecification
    of the model (link function).

  \item Interpretation of coefficients, and deriving
    \alert{marginal effects} and \alert{average partial derivative}.

  \item
    Why \alert{FE in a logit} model is a bad idea without further
    modification.

    Gives an example of why nuisance parameters like FE that can't be
    estimated consistently can contaminate estimates of coefficients of
    interest in nonlinear models.
    Called an \alert{incidental parameter problem}.
\end{itemize}
\end{frame}
}


%\subsection{Microfoundations for General}

%{\footnotesize
%\begin{frame}
%Latent variable, or index function.
%\end{frame}
%}


\section{LPM}

\subsection{Overview}

{\footnotesize
\begin{frame}{Linear Probability Model}
Suppose we have binary $Y_i\in\{0,1\}$ and we estimate a linear
probability model
\begin{align*}
  Y_i = X_i'\beta + \varepsilon_i
\end{align*}
As usual, two possible justifications for doing this:
\begin{enumerate}
  \item Whatever the true form of the (generally arbitrarily nonlinear)
    conditional probability function $P[Y_i=1|X_i]$ is, OLS will
    consistently estimate, the \alert{best linear approximation}, as
    always:
    \begin{align*}
      \hat{\beta}
      \pto
      \E[X_iX_i']^{-1}\E[X_iY_i]
      =
      \E[X_iX_i']^{-1}\E\big[X_i\cdot\E[Y_i|X_i]\big]
      =
      \E[X_iX_i']^{-1}\E\big[X_i\cdot P[Y_i=1|X_i]\big]
    \end{align*}

  \item Suppose, by dumb luck, we really have a
    \alert{linear conditional probability function}:
    \begin{align*}
      P[Y_i=1|X_i]
      =
      \E[Y_i|X_i]
      =
      X_i'\beta
    \end{align*}
    This implies $\E[\varepsilon_i|X_i]=0$ (a justification that,
    recall, is \emph{stronger} that the predetermined
    $\E[\varepsilon_iX_i]=0$ that we usually use to justify regression),
    and the OLS estimator will be consistent for the true $\beta$ that
    characterizes the CEF (rather than some best approximation to the
    CEF).
\end{enumerate}
\end{frame}
}


\subsection{Standard Errors}

{\scriptsize
\begin{frame}{LPM: Necessity of Heteroskedasticity-Robust Standard Errors}
In \alert{either} case, we need to use heteroskedasticity-robust
standard errors.
\begin{enumerate}
  \item
    In this case, the CEF generally isn't linear, and so there will
    generally be \alert{heteroskedasticity} due to this nonlinearity,
    which can be seen from a picture.

    Plot outcomes generated from a logit or probit
    $P[Y_i=1|X_i]$ for scalar $X_i$, and draw a best fit straight line.
    Notice the errors will not be homoskedastic.

  \item Even in this stronger case, we can never have homoskedasticity
    (like we might have once-upon-a-time-in-econometrics done in
    a continuous $Y_i$ setting) because, by assumption,
    \begin{align*}
      P[Y_i=1|X_i]
      =
      \E[Y_i|X_i]
      =
      X_i'\beta
    \end{align*}
    And also, because $Y_i$ is Bernoulli with ``success'' probability
    $P[Y_i=1|X_i]$,
    \begin{align*}
      \Var(Y_i|X_i)
      =
      P[Y_i=1|X_i]
      \cdot
      (1- P[Y_i=1|X_i])
      =
      X_i'\beta
      (1-X_i'\beta)
    \end{align*}
    which necessarily varies with $X_i$.
    So homoskedasticity is an \alert{impossibility} in LPM.
\end{enumerate}
Punchline:
You \alert{could} assume homoskedasticity in a continuous $Y_i$ setting
(although it's generally a bad idea).
In an LPM model, it's not just a bad idea; it's logically inconsistent
and totally at odds with the whole point of estimating an LPM model.
\end{frame}
}


\subsection{Estimation via GLS}

{\footnotesize
\begin{frame}{LPM and GLS}
If we assume that we really do have a linear conditional probability
function (Case 2)
\begin{align*}
  P[Y_i=1|X_i]
  =
  \E[Y_i|X_i]
  =
  X_i'\beta
\end{align*}
we just argued that we will also have
\begin{align*}
  \Var(\varepsilon_i|X_i)
  =
  \Var(Y_i|X_i)
  =
  P[Y_i=1|X_i]
  \cdot
  (1- P[Y_i=1|X_i])
  =
  X_i'\beta
  (1-X_i'\beta)
\end{align*}
This specifies the variance of the errors.
Therefore, could use this to obtain \alert{GLS} estimates, rather
than OLS estimates.
Specifically, FGLS implementation entails
\begin{itemize}
  \item Estimating $\beta$ in some first step (by OLS)
  \item Form weights based on
    $\Var(\varepsilon_i|X_i)= X_i'\hat{\beta} (1-X_i'\hat{\beta})$
  \item Getting the FGLS estimate by minimizing the weighted sum of
    squared errors, with weights for each observation equal to the
    inverse of the variance.
\end{itemize}
But note that this might not be a great idea because $p(1-p)$ is
maximized at $p=0.5$, with smaller values for $p$ close to $0$ or 1.
And so the small variance, ``precisely'' estimated and highly-weighted
observations will be those observations in the tails, where we generally
worry most about the LPM's performance.
\end{frame}
}




\section{General Link Function}

\subsection{Estimation}

{\scriptsize
\begin{frame}{Binary Choice: Estimation under General Link Function}
Consider a binary choice model with arbitrary link function
$G(\,\cdot\,)$:
\begin{align*}
  \E[Y_i|X_i]
  =
  P[Y_i=1|X_i]
  =
  G(X_i'\beta)
\end{align*}
Given iid sample $\{Y_i,X_i\}$, two options for estimation
\begin{enumerate}
  \item
    Can use orthogonality condition
    \vspace{-5pt}
    \begin{align*}
      0 = \E[Y_i-G(X_i'\beta)|X_i]
    \end{align*}
    Multiply by functions of $X_i$ to generate a GMM moment condition.
    Most common: Use the following moment condition
    \vspace{-5pt}
    \begin{align*}
      0 = \E[(Y_i-G(X_i'\beta))X_i]
    \end{align*}

  \item
    As model \alert{fully} specifies conditional distribution of
    $Y_i|X_i$, can form likelihood
    \vspace{-5pt}
    \begin{align*}
      \prod_{i=1}^N
      G(X_i'\beta)^Y_i
      [1-G(X_i'\beta)]^{1-Y_i}
    \end{align*}
    Log-likelihood
    \vspace{-5pt}
    \begin{align*}
      \calL(\beta)
      &=
      \sum_{i=1}^N
      \bigg[
      Y_i
      \log
      G(X_i'\beta)
      +
      (1-Y_i)
      \log
      (1-G(X_i'\beta))
      \bigg]
    \end{align*}
    Can do (conditional-on-$X_i$) MLE.
    Generally also prudent to allow for misspecification.

\end{enumerate}
\end{frame}
}

{\scriptsize
\begin{frame}{Binary Choice: MLE under General Link Function}
For a single observation, we have log-likelihood of
\begin{align*}
  \log P[Y_i|X_i,\beta]
  =
  Y_i
  \log
  G(X_i'\beta)
  +
  (1-Y_i)
  \log
  (1-G(X_i'\beta))
\end{align*}
This implies derivatives
\begin{align*}
  \frac{\partial \log P[Y_i|X_i,\beta]}{\partial \beta}
  &=
  Y_i
  \frac{%
    g(X_i'\beta)
  }{%
    G(X_i'\beta)
  }
  X_i
  -
  (1-Y_i)
  \frac{%
    g(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  X_i
  =
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{%
    G(X_i'\beta)
    (1-G(X_i'\beta))
  }
  \right]
  g(X_i'\beta)
  X_i
  \\
  \frac{\partial^2 \log P[Y_i|X_i,\beta]}{\partial \beta\partial \beta'}
  &=
  (\text{A mess})
\end{align*}
Can then estimate $\hat{A}$, $\hat{\Omega}_1$, or $\hat{\Omega}_2$
using these derivatives and formulas on the previous slide.
Can also bootstrap. See later slide after discussion of marginal
effects.

To get analytical standard errors, can assume \alert{correct
specification}, estimate $I_{\theta_0}=A=\Omega$ using any of the above
estimators, and approximate
\begin{align*}
  \sqrt{n}(\hat{\theta}-\theta_0)
  \overset{a}{\sim}
  \calN(0,I_{\theta_0}^{-1})
\end{align*}
Alternatively, can guard against misspecification and instead use
\begin{align*}
  \sqrt{n}(\hat{\theta}-\theta_0)
  \overset{a}{\sim}
  \calN(0,A^{-1}\Omega A^{-1}{}')
\end{align*}
In general, all these different choices give you distinct standard
errors for the same estimate.
\end{frame}
}


\subsection{Marginal and Average Partial Derivative \& Effects}

{\footnotesize
\begin{frame}{Interpretation of Coefficients}

In linear models
\begin{align*}
  Y_i = X_i'\beta + \varepsilon_i
\end{align*}
we often interpret $\beta$ as the \alert{marginal effect} $dY/dX$, not
just a parameter that's useful for determining the predicted value
the predicted value $\hat{Y}$ or the best linear approximation to the
CEF.

In binary choice models
\begin{align*}
  P[Y_i=1|X_i]
  =
  G(X_i'\beta)
\end{align*}
the nonlinear link function means that it is no longer the case that
$\beta$ is immediately the marginal effect.
We must compute it.

Similar to how, if we include Age and $\text{Age}^2$ as covariates, the
marginal effect of age involves the coefficients on both.
\end{frame}
}


{\footnotesize
\begin{frame}{Binary Choice Model: Marginal Effects}
Binary choice model
\begin{align*}
  P[Y=1|X=x]
  =
  G(x'\beta)
  =
  G(x_1\beta_{1} + x_{-1}'\beta_{-1})
\end{align*}
\alert{Marginal Effect}
\begin{itemize}
  \item \alert{Continuous $x_1$}:
    The marginal effect with respect to the 1st regressor at a
    particular value $x=(x_1,\,x_{-1}')'$ of the regressors is
    \begin{align*}
      \frac{\partial}{\partial x_1}
      P[Y=1|X=x]
      =
      g(x'\beta)\beta_{1}
    \end{align*}

  \item \alert{Binary $x_1$}:
    When the 1st regressor is binary, use instead
    \begin{align*}
      &
      P[Y=1|X=(1,x_{-1})']
      -
      P[Y=1|X=(0,x_{-1})']
      \\
      &=
      G(\beta_{1} + x_{-1}'\beta_{-1})
      -
      G(x_{-1}'\beta_{-1})
    \end{align*}

\end{itemize}
Estimate these quantities by plug-in estimator.
Get s.e.'s from delta method or bootstrap.
\end{frame}
}


{\scriptsize
\begin{frame}{Binary Choice Model}
Binary choice model
\begin{align*}
  P[Y=1|X=x]
  =
  G(x'\beta)
  =
  G(x_1\beta_{1} + x_{-1}'\beta_{-1})
\end{align*}
\alert{Average Partial Derivative and Effect}:
Marginal effects (MEs) are defined \emph{at a particular} $X$.  Might
want some weighted average ME, where we weight according to the
distribution of $X$.
\begin{itemize}
  \item \alert{Continuous $x_1$}:
    Taking $\E$ over the population distribution of $X$,
    \begin{align*}
      \tau
      =
      \E\bigg[
      \frac{\partial }{\partial x_1}
      P[Y=1|X=x]
      \bigg|_{x=X}
      \bigg]
      =
      \E[g(x'\beta)\beta_{1}]
      \quad\implies\quad
      \hat{\tau}
      =
      \frac{1}{N}
      \sumin
      g(X_i'\hat{\beta})\hat{\beta}_1
    \end{align*}
  \item \alert{Binary $x_1$}:
    When the 1st regressor is binary,
    \begin{align*}
      \tau
      =
      \E\bigg[
      P[Y=1|X=(1,x_{-1})']
      -
      P[Y=1|X=(0,x_{-1})']
      \bigg]
      =
      \E\bigg[
      G(\beta_{1} + X_{-1}'\beta_{-1})
      -
      G(X_{-1}'\beta_{-1})
      \bigg]
    \end{align*}
    Estimator is sample counterpart:
    \begin{align*}
      \hat{\tau}
      =
      \frac{1}{N}
      \sumiN
      \bigg[
      G(\hat{\beta}_{1} + X_{-1}'\hat{\beta}_{-1})
      -
      G(X_{-1}'\hat{\beta}_{-1})
      \bigg]
    \end{align*}

\end{itemize}
Standard errors by 2-step GMM or bootstrap.
\end{frame}
}

\begin{frame}[shrink]{Summary}
Models
\begin{align*}
  \text{LPM}
  \qquad
  P[Y_i=1|X_i] &= X_i\beta \\
  \text{Probit}
  \qquad
  P[Y_i=1|X_i] &= \Phi(X_i\beta) \\
  \text{Logistic}
  \qquad
  P[Y_i=1|X_i] &= F(X_i\beta)
  \qquad\text{where}\quad
  F(x) = \frac{e^x}{1+e^x}
\end{align*}
Marginal effects for continuous covariate $X_{ji}$
\begin{align*}
  \text{LPM}
  \qquad
  \frac{\partial P[Y_i=1|X_i]}{\partial X_{ji}}
  &= \beta_j
  \\
  \text{Probit}
  \qquad
  \frac{\partial P[Y_i=1|X_i]}{\partial X_{ji}}
  &= \phi(X_i\beta)\beta_j
  \\
  \text{Logistic}
  \qquad
  \frac{\partial P[Y_i=1|X_i]}{\partial X_{ji}}
  &= F'(X_i\beta)\beta_j
  \qquad\text{where}\quad
  F'(x) = \frac{e^x}{(1+e^x)^2}
\end{align*}
Marginal effects for dummy variables
\begin{align*}
  P[Y_i=1|X_{1i},\ldots,X_{ji}=1,\ldots,X_{ki}]
  -P[Y_i=1|X_{1i},\ldots,X_{ji}=0,\ldots,X_{ki}]
\end{align*}
\end{frame}


\subsubsection{Standard Errors}

{\scriptsize
\begin{frame}{Standard Errors}

In practice, the easiest way to get asymptotically valid standard errors
is to \alert{bootstrap}.
\begin{itemize}
  \item Simply resample your data, estimate $\beta$ by probit, logit,
    whatever method you choose, and then compute the desired marginal
    effect or average partial derivative/effect across these boostrap
    samples, and use the resulting bootstrapped sampling distribution

  \item If the model/link is \alert{misspecified}, this nonparametric
    bootstrap procedure will be robust to misspecification, and is
    therefore like using the sandwich asymptotic variance, rather than
    the inverse information matrix.

  \item If you really believe your model is \alert{correctly specified},
    you can do a \alert{parametric} bootstrap where you simulate
    datasets from the binary choice model under $\hat{\beta}$.
    The resulting bootstrap sampling distribution will generally not be
    robust to misspecification, and is like using the inverse Fisher
    information matrix, rather than the sandwich asymptotic variance.
\end{itemize}
If you desire analytical variance formulas instead of taking
advantage of the bootstrap, the next few slides show how derive the
asymptotic distribution of the marginal effect and average partial
derivative/effect estimators.
These are purely for reference.

Of course, if you are a Bayesian, you should simply draw from the
posterior and compute marginal and average partial effects using
posterior draws.
\end{frame}
}


{\footnotesize
\begin{frame}{Asymptotic Distribution for ME, Continuous $x_1$}
Marginal effect at $x$ with respect to 1st covariate:
\begin{align*}
  \frac{\partial}{\partial x_1}
  P[Y=1|X=x]
  =
  g(x'\beta_0)\beta_{0,1}
\end{align*}
Let $e_1$ denote the 1st column of $I_k$.
Define
\begin{align*}
  h(b)&= g(x'b)b_1=g(x'b)e_1'b \\
  \implies\quad
  H
  &=
  \frac{\partial}{\partial b'}
  h(b)
  \big|_{b=\beta_0}
  =
  \big[
  g'(x'b)b_1x'
  +
  g(x'b)e_1'
  \big]_{b=\beta_0}
\end{align*}
Suppose
\begin{align*}
  \sqrt{N}(\hat{\beta}-\beta_0)
  \quad\dto\quad
  \calN(0,V)
\end{align*}
where $V$ is appropriate variance corresponding to any of the following,
depending up which estimation method you choose:
The inverse Fisher information matrix (correctly specified MLE),
sandwich misspecification-robust estimator (incorrectly specified MLE),
GMM variance (using moment conditions).

Then by the delta method
\begin{align*}
  &\sqrt{N}(h(\hat{\beta})-h(\beta_0))
  =
  \sqrt{N}\big(
    g(x'\hat{\beta})\hat{\beta}_1-g(x'\beta_0)\beta_{0,1}
  \big)
  \quad\dto\quad
  \calN(0,HVH')
\end{align*}
\end{frame}
}

{\footnotesize
\begin{frame}{Asymptotic Distribution for ME, Binary $x_1$}
Marginal effect at $x$ with respect to 1st covariate:
\begin{align*}
  &
  P[Y=1|X=(1,x_{-1}')']
  -
  P[Y=1|X=(0,x_{-1}')']
  \\
  &=
  G(\beta_{0,1} + x_{-1}'\beta_{0,-1})
  -
  G(x_{-1}'\beta_{0,-1})
\end{align*}
Define
\begin{align*}
  h(b)
  &=
  G(b_1 + x_{-1}'b_{-1})
  -
  G(x_{-1}'b_{-1})
  \\
  &=
  G\big(
  (1,\,x_{-1})'b
  \big)
  -
  G\big(
  (0,\,x_{-1})'b
  \big)
  \\
  \implies\quad
  H=
  \frac{\partial}{\partial b'}
  h(b)
  \bigg|_{b=\beta_0}
  &=
  g\big(
  (1,\,x_{-1})'b
  \big)
  \begin{pmatrix}
    1 & x_{-1}'
  \end{pmatrix}
  -
  g\big(
  (0,\,x_{-1})'b
  \big)
  \begin{pmatrix}
    0 & x_{-1}'
  \end{pmatrix}
  \bigg|_{b=\beta_0}
  %\\
  %&=
  %\begin{pmatrix}
    %g\big(
    %(1,\,x_{-1})'b
    %\big)
    %&
    %x_{-1}
    %\big[
    %g\big(
    %(1,\,x_{-1})'b
    %\big)
    %-
    %g\big(
    %(0,\,x_{-1})'b
    %\big)
    %\big]
  %\end{pmatrix}
  %\bigg|_{b=\beta_0}
\end{align*}
Suppose
\begin{align*}
  \sqrt{N}(\hat{\beta}-\beta_0)
  \quad\dto\quad
  \calN(0,V)
\end{align*}
Then by the delta method
\begin{align*}
  &\sqrt{N}(h(\hat{\beta})-h(\beta_0))
  \\
  &=
  \sqrt{N}\big(
    [
    G(\hat{\beta}_{1} + x_{-1}'\hat{\beta}_{-1})
    -
    G(x_{-1}'\hat{\beta}_{-1})
    ]
    -
    [
    G(\beta_{0,1} + x_{-1}'\beta_{0,-1})
    -
    G(x_{-1}'\beta_{0,-1})
    ]
  \big)
  \quad\dto\quad
  \calN(0,HVH')
\end{align*}
\end{frame}
}



{\scriptsize
\begin{frame}{Asymptotic Distribution for Average Partial Derivative}
For continuous $x_1$,
\begin{align*}
  \gamma_0
  =
  \E\bigg[
  \frac{\partial }{\partial x_1}
  P[Y=1|X=x]
  \bigg|_{x=X}
  \bigg]
  =
  \E[g(x'\beta_0)\beta_{0,1}]
  \qquad
  \hat{\gamma}
  =
  \left[
    \frac{1}{N}
    \sumin
    g(X_i'\hat{\beta})\hat{\beta}_1
  \right]
\end{align*}
Can write as \alert{2-step GMM estimator}.
\begin{itemize}
  \item Recall that $\hat{\beta}$ the estimator of $\beta_0$ defined by
    some moment conditions
    \begin{align*}
      0 = \E[h(Y_i,X_i,\beta_0)]
      %0 = \E[s(Y_i,X_i,\beta_0)]
      %=
      %\E\left[
        %\left(
        %\frac{Y_ig(X_i'\beta_0)}{G(X_i'\beta_0)}
        %-
        %\frac{(1-Y_i)g(X_i'\beta_0)}{1-G(X_i'\beta_0)}
        %\right)
        %X_i
      %\right]
    \end{align*}
    If doing MLE, then $h(Y_i,X_i,\beta_0)=s(Y_i,X_i,\beta_0)$, the
    score function.

  \item
    Can also let $\gamma_0$ denote the average partial derivative,
    defined by moment condition
    \begin{align*}
      0 = \E[\gamma_0-g(X_i'\beta_0)\beta_{0,1}]
    \end{align*}

  \item
    Can write estimation as just-identified GMM:
    \begin{align*}
      0
      &=
      \E
        \begin{bmatrix}
          h(Y_i,X_i,\beta_0)
          \\
          \gamma_0-g(X_i'\beta_0)\beta_{0,1}
        \end{bmatrix}
    \end{align*}
\end{itemize}
\end{frame}
}



{\scriptsize
\begin{frame}{Asymptotic Distribution for Average Partial Derivative}
Moment condition and asymptotic distribution of estimator
\begin{align*}
  0
  &=
  \E
  \begin{bmatrix}
    h(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{bmatrix}
  \qquad
  \sqrt{N}(\hat{\theta}-\theta_0)
  \quad\dto\quad
  \calN(0,(G_{gmm}'\Omega^{-1}G_{gmm})^{-1})
\end{align*}
where we have expected derivative of moment condition w.r.t.\
$\theta:=(\beta',\gamma)'$:
\begin{align*}
  G_{gmm}
  &
  :=
  \E\left[
    \frac{\partial}{\partial \theta'}
    \begin{pmatrix}
      h(Y_i,X_i,\beta)
      \\
      \gamma-g(X_i'\beta)e_1'\beta
    \end{pmatrix}_{\theta=\theta_0}
  \right]
  \\
  &\hphantom{:}=
  \E
  \begin{bmatrix}
    \frac{\partial }{\partial \beta'}
    \big(h(Y_i,X_i,\beta)\big)\big|_{\theta=\theta_0}
    &
    \frac{\partial}{\partial \gamma}
    \big( h(Y_i,X_i,\beta)\big)\big|_{\theta=\theta_0}
    \\
    \frac{\partial}{\partial \beta'}
    \big(
    \gamma-g(X_i'\beta)e_1'\beta
    \big)
    \big|_{\theta=\theta_0}
    &
    \frac{\partial}{\partial \gamma}
    \big(
    \gamma-g(X_i'\beta)e_1'\beta
    \big)
    \big|_{\theta=\theta_0}
  \end{bmatrix}
  \\
  &\hphantom{:}=
  \E
  \begin{bmatrix}
    \frac{\partial h(Y_i,X_i,\beta_0)}{\partial \beta'}
    &
    0
    \\
    -g'(X_i'\beta_0)e_1'\beta_{0}X_i'
    -g(X_i'\beta_0)e_1'
    &
    1
  \end{bmatrix}
\end{align*}
and variance of the moment condition
\begin{align*}
  \Omega
  &=
  \Var
  \begin{pmatrix}
    h(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{pmatrix}
  =
  \E
  \left[
  \begin{pmatrix}
    h(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{pmatrix}
  \begin{pmatrix}
    h(Y_i,X_i,\beta_0)
    \\
    \gamma_0-g(X_i'\beta_0)\beta_{0,1}
  \end{pmatrix}'
  \right]
\end{align*}
Both of which we can estimate via their sample counterparts.
\end{frame}
}








\section{FE in Logit}

{\scriptsize
\begin{frame}{Binary Choice: Panel Data}
Given a panel of binary choice data $\{Y_{it},X_{it}\}$ where $i$
represent individuals and $t$ is time, might be tempted to
add \alert{individual FE} to logit or probit, e.g.\ to specify a model
of the form
\begin{align*}
  P[Y_{it}=1|X_{it}]
  =
  G(\alpha_i + X_{it}'\beta)
\end{align*}
Note
\begin{itemize}
  \item We do this all the time in regression models, so in principle
    this seems like it should be fine.
    But this is an example where FE are a \alert{bad idea}
  \item The model is inherently \alert{nonlinear}
  \item With a few obs.\ (time periods) per unit $i$, cannot
    consistently estimate $\alpha_i$'s.
  \item In regression, this is no problem since we different them out
    anyway because these so called \alert{incidental parameters} enter
    the model linearly.
  \item In nonlinear models like this, the FE are not automatically
    differenced out.
  \item Inconsistent estimation of the $\alpha_i$ spills over and
    \alert{contaminates} estimation of $\beta$ leading to bias in our
    estimates of $\beta$ that doesn't vanish asymptotically.
\end{itemize}
This is an example of a general rule:
In \alert{nonlinear} models, inconsistent estimation of certain
parameters can spill over and affect other parameters.
This is called the \alert{incidental parameters problem}.

Luckily, in logit models, you can do something about, as we now give an
example of.
But not for probit or other link functions.
\end{frame}
}



{\footnotesize
\begin{frame}{Binary Choice: Panel Data}
Suppose we have a panel $\{Y_{it},X_{it}\}$ where $Y_{it}$ is binary.
Let $\pi_{it} := P[Y_{it}=1|x_{it}]$, for some model or parametric
functional form for the conditional success probability.

Write the joint likelihood for a single entity:
\begin{align*}
  \calL(Y_{i1},\ldots,Y_{iT})
  &=
  \exp\left(
  \sumtT
  \big[
  Y_{it}\log \pi_{it}
  +
  (1-Y_{it})\log\big(1-\pi_{it}\big)
  \big]
  \right)
  \\
  &=
  \exp\left(
  \sumtT
  Y_{it}
  \log\left(\frac{\pi_{it}}{1-\pi_{it}}\right)
  +
  \sumtT
  \log\big(1-\pi_{it}\big)
  \right)
\end{align*}
Suppose we choose a model for $\pi_{it}$ so that
\begin{align*}
  \log\left(\frac{\pi_{it}}{1-\pi_{it}}\right)
  =
  \alpha_i + X_{it}'\beta
\end{align*}
Then, substituting in,
\begin{align*}
  \calL(Y_{i1},\ldots,Y_{iT})
  =
  \exp\left(
  \alpha_i
  \sumtT
  Y_{it}
  +
  \beta'
  \sumtT
  Y_{it} X_{it}
  +
  \sumtT
  \log\big(1-\pi_{it}\big)
  \right)
\end{align*}
This is a distribution in the exponential family with
$\sumtT Y_{it}$ sufficient for $\alpha_i$.
\end{frame}
}


\begin{frame}{Binary Choice: Panel Data}
Summarizing:
If we choose
\begin{align*}
  \log\left(\frac{\pi_{it}}{1-\pi_{it}}\right)
  &=
  \alpha_i + X_{it}'\beta
  \\
  \iff\qquad
  \pi_{it}
  =
  P[Y_{it}=1|X_{it}]
  &=
  \frac{%
    \exp(\alpha_i+X_{it}'\beta)
  }{%
    1+\exp(\alpha_i+X_{it}'\beta)
  }
  =
  G(\alpha_i+X_{it}'\beta)
\end{align*}
then a sufficient statistic for $\alpha_i$ is
the total number of successes
\begin{align*}
  \sumtT
  Y_{it}
\end{align*}
This is useful because then we can form the conditional likelihood
(conditional on the sufficient stat above), in which the parameter
$\alpha_i$ will drop out.

So one solution to the incidental parameters problem that's available in
this logit setting is to just do ML using the conditional likelihood.
\end{frame}


{\footnotesize
\begin{frame}{Binary Choice: Panel Data}
To implement, first recall the likelihood:
\begin{align*}
  &\calL(Y_{i1},\ldots,Y_{iT})
  =
  \exp\left(
  \alpha_i
  \sumtT
  Y_{it}
  +
  \beta'
  \sumtT
  Y_{it} X_{it}
  -
  \sumtT
  \log\big(1+\exp(\alpha_i+X_{it}'\beta)\big)
  \right)
\end{align*}
We can then compute the distribution of the sufficient stat by computing
the probabilities that the sufficient stat equals some total $\tau_i$.
This is done by adding up the probabilities of all configurations that
induce total $\tau_i$:
\begin{align*}
  &
  P\left[
  \sumtT
  Y_{it}
  =
  \tau_i
  \right]
  =
  \sum_{%
    {Y}_i\,:\,\sum_{t=1}^TY_{it}=\tau_i
  }
  \exp\left(
  \alpha_i
  \tau_i
  +
  \beta'
  \sumtT
  {Y}_{it} X_{it}
  -
  \sumtT
  \log\big(1+\exp(\alpha_i+X_{it}'\beta)\big)
  \right)
\end{align*}
Compute the likelihood conditional on the sufficient statistic by
dividing:
\begin{align*}
  P\left[
  Y_{i1},\ldots,Y_{iT}
  \;\bigg|\;
  \sumtT
  Y_{it}
  =
  \tau_i
  \right]
  &=
  \frac{%
    \calL(Y_{i1},\ldots,Y_{iT})
  }{%
    P\left[
    \sumtT
    Y_{it}
    =
    \tau_i
    \right]
  }
  =
  \frac{%
    \exp\left(
    \beta'
    \sumtT
    Y_{it} X_{it}
    \right)
  }{%
    \sum_{%
      \tilde{Y}_i\,:\,\sum_{t=1}^T\tilde{Y}_{it}=\tau_i
    }
    \exp\left(
    \beta'
    \sumtT
    \tilde{Y}_{it} X_{it}
    \right)
  }
\end{align*}
Notice that this is a distribution characterizing $\beta$ without
$\alpha_i$ terms.
We've ``conditioned them out'' similarly to how we difference out the FE
in linear FE models.
\end{frame}
}


\begin{frame}[shrink]{Remarks}
Remarks
\begin{itemize}
  \item Clear from this how the logit assumption was crucial.
    It was just the right assumption for $\pi_{it}=P[Y_{it}=1|X_{it}]$
    so that we could get a sufficient statistic for the $\alpha_i$ and
    condition them out.
    Other choices of $G(\,\cdot\,)$ do not deliver that.
  \item
    If $\tau_i=0$ or $\tau_i=T$, you don't use those observations
    because the conditional likelihood just equals 1 for all values of
    the parameters.
    So if many individuals don't change states, you throw away a lot of
    observations.
\end{itemize}
\end{frame}


\begin{frame}[shrink]{Binary Choice: Panel Data}
Suppose $T=2$.
As stated, $\tau_i=0$ and $\tau_i=2$ are trivial.
So consider $\tau_i=1$.
Compute
\begin{align*}
  P\left[
  Y_{i1}=0,Y_{i2}=1
  \;\bigg|\;
  \sumtT
  Y_{it}
  =
  \tau_i
  \right]
  &=
  \frac{%
    \exp\left(
    \beta' X_{i2}
    \right)
  }{%
    \exp\left(
    \beta'X_{i1}
    \right)
    +
    \exp\left(
    \beta'X_{i2}
    \right)
  }
  \\
  &=
  \frac{%
    1
  }{%
    1+
    \exp\left(
    -\beta'(X_{i2}-X_{i1})
    \right)
  }
\end{align*}
Parallels with FE here:
\begin{itemize}
  \item Using changes in $X$'s to identify $\beta$'s.
  \item Can't identify parameters for covariates fixed across time.
\end{itemize}
\end{frame}


%\begin{frame}[shrink]{Exponential Family}
%Family of distributions for $\{X_i\}_{i=1}^n$ belongs to exponential
%family if it can be written
%\begin{align*}
  %f(x|\theta)
  %=
  %h(x)\exp\left(
  %\sum_{i=1}^s \eta_i(\theta)T_i(x)
  %- A(\theta)
  %\right)
%\end{align*}
%Note that $T_i(x)$ is sufficient for $\eta_i(\theta)$.
%\end{frame}




\section{Tobit Alternatives}

%{\footnotesize
%\begin{frame}
%LAD, write down the likelihood
%\end{frame}
%}





\end{document}



\begin{frame}[shrink]
Binary choice model, for some link function $G(\,\cdot\,)$ with the
properties of a CDF:
\begin{align*}
  P[Y=1|X=x]
  =
  G(x'\beta_0)
\end{align*}
For iid data, the MLE maximizes the log-likelihood function
\begin{align*}
  \hat{\calL}(\beta)
  &=
  \sumiN
  G(X_i'\beta)^{Y_i}
  [1-G(X_i'\beta)]^{1-Y_i}
  \\
  \log\hat{\calL}(\beta)
  &=
  \sumiN
  \bigg\{
  Y_i\log\big( G(X_i'\beta)\big)
  +
  (1-Y_i)
  \log\big(
  1-G(X_i'\beta)
  \big)
  \bigg\}
\end{align*}
Want to show that if $\eta \mapsto \log G(\eta)$ concave, then the
log-likelihood function is globally concave.

For any two points $b_1$ and $b_2$, consider for $\lambda\in[0,1]$:
\begin{align*}
  \log\hat{\calL}(\lambda b_1 + (1-\lambda)b_2)
  &=
  \sumiN
  \bigg\{
  Y_i\log\big( G(X_i'[\lambda b_1 + (1-\lambda)b_2])\big)
  +
  (1-Y_i)
  \log\big(
  1-G(X_i'[\lambda b_1 + (1-\lambda)b_2])
  \big)
  \bigg\}
\end{align*}
Note
\begin{align*}
  G(X_i'[\lambda b_1 + (1-\lambda)b_2])
  =
  G(\lambda X_i'b_1 + (1-\lambda)X_i'b_2)
\end{align*}
By concavity
\begin{align*}
  \log G(\lambda X_i'b_1 + (1-\lambda)X_i'b_2)
  \geq
  \lambda \log G(X_i'b_1)
  +
  (1-\lambda)\log G(X_i'b_2)
\end{align*}

\end{frame}


\begin{frame}[shrink]
Define
\begin{align*}
  \tilde{G}(\eta)
  =
  \log G(\eta)
\end{align*}
Note
\begin{align*}
  \tilde{G}'(\eta)
  =
  \frac{\partial}{\partial \eta}
  \log G(\eta)
  &=
  \frac{G'(\eta)}{G(\eta)}
  \\
  \tilde{G}''(\eta)
  =
  \frac{\partial}{\partial \eta^2}
  \log G(\eta)
  &=
  \frac{G''(\eta)}{G(\eta)}
  -
  \frac{G'(\eta)^2}{G(\eta)^2}
  =
  \frac{G''(\eta)}{G(\eta)}
  -
  \tilde{G}'(\eta)^2
\end{align*}
If $\tilde{G}(\eta)=\log G(\eta)$ concave, then
\begin{align*}
  \tilde{G}''(\eta)
  &\leq 0
  \\
  \implies\quad
  \frac{G''(\eta)}{G(\eta)}
  -
  \tilde{G}'(\eta)^2
  &\leq
  0
  \\
  \frac{G''(\eta)}{G(\eta)}
  &\leq
  \tilde{G}'(\eta)^2
  \\
  G''(\eta)
  &\leq
  \frac{G'(\eta)^2}{G(\eta)}
\end{align*}
\end{frame}


\begin{frame}[shrink]
\begin{align*}
  \log \calL_i(\beta)
  &=
  Y_i\log\big( G(X_i'\beta)\big)
  +
  (1-Y_i)
  \log\big(
  1-G(X_i'\beta)
  \big)
\end{align*}
Derivative
\begin{align*}
  &=
  \left[
  \frac{Y_i}{G(X_i'\beta)}
  -
  \frac{1-Y_i}{1-G(X_i'\beta)}
  \right]
  G'(X_i'\beta)X_i
  \\
  &=
  \left[
  \frac{%
    Y_i(1-G(X_i'\beta))
    -
    (1-Y_i)
    G(X_i'\beta)
  }{G(X_i'\beta)(1-G(X_i'\beta))}
  \right]
  G'(X_i'\beta)X_i
  \\
  &=
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{G(X_i'\beta)(1-G(X_i'\beta))}
  \right]
  G'(X_i'\beta)X_i
  \\
  &=
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{1-G(X_i'\beta)}
  \right]
  \frac{G'(X_i'\beta)}{G(X_i'\beta)}
  X_i
  \\
  &=
  \left[
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{1-G(X_i'\beta)}
  \right]
  \tilde{G}'(X_i'\beta)
  X_i
  \\
  &=
  [Y_i - G(X_i'\beta)]
  [
  1-G(X_i'\beta)
  ]^{-1}
  \tilde{G}'(X_i'\beta)
  X_i
\end{align*}
\end{frame}


\begin{frame}[shrink]
Derivative again
\begin{align*}
  &=
  -
  G'(X_i'\beta)
  [
    1-G(X_i'\beta)
  ]^{-1}
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  [
    1-G(X_i'\beta)
  ]^{-2}
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  [
    1-G(X_i'\beta)
  ]^{-1}
  \tilde{G}''(X_i'\beta)
  X_iX_i'
\end{align*}
\begin{align*}
  &=
  -
  \frac{1}{%
    1-G(X_i'\beta)
  }
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  \frac{%
    1
  }{%
    (1-G(X_i'\beta))^2
  }
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  [
    Y_i
    -
    G(X_i'\beta)
  ]
  \frac{%
    1
  }{%
    (1-G(X_i'\beta))
  }
  \tilde{G}''(X_i'\beta)
  X_iX_i'
\end{align*}

Collect terms
\begin{align*}
  &=
  \bigg[
  Y_i
  \left(
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  +
  \tilde{G}''(X_i'\beta)
  \right)
  \\
  &
  -
  \left(
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  +
  G(X_i'\beta)
  \tilde{G}''(X_i'\beta)
  \right)
  \bigg]
  \frac{1}{%
    1-G(X_i'\beta)
  }
  X_iX_i'
\end{align*}


\end{frame}

\begin{frame}[shrink]
Simplify
\begin{align*}
  &=
  -
  \left[
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
    +
    G(X_i'\beta) \tilde{G}''(X_i'\beta)
    -
    G(X_i'\beta)^2\tilde{G}''(X_i'\beta)
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &
  +
  Y_i
  \left[
  \frac{%
    G'(X_i'\beta)
    \tilde{G}'(X_i'\beta)
  }{%
    (1-G(X_i'\beta))^2
  }
  +
  \frac{%
    \tilde{G}''(X_i'\beta)
    (1-G(X_i'\beta))
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &=
  -
  \left[
  \frac{%
    G'(X_i'\beta) \tilde{G}'(X_i'\beta)
    +
    G(X_i'\beta) \tilde{G}''(X_i'\beta)
    -
    G(X_i'\beta)^2\tilde{G}''(X_i'\beta)
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &
  +
  Y_i
  \left[
  \frac{%
    G'(X_i'\beta)
    \tilde{G}'(X_i'\beta)
    +
    \tilde{G}''(X_i'\beta)
    (1-G(X_i'\beta))
  }{%
    (1-G(X_i'\beta))^2
  }
  \right]
  X_iX_i'
  \\
  &=
  \left(
  \frac{%
    Y_i
    -
    1
  }{%
    (1-G(X_i'\beta))^2
  }
  \right)
  G'(X_i'\beta)
  \tilde{G}'(X_i'\beta)
  X_iX_i'
  \\
  &
  +
  \left(
  \frac{%
    Y_i
    -
    G(X_i'\beta)
  }{%
    (1-G(X_i'\beta))
  }
  \right)
  \tilde{G}''(X_i'\beta)
  X_iX_i'
\end{align*}

\end{frame}









% Sample code
\pause to add breaks
\textcolor{red}{text to show}.
\vspace{20pt}

\usebeamercolor{dull text}
\alert<+>{Words that you want to alert}

