\documentclass[12pt]{article}

\author{Matthew Cocci}
\title{Machine Learning}
\date{}

%% Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{fullpage}
\usepackage{setspace}
%\onehalfspacing


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt}
    %---Width of the line

%\setlength{\headsep}{0.2in}
    %---Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm} %allows for labeling of theorems
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
%\numberwithin{equation}{section}
    %---This labels the equations in relation to the sections
    %---rather than other equations
%\numberwithin{equation}{subsection}
    %---This labels relative to subsections


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure}
    %---For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{
    colorlinks,
        %---This colors the links themselves, not boxes
    citecolor=black,
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},%
            %---Size of the numbers
        numbersep=9pt,%
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc.


%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %---Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %---For bibliographies
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

%\tableofcontents %adds it here

\section{Machine Learning Problems}

There are two main types of learning in the field of machine learning:
\begin{enumerate}

   \item Supervised Learning: In this case, we know the right answer
      to our problem.  For example, we might have data on house prices
      and a number of covariates, including bedrooms, age, location,
      etc.  From there, we try to fit a model that best predicts price
      using the covariates, and we can check the fit of our model
      against the observed values.

   \item Unsupervised Learning: Here, you don't know the right answer.
      For example, you might want to group a set of customers
      into clusters; however, there's no known right answer that you can
      use to check the quality of your clustering model.

\end{enumerate}
These learning methods are applied to serve two main types of
learning problems:
\begin{enumerate}
   \item Regression Problems: The predicted or response variable
      takes on continuous values.
   \item Classification Problems: Each observation or training example
      belongs to some category, and must be classified. This might
      be supervised (like a binary dead-alive type problem) or
      unsupervised (like clustering).
\end{enumerate}



\section{Terminology and Notation}

In order to calibrate an algorithm, we use a set of
\emph{training examples} (the data), denoted
$(\mathbf{X}^{(1)}, Y^{(1)}),
(\mathbf{X}^{(2)}, Y^{(2)}), \ldots, (\mathbf{X}^{(m)}, Y^{(m)})$,
where $(\mathbf{X}^{(i)}, Y^{(i)})$ is the $i$th training example. Each
training example consists of a vector of $n$ \textbf{features}
(covariates)
   \[ \mathbf{X}^{(i)} = \left(X_1^{(i)},X_2^{(i)},\ldots,
      X_n^{(i)}\right)'\]
We use these training examples to fit the parameters of
a \emph{hypothesis} (model), denoted $h_\theta(\mathbf{X})$. If we
consider a simple linear model and that might look like
   \[ h_\theta(X) = \theta_0 + \theta_1 X\]
If we have a multivariate linear model, that would look like
\begin{align*}
   h_\theta(\mathbf{X}) &= \theta' \mathbf{X}\\
   \mathbf{X} &= \left(1, X_1, X_2, \ldots, X_n\right)' \in \mathbb{R}^{n+1},
      \qquad X_0 = 1\\
   \theta &= \left(\theta_0, \theta_1, \theta_2, \ldots, \theta_n\right)'
   \in \mathbb{R}^{n+1}
\end{align*}
{\sl Note}: Above, everything is denoted in upper case letters
to denote random variables, but specific realizations will be
denoted by lowercase analogues.


\section{Statistical Decision Theory}

Now how do we make progress in Machine Learning? Where do
the many results, which we'll detail below, come from?

\subsection{Overview}

Most of it reduces to specifying or assuming a particular
joint distribution of the data, denoted $p(\mathbf{X}, Y)$,
then deriving an \emph{Hypothesis} or \emph{Model} that
best predicts the data.

\paragraph{Note} Oftentimes, in practice, we
jump right to models in the spirit of ``Here's our data,
let's fit the model.'' \emph{However}, this type of thinking
assumes that the joint distribution is well approximated
by the model you have in mind. \emph{Don't forget that.}
You can fit always specify a model, fit a model, then
make predictions, but if you want your predictions to be
reasonable, make sure that you think about the joint
distribution of the data, and make sure the model can
approximate that well.

So once we have a model or hypothesis, we have to fit
the model. We do so by specifying a \emph{Loss Function},
which defines how exactly to penalize errors or misclassification
by our model.  We'll choose the parameters of our model
to minimize the error or misclassification among our
training set observations.

To make this more precise, denote the loss function as
$L\left(Y, h_\theta(\mathbf{X})\right)$, which makes explicit the
dependence of error/misclassification upon the model,
$h_\theta(X)$.
Define the {\sl Expected Prediction Error} (EPE) as
\begin{equation}
    \text{EPE}(L) = E\left[ L\left(Y, h_\theta(\mathbf{X})\right)\right] =
        \int L\left(y, h_\theta(x)\right) p(x, y) \; dx dy
\end{equation}
This makes clear that the EPE is a function of both the
the model and the loss function.

Different models will naturally have different EPE's
given the same training set, but perhaps less obvious:
different loss functions will lead to different fits
and parameterizations given the same model. So fitting
a model is a three step process:
\begin{enumerate}
    \item Specify a model to describe the joint
        distribution of the data.
    \item Specify a Loss Function that defines how
        to penalize errors or misclassifications.
    \item Fit the model by choosing the values of the
        parameters that minimize the chosen Loss Function.
\end{enumerate}

\subsection{Quantitative Response}

The \emph{squared error loss function} remains as a
common loss function for quantitative responses:
\begin{equation}
    \label{sqerror}
    L\left(Y, h_\theta(\mathbf{X})\right) = \left(Y -
        h_\theta(\mathbf{X})\right)^2
\end{equation}
If we try to minimize the



\subsection{Categorical Response}


\section{Gradient Descent}

Again, suppose we want to find point estimates of the components in
a vector $\theta = (\theta_1, \theta_2, \ldots, \theta_n)$ by
minimizing some cost function, $\mathcal{J}(\theta)$.\footnote{We
can easily make the necessary changes if we want to maximize. In
particular, we can minimize the negative of some target function. (The
name ``cost function'' isn't really appropriate if we're trying to
maximize.)} Then we update our parameters all at once as follows:
\begin{align*}
   \theta_1^{(t+1)} &= \theta_1^{(t)} - \alpha \frac{\partial}{
      \partial \theta_1^{(t)}}\left[ \mathcal{J}(\theta)\right]\\
   \vdots \qquad & \qquad \vdots \\
   \theta_n^{(t+1)} &= \theta_n^{(t)} - \alpha \frac{\partial}{
      \partial \theta_n^{(t)}}\left[ \mathcal{J}(\theta)\right]
\end{align*}
where $\alpha$ is the learning rate. This parameter controls the
size of our steps and influences the aggressiveness of our
descent. So use this process to keep updating and iterating
until convergence.

\paragraph{Note} At each iteration, we update all the components of
$\theta$ \emph{simultaneously}. That is, the equations above are a
package deal. We don't update one parameter at a time as we did
with Gibbs Sampling. Instead, we take all the partial derivatives
of the cost function, $\mathcal{J}(\theta)$, then plug into the RHS
of the above expressions.
\\
\\
{\sl Feature Scaling} If we have multiple features, the gradient
of descent may need some help to converge more quickly.  In particular,
if the features can take on ranges of values that are very
different---i.e. $x_1^{(i)} \in (1,3)$ and $x_2^{(i)}
\in (500,10000)$---then our contours will appear very elongated and the
gradient descent algorithm may converge slowly.  To fix this, we might
want to normalize the data and make each feature into a z-score or
apply some other reasonable normoalization.
\\
\\
{\sl Choice of $\alpha$:} First, notice how this process
self-adjusts in the
magnitude of the update.  In particular, as you get close to
the minimum, the derivatives approach zero.  So even with
$\alpha$ constant, the shrinking derivative means that the gradient
descent algorithm will take smaller steps, which is desireable.
To choose a particular value of $\alpha$, you face a tradeoff. Very
small values of $\alpha$ will always allow the gradient descent
algorithm to converge, but it will do so very slowly.  Larger values
of $\alpha$ risk overshooting the local minimum and may cause the
algorithm to diverge.

\section{Multiple Regression}

Recall the \emph{normal equation} for multiple regression, which
in matrix notation, says that parameter estimates should be
   \[ \theta = (X^T X)^{-1} X^T y\]
where $X$ will be an $m\times n+1$ matrix for $m$ training examples
and $n$ features.  The first feature of our $n+1$ features in $X$
is a 1 for each training example to include a constant in our
regression.
\textbf{Note}
that we must have $m\geq n$---that is the number of training
examples greater than the number of features---in order to invert
the matrix in parentheses. Otherwise, the matrix is
\emph{singular} (non-invertible).
\\
\\
The typical cost function that we want to minimize (over $m$
training examples) is:
\begin{equation}
   \label{cost}
   \mathcal{J}(\theta) = \frac{1}{2} \sum^m_{i=1} (h_\theta(x^{(i)}) -
      y^{(i)})^2
\end{equation}


\section{Logistic Regression}

\subsection{Single-Class Problems}

Recall that the response variable can take on one of two
values---typically positive or negative, or $y \in \{0,1\}$.
This is a classification problem where we want to assign observations
(or training examples) to a group based on features. We set our
hypothesis as follows:
\begin{align*}
   h_\theta(x) &= \frac{1}{1+e^{-\theta^T \; x}}
\end{align*}
where $\theta^T$ is a vector of $n+1$ parameters and $x$ is a vector
of $n+1$ features.
The term on the right is called the \emph{logistic} or \emph{sigmoid}
function, and it is guaranteed to lie between 0 and 1.
The resulting output from $h_\theta(x)$ is a conditional probability
that an observation is in the 1 group given the observed features.
\\
\\
{\sl Cost Function:} It turns out that the cost function we used
for multiple regression in Equation \ref{cost} will be
\emph{non-convex}, so the gradient descent algorithm will get stuck
at local modes.  This leads us to modify our cost function in
line with maximum likelihood so that
the cost for a particular training example is
\begin{align}
    \text{Cost}(h_\theta(x),\;y) &= \begin{cases}
      -\ln h_\theta(x) & \text{if $y=1$} \\
      -\ln (1-h_\theta(x)) &\text{if $y=0$} \end{cases}\notag\\
    &=-y \ln h_\theta(x)- (1-y) \ln(1-h_\theta(x))\\
    \Rightarrow \quad
    \mathcal{J}(\theta) &= -\frac{1}{m}\sum^m_{i=1}\left[
    y^{(i)} \ln h_\theta(x^{(i)}) +(1-y^{(i)})\ln (1-h_\theta(x^{(i)})
    )\right]
\end{align}


\subsection{Multi-Class Problems}

Suppose that we want to generalize single-class logistic regression
to the case of multiple groups. Specifically, suppose that each
$y^{(i)}$ can belong to groups $\{1,2,\ldots,k\}$. Then the
\emph{one-vs-all} approach says
\begin{enumerate}
   \item Run $k$ different logistic regressions, where you take one
      group, $i$, and treat it as the posive case. The remaining
      $k-1$ groups are treated as one big negative group.
   \item Using your training examples
      to fit a hypothesis for each such logistic regression so that
      there are $k$ hypotheses in total: $h_\theta^{(i)}(x), \ldots,
      h_\theta^{(k)}(x)$.
   \item Then, given some covariates, $x$, you predict the group
      for this observation to be
      \[ \max_i \{ h_\theta^{(1)}(x), \ldots, h_\theta^{(k)}(x)\} \]
      i.e. group $i$, the group that has the maximum chance
      of containing the observation.
\end{enumerate}



\section{Regularization}

In order to avoid the problem of overfitting, we typically apply
\emph{regularization}, where we penalize large values of the
parameters we're trying to fit by adding a term to the cost
function. This means we want to find
\begin{equation}
   \label{regularized}
   \arg \min_\theta \mathcal{J}(\theta) = f\left(h_\theta(x), y\right)
   + g(\theta)
\end{equation}
where $\theta = (\theta_1, \ldots, \theta_n)$ and where $x$ and
$y$ are a vector of $m$ training examples. Note that
$\theta_0$---the conventional constant parameter---is not included
in the regularization.
\\
\\
As an example of a standard form for Equation \ref{regularized},
we might choose
\begin{equation}
   \label{regstd}
   \arg \min_\theta \mathcal{J}(\theta) = \arg\min \;
   \frac{1}{2m}\left[ \sum^m_{i=1} \left(h_\theta(x^{(i)})-y^{(i)}
   \right)^2 + \lambda \sum^n_{j=1} \theta_j^2
   \right]
\end{equation}


\subsection{Implementation by Normal Equations, Multiple Regression}

Suppose we consider the problem of multiple regression, and we
need to choose the regression parameters with the regularization
and cost function in Equation \ref{regstd}. Then the normal equations
are modified to
\begin{equation}
   \label{normreg}
   \theta = \left(X^T X + \lambda R\right)^{-1}  X^T y
\end{equation}
where $R$ is nearly the identity matrix $I_n$, but with entry
$R_{1,1}=0$ instead of 1.
\\
\\
Note that \emph{without} regularization
we must have $m\geq n$---that is the number of training
examples greater than the number of features. Otherwise the
matrix in parentheses is \emph{singular} (non-invertible).
\textbf{But} if we choose $\lambda>0$ and apply regularization,
then the matrix in parentheses is \emph{guaranteed} to
be invertible, even if $m < n$.


\subsection{Implementation by Gradient Descent, Multiple
Regression}

If this were used in an implementation of gradient descent, this
would yield an updating rule of:
\begin{align*}
   \theta_0 &:= \theta_0 - \alpha \; \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_0^{(i)}\\
   \theta_j &:= \theta_j - \alpha \left[ \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_j^{(i)} +
   \frac{\lambda}{m} \theta_j \right]\\
   &:= \theta_j\left(1-\alpha \frac{\lambda}{m}\right) - \alpha \;
   \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_j^{(i)}
\end{align*}
where $j=1, \ldots, n$ and $h_\theta(x)$ is the standard
multiple regression form.
It's clear that the first term is $\theta_j$
times a constant less than 1, so we're \emph{shrinking} the
parameter towards zero, then the second term updates as before the
shrunken parameter.


\subsection{Implementation by Gradient Descent, Logistic
Regression}

If we want to implement a regularized logistic regression, we'll take
the cost function to be
\begin{equation}
   \label{logreg}
   \mathcal{J}(\theta) = -\left[\frac{1}{m} \sum_{i=1}^m
   y^{(i)} \ln h_\theta(x^{(i)}) + (1-y^{(i)}) \ln \left(1-
   h_\theta(x^{(i)})\right)\right] + \frac{\lambda}{2m} \sum^n_{j=1}
   \theta_j^2
\end{equation}
Given this cost function, the update rule for gradient descent
becomes
\begin{align*}
   \theta_0 &:= \theta_0 - \alpha \; \frac{1}{m} \sum^m_{i=1}
   \left( h_\theta(x^{(i)})-y^{(i)}\right) x_0^{(i)}\\
   \theta_j &:=
      \theta_j - \alpha \left[ \frac{1}{m} \sum^m_{i=1}
      \left( h_\theta(x^{(i)})-y^{(i)}\right) x_j^{(i)} +
      \frac{\lambda}{m} \theta_j \right]
\end{align*}
where $j=1,\ldots,n$.  Now this looks very similar to the update
rule for multiple regression, except it doesn't make explicit
the different hypothesis functions.  In particular, for logistic
regression, we have
\[ h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} \]


\newpage
\section{Neural Networks}

This is useful for classification problems involving a high
number of features.  We saw with logistic regression that including
different powers and cross-products of the features can give a
better fit for the classification problem, fitting circles and ellipses
to the data; \emph{however},
while this works well for, say, two features, the parameter
optimization problem grows very quickly with more features.
\\
\\
Neural networks are also very useful because they can represent
highly non-linear functions and classifications.  In particular,
they can even be used to construct logical decision-making and
classification.

\subsection{Model Representation}

Here's the basic idea:
between the \emph{input layer} (the features) and the
\emph{output layer} (the hypothesis), we introduce at least one
\emph{hidden layer} transforming the inputs into new variables
that will be used in the hypothesis. This forms the
\emph{architecture} of the network.
\\
\\
We introduce the following notation to be used in computing/evaluating
layers,\footnote{
Don't take ``activation'' and ``weights'' too seriously.  ``Activation''
just indicates the \emph{value} computed and output by a specific
layer or unit in the layer. In addition, ``weights'' are just
the name for parameters in the neural networks literature.}
\begin{align*}
   a^{(j)}_i &= \text{the ``activation'' of unit $i$ in layer $j$} \\
   \Theta^{(j)} &= \text{matrix of weights controlling function}\\
   &\qquad \text{mapping from layer $j$ to $j+1$}
\end{align*}
Now suppose there are $n$ features, $L$ hidden layers, and $s_j$
units in layer $j$. Note that in each layer, we'll typically introduce
a constant term, $x_0$ (for the features) or $a^{(j)}_0$ in
the intermediate layers. This is commonly called the ``bias unit.''
\begin{align*}
   a^{(2)}_i &= f^{(1)}_i\left(x_0, \ldots, x_n,
      \Theta_{i0}^{(1)},\ldots,\Theta_{in}^{(1)}\right), \qquad
      i=1,\ldots,n
   \\	\\
   a^{(j+1)}_i &= f^{(j)}_i\left(a_0^{(j)}, \ldots,
      a_{s_j}^{(j)}, \;
      \Theta_{i0}^{(j)},\ldots,\Theta_{is_j}^{(j)}\right), \qquad
      \begin{cases}
	 i=1,\ldots,s_{j+1} \\  j = 1, \ldots, L-1 \end{cases}
      \\
      \\
   h_\Theta(x) =
      a^{(L)}_1 &= f^{(L-1)}_1\left(a_0^{(L-1)},
      \ldots,
      a_{s_{L-1}}^{(L-1)}, \;
      \Theta_{i0}^{(L-1)},\ldots,\Theta_{is_{L-1}}^{(L-1)}
      \right)
\end{align*}
It follows, by our definition of $s_j$ and $\Theta^{(j)}$, that
$\Theta^{(j)}$ is of dimension $s_{j+1} \times (s_j + 1)$.
I know; there's a lot of notation. But basically, it says each unit
in the first layer is some function of the features and parameters.
Then the activiation of all units in subsequent layers are functions of
the activations of units in the immediately previous layer. This
goes on until you get to your hypothesis, and this process is called
\emph{forward propogation}.

\newpage
\subsection{Vectorized Implementation}

Here, we simplify notation a bit and take the special case where the
function of the attributes, $g(\cdot)$, is the sigmoid
function. So we define the stage 2 attributes as follows
\begin{align*}
    a_1^{(2)} &= g\left(\Theta_{10}^{(1)} x_0
	+ \Theta_{11}^{(1)} x_1 + \cdots
	+ \Theta_{1n}^{(1)} x_n\right) = g\left(z_1^{(2)}\right)
    \\
    \vdots \quad & \qquad \vdots\\
    a_{s_2}^{(2)} &= g\left(\Theta_{s_2 0}^{(1)} x_0
	+ \Theta_{s_2 1}^{(1)} x_1 + \cdots
	+ \Theta_{s_2 n}^{(1)} x_n\right)
	= g\left(z_{s_2}^{(2)}\right)
\end{align*}
Now let's vectorize, redefining the vector $x$ of dimension
$(n+1)\times1$ (with one more row than the number of features for the
extra constant term) as follows to make notation
simpler: $x = a^{(1)}$. Next, we define $z^{(j)}$ as a vector of
dimension $s_j \times 1$, where $s_j$ is the number of attributes in
layer $j$. We do the following steps to compute the hypothesis:
\begin{enumerate}
    \item Compute $z^{(2)} = \Theta^{(1)} x = \Theta^{(1)} a^{(1)}$,
	then use $z^{(2)}$ to compute $a^{(2)} = g(z^{(2)})$.
    \item Prepend $a_0^{(2)}=1$ to $a^{(2)}$ to allow for a
	constant/bias term, then compute $z^{(3)} = \Theta^{(2)}a^{(2)}$.
    \item Continue this process, prepending $a_0^{(j)}=1$ to
	$a^{(j)}$, computing $z^{(j+1)} = \Theta^{(j)} a^{(j)}$ for
	all $j$ number of layers until you reach $h_\Theta(x)$, a
	process called \emph{forward propagation}.
\end{enumerate}


\subsection{Cost Function for a Classfication Neural Network}

{\sl Binary Classification:} In this special case, $y \in \{ 0, 1\}$
and there is 1 output unit that computes $h_\Theta(x) \in \mathbb{R}$.
So $s_L = 1$, where $s_\ell$ is the number of units in layer $\ell$.
\\
\\
{\sl Multi-Class Classification:} In this more general case,
$y\in \mathbb{R}^K$ where there are $K$ distinct classes.
For ex., $y$ could be one of four classes (pedestrian,
car, motorcycle, truck), denoted
\[ y = \begin{bmatrix} 1 & 0 & 0 & 0 \end{bmatrix}, \qquad
   \begin{bmatrix} 0 & 1 & 0 & 0 \end{bmatrix}, \qquad
   \begin{bmatrix} 0 & 0 & 1 & 0 \end{bmatrix}, \qquad\text{or}\qquad
   \begin{bmatrix} 0 & 0 & 0 & 1 \end{bmatrix} \]
So there are $K$ output units. Typically, we only use this setup
when $K\geq 3$.
\\
\\
{\sl Cost Function:}
This is a generalization of the cost function for
logistic regression:
\begin{align}
   h_\Theta(x) &\in \mathbb{R}^K \qquad \left(h_\Theta(x)\right)_i =
   \text{$i$th output} \notag\\
   J(\Theta) &= -\frac{1}{m} \left[\sum^m_{i=1}\sum^K_{k=1} y_k^{(i)}
   \ln \left(h_\Theta(x^{(i)})\right)_k + \left(1-y_k^{(i)}\right)
   \ln \left(1-\left(h_\Theta(x^{(i)})\right)_k\right)
   \right] \notag\\
   &\qquad + \frac{\lambda}{2m} \sum^{L-1}_{\ell=1} \sum^{s_\ell}_{i=1}
   \sum^{s_{\ell+1}}_{j=1} \left(\Theta^{(\ell)}_{ji}\right)^2
\end{align}
The first term is a simple generalization of the regular
logistic regression cost function---we just sum over the $K$ different
classes. Note that the next term in $J(\Theta)$ is a summation over
weights \emph{not} including the constant bias terms (so we don't
include $i=0$).

\newpage
\subsection{Backpropogation Algorithm}

In order to use gradient descent, we need the value of the
cost function, $J(\Theta)$, along with the partial derivatives,
$\frac{\partial J(\Theta)}{\partial \Theta^{(\ell)}_{ij}}$.
\\
\\
To compute the derivatives, we use the \emph{backpropogation} algorithm.
Here's the algorithm (vectorized for speed) for multi-class logistic
classification.

Let $\Theta$ denote the set of parameters. Let $Y$ denote
the $K\times N$ matrix of outcomes (where $K$ is the number of
classes and $N$ is the number of training examples).
Similarly, let $X$ denote the $F\times N$ matrix of inputs.
Let $g(z)$ denote the logistic function. If $z$ is an array, then the
logistic function is applied element-wise.

The derivative of $J(\,\cdot\,)$ at that $\Theta$ can be computed
as follows.
\begin{enumerate}
  \item Use forward propagation to get the values of $a^{(\ell)}$ and
    $J(\Theta)$
    \begin{enumerate}
      \item
        Initialize $a^{(1)}=X$.
        If the first row is not a row of zeros (to account for the bias
        term), add it.
      \item
        For $\ell=1,\ldots,L-1$, compute
        \begin{align*}
          a^{(\ell+1)} =
          \begin{pmatrix}
            \mathbf{1}
            \\
            g\left(
            \Theta^{(\ell)}
            a^{(\ell)}
            \right)
          \end{pmatrix}
        \end{align*}
        where $\mathbf{1}$ is a $1\times N$ matrix of ones to account
        for the bias term.
      \item
        Set $h_\Theta(X) = g\left(\Theta^{(L)}a^{(L)}\right)$
    \end{enumerate}
  \item
    Next, we use back propagation to compute the derivative.
    \begin{enumerate}
      \item
        Initialize the error vector
        \begin{align*}
          \delta^{(L+1)} &= Y - h_\Theta(X)
        \end{align*}
      \item Iterate backwards and compute
        \begin{align*}
          \delta^{(\ell)}
          &=
          \big[
          \Theta^{(\ell)}
          \big]^T
          \delta^{(\ell+1)}
          .\times
          g'\big(\Theta^{(\ell-1)}a^{(\ell-1)}\big)
          \qquad \ell = 2,\ldots,L
          \\
          \iff\qquad
          &=
          \big[
          \Theta^{(\ell)}
          \big]^T
          \delta^{(\ell+1)}
          .\times
          \big[
          a^{(\ell)}
          .\times
          \left(\mathbf{1}- a^{(\ell)}\right)
          \big]
        \end{align*}
        where $\mathbf{1}$ is here array of ones, the same dimension as
        $a^{(\ell)}$.

      \item From there, we can compute
        \begin{align*}
          \frac{\partial J(\Theta)}{\partial \Theta^{(\ell)}_{ij}}
          &=
          \delta^{(\ell+1)}_{i,\cdot}
          \cdot
          a^{(\ell)}_{j,\cdot}
        \end{align*}
        where we are taking the dot product between row $i$ of
        $\delta^{(\ell+1)}$ (which has $N$ columns) and row $j$ of
        $a^{(\ell)}$ (which also has $N$ columns).
    \end{enumerate}
\end{enumerate}
To understand where this comes from, we first consider what happens with
only one observation. In that case, the cost function is given by
\begin{align*}
  J(\Theta)
  &=
  -
  \underbrace{%
  \sum^K_{k=1}
  \bigg[
    y_k
    \ln \left( h_\Theta(x)_k \right)
    +
    (1-y_k)
    \ln \big(1-h_\Theta(x)_k\big)
  \bigg]
  }_{=:\tilde{J}(\Theta)}
  +
  \frac{\lambda}{2}
  \sum^{L}_{\ell=1} \sum^{s_\ell}_{i=1}
   \sum^{s_{\ell+1}}_{j=1} \left(\Theta^{(\ell)}_{ij}\right)^2
\end{align*}
We first focus on computing the derivative of $\tilde{J}$ only.
To start, note that
\begin{align*}
  h_{\Theta}(x)_k
  = g\left(\Theta_{k\cdot}^{(L)}a^{(L)}\right)
  \qquad
  a^{(\ell)}_m
  = g\left(\Theta^{(\ell-1)}_{m\cdot}a^{(\ell-1)}\right)
\end{align*}
where $\Theta_{i\cdot}^{(\ell)}$ denotes the $i$th row of
$\Theta^{(\ell)}$ and
\begin{align}
  g(z) = \frac{1}{1+\exp(-z)}
  \quad
  \implies\quad
  g'(z) = g(z)(1-g(z))
  \label{logisticderiv}
\end{align}
Now, let's start computing
$\frac{\partial \tilde{J}(\Theta)}{\partial \Theta^{(\ell)}_{ij}}$,
replacing $g_\Theta(x)_k$ with
$g\left(\Theta_{k\cdot}^{(L)}a^{(L)}\right)$ in the process
\begin{align*}
  \frac{\partial \tilde{J}(\Theta)}{\partial \Theta^{(\ell)}_{ij}}
  &=
  \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
  \sum_{k=1}^K
  \left[
    y_k \ln g\left(\Theta^{(L)}_{k\cdot} a^{(L)}\right)
    +
    (1-y_k) \ln\left(
    1-
    g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
  \right)
  \right]
  %\\
  %&=
  %\sum_{k=1}^K
  %\left[
  %y_k
  %\frac{%
    %\frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
    %\left[
    %g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
    %\right]
  %}{g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)}
  %-
  %(1-y_k)
  %\frac{%
    %\frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
    %\left[
    %g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
    %\right]
  %}{%
    %1-
    %g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
  %}
  %\right]
  \\
  &=
  \sum_{k=1}^K
  \left[
  y_k
  \frac{%
    g'\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
    \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
    \left[
    \Theta^{(L)}_{k\cdot} a^{(L)}
    \right]
  }{g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)}
  -
  (1-y_k)
  \frac{%
    g'\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
    \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
    \left[
    \Theta^{(L)}_{k\cdot} a^{(L)}
    \right]
  }{%J
    1-
    g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
  }
  \right]
\end{align*}
We can use Result~\ref{logisticderiv} to substitute in for the
derivative $g'$. That simplifies things considerably
\begin{align*}
  \frac{\partial \tilde{J}(\Theta)}{\partial \Theta^{(\ell)}_{ij}}
  &=
  \sum_{k=1}^K
  \left\{
  \left[
  y_k
    \left(
    1-
    g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
    \right)
  -
    (1-y_k)
    g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
  \right]
  \cdot
  \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
  \left[
  \Theta^{(L)}_{k\cdot} a^{(L)}
  \right]
  \right\}
  \\
  &=
  \sum_{k=1}^K
  \left\{
  \left[
    y_k
  -
    g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
  \right]
  \cdot
  \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
  \left[
  \Theta^{(L)}_{k\cdot} a^{(L)}
  \right]
  \right\}
\end{align*}
Now define vector $\delta^{(L+1)}$ as
\begin{align*}
  \delta^{(L+1)}_k :=
  y_k - g\big(\Theta^{(L)}_{k\cdot} a^{(L)}\big)
\end{align*}
Therefore, we can write
\begin{align}
  \frac{\partial \tilde{J}(\Theta)}{\partial \Theta^{(\ell)}_{ij}}
  &=
  \sum_{k=1}^K
  \left[
    \delta^{(L+1)}_k
  \cdot
  \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
  \left[
  \Theta^{(L)}_{k\cdot} a^{(L)}
  \right]
  \right]
  \label{tildeJ}
\end{align}
Next, we consider a generalization (with some name changes for indices)
of the other term in this sum
\begin{align*}
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \left[
  \Theta^{(\ell)}_{n_{\ell+1},\cdot} \, a^{(\ell)}
  \right]
  =
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \left[
  \sum_{n_\ell=0}^{s_\ell}
  \Theta^{(\ell)}_{n_{\ell+1},n_\ell} a^{(\ell)}_{n_\ell}
  \right]
  \qquad \text{for}\quad
  \ell^* \leq \ell
\end{align*}
where $s_\ell$ is the number of non-bias-term nodes in layer $\ell$.
First, it's clear that if $\ell^*=\ell$, this derivative is
\begin{align}
  \frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
  \left[
  \Theta^{(\ell)}_{n_{\ell+1},\cdot} a^{(\ell)}
  \right]
  %=
  %\sum_{n_\ell=0}^{s_\ell}
  %\frac{\partial}{\partial \Theta^{(\ell)}_{ij}}
  %\left[
  %\Theta^{(\ell)}_{n_{\ell+1},n_\ell} a^{(\ell)}_{n_\ell}
  %\right]
  &=
  \begin{cases}
    a_j^{(\ell)} & n_{\ell+1} = i \\
    0 & n_{\ell+1}\neq i
  \end{cases}
  \qquad\iff\qquad
  a_j^{(\ell)} \cdot \mathbf{1}\{n_{\ell+1}=i\}
  \label{indicator}
\end{align}
Next, for $\ell^*<\ell$, we have have to use recursion
\begin{align}
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \left[
  \Theta^{(\ell)}_{n_{\ell+1},\cdot} \,a^{(\ell)}
  \right]
  &=
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \left[
  \sum_{n_\ell=0}^{s_\ell}
  \Theta^{(\ell)}_{n_{\ell+1},n_\ell} a^{(\ell)}_{n_\ell}
  \right]
  =
  \sum_{n_\ell=0}^{s_\ell}
  \left\{
  \Theta^{(\ell)}_{n_{\ell+1},n_\ell}
  \cdot
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \left[
  a^{(\ell)}_{n_\ell}
  \right]
  \right\}
  \notag
  \\
  &=
  \sum_{n_{\ell}=0}^{s_\ell}
  \left\{
  \Theta^{(\ell)}_{n_{\ell+1},n_\ell}
  \cdot
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \bigg[
  g\left(
  \Theta_{n_\ell,\cdot}^{(\ell-1)}
  a^{(\ell-1)}
  \right)
  \bigg]
  \right\}
  \notag
  \\
  &=
  \sum_{n_\ell=0}^{s_\ell}
  \left\{
  \Theta^{(\ell)}_{n_{\ell+1},n_\ell}
  \cdot
  g'\left(
  \Theta_{n_\ell,\cdot}^{(\ell-1)}
  a^{(\ell-1)}
  \right)
  \cdot
  \frac{\partial}{\partial \Theta^{(\ell^*)}_{ij}}
  \bigg[
  \Theta_{n_\ell,\cdot}^{(\ell-1)}
  a^{(\ell-1)}
  \bigg]
  \right\}
  \label{recurse}
\end{align}
Notice that the derivative in the term of the sum is just like what we
started with, only one level deeper.
Therefore, we can just keep recursing down $r=\ell-\ell^*$ times until
we hit $\ell-r=\ell^*$.

Given this structure, we can compute the derivatives recursively. To do
so, first suppose that $g(z)$ is the vectorized logistic function, so
that if $z$ is a vector, $g(z)$ returns another vector with $i$th
element $g(z_i)$. Then, start from $\delta^{(L+1)}$ defined above to
define the follow objects
\begin{align*}
  \delta^{(L+1)} &= y - g\left(\Theta^{(L)}a^{(L)}\right)
  \\
  \delta^{(\ell)} &=
  \big[
  \delta^{(\ell+1)}
  \big]^T
  \Theta^{(\ell)}
  .\times
  g'\big(\Theta^{(\ell-1)}a^{(\ell-1)}\big)
  \qquad \ell = 1,\ldots,L
\end{align*}
where $.\times$ denotes elementwise multiplication.
It can be verified by expanding out Equation~\ref{tildeJ} using
Equations~\ref{indicator} and~\ref{recurse} that
\begin{align*}
  \frac{\partial \tilde{J}(\Theta)}{\partial \Theta^{(\ell)}_{ij}}
  &=
  \delta^{(\ell+1)}_i a^{(\ell)}_j
\end{align*}






%%%% APPPENDIX %%%%%%%%%%%

% \appendix




\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf}
%	 }
%      }
%   \end{figure}


%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
