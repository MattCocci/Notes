
\section{Discrete Time Review}

\subsection{Basic Building Block}

The basic building block is random iid noise:
    \[ \varepsilon_t \sim \text{iid} \qquad
	E_t\varepsilon_{t+1} = 0 \qquad 
	E_t\varepsilon^2_{t+1} = \sigma_\varepsilon^2
    \]
where $E_t(\cdot)$ represents the expectation \emph{conditional} on
the information at time $t$. In practice, that means any variable
with a $t$ subscript inside a expectation, variance, covariance, etc.
operator can be taken out because it is \emph{known} at time $t$.
\\
\\
Often $\varepsilon_t$ will be draws
from a normal distribution, but they don't have to be.

\subsection{Canonical AR(1) Model}

More complicated than simple random noise, we have the AR(1) process,
which is written in one of either two standard ways, with the latter
being more common as we move to continuous time:
\begin{align*}
    x_{t+1} &= \rho x_t + \varepsilon_{t+1} \qquad 
	E_t\varepsilon_{t+1} = 0 \qquad
	E_t\varepsilon^2_{t+1} = \sigma^2_\varepsilon \\
    x_{t+1} &= \rho x_t + \sigma_\varepsilon \varepsilon_{t+1} \qquad 
	E_t\varepsilon_{t+1} = 0 \qquad
	E_t\varepsilon^2_{t+1} = 1 
\end{align*}
From there, it's easy to show via the linearity property of the 
expectation operator and by successive substitution that
\begin{align*}
    E_t x_{t+1} &= E_t[\rho x_t + \varepsilon_{t+1}] \\
    &= \rho x_t \\
    E_t x_{t+k} &= \rho^k x_t
\end{align*}
As for the variance conditional on information at time $t$, denoted
by $\text{Var}_t$, we get
\begin{align*}
    \text{Var}_t(x_{t+1}) &= \text{Var}_t(\rho x_t + \varepsilon_{t+1}) \\
	&= \sigma_\varepsilon^2 \\
    \text{Var}_t(x_{t+2}) &= \text{Var}_t(\rho x_{t+1} + \varepsilon_{t+2}) \\
	&= \text{Var}_t(\rho x_{t+1}) + \text{Var}_t(\varepsilon_{t+2}) 
	    + 2 \; \text{Cov}_t(\rho x_{t+1}, \; \varepsilon_{t+2}) \\
	&= \rho^2 \; \text{Var}_t(x_{t+1}) + \text{Var}_t(\varepsilon_{t+2}) 
	    + 0 
	= \rho^2 \sigma_\varepsilon^2 + \sigma_\varepsilon^2 \\
	&= (1+\rho^2) \sigma_\varepsilon^2 \\
    \text{Var}_t(x_{t+k}) &= (1 + \rho^2 + \cdots + \rho^{2(k-1)}) 
	\sigma^2_\varepsilon
\end{align*}
The last line we prove by induction.
\begin{proof} In the case of $k=3$, which we will take as the base
case,
\begin{align*}
    \text{Var}_t(x_{t+3}) &=  \text{Var}_t(\rho x_{t+2} + \varepsilon_{t+3}) \\
    &=\rho^2 \; \text{Var}_t(x_{t+2}) + \text{Var}_t(\varepsilon_{t+3})
	+ \text{Cov}_t(x_{t+2}, \; \varepsilon_{t+3}) \\
    \text{Var($x_{t+2}$) from above } \quad &= \rho^2 ( 1 + \rho^2) \sigma^2_\varepsilon 
	+ \sigma^2_\varepsilon + 0 \\
    &= (1 + \rho^2 + \rho^4) \sigma^2_\varepsilon 
\end{align*}
So the base case holds.  Now assume it holds for arbitrary $k$,
and show that the condition also holds for $k+1$:
\begin{align*}
    \text{Var}_t(x_{t+k+1}) &= \text{Var}_t(\rho x_{t+k} + 
	\varepsilon_{t+k+1}) \\
    &= \rho^2 \; \text{Var}_t( x_{t+k}) + \text{Var}_t(\varepsilon_{t+k+1})
	+ \text{Cov}_t(x_{t+k}, \; \varepsilon_{t+k+1}) \\
    &= \rho^2 (1 + \rho^2 + \cdots + \rho^{2(k-1)})\sigma^2_\varepsilon
	+ \sigma^2_\varepsilon \\
    &= (1 + \rho^2 + \cdots + \rho^{2([k+1]-1)})\sigma^2_\varepsilon
\end{align*}
so it holds for $k+1$.
\end{proof}

\subsection{Canonical MA(1) Model}

The basic form of the moving average MA(1) model, with conditional 
means:
\begin{align*}
    x_{t+1} &= \varepsilon_{t+1} + \theta \varepsilon_t  
    \qquad E_t \varepsilon_{t+1} = 0, \quad 
	E^2_t \varepsilon_{t+1} = \sigma^2_\varepsilon
\end{align*}
The first moments for the process, conditional on information 
at time $t$, are as follows:
\begin{align*}
    \Rightarrow \quad E_t x_{t+1} &= E_t\varepsilon_{t+1} + E_{t} [\theta \varepsilon_t ]\\
		&= \theta \varepsilon_t \\
    E_t x_{t+2} &= E_t[ \varepsilon_{t+2}  + \theta \varepsilon_{t+1} ] 
	= E_t[\varepsilon_{t+2}] + E_t[ \theta \varepsilon_{t+1} ] \\
    &= 0 \\
    E_t x_{t+k} &= 0
\end{align*}
Now for the conditional variances, dropping the covariance terms
which we included above
since innovations at different times are always independent:
\begin{align*}
    \text{Var}_t(x_{t+1}) &= \text{Var}_t(\varepsilon_{t+1} +
	\theta \varepsilon_t) \\
	&= \text{Var}_t(\varepsilon_{t+1}) 
	    + \theta^2 \; \text{Var}_t(\varepsilon_t)  \\
	&= \sigma_\varepsilon^2 \\
    \text{Var}_t(x_{t+k}) &= 
	\text{Var}_t(\varepsilon_{t+k} + \theta \varepsilon_{t+k-1}) \\
	&=  \text{Var}_t(\varepsilon_{t+k}) 
	    + \theta^2 \text{Var}_t(\varepsilon_{t+k-1}) \\
	&=  (1 + \theta^2) \sigma_\varepsilon^2 \\
\end{align*}
   
\newpage
\subsection{Unconditional Moments}

So far, all of the means and variances above were \emph{conditional}
on information at time $t$.  So that implies computing $E_t x_{t+1}$
and $\text{Var}_t(x_{t+1})$ via the formulas above requires knowledge 
of $x_t$.
\\
\\
{\sl Mean}:
However, suppose you wanted to generate the series from scratch.  What
is the \emph{unconditional} mean and variance so that you can draw
$x_t$ \emph{without} prior values $x_{t-1}, x_{t-2}, \ldots$?
\begin{align*}
    Ex_t &= E[\rho x_{t-1}] + E\varepsilon_t \\
	&= \rho Ex_{t-1} + 0  \\
    \Rightarrow \quad Ex_t &= 0
\end{align*}
The last line follows because the unconditional average $Ex_t$ must
equal the unconditional average $Ex_{t-1}$, forcing the expecation
to be zero.
\\
\\
{\sl Variance}: Now we do the same thing for the variance. We'll
also use the fact that the unconditional variance for $x_t$ must
equal that for $x_{t-1}$, which allows us to solve:
\begin{align*}
    \text{Var}(x_{t}) &= \text{Var}(\rho x_{t-1} + \varepsilon_t) \\
    &= \rho^2 \; \text{Var}(x_{t-1}) + \text{Var}( \varepsilon_t) \\
	\text{Using $\text{Var}(x_{t-1}) = \text{Var}(x_{t})$:} \quad
    &= \rho^2 \; \text{Var}(x_{t}) + \sigma_\varepsilon^2 \\
    \Rightarrow \quad \text{Var}(x_{t}) &= 
	\frac{\sigma^2_\varepsilon}{1-\rho^2}
\end{align*}


\subsection{Translating Between AR and MA Models}

{\sl AR to MA}: First, 
you can transform an AR(1) model into a MA($\infty$) model
by ``solving'' the AR model through successive substitution:
\begin{align*}
    x_t &= \rho x_{t-1} + \varepsilon_t \\
    &= \rho (\rho x_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \\
    &= \rho^2 x_{t-2} + \rho \varepsilon_{t-1}) + \varepsilon_t \\
    &= \quad \vdots \qquad \vdots \\
    &= \sum^\infty_{j=1} \rho^j \varepsilon_{t-j}
\end{align*}
{\sl MA to AR}: Similarly, you can transform an MA(1) model to
an AR($\infty$) model as follows:


\newpage
\section{Continuous Time}

\subsection{Brownian Motion}

We define Brownian Motion, the continuous version of the random walk,
as follows:
\begin{enumerate}
    \item Normal Increments for all $\Delta \in \mathbb{R}$:
	\[ z_{t+\Delta} - z_t \sim \text{N}(0, \; \Delta) \]
    \item Independence of non-overlapping increments.
    \item Non-overlapping increments of the same length are
	identically (and normally) distributed.
\end{enumerate}

\subsection{Fundamental Building Block, $dz_t$}

Just as random noise, $\varepsilon_t$ was the basic building block
for discrete time, the analogous building block in continuous time is
\begin{equation}
    dz_t = \lim_{\Delta \rightarrow 0} z_{t + \Delta} - z_t
\end{equation}
Note, $d$ is \emph{forward} difference operator.
\\
\\
We also have the result that $dz$ is of size (or order) $\sqrt{dt}$ 
(which is equivalent to the limit of $\Delta$), since recall
\begin{align*}
    \text{Var}(z_{t + \Delta} - z_t) &= \Delta \\
    \Leftrightarrow \quad \text{Sd}(z_{t + \Delta} - z_t) &= 
	\sqrt{\Delta}
\end{align*}
So now we can see why $z_t$ is not differentiable.  Namely, if 
we tried to consider $dz_t/dt$, we know that $dz_t$ is of order
$\sqrt{dt}$; therefore, the ratio $dz_t/dt$ diverges to $\pm \infty$
since $dt$ is close to zero, while the numerator, $dz_t$ of order
$\sqrt{dt}$, is larger. So we can talk about \emph{the change}, $dz_t$,
because that change is going to be very small over short time horizons.
But we can't talk about the \emph{rate of change}, because compared
to ``the change'' in the process, the change in time is smaller still,
such that the ratio diverges.
\\
\\
{\sl Moments}: Next, we have
\begin{align*}
    E_t dz_t &= 0 \\
    \text{Var}_t(dz_t) &= dt \\
    \text{Cov}(dz_t, \; dz_s) &= 0 \qquad s\neq t
\end{align*}


