\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Kalman Filter}
\date{\today}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Basic Idea and Terminology}

Here's the basic procedure associated with the Kalman
Filter:
\begin{enumerate}
    \item Start with a prior for some variable of interest
	in the current period, $p(x)$.
    \item Observe the current measurement $y_t$.
    \item ``Filter'' out the noise and 
	compute the filtering distribution: $p_t(x | y)$.
    \item Compute the predictive distribution $p_{t+1}(x)$
	from the filtering distribution and your model.
    \item Increment $t$ by one, and go back to step 1, taking
	the predictive distribution as your prior.

\end{enumerate}

\section{Normal Example}

Suppose we want to measure some variable $x$. We will 
assume a \emph{prior} that is multivariate normal such that
    \[ x \sim \text{N}(\hat{x}, \Sigma) \]
Next, we ``measure'' $x$ by matching it to an observable in
a \emph{measurement equation}:
    \[ y = G x + v \qquad v\sim \text{N}(0, R) \]
where $R$ is positive definite, while $G$ and $R$ are both 
$2 \times 2$. This forms the \emph{likelihood}.
\\
\\
We then ``filter'' out the noise, updating our view of $x$ in
light of the data in the filtering step using Bayes' Rule:
\begin{align}
    p(x \; | \; y) &= \frac{p(y \; | \; x) \cdot p(x)}{p(y)} 
    \propto p(y \; | \; x) \cdot p(x) \label{xgiveny} \\
    &\propto \exp\left\{-\frac{1}{2}
	\left( y - Gx \right)' R^{-1} \left(y-Gx\right)
	\right\} \exp\left\{-\frac{1}{2}( x - \hat{x} )' 
	\Sigma^{-1} (x-\hat{x})\right\} \notag
\end{align}
Now let's expand the term the lefthand exponential:
\begin{align*}
    A = \left( y - Gx \right)' R^{-1} \left(y-Gx\right) &= 
	\left( y' - x'G' \right) R^{-1} \left(y-Gx\right) \\
    &= \left( y'R^{-1} - x'G'R^{-1} \right)  \left(y-Gx\right) \\
    &= \left( y'R^{-1}y - y'R^{-1} Gx - x'G'R^{-1} y 
	+ x'G'R^{-1} Gx\right)
\end{align*}
And now the same for the righthand exponential:
\begin{align}
     B = ( x - \hat{x} )' \Sigma^{-1} (x-\hat{x}) 
	&= ( x' - \hat{x}' ) \Sigma^{-1} (x-\hat{x}) \notag\\
    &=  (x'\Sigma^{-1} - \hat{x}'\Sigma^{-1})   
	(x-\hat{x}) \notag \\
    &=  x'\Sigma^{-1} x - x'\Sigma^{-1} \hat{x} - 
	\hat{x}'\Sigma^{-1} x
	+ \hat{x}'\Sigma^{-1} \hat{x} \label{B}
\end{align}
Adding the two exponentials, we get:
\begin{align*}
    C = A + B &= x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \\
	& \qquad + \hat{x}' \Sigma^{-1} \hat{x} + y' R^{-1} y 
\end{align*}
Now notice that Expression \ref{xgiveny} is the probability
distribution of $x$ \emph{conditional} on $y$ and pretty
much anything else that isn't $x$.  And because of the
wonderful properties of the exponential function and the
black-hole nature of the proportionality constant, we'll be
able to simplify things nicely (and we'll worry that the
distribution $p(x\;|\;y)$ integrates to one later on).  

Specifically, in the expression for $C$, the two terms
in the second row \emph{don't} depend upon $x$. Therefore,
letting $C(x)$ be the portion of $C$ that depends upon
$x$, and letting $C(\lnot x)$ bet the additive terms which
don't depend upon $x$, we can simplify
\begin{align*}
    p(x\; | \; y) &\propto \exp\left\{ -\frac{1}{2}
	C \right\} = \exp\left\{ -\frac{1}{2}
	\left[C(x) + C(\lnot x) \right]\right\}  \\
    &\propto \exp\left\{ -\frac{1}{2}
	C(x)\right\} +  \exp\left\{ -\frac{1}{2} C(\lnot x) 
	\right\}  \\
    &\propto \exp\left\{ -\frac{1}{2}
	C(x)\right\}
\end{align*}
We just absorb the portion not relevant to $p(x\;|\;y)$ into
the proportionality constant.  This means our the work we
did above to get $C$ simplifies our target expression to
\begin{align}
    p(x\;|\;y) &\propto \exp\left\{-\frac{1}{2} \left[
	x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \right]\right\}
	\label{tosimp}
\end{align}
Now this doesn't look too helpful, but with a little bit of 
work, we can turn this into the probability distribution
for a multivariate normal random variable. So let's do it.
\\
\\
\\
First, the variance of the 
normal distribution corresponding to $p(x\;|\;y)$ can
be derived by examining Equation \ref{tosimp} and likening
it to Equation \ref{B} (which has the contents of the 
exponential in the prior distribution of $x$).  
Namely, the inverse of the new variance,
which we'll denote as $\Sigma_F$ will be sandwiched in 
between $x'$ and $x$ in Equation \ref{tosimp}, just as it 
was sandwiched between $x'$ and $x$ in Equation \ref{B}.
We use this fact, along with the 
the Woodbury matrix identity, stated in 
the appendix, to derive:
\begin{align*}
    \Sigma_{F} &= \left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} \\
    \text{Woodbury Identity} \Rightarrow
	\qquad &=  \Sigma - \Sigma G'(R 
	    + G\Sigma G')^{-1}
	    G\Sigma
\end{align*}
Next, we want to get the mean of the distribution of 
$p(x|y)$.  Again, once we
take a second and compare Expression \ref{tosimp}
to Expression \ref{B}, it's becomes clear from inspection 
that we must have 
\begin{equation}
    \label{notobvious}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) 
	= \left( \Sigma^{-1} + G'R^{-1}G\right) Z
\end{equation}
To see this, 
liken the lefthand side of Equation \ref{notobvious} to
the result of the matrix multiplication $\Sigma^{-1}\hat{x}$ in
Equation \ref{B}. To get the righthand side, use the fact
that we \emph{know} the Equation \ref{notobvious} analogue
to Equation \ref{B}'s $\Sigma^{-1}$, which we just derived.
\\
\\
So all that's left to do is solve for $Z$ in Equation
\ref{notobvious}.
And so we solve Equation \ref{B} by using the Woodbury matrix
identity representation from above:
\begin{align*}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) &= 
	\left( \Sigma^{-1} + G'R^{-1}G\right) Z \\
    \Rightarrow \quad Z &=
	\left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
	(\Sigma^{-1} \hat{x} + G'R^{-1}y) \\
    Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right)
\end{align*}
Now let's simplify this representation a bit, particularly
using the trick $(AB)^{-1} = B^{-1}A^{-1}$ and
especially in reverse:
\begin{align*}
    Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right) \\
     &= \Sigma \Sigma^{-1} \hat{x} 
	+ \Sigma G'R^{-1}y 
	- \Sigma G'(R + G\Sigma G')^{-1} G\Sigma
	    \Sigma^{-1} \hat{x} \\
	& \qquad - \Sigma G'(R + G\Sigma G')^{-1} G\Sigma
	     G'R^{-1}y \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	 - \Sigma G'(R + G\Sigma G')^{-1} G\Sigma
	     G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	- (G'^{-1}\Sigma^{-1})^{-1} (R + G\Sigma G')^{-1} G\Sigma
	     G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	- \left\{(R + G\Sigma G')(G'^{-1}\Sigma^{-1}) 
	    \right\}^{-1}  G\Sigma G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	- \left\{RG'^{-1}\Sigma^{-1} 
	    + G\Sigma G'G'^{-1}\Sigma^{-1} 
	    \right\}^{-1}  G\Sigma G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	- \left\{RG'^{-1}\Sigma^{-1} 
	    + G\Sigma^{-1} 
	    \right\}^{-1}  G\Sigma G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	- \left\{G^{-1} \left(RG'^{-1}\Sigma^{-1} 
	    + G\Sigma^{-1} \right)
	    \right\}^{-1}  \Sigma G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
     &= \hat{x} 
	+ \left( \Sigma G'R^{-1} 
	- \left\{G^{-1} RG'^{-1}\Sigma^{-1} 
	    + \Sigma^{-1} 
	    \right\}^{-1}  \Sigma G'R^{-1}\right)y \\
	&\qquad - \Sigma G'(R + G\Sigma G')^{-1} G
	    \hat{x} \\
\end{align*}


%%%% APPPENDIX %%%%%%%%%%%

\newpage
\appendix

\section{Woodbury Matrix Identity}

For matrices $A$, $U$, $C$, and $V$:
\begin{equation}
    (A+UCV)^{-1} = A^{-1} - A^{-1} U(C^{-1} + VA^{-1}U)^{-1}
	VA^{-1}
\end{equation}
Now consider the special case we have above with the
Kalman filter:
\begin{equation}
    \label{special}
    (A + V'CV)^{-1} = A^{-1} - A^{-1} V'(C^{-1} + VA^{-1}V')^{-1}
	VA^{-1}
\end{equation}
We'll use the trick $(AB)^{-1} = B^{-1} A^{-1}$ often, and
typically in reverse, going from the right side of the equality
to the left to simplify Equation \ref{special}.
\begin{align*}
    (A + V'CV)^{-1} &= A^{-1}-A^{-1} V'(C^{-1} + VA^{-1}V')^{-1}
	VA^{-1} \\
    &= A^{-1}- \left(V'^{-1} A\right)^{-1} 
	(C^{-1} + VA^{-1}V')^{-1} VA^{-1} \\
    &= A^{-1}- \left\{  (C^{-1} + VA^{-1}V')
	\left(V'^{-1} A\right)\right\}^{-1} VA^{-1} \\
    &= A^{-1}- \left\{C^{-1} \left(V'^{-1} A\right)
	+ VA^{-1}V' \left(V'^{-1} A\right)\right\}^{-1}
	 VA^{-1} \\
    &= A^{-1}- \left\{C^{-1} \left(V'^{-1} A\right)
	+ V\right\}^{-1}
	 \left(AV^{-1}\right)^{-1} \\
    &= A^{-1}- \left\{\left(AV^{-1}\right) 
	\left[C^{-1} \left(V'^{-1} A\right)
	+ V\right]\right\}^{-1} \\
    &= A^{-1}- \left\{AV^{-1} 
	C^{-1} \left(V'^{-1} A\right)
	+ AV^{-1}V\right\}^{-1} \\
    &= A^{-1}- \left\{AV^{-1} 
	C^{-1} V'^{-1} A
	+ A\right\}^{-1} \\
    &= A^{-1}- \left\{AV^{-1} 
	C^{-1} V'^{-1} A
	+ A\right\}^{-1} \\
\end{align*}



\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
