\documentclass[a4paper,12pt]{article}

\author{Matthew Cocci}
\title{Kalman Filter}
\date{\today}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
%\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{fullpage}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
%\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Basic Idea and Terminology}

Here's the basic procedure associated with the Kalman
Filter:
\begin{enumerate}
    \item Start with a prior for some variable of interest
	in the current period, $p(x)$.
    \item Observe the current measurement $y_t$.
    \item ``Filter'' out the noise and 
	compute the filtering distribution: $p_t(x | y)$.
    \item Compute predictive distribution $p_{t+1}(x)$
	from  filtering distribution and your model.
    \item Increment $t$ and return to step 1, taking
	the predictive distribution as your prior.

\end{enumerate}

\section{Filtering Step}

Suppose we want to measure some latent state variable $x$. We will 
assume a \emph{prior} that is multivariate normal such that
    \[ x \sim \text{N}(\hat{x}, \Sigma) \]
Next, we ``measure'' $x$ by matching it to an observable in
a \emph{measurement equation}:
    \[ y = G x + v \qquad v\sim \text{N}(0, R) \]
where $R$ is positive definite, while $G$ and $R$ are both 
$2 \times 2$. This forms the \emph{likelihood}.
\\
\\
We then ``filter'' out the noise, updating our view of $x$ in
light of the data in the filtering step using Bayes' Rule.
Note, this is called ``filtering'' because we don't use the prior
and likelihood to forecast into
the future. We combine the prior with the likelihood only to filter
out noise and get closer to the true value of $x$ based on
the data, summarized in the posterior, or the
\emph{filtering distribution}:
\begin{align}
    p(x \; | \; y) &= \frac{p(y \; | \; x) \cdot p(x)}{p(y)} 
    \propto p(y \; | \; x) \cdot p(x) \label{xgiveny} \\
    &\propto \exp\left\{-\frac{1}{2}
	\left( y - Gx \right)' R^{-1} \left(y-Gx\right)
	\right\} \exp\left\{-\frac{1}{2}( x - \hat{x} )' 
	\Sigma^{-1} (x-\hat{x})\right\} \notag
\end{align}
Now let's expand the term in the lefthand exponential:
\begin{align*}
    A = \left( y - Gx \right)' R^{-1} \left(y-Gx\right) &= 
	\left( y' - x'G' \right) R^{-1} \left(y-Gx\right) 
    = \left( y'R^{-1} - x'G'R^{-1} \right)  \left(y-Gx\right) \\
    &= \left( y'R^{-1}y - y'R^{-1} Gx - x'G'R^{-1} y 
	+ x'G'R^{-1} Gx\right)
\end{align*}
And now the same for the righthand exponential:
\begin{align}
     B = ( x - \hat{x} )' \Sigma^{-1} (x-\hat{x}) 
	&= ( x' - \hat{x}' ) \Sigma^{-1} (x-\hat{x}) \notag\\
    &=  (x'\Sigma^{-1} - \hat{x}'\Sigma^{-1})   
	(x-\hat{x}) \notag \\
    &=  x'\Sigma^{-1} x - x'\Sigma^{-1} \hat{x} - 
	\hat{x}'\Sigma^{-1} x
	+ \hat{x}'\Sigma^{-1} \hat{x} \label{B}
\end{align}
Adding the two exponentials, we get:
\begin{align*}
    C = A + B &= x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \\
	& \qquad + \hat{x}' \Sigma^{-1} \hat{x} + y' R^{-1} y 
\end{align*}
Now notice that Expression \ref{xgiveny} is the probability
distribution of $x$ \emph{conditional} on $y$ and pretty
much anything else that isn't $x$.  And because of the
wonderful properties of the exponential function and the
black-hole powers of the proportionality constant, we'll be
able to simplify things nicely (and we'll worry that the
distribution $p(x|y)$ integrates to one later on).  

So in the expression for $C$, the two terms
in the second row \emph{don't} depend upon $x$. Therefore,
letting $C(x)$ be the portion of $C$ that depends upon
$x$, and letting $C(\lnot x)$ bet the additive terms which
don't depend upon $x$, we can simplify
\begin{align*}
    p(x\; | \; y) &\propto \exp\left\{ -\frac{1}{2}
	C \right\} = \exp\left\{ -\frac{1}{2}
	\left[C(x) + C(\lnot x) \right]\right\}  \\
    &\propto \exp\left\{ -\frac{1}{2}
	C(x)\right\} +  \exp\left\{ -\frac{1}{2} C(\lnot x) 
	\right\}  \\
    &\propto \exp\left\{ -\frac{1}{2}
	C(x)\right\}
\end{align*}
We just absorb the portion not relevant to $p(x|y)$ into
the proportionality constant.  This means our the work we
did above to get $C$ simplifies our target expression to
\begin{align}
    p(x\;|\;y) &\propto \exp\left\{-\frac{1}{2} \left[
	x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \right]\right\}
	\label{tosimp}
\end{align}
\paragraph{Goal}
Now this doesn't look too helpful, but with a little bit of 
work, we can turn this into the probability distribution
for a multivariate normal random variable. 
In fact, the rest of the section may look complicated, but
keep in mind the big picture: the likelihood and prior
were both multivariate normal, so 
the posterior $p(x|y)$ is going to be normal. We just want
to identify the mean vector and variance-covariance matrix;
then we're home.

\paragraph{Variance} So first, the {variance} of the 
normal distribution corresponding to $p(x|y)$ can
be derived by examining Equation \ref{tosimp} and likening
it to Equation \ref{B} (which gives the contents of the 
exponential in the prior MVN distribution of $x$).  

Namely, the inverse of the new variance,
which we'll denote as $\Sigma^F$ will be sandwiched in 
between $x'$ and $x$ in Equation \ref{tosimp}, just as it 
was sandwiched between $x'$ and $x$ in Equation \ref{B}.
We use this fact, along with the 
the Woodbury matrix identity (stated in 
the appendix) to derive:
\begin{align}
    \Sigma^{F} &= \left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
	\notag \\
    \text{Woodbury Identity} \Rightarrow
	\qquad &=  \Sigma - \Sigma G'(R 
	    + G\Sigma G')^{-1}
	    G\Sigma
	    \label{covar}
\end{align}
\paragraph{Mean} 
Next, we want to get the {mean} of the distribution of 
$p(x|y)$, which we'll denote by $\hat{x}^F$.  Again, once we
take a second and compare Expression \ref{tosimp}
to Expression \ref{B}, it's becomes clear from inspection 
that we must have 
\begin{equation}
    \label{notobvious}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) 
	= \left( \Sigma^{-1} + G'R^{-1}G\right) Z
\end{equation}
To see this, 
liken the lefthand side of Equation \ref{notobvious} (which 
itself comes from Expression \ref{tosimp}) to
the result of the matrix multiplication $\Sigma^{-1}\hat{x}$ in
Equation \ref{B}. To get the righthand side, use the fact
that we \emph{know} the Equation \ref{notobvious} analogue
to Equation \ref{B}'s $\Sigma^{-1}$, which we just 
derived in the variance section and called $\Sigma^F$.
\\
\\
So all that's left to do is solve for $Z$ in Equation
\ref{notobvious}.  The result will turn out to be our mean
vector for the posterior, $\hat{x}_F$:
\begin{align}
    \label{mean}
    \hat{x}^F = Z 
        &= \hat{x} 
        + \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
        \left( y - G \hat{x} \right)  
\end{align}
If you want to see the nasty linear algebra that gets you
to this result, you can check out the appendix. Or you can 
you just take this result as given and save yourself an
hour of painstaking derivation and eye-crossing complications,
unlike myself.\footnote{But if you \emph{do} look at the appendix,
you might just give my semi-wasted hour some meaning, in which 
case---thank you.}
\\
\\
Putting together the expressions for the mean and variance
(see Equations \ref{mean} and \ref{covar}, respectively)
of the posterior estimate for $x|y$ (i.e. the ``filtering
distribution''), we get that
\begin{align}
    x | y &\sim \text{N} \left(\hat{x}^F, \; \Sigma^F\right) 
    \label{filtered} \\
    \notag\\
    \text{where} \quad \hat{x}^F &= \hat{x} 
	+ \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( y - G \hat{x} \right)  \notag \\
    \Sigma^F &= \Sigma - \Sigma G'(R 
	    + G\Sigma G')^{-1}
	    G\Sigma \notag
\end{align}
Notice that our new ``filtered'' mean is simply a 
combination of our prior mean, $\hat{x}$, and 
a transformation of the ``error'' between our
observed value and the prior guess for that
observable ($y - G\hat{x}$).

\newpage
\paragraph{Recap} Okay, so what did we just do? 
\begin{enumerate} 
    \item We took a Multivariate Normal (MVN) prior to 
	summarize our beliefs about a latent, imperfectly 
    observable state variable $x$. 
    \item Knowing that we'll observe some data, $y$, which
	provides a ``noisy'' measure of $x$, we postulated a 
	likelihood $p(y|x)$ that is also MVN. 
    \item Then, using Bayes' Rule, we combine the information
	contained in our prior $p(x)$ and the data (via
	the likelihhod $p(y|x)$) to get a ``filtered'' 
	distribution of $x$, $p(x|y)$, given the data and our
	prior.
\end{enumerate} 
Why might this long, tortuous, painful process help us in
economics?  Well, imagine that in our model, there's some 
state for the ``natural rate of unemployment,'' denoted by $x$.  
Now of course, we can't observe that value.  But we'll have
economic statistics, like measurements of unemployment itself along 
with other informative statistics such as GDP
and hours, which might provide information about the natural
rate of unemployment.
However, those statistics are imperfect and noisy.  The
Kalman Filter gives us a way to combine those noisy estimates
in with our beliefs in a principled, sensible manner.


\section{Forecasting Step}

Now let's make our model a little more dynamic and consider 
forecasting ahead. To do so, we specify a model
of how the state, $x$, evolves.  To make it easy on ourselves,
let's assume everything's Gaussian (woohoo! that's easy):
\begin{equation}
    \label{lom}
    x_{t+1} = Ax_t + w_{t+1} \qquad w_t \sim \text{N}(0, Q)
\end{equation}
Now, we want to come up with a \emph{predictive distribution}
given our prior and the current information encapsulated in 
our filtering distribution, $p(x|y)$. Since we're assuming
everything is normal, we need only pin down the
mean and variance of the forecast, since linear combinations
of Gaussian variables are Gaussian.  

Of course, these kinds of things are well known for  
MVN random variables, which has the nice properties
\begin{align*}
    E[AX] &= A E[X] = A \mu \\
    \text{Var}(AX) &= A \text{Var}(X) A' = A\Sigma A' \\
    \text{where} \quad X &\sim \text{N}(\mu, \Sigma)
\end{align*}
Now let's use these facts, along with the assumption
that we're predicting $x_{t+1}$ by starting with a 
\emph{filtered} $x_t$ (denoted $x^F_t$ which has the 
distribution in Equation \ref{filtered}), and 
assuming $x^F_t$ is uncorrelated with $w_{t+1}$:
\begin{align}
    E[x_{t+1}] &= E\left[A{x}^F_{t} + w_{t+1} \right] 
        = AE\left[{x}^F_{t}\right] + E\left[w_{t+1} 
            \right] \notag\\
        &= A \hat{x}^F_{t} + 0 = A \hat{x}^{F}_t
            \label{intfore_mean}\\
    \text{Var}({x}_{t+1}) &= 
        \text{Var}(A{x}^F_{t} + w_{t+1}) 
        = A\text{Var}\left({x}^F_{t}\right)A' 
            +\text{Var}(w_{t+1})\notag \\
        &= A\Sigma^F_t A' + Q \label{intfore_var}
\end{align}
where $\hat{x}^F_t$ and $\Sigma_t^F$ are as above in Equation 
\ref{filtered}. This characterizes the distribution for the
one step ahead forecasting distribution.

\newpage
Now, we can simplify what we have a bit more by defining 
the \emph{Kalman Gain}:
\begin{equation}
    \label{kalgain}
    K_\Sigma = A\Sigma G' (R + G\Sigma G')^{-1}
\end{equation}
We see the practical use of this by subbing in the full
expressions for $\hat{x}_t^F$ and $\Sigma_t^F$ into Equations 
\ref{intfore_mean} and \ref{intfore_var} above,
and then simplifying our expressions for the
mean and variance as a function of the Kalman Gain:
\begin{align}
    \hat{x}_{t+1} &= E[x_{t+1}] = 
        A\left\{ \hat{x}_t
	    + \left[\Sigma_t G' (R + G \Sigma_t G')^{-1}  \right]
        \left( y - G \hat{x}_t \right) \right\} \notag \\
        &= A\hat{x}_t + K_{\Sigma_t} (y - G \hat{x}_t) 
        \label{foremean} \\
    \Sigma_{t+1} &= \text{Var}(x_{t+1}) = 
        A \left\{ \Sigma_t - \Sigma_t G'(R 
	    + G\Sigma_t G')^{-1}
        G\Sigma_t \right\} A' + Q \notag \\
    &= A\Sigma_t A' - K_{\Sigma_t} G \Sigma_t A' + Q
        \label{forevar}
\end{align}


\section{Full Recursive Procedure}

Now we need to build up the recursive algorithm to forecast 
further into the future. Let $t$ be the current time period, 
and we proceed as follows:
\begin{enumerate}
    \item Start with a prior in the current period at 
        time $t$, as given above
        \[ p_{t}(x) \sim \text{N}\left(\hat{x}_t, \; 
            \Sigma_t\right) \]
    \item Observe the current measurement, $y_t$.
    \item Update and filter out the noise as we did in
        the Filtering Section to get, you guessed it,
        the filtering distribution: $p_t(x | y) = 
        \text{N}(\hat{x}^F_t, \Sigma^F_t)$.
    \item Compute the predictive distribution 
        $p_{t+1}(x) = \text{N}(\hat{x}_{t+1}, \Sigma_{t+1})$
        from the filtering distribution, $p_t(x|y)$, and
        the law of motion in Equation \ref{lom}.
    \item Increment $t$. Return to step 1, taking
        the predictive distribution, $p_{t+1}(x)$, as
        the prior.
\end{enumerate}


\section{Convergence}

Now since $x_{t}$ is random from the perspective
of time $t-1$ (i.e. there is some irreducable uncertainty
resulting from shocks at time $t$), we know that $\Sigma_t$
will never be zero, unless $w_t$ in Equation \ref{lom} is 
degenerate. However, we might ask whether $\Sigma_t$, our
measure of uncertainty for our prediction $\hat{x}_t$
of $x_t$, will ever converge to a \emph{constant} matrix over
time. 

Recall how $\Sigma_t$ evolves, as specified in Equation 
\ref{forevar} (substituting in for the Kalman Gain, 
$K_\Sigma$), which gives the non-linear difference equation:
\begin{equation}
    \label{sig_euler}
    \Sigma_{t+1} = A\Sigma_t A' - K_{\Sigma_t} G \Sigma_t A' + Q
\end{equation}
If it were the case that $\Sigma_t$ converges to some fixed
matrix, then there would be a fixed point, $\Sigma^*$, satisfying 
Equation \ref{sig_euler} as follows:
\begin{equation}
    \label{riccati}
    \Sigma^* =
        A\Sigma^* A' - A\Sigma^* G' (R + G\Sigma^* G')^{-1} G \Sigma^* A' + Q
\end{equation}
This is known as the \emph{Discrete Time Algebraic Riccati Equation}.
A sufficient condition for convergence is that all the eigenvalues of
$A$, $\lambda_i$, satisfy $|\lambda_i|<1$. 








%%%% APPPENDIX %%%%%%%%%%%

\newpage
\appendix

\section{Woodbury Matrix Identity}

For matrices $A$, $U$, $C$, and $V$:
\begin{equation}
    (A+UCV)^{-1} = A^{-1} - A^{-1} U(C^{-1} + VA^{-1}U)^{-1}
	VA^{-1}
\end{equation}
Now consider the special case we have above with the
Kalman filter:
\begin{equation}
    \label{special}
    (A + V'CV)^{-1} = A^{-1} - A^{-1} V'(C^{-1} + VA^{-1}V')^{-1}
	VA^{-1}
\end{equation}

\section{Derivation of the Mean}

Recall what we want to show.  For $\hat{x}_F \equiv Z$, we want
to show that
\begin{align*}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) 
	&= \left( \Sigma^{-1} + G'R^{-1}G\right) Z \\
    \Rightarrow 
    \hat{x}_F = Z 
        &= \hat{x} 
        + \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
        \left( y - G \hat{x} \right)  
\end{align*}
And so we solve this equation by using the Woodbury matrix
identity representation from above:
\begin{align*}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) &= 
	\left( \Sigma^{-1} + G'R^{-1}G\right) Z \\
    \Rightarrow \quad Z &=
	\left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
	(\Sigma^{-1} \hat{x} + G'R^{-1}y) \\
    Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right)
\end{align*}
Now let's simplify $\hat{x}_F = Z$ a bit, 
expanding out the multiplication:
\begin{align*}
    \hat{x}_F = Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right) \\
    \text{FOIL} \quad &= \hat{x} + \Sigma G' R^{-1} y - 
	\left[\Sigma G' (R + G \Sigma G')^{-1} G \Sigma\right]
	\left[ \Sigma^{-1} \hat{x} \right]  \\
    &\qquad - \left[\Sigma G' (R+ G \Sigma G')^{-1}G\Sigma \right]
	\left[G' R^{-1} y \right] \\
    \text{Simpify} \quad &= \hat{x} + \Sigma G' R^{-1} y - 
	\left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad - \Sigma G' (R+ G \Sigma G')^{-1}G\Sigma 
	G' R^{-1} y  \\
    \text{Change Order} \quad &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad 
	+ \Sigma G'  R^{-1} y 
	- \Sigma G' (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} y \\
    \text{Regroup} \quad &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad 
	+ \Sigma G'  \left\{ R^{-1}  
	-  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \right\} y 
\end{align*}
Okay, now let's take a breather.  We'll make this a bit
easier on ourselves, and just consider simplifying the guy
in the brackets, $\{\}$ by using 
$A^{-1} A = I$ with a very special choice of $A$:
\begin{align*}
    \left\{ R^{-1}  -  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \right\}   &=   
	(R+ G \Sigma G')^{-1} (R+ G \Sigma G') R^{-1}  \\
    &\qquad -  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \\
    \text{Group} \quad &=   (R+ G \Sigma G')^{-1} \left[ (R+ G \Sigma G') R^{-1}  
	-  G\Sigma   G' R^{-1}\right] \\
    \text{Distribute} \quad &=   (R+ G \Sigma G')^{-1} \left[ RR^{-1} 
	+ G \Sigma G'R^{-1} -  G\Sigma   G' R^{-1}\right] \\
    \text{Simplify} \quad &=   (R+ G \Sigma G')^{-1} \left[ I 
	+ 0\right] \\
    &=   (R+ G \Sigma G')^{-1} 
\end{align*}
Substituting back in above for the term in braces, 
$\{\}$, we get the following expression
for $Z = \hat{x}_F$:
\begin{align}
    \hat{x}_F = Z &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  
	+ \Sigma G'  (R+ G \Sigma G')^{-1} y \notag \\
    &= \hat{x} 
	+ \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( y - G \hat{x} \right)  
\end{align}

\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
