\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Kalman Filter}
\date{\today}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Basic Idea and Terminology}

Here's the basic procedure associated with the Kalman
Filter:
\begin{enumerate}
    \item Start with a prior for some variable of interest
	in the current period, $p(x)$.
    \item Observe the current measurement $y_t$.
    \item ``Filter'' out the noise and 
	compute the filtering distribution: $p_t(x | y)$.
    \item Compute predictive distribution $p_{t+1}(x)$
	from  filtering distribution and your model.
    \item Increment $t$ and return to step 1, taking
	the predictive distribution as your prior.

\end{enumerate}

\section{Normal Example, Filtering Step}

Suppose we want to measure some variable $x$. We will 
assume a \emph{prior} that is multivariate normal such that
    \[ x \sim \text{N}(\hat{x}, \Sigma) \]
Next, we ``measure'' $x$ by matching it to an observable in
a \emph{measurement equation}:
    \[ y = G x + v \qquad v\sim \text{N}(0, R) \]
where $R$ is positive definite, while $G$ and $R$ are both 
$2 \times 2$. This forms the \emph{likelihood}.
\\
\\
We then ``filter'' out the noise, updating our view of $x$ in
light of the data in the filtering step using Bayes' Rule.
Note, this is called ``filtering'' because we don't use the prior
and likelihood forecast into
the future. We combine the prior with the likelihood to filter
out noise and get closer to the true value of $x$ based on
the data, summarzed in the posterior, or the
\emph{filtering distribution}:
\begin{align}
    p(x \; | \; y) &= \frac{p(y \; | \; x) \cdot p(x)}{p(y)} 
    \propto p(y \; | \; x) \cdot p(x) \label{xgiveny} \\
    &\propto \exp\left\{-\frac{1}{2}
	\left( y - Gx \right)' R^{-1} \left(y-Gx\right)
	\right\} \exp\left\{-\frac{1}{2}( x - \hat{x} )' 
	\Sigma^{-1} (x-\hat{x})\right\} \notag
\end{align}
Now let's expand the term the lefthand exponential:
\begin{align*}
    A = \left( y - Gx \right)' R^{-1} \left(y-Gx\right) &= 
	\left( y' - x'G' \right) R^{-1} \left(y-Gx\right) 
    = \left( y'R^{-1} - x'G'R^{-1} \right)  \left(y-Gx\right) \\
    &= \left( y'R^{-1}y - y'R^{-1} Gx - x'G'R^{-1} y 
	+ x'G'R^{-1} Gx\right)
\end{align*}
And now the same for the righthand exponential:
\begin{align}
     B = ( x - \hat{x} )' \Sigma^{-1} (x-\hat{x}) 
	&= ( x' - \hat{x}' ) \Sigma^{-1} (x-\hat{x}) \notag\\
    &=  (x'\Sigma^{-1} - \hat{x}'\Sigma^{-1})   
	(x-\hat{x}) \notag \\
    &=  x'\Sigma^{-1} x - x'\Sigma^{-1} \hat{x} - 
	\hat{x}'\Sigma^{-1} x
	+ \hat{x}'\Sigma^{-1} \hat{x} \label{B}
\end{align}
Adding the two exponentials, we get:
\begin{align*}
    C = A + B &= x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \\
	& \qquad + \hat{x}' \Sigma^{-1} \hat{x} + y' R^{-1} y 
\end{align*}
Now notice that Expression \ref{xgiveny} is the probability
distribution of $x$ \emph{conditional} on $y$ and pretty
much anything else that isn't $x$.  And because of the
wonderful properties of the exponential function and the
black-hole nature of the proportionality constant, we'll be
able to simplify things nicely (and we'll worry that the
distribution $p(x|y)$ integrates to one later on).  

Specifically, in the expression for $C$, the two terms
in the second row \emph{don't} depend upon $x$. Therefore,
letting $C(x)$ be the portion of $C$ that depends upon
$x$, and letting $C(\lnot x)$ bet the additive terms which
don't depend upon $x$, we can simplify
\begin{align*}
    p(x\; | \; y) &\propto \exp\left\{ -\frac{1}{2}
	C \right\} = \exp\left\{ -\frac{1}{2}
	\left[C(x) + C(\lnot x) \right]\right\}  \\
    &\propto \exp\left\{ -\frac{1}{2}
	C(x)\right\} +  \exp\left\{ -\frac{1}{2} C(\lnot x) 
	\right\}  \\
    &\propto \exp\left\{ -\frac{1}{2}
	C(x)\right\}
\end{align*}
We just absorb the portion not relevant to $p(x\;|\;y)$ into
the proportionality constant.  This means our the work we
did above to get $C$ simplifies our target expression to
\begin{align}
    p(x\;|\;y) &\propto \exp\left\{-\frac{1}{2} \left[
	x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \right]\right\}
	\label{tosimp}
\end{align}
Now this doesn't look too helpful, but with a little bit of 
work, we can turn this into the probability distribution
for a multivariate normal random variable. 
In fact, the rest of the section may look complicated, but
keep in mind the big picture: the likelihood and prior
were both multivariate normal, so 
the posterior $p(x|y)$ is going to be normal. We just want
to identify the mean vector and variance-covariance matrix;
then we're home.

\paragraph{Variance} So first, the {variance} of the 
normal distribution corresponding to $p(x|y)$ can
be derived by examining Equation \ref{tosimp} and likening
it to Equation \ref{B} (which has the contents of the 
exponential in the prior distribution of $x$).  

Namely, the inverse of the new variance,
which we'll denote as $\Sigma_F$ will be sandwiched in 
between $x'$ and $x$ in Equation \ref{tosimp}, just as it 
was sandwiched between $x'$ and $x$ in Equation \ref{B}.
We use this fact, along with the 
the Woodbury matrix identity (stated in 
the appendix) to derive:
\begin{align}
    \Sigma_{F} &= \left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
	\notag \\
    \text{Woodbury Identity} \Rightarrow
	\qquad &=  \Sigma - \Sigma G'(R 
	    + G\Sigma G')^{-1}
	    G\Sigma
	    \label{covar}
\end{align}
\paragraph{Mean} 
Next, we want to get the {mean} of the distribution of 
$p(x|y)$, which we'll denote by $\hat{x}_F$.  Again, once we
take a second and compare Expression \ref{tosimp}
to Expression \ref{B}, it's becomes clear from inspection 
that we must have 
\begin{equation}
    \label{notobvious}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) 
	= \left( \Sigma^{-1} + G'R^{-1}G\right) Z
\end{equation}
To see this, 
liken the lefthand side of Equation \ref{notobvious} (which 
itself comes from Expression \ref{tosimp}) to
the result of the matrix multiplication $\Sigma^{-1}\hat{x}$ in
Equation \ref{B}. To get the righthand side, use the fact
that we \emph{know} the Equation \ref{notobvious} analogue
to Equation \ref{B}'s $\Sigma^{-1}$, which we just 
derived and called $\Sigma_F$.
\\
\\
So all that's left to do is solve for $Z$ in Equation
\ref{notobvious}.  The result will turn out to be our mean
vector for the posterior, $\hat{x}_F$.
And so we solve Equation \ref{B} by using the Woodbury matrix
identity representation from above:
\begin{align*}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) &= 
	\left( \Sigma^{-1} + G'R^{-1}G\right) Z \\
    \Rightarrow \quad Z &=
	\left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
	(\Sigma^{-1} \hat{x} + G'R^{-1}y) \\
    Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right)
\end{align*}
Now let's simplify $\hat{x}_F = Z$ a bit, 
expanding out the multiplication:
\begin{align*}
    \hat{x}_F = Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right) \\
    &= \hat{x} + \Sigma G' R^{-1} y - 
	\left[\Sigma G' (R + G \Sigma G')^{-1} G \Sigma\right]
	\left[ \Sigma^{-1} \hat{x} \right]  \\
    &\qquad - \left[\Sigma G' (R+ G \Sigma G')^{-1}G\Sigma \right]
	\left[G' R^{-1} y \right] \\
    &= \hat{x} + \Sigma G' R^{-1} y - 
	\left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad - \Sigma G' (R+ G \Sigma G')^{-1}G\Sigma 
	G' R^{-1} y  \\
    &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad 
	+ \Sigma G'  R^{-1} y 
	- \Sigma G' (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} y \\
    &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad 
	+ \Sigma G'  \left\{ R^{-1}  
	-  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \right\} y 
\end{align*}
Okay, now let's take a breather.  We'll make this a bit
easier on ourselves, and just consider simplifying the guy
in the brackets, $\{\}$:
\begin{align*}
    \left\{ R^{-1}  -  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \right\}   &=   
	(R+ G \Sigma G')^{-1} (R+ G \Sigma G') R^{-1}  \\
    &\qquad -  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \\
    &=   (R+ G \Sigma G')^{-1} \left[ (R+ G \Sigma G') R^{-1}  
	-  G\Sigma   G' R^{-1}\right] \\
    &=   (R+ G \Sigma G')^{-1} \left[ RR^{-1} 
	+ G \Sigma G'R^{-1} -  G\Sigma   G' R^{-1}\right] \\
    &=   (R+ G \Sigma G')^{-1} \left[ I 
	+ 0\right] \\
    &=   (R+ G \Sigma G')^{-1} 
\end{align*}
Substituting back in above for the term in braces, 
$\{\}$, we get the following expression
for $Z = \hat{x}_F$:
\begin{align}
    \hat{x}_F = Z &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  
	+ \Sigma G'  (R+ G \Sigma G')^{-1} y \notag \\
    &= \hat{x} 
	+ \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( y - G \hat{x} \right)  
    \label{mean}
\end{align}
Putting together the expressions for the mean and variance
(see Equations \ref{mean} and \ref{covar}, respectively)
of the posterior estimate for $x|y$, we get that
\begin{align}
    x | y &\sim \text{N} \left(\hat{x}_F, \; \Sigma_F\right) \\
    \notag\\
    \text{where} \quad \hat{x}_F &= \hat{x} 
	+ \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( y - G \hat{x} \right)  \notag \\
    \Sigma_F &= \Sigma - \Sigma G'(R 
	    + G\Sigma G')^{-1}
	    G\Sigma \notag
\end{align}
\paragraph{Conclusion} Okay, so what did we just do? 
\begin{enumerate} 
    \item We took a Multivariate Normal (MVN) prior to 
	summarize our beliefs about $x$. 
    \item Knowing that we'll observe some data, $y$, that 
	provides a ``noisy'' measure of $x$, we postulated a 
	likelihood $p(y|x)$ that is also MVN. 
    \item Then, using Bayes' Rule, we combine the information
	contained in our prior $p(x)$ and the data (via
	the likelihhod $p(y|x)$) to get a ``filtered'' 
	distribution of $x$, $p(x|y)$, given the data and our
	prior.
\end{enumerate} 
Why might this long, tortuous, painful process help us in
economics?  Well, imagine that in our model, there's some 
state for the level of output, denoted by $x$.  Now we'll have
economic statistics, like measurements of GDP itself along 
with other informative statistics such as unemployment 
and investment, which might provide information about output.
But of course, those statistics are imperfect and noisy.  The
Kalman Filter gives us a way to combine those noise estimates
in with our beliefs in a principled, sensible manner.


\section{Normal Expample, Forecasting Step}

Now let's make our model a little more dynamic and include
forecasting in our method. To do so, we specify a model
of how the state, $x$ evolves.  To make it easy on ourselves,
let's assume everything's Gaussian (woohoo! that's easy):
\begin{equation}
    x_{t+1} = Ax_t + w_{t+1} \qquad w_t \sim \text{N}(0, Q)
\end{equation}
Now, we want to come up with a \emph{predictive distribution}






%%%% APPPENDIX %%%%%%%%%%%

\newpage
\appendix

\section{Woodbury Matrix Identity}

For matrices $A$, $U$, $C$, and $V$:
\begin{equation}
    (A+UCV)^{-1} = A^{-1} - A^{-1} U(C^{-1} + VA^{-1}U)^{-1}
	VA^{-1}
\end{equation}
Now consider the special case we have above with the
Kalman filter:
\begin{equation}
    \label{special}
    (A + V'CV)^{-1} = A^{-1} - A^{-1} V'(C^{-1} + VA^{-1}V')^{-1}
	VA^{-1}
\end{equation}



\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
