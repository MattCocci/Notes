\documentclass[a4paper,12pt]{scrartcl}

\author{Matt Cocci}
\title{Convergence}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\numberwithin{equation}{section} 
%, This labels the equations in relation to the sections rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}



\begin{document}
\begin{center}
   \LARGE
   \textbf{Convergence}
\end{center}

\section{Overview}

This note will discuss the probability concepts associated
with ``convergence,'' along with the applications and 
results that these concepts permit.

Namely, the first section dicussions convergence 
\emph{in probability} (also known as ``{weak convergence}'')
before moving onto the stronger
concept of \emph{almost sure convergence} (also known 
as ``{strong convergence}'').  It concludes
with convergence \emph{in distribution}.

Finally, the remainder of the note uses these concepts to
define the famous \emph{Law of Large Numbers} (LLN), which
gives the conditions under which sample moments converge
to population moments as $n\rightarrow \infty$.
Both the weak and strong versions will be covered, which will
each use the concepts of weak convergence and strong 
convergence, respectively, discussed above and defined below.

Beyond that, the \emph{Central Limit Theorem} (CLT) provides a
refinement of the LLN, describing the \emph{rate at which}
sample moments converge to population moments as 
$n\rightarrow\infty$. 



\section{Types of Convergence}

\subsection{Preliminary Definition}

A sequence $\{x_n\}$ has a limit $x$, written 
\begin{equation}
    \lim_{n\rightarrow\infty} x_n = n
\end{equation}
if $\forall \epsilon > 0$, there exists an $n_\epsilon < \infty$
such that for all $n \geq n_\epsilon$,  
    \[  |x_n - x| \leq \epsilon \]
In words: ``You tell me how arbitrarily close $x_n$ should
be to $x$. I'll tell you an index $n_\epsilon$, past which,
that will happen.''

But this, of course, is for a sequence of numbers, $\{x_n\}$.
There's no randomness there.  So what about a sequence of 
Random Variables---something non-deterministic, like an 
average $\bar{X}$? For that, 
we turn to the topics of the next few subsections.

\subsection{Convergence in Probability}

Consider a sequence of random variables $\{ X_n \}$, 
each with corresponding distribution function $F_n$.
Then a random variable $X_n$ \emph{converges in probability}
to $X$ if
\begin{equation}
    \label{plim}
    \lim_{n\rightarrow\infty} \text{Pr}\left(\left\lvert X_n
	- X \right\rvert \leq \epsilon \right) = 1
    \qquad \forall \epsilon > 0
\end{equation}
This is denoted $\text{plim}_{n\rightarrow\infty} X_n = X$ or
($X_n\overset{p}{\to}X$),
while $X$ is called the \emph{probability limit} (or \emph{plim})
of $X_n$. Convergence in probability is also known as 
\emph{weak convergence}.

To get the intuition, consider the Definition given in 
\ref{plim}. Notice that it uses the traditional definition
of a limit, but applied to a sequence of 
\emph{probabilities}.  It does \textbf{not} say that 
realizations equal the plim (i.e. $X_n = X$)
as $n\rightarrow\infty$. Instead, it describes the distribution
of $|X_n - X|$ and stipulates that the realizations
cluster very close to $X$ as $n\rightarrow\infty$.

Now for some final notes. Convergence in probability
is \textbf{not} convergence in expectation. The former
concerns a sequence of probabilities, while the latter a
sequence of expectations. Finally, the probability limit
$X$ must be free of all dependence upon the sample size $n$.

\subsection{Almost Sure Convergence}

Now, we turn to a concept stronger
than convergence in probability, \emph{almost sure
convergence}---also known as ``strongli
convergence.''\footnote{In probability terminology, 
a random event which occurs
with probability one is called ``almost sure.''}
A random variable, $X_n$ converges \emph{almost surely}
to $X$ if
\begin{equation}
    \label{almsure}
    \text{Pr}\left(\lim_{n\rightarrow \infty}
	\left\lvert X_n - X \right\rvert \right) = 1
    \qquad \forall \epsilon > 0
\end{equation}
We denote this form of convergence by 
$X_n\overset{a.s.}{\to}X$. It is stronger than 
convergence in probability because it computes the probability
of a limit, rather than the limit of a probability.

%\paragraph{In $p$-norm} All $X_n$ and $X$ have finite $p$th moment
%and 
%\[ \lim_{n\rightarrow\infty} E\left[|X_n - X|^p\right] = 0, \qquad
%   0<p<\infty \]
%
%\paragraph{In Distribution} We say $X_n$ with distributions $F_n$ 
%converge \emph{In Distribution} to $X$ with distribution $F$ if
%   \[ \lim_{n\rightarrow\infty} F_n(x) = F(x) \]
%for all $x \in \mathbb{R}$ at which $F$ is continuous. Also called
%\textbf{Weak Convergence}.

\section{Relationships}

The concepts just defined are related in the following way:
\begin{itemize}
   \item[-]{Almost sure $\Rightarrow$ In Probability.}
   \item[-]{In Probability $\Rightarrow$ there's a deterministic 
      subsequence that converges Almost Surely.}
   \item[-]{In $p$-norm $\Rightarrow$ In Probability.}
   \item[-]{Almost Surely and In $p$-norm, undecidable.}
   \item[-]{Almost Surely, In Probability, and In $p$-norm each
      $\Rightarrow$ In Distribution.}
\end{itemize}

\section{Law of Large Numbers (LLN)}

We now use the concept of convergence in probability 
along with the following relation, \emph{Chebyshev's Inequality},
to define the Weak LLN:
\begin{equation}
    \label{chebyshev}
    \text{Pr}\left(\left\lvert X_n - \mathbb{E}X_n \right\rvert > 
	\delta\right) \leq \frac{\text{Var}(X_n)}{\delta^2}
\end{equation}
Now for a proof of the inequality, which is refreshingly
simple given how useful Chebyshev's Inequality is.
\begin{proof} Assume that $X_n$ has finite variance, 
    $\sigma^2$, and let $F_n(x)$ denote the distribution
    of $X_n$. Then 
    \begin{equation}
	\label{chebyproof}
	\text{Pr}\left(\left\lvert X_n - \mathbb{E}X_n 
	    \right\rvert > \delta\right)
	    = \text{Pr}\left(\left( X_n - \mathbb{E}X_n 
	    \right)^2 > \delta^2\right)
    \end{equation}
\end{proof}



\section{Related Concepts}

\paragraph{Consistency} A sequence of estimators $\{ \hat{\theta}_n \}$
where $n=1,2,\ldots$ is \emph{consistent} for parameter $\theta$ if
$\hat{\theta}_n$ converges \emph{In Probability} to $\theta$.

\paragraph{Strongly Consistent} If convergence of $\hat{\theta}_n$
to $\theta$ holds with probability 1.



\end{document}

