\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Factor Models}
\date{\today}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}

%% Formatting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=1in]{geometry}
    %   Adjust the margins with geometry package
%\usepackage{pdflscape}
    %   Allows landscape pages
%\usepackage{layout}
    %   Allows plotting of picture of formatting 



%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr} 
%\pagestyle{fancy} 
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt} 
    %   Make the header bigger to avoid overlap

%\fancyhf{}
    %   Erase header settings

%\renewcommand{\headrulewidth}{0.3pt} 
    %   Width of the line

%\setlength{\headsep}{0.2in}    
    %   Distance from line to text
            

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure} 
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref} 
\hypersetup{%
    colorlinks,		
        %   This colors the links themselves, not boxes
    citecolor=black,	
        %   Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{verbatim} 
    %   For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %   Without this there will be a symbol in 
            %   the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},% 
            %   Size of the numbers
        numbersep=9pt,% 
            %   Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %   Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %   For including Matlab code from .m file with colors,
    %   line numbering, etc. 

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%\usepackage{natbib} 
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{enumitem} 
    %   Has to do with enumeration	
\usepackage{appendix}
%\usepackage{natbib} 
    %   For bibliographies
\usepackage{pdfpages}
    %   For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\begin{document}
\maketitle

\tableofcontents 

\clearpage
\section{Key Result for Factor Models}

The conditional distribution of jointly multivariate normal random
variables is one of the most important results that underlies the
estimation and application of factor, regression, and state space
models. It can be summarized as follows:
\begin{align}
  \text{Given} \qquad 
    \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}
    &\sim 
    N\left(
    \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix},
    \begin{bmatrix} 
      \Sigma_{11} & \Sigma_{12} \\
      \Sigma_{21} & \Sigma_{22} 
    \end{bmatrix} 
    \right) \notag\\
    \label{reg} \\
  X_1 | X_2 &\sim N(\hat{\mu}, \hat{\Sigma})  \notag\\
  \text{where} \quad
  \hat{\mu} &= \mu_1 + \Sigma_{12} \Sigma^{-1}_{22} 
    (X_2-\mu_2) \notag\\
  \hat{\Sigma} &= \Sigma_{11} - \Sigma_{12} \Sigma^{-1}_{22} 
    \Sigma_{21}\notag
\end{align}
This will be used again and again to derive the results presented below.

\section{Static Factor Models}

To begin with the simplest case, this section considers ``static ''
factor models that ignore dynamics and any autocorrelation between
observations even when dealing with a panel.

\subsection{General Case}

Suppose that we have data $\{x_{it}\}$ for $i=1,\ldots,n$ and
$t=1,\ldots,T$. In economics, this will often be a panel dataset, with
individuals or data series along the $i$ dimension and time across the
$t$ dimension. However, the model presented below could just as well
characterize any dataset that varies along two dimensions.\footnote{For
example, factor analysis was developed early on to extract certain
components of intelligence (general intelligence, verbal intelligence,
analytical intelligence, etc.) that explain the types of questions that
people answer correctly and how often they answer correctly. In that
case, you have $T$ individuals answering $n$ different questions that
test a range of abilities.  Then, the $x_{it}$ would be variables
denoting whether individual $t$ got question $i$ right, and you could
extract common factors across individuals which correspond to these
different types of intelligence.}

The data $\{x_{it}\}$ are assumed to be explained by $r$ factors,
$\{f_j\}_{j=1}^r$, plus random error. 
\begin{equation}
  x_{it} = \mu_i + \lambda_{i1} f_{1t} + \cdots + \lambda_{ir} f_{rt} + e_{it}
  \qquad i=1\ldots,n
\end{equation}
where errors are orthogonal to errors (across $i$ and $t$) and factors
are orthogonal to factors (across $r$ and $t$).

We can write out the model more explicitly and compactly in matrix
notation.  To do so, we stack all ovservations for a given $t$ and write
out the distributional assumptions as:
\begin{align}
  \underset{(n \times 1)}{X_t} &= \underset{(n \times 1)}{\mu} + 
    \underset{(n \times r)}{\Lambda} \underset{(r \times 1)}{F_t}
    + \underset{(n \times 1)}{e_t} 
    \qquad\quad \text{with } F_t \perp e_t
    \notag\\
  \begin{bmatrix} x_{1t} \\ \vdots \\ x_{nt} \end{bmatrix}
    &= \begin{bmatrix} \mu_{1t} \\ \vdots \\ \mu_{rt} \end{bmatrix}+
        \begin{bmatrix} 
          \lambda_{11} & \cdots & \lambda_{1r} \\
          \vdots & \ddots & \vdots \\
          \lambda_{n1} & \cdots & \lambda_{nr} 
        \end{bmatrix}
      \begin{bmatrix} f_{1t} \\ \vdots \\ f_{rt} \end{bmatrix}
      +  \begin{bmatrix} e_{1t} \\ \vdots \\ e_{nt} \end{bmatrix}
      \label{fm} \\\notag\\
  \text{Where} \qquad
    \begin{bmatrix} e_t \\ F_t \end{bmatrix} &\sim
    N\left(
      \begin{bmatrix} 0 \\ 0 \end{bmatrix} ,
      \begin{bmatrix} 
        \Psi & 0 \\
        0 & I \\
      \end{bmatrix}
      \right)\label{fmjoint}
\end{align}
where the \emph{loading matrix} $\Lambda$ is not necessarily square
(though it may look like it above). Note that in the case where $\Psi$
is diagonal, we call this setup an \emph{exact factor model}. In an
exact factor model, all correlation between observations $x_{it}$ is
driven by common factors. The alternative, where $\Psi$ is not diagonal,
is called an \emph{approximate factor model}.

Given the definition of $X_t$ in Equation~\ref{fm}, we can rewrite the
joint distribution in (\ref{fmjoint}) in terms of $X_t$ and $F_t$ by
deriving the means, variances, and covariances:
\begin{align*}
  EX_t &= E\left[\mu + \Lambda F_t + e_t\right]
    = E[\mu] + \Lambda E[F_t] + E[e_t]\\
  \text{By~\ref{fm}} \quad\Rightarrow\quad
    EX_t &= \mu \\\\
  E[(X_t-EX_t)(X_t-EX_t)'] &= EX_tX_t' - (EX_t)^2\\
  &= E[(\mu+\Lambda F_t + e_t)(\mu+\Lambda F_t + e_t)'] - \mu\mu'\\
  &= \mu\mu' + 2\mu E[F'_t]\Lambda' + 2\mu Ee_t' 
    + \Lambda E[F_tF'_t] \Lambda' + E[e_te_t'] - \mu\mu'\\
  \text{By~\ref{fm}} \quad\Rightarrow\quad
    \text{Var}(X_t)
    &= \Lambda \Lambda' + \Psi \\\\
  E[(X_t-EX_t)(F_t-EF_t)'] &= 
    E[X_tF_t'] - (EX_t)(EF_t)' = 
      E[(\mu+\Lambda F_t + e_t) F_t'] - \mu\cdot 0\\
    &= \Lambda E[F_tF_t'] + Ee_t\\
  \text{By~\ref{fm}} \quad\Rightarrow\quad
    \text{Cov}(X_t, F_t)
    &= \Lambda 
\end{align*}
Putting all of this together to rewrite Distribution~\ref{fmjoint}, we
can write
\begin{equation}
  \begin{bmatrix} X_t \\ F_t \end{bmatrix} \sim
  N\left(
    \begin{bmatrix} \mu \\ 0 \end{bmatrix} ,
    \begin{bmatrix} 
      \Lambda\Lambda' + \Psi & \Lambda \\
      \Lambda' & I \\
    \end{bmatrix}
    \right)
\end{equation}
Then, conditional on the data, the factor structure, factor loadings
$\Lambda$, and the variance-covariance matrix of the errors, $\Psi$, we
can apply (\ref{reg}) to get the distribution of the factors:
\begin{align}
  F_t|X_t &\sim N\left(\mu_F, \Sigma_F\right) \label{fgivenx}\\
  \mu_F &:= \Lambda'(\Lambda\Lambda'+\Psi)^{-1}(X_t-\mu) \notag\\
  \Sigma_F &:= I- \Lambda'(\Lambda\Lambda'+\Psi)^{-1}\Lambda \notag
\end{align}
We postpone estimation of the loadings and identification to later
sections. Lastly, see Appendix~\ref{sec:inv} for a matrix identity that
enables a less costly computation of $\mu_F$ and $\Sigma_F$.

\subsection{Single Factor, Identical Loadings, and Spherical Errors}

Suppose that we take the special case of a static factor model where
there is one single factor for each time period, $f_t$, that all
observations load on equally. In addition, assume that the errors are
all iid $e_{it}\sim N(0, \sigma^2)$ (i.e.\ spherical) and uncorrelated
across time and observations:
\begin{align*}
  x_{it} &= f_{t} + e_{it}\qquad \text{Var}(e_{it})=\sigma^2\\
  X_t &= \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix} F_t + 
    \begin{bmatrix} e_{1t} \\ \vdots \\ e_{nt} \end{bmatrix}
    \qquad \text{Cov}(e_t) = \Psi = \sigma^2 I_n
\end{align*}
Again, taking $\Psi$ and $\Lambda$ as known, the factor given the data
can be written out explicitly as 
\begin{align}
  F_t = f_t &\sim N(\mu_F, \Sigma_F)\label{ex1}\\
  \mu_F &= \frac{1}{\left(1+\frac{n}{\sigma^2}\right)}
  \sum^n_{i=1} \frac{x_{it}-\mu}{\sigma^2}
  = \frac{1}{\sigma^2+n} \sum^n_{i=1} \left(x_{it}-\mu\right)\notag\\
  \Sigma_F &= \frac{1}{\left(1+\frac{n}{\sigma^2}\right)}\notag
\end{align}

\paragraph{Bayesian Intuition} We can see from the expression above that
the mean estimate of the factor at time $t$ is something like a simple
average across all demeaned observations at time $t$.  It is, however,
biased towards zero. To understand why we opt for this biased estimator
of $F_t$ rather than simply the (unbiased) cross-sectional avarage,
let recast this whole effort within the context of Bayesian regression.

Recall the equation relating the factors to the observations:
\begin{align}
  \underset{(n \times 1)}{X_t} &= \underset{(n \times 1)}{\mu} + 
    \underset{(n \times r)}{\Lambda} \underset{(r \times 1)}{F_t}
    + \underset{(n \times 1)}{e_t} 
    \label{regintuition}
\end{align}
While we naturally want to think of $\Lambda$ as the coefficients on the
righthand-side factors ($F_t$) we can also flip that logic. Since we
always take $\Lambda$ and $X_t$ as given in the derivations above, think
instead of the factors $F_t$ as the ``coefficients'' to estimate,
while $\Lambda$ is like the righthand side data in a typical regression
setup. 

Then, the distributional assumptions about the factors 
\[
  F_t \sim N(0, I)
\]
amount to a \emph{prior} on the factors (our ``coefficients'' to
estimate), while the expressions in Result~\ref{fgivenx} and
Result~\ref{ex1} amount to the \emph{posterior} of the factors after
oberving the data $X_t$. To complete the story,
Equation~\ref{regintuition} characterizes the likelihood.

Cast in this context, the expressions for $\mu_F$ and $\Sigma_F$ are
entirely conventional, familiar, and expected since they match exactly
the mean and variance of the posterior distribution for the
``coefficients'' $F_t$ that you would get in Bayesian regression with a
shrinkage prior.


\subsection{Single Factor with Different Loadings and Error
Distributions}

Suppose that we take the special case of a static factor model where
there is one single factor for each time period, $f_t$, that all
observations load on, but with different weights. In addition, assume
that the errors are all independent and distributed with variances
unique to each series---i.e. $e_{it}\sim N(0, \sigma_i^2)$:
\begin{align*}
  x_{it} &= \lambda_i f_{t} + e_{it}\qquad \text{Var}(e_{it})=\sigma_i^2\\
  X_t &= \begin{bmatrix} \lambda_1 \\ \vdots \\ \lambda_n \end{bmatrix} 
    F_t + 
    \begin{bmatrix} e_{1t} \\ \vdots \\ e_{nt} \end{bmatrix}
    \qquad \text{Cov}(e_t) = \Psi = 
    \begin{bmatrix}
      \sigma_1 & \cdots & 0 \\
      \vdots & \ddots & \vdots \\
      0 & \cdots & \sigma_n \\
    \end{bmatrix}
\end{align*}
Again, taking $\Psi$ and $\Lambda$ as known, the factor given the data
can be written out explicitly as 
\begin{align}
  F_t = f_t &\sim N(\mu_F, \Sigma_F)\label{ex2}\\
  \mu_F &=
  \frac{1}{\left(1+\sum^n_{i=1}\frac{\lambda_i^2}{\sigma_i^2}\right)}
  \sum^n_{i=1} \frac{\lambda_i}{\sigma_i^2}(x_{it}-\mu) \notag\\
%  \Sigma_F &= \frac{1}{\left(1+\frac{n}{\sigma^2}\right)}\notag
\end{align}



%% APPPENDIX %%

\clearpage
\appendix
\section{Matrix Inversion Identity}
\label{sec:inv}

\paragraph{Motivation} Recall the distribution of the factors
conditional on the data and the parameters from Result~\ref{fgivenx}:
\begin{align}
  F_t|X_t &\sim N\left(\mu_F, \Sigma_F\right) \notag\\
  \mu_F &:= \Lambda'(\Lambda\Lambda'+\Psi)^{-1}(X_t-\mu) \notag\\
  \Sigma_F &:= I- \Lambda'(\Lambda\Lambda'+\Psi)^{-1}\Lambda \notag
\end{align}
This requires the inversion of an $n\times n$ matrix to compute the mean
and variance. We typically use factor analysis when $n$ is large,
seeking a small number $r$ of factors that efficiently summarize the
data. So it would be nice if we exploit matrix identities that allow us
to invert an $r\times r$ matrix instead, saving on computation.

\paragraph{Simplifying the Mean}
Here, we can use a well-known matrix identity:
\begin{align}
  P(I+QP) &= (I+PQ)P \notag\\
  \Rightarrow \qquad
  (I+PQ)^{-1}P &= P (I+QP)^{-1} \label{invid}
\end{align}
Now, let's take $\Lambda'(\Lambda \Lambda' +\Psi)^{-1}$ and try to apply
some matrix algebra so we can use that result:
\begin{align*}
    \Lambda'(\Lambda \Lambda' +\Psi)^{-1} &=
    \Lambda' \left(\Psi^{-1}\Psi\right)(\Lambda \Lambda' +\Psi)^{-1}\\
    &= \Lambda' \Psi^{-1}\left(\Psi(\Lambda \Lambda' +\Psi)^{-1}\right)\\
    &= \Lambda' \Psi^{-1}\left((\Lambda \Lambda'
      +\Psi)\Psi^{-1}\right)^{-1}\\
    &= \Lambda' \Psi^{-1}\left(I+\Lambda \Lambda'\Psi^{-1}\right)^{-1}
\end{align*}
But notice that if we set $P:=\Lambda'\Psi^{-1}$ and $Q:=\Lambda$, we
have the righthand side of Identity~\ref{invid}. So just use the
Identity~\ref{invid} to exchange for the lefthand side and write out the
main result
\begin{equation}
  \Rightarrow\qquad   
  \Lambda'(\Lambda \Lambda' +\Psi)^{-1} = 
  (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1} 
\end{equation}
So now, we only ever invert $\Psi$ or $(I+\Lambda'\Psi^{-1}\Lambda)$,
both of which are $r\times r$ matrices.

\paragraph{Simplifying the Variance}
Rewriting our conditional factor distribution, we have
\begin{align}
  F_t|X_t &\sim N\left(\mu_F, \Sigma_F\right) \notag\\
  \mu_F &:= 
    (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1} (X_t-\mu)\notag\\
  \Sigma_F &:= 
    I- (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1}\Lambda 
    \label{halfway}
\end{align}
Let's simplify the variance a little more with the following identity 
\begin{align}
  (I+P)^{-1} 
    &= (I+P)^{-1} ( I + P - P)\notag\\
    &= (I+P)^{-1} ( I + P) - (I+P)^{-1}P\notag\\
  \Rightarrow (I+P)^{-1} 
  &= I - (I+P)^{-1}P\label{invid2}
\end{align}
Now in Expression~\ref{halfway}, let's set
$P:=\Lambda'\Psi^{-1}\Lambda$ and exchange the righthand side of
Identity~\ref{invid2} for the lefthand side:J
\begin{align}
  \Sigma_F &= 
    I- (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1}\Lambda\notag\\
  &= (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1}\Lambda
  \notag
\end{align}
Again, this only entails the inversion of $r\times r$ matrices. 

\paragraph{Conclusion} Writing out the full result, we have now written
the contribution of the Factors as
\begin{align}
  F_t|X_t &\sim N\left(\mu_F, \Sigma_F\right) \notag\\
  \mu_F &:= 
    (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1} (X_t-\mu)\notag\\
  \Sigma_F &:= 
    (I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1}\Lambda\notag
\end{align}
You only have to invert $r\times r$ matrices, and you can compute 
$(I+\Lambda'\Psi^{-1}\Lambda)^{-1} \Lambda'\Psi^{-1}$ once, then use it
for computing both $\mu_F$ and $\Sigma_F$ with one more matrix
multiplication each.

\clearpage
\section{Principal Component Analysis}

This section first summarizes Principle Component Analysis (PCA) and
then examines the close relationship between PCA and factor analysis. 

Suppose that we have a population covariance matrix $\Sigma$. Since
$\Sigma$ is symmetric, we can do an eigenvalue decomposition and write 
\begin{align*}
 i 
\end{align*}






\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% VIEW LAYOUT %%
    
        \layout

    %% LANDSCAPE PAGE %%

        \begin{landscape}
        \end{landscape}

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}  %Use in text; cites
        \citep{LabelInSourcesFile} %Use in text; cites in parens

        \nocite{LabelInSourceFile} % Includes in refs w/o specific citation
        \bibliographystyle{apalike}  % Or some other style

        % To ditch the ``References'' header
        \begingroup
        \renewcommand{\section}[2]{} 
        \endgroup

        \bibliography{sources} % where sources.bib has all the citation info

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%

        \verbatiminput{file.ext}    
            %   Includes verbatim text from the file
        \texttt{text}	  
            %   Renders text in courier, or code-like, font

        \matlabcode{file.m}	  
            %   Includes Matlab code with colors and line numbers


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}


        % Side by Side figures
            \begin{figure}[h!]
                \centering
                \mbox{\subfigure{
                    \includegraphics[scale=1]{file1.pdf}
                }\quad\subfigure{
                    \includegraphics[scale=1]{file2.pdf} 
                }
                }
            \end{figure}
    

