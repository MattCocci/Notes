\documentclass[a4paper,12pt]{scrartcl}
\title{Continuous Time Finance}
\author{Matthew Cocci}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
%\numberwithin{equation}{section} %, This labels the equations in relation to the sections rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}




\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Blah}
\subsection{Slogan of It\={o} Calculus}
      
Finally, that last property says that
\begin{equation}
    \label{slogan}
     (dW_t)^2 = dt 
\end{equation}
This follows from Equation, which was a condition for
the construction of Brownian Motion, and the fact that $D=1/2$
for Brownian Motion. We can also show that $(dW_t)^p = 0$ for all $p>2$.
In it's precise form, the above facts mean
\begin{align*}
    \int^t_a \phi_s (dW_s)^2 &=  \int^t_a \phi_s ds \\
    \int^t_a \phi_s (dW_s)^n &= 0 \qquad n>2
\end{align*}
both of which will make more sense below.
\\
\\
{\sl Elaboration}: Equation \ref{slogan} is a pretty remarkable fact.
It says that the square of a random process becomes \emph{deterministic}.
We're not just talking about $dW^2_t$ being $dt$ in expectation---it 
actually \emph{is} $dt$.




\section{The It\={o} Integral}

The ultimate representation that we want for the \textbf{It\={o}
Integral}, 
   \[ \int_0^T \phi_t \;dW_t, \]
is the result of the limit of specific sums, which must first be defined.

\subsection{It\={o} Sums}

So let us define \textbf{It\={o}'s Sums}, for stochastic processes
$\phi_t$ and $W_t$ as
   \[ IS(\phi,\Delta)=\sum^n_{i=1} \phi_{t_{i-1}}(W_{t_i}-W_{t_{i-1}}).\]
where $\Delta = (t_0, t_1,\ldots,t_n)$ is some partition of the time 
inteval, $[0,T]$. Note, the It\={o} Integral 
is a bit more general in that we
could substitute any stochastic process, $X_t$, in for $W_t$.  But 
the Wiener Process has some nice properties, so we'll stick to it.

\paragraph{Note} Very important is the subscript on $\phi$.  Notice that
we evaluate at the \emph{left} endpoint of our chopped-up time 
intervals: $t_{i-1}$.  This turns out to make a difference, because it
means that the process, $\phi$, doesn't overlap the difference in the 
Brownian Motion terms, allowing us to assert independence.

Moreover, when we do this, if $W_t$ (or $X_t$ more generally) is a 
Martingale, then the resulting It\={o} Sum will be as well. There's 
even an added benefit in that the choice of the left endpoint
imposes what looks like a ``previsibility condition''---which will
be very useful for stock prices and portfolio determination.

\subsection{It\={o}'s Integral}

We define the \textbf{It\={o} Integral} as the limit of of It\={o}
Sums:
   \[ \lim_{||\Delta|| \rightarrow 0}IS(\phi,\Delta) =  
      \lim_{||\Delta|| \rightarrow 0} \sum^n_{i=1} \phi_{t_{i-1}}(W_{t_i}
      -W_{t_{i-1}})= \int_0^T \phi_t \;dW_t \]

\paragraph{Theorem 4.3.7} If $\phi_s$ is adapted to $W_s$ (definition
below), then 
   \[ \int_a^t \phi_s \; (dW_s)^2 = \int_a^t \phi_s \; ds.\]
In addition, for $n>2$,
   \[ \int_a^t \phi_s \; (dW_s)^2 = 0. \]


\subsection{Properties of the It\={o} Integral}

Given processes $\phi_s$ and $\psi_s$ which are adapted (either to 
$X_s$ or $W_s$), we obtain the following results:
\begin{enumerate}
   \item{$\int_a^t dX_s = X_t - X_a$.}
   \item{$\int_a^t (\phi_s + \psi_s)\; dX_s = \int_a^t \phi_s \; dX_s + 
      \int_a^t \psi_s \; dX_s$.}
   \item{$E\left[\int_a^t \phi_s \;dW_s \right] = 0$.}
   \item{$E\left[\left(\int_a^t \phi_s \;dW_s\right)^2 \right] = 
      E\left[\int_a^t \phi^2_s \;dW_s \right]$.}
\end{enumerate}




\subsection{Existence Conditions}

For the limit to exist---and, therefore, the It\={o} Integral---we must
satisfy certain conditions.
\begin{enumerate}[]
   \item[(i)]{$\phi_t$ is \emph{adapted} to $W_t$.}
   \item[(ii)]{$\phi_t$ is \emph{square-integrable}.} 
\end{enumerate}

\paragraph{Definition} We say that a process, $\phi_t$ is 
\textbf{adapted} to $W_t$ if, for any $s > t$, $\phi_t$ is independent
of $W_s - W_t$.

\paragraph{Definition} We say that a random variable process, $\phi_t$ is
\textbf{square-integrable} if, for any $T>0$,
   \[E \left[ \int^T_0 |\phi_t|^2 \; dt \right] < \infty.\]
This is equivalent to checking that the first and second moments are
finite.  
And as a side note, you can take the expectation operater inside the 
integral because $|\phi_t|^2$ is a positive random variable.

\subsection{Mode of Convergence}

If $\phi_t$ satisfies the two conditions above, then the sequence of 
sums $IS(\phi, \Delta)$ will converge as $||\Delta||\rightarrow 0$ to a 
random variable, denoted by the It\={o} integral of $\phi_t$ against
$W_t$. What's more, it so happens that the It\={o} Sums converge to 
the It\={o} Integral in mean square.

\paragraph{Definition} We say that a sequence of random variables, 
$\{ X_n | n \in \mathbb{N} \}$ converges to a random variable $X$ in 
\textbf{mean-square} if
   \[\lim_{n \rightarrow \infty} E\left[ |X_n - X|^2 \right] = 0, \]
which, as can be seen upon expanding, checks that $EX_n \rightarrow
EX$ and $Var(X_n) \rightarrow Var(X)$---i.e. that the the two random 
variables are the same. 
In the case of the It\={o} Integral, this means that 
   \[\lim_{||\Delta||\rightarrow 0} E \left[ \left\lvert IS(\phi, \Delta)
      - \int_0^T \phi_t dW_t \right\rvert^2 \right] = 0.\]

\subsection{Useful Examples}

\begin{enumerate} 
   \item{In the derivation of the following result, 
      one uses the property of 
      convergence in mean square to obtain:
      \[ \int_0^T W_t \; dW_t = \frac{1}{2} W_T^2 - \frac{1}{2} T \]}
   \item{Follows because it's a telescoping sum:
      \[ \int_a^b dX_t = X_b - X_a  \]
      \[ \int_0^T dW_t = W_T - W_0 = W_T \]}
   \item{This next result follows from the fact that $M_t = \int_0^T
      \phi_s dW_s$ is a martingale:
      \[ E\left[\int_0^T \phi_t \; dW_t\right] = 0 \]}
   \item{The second order variation comes into play here: 
      \[ E\left[\left(\int_0^T \phi_t \; dW_t\right)^2\right] = 
      E\left[\int_0^T \phi^2_t \; dW_t\right] \]}
\end{enumerate}
   



\section{Stochastic Differential Equations}

Our models for stock prices will be solutions to \textbf{stochastic
differential equations}, which are expressions of the form
\begin{equation}
   \label{sde}
   dX_t = a(X_t,t)\;dt + b(X_t,t)\;dW_t. 
\end{equation}
A stochastic process $X_t$ that solves this SDE will be of the form 
   \[ X_t - X_a = \int_a^t dX_s = \int_a^t a(X_s,s)\;ds + b(X_s,s)\;W_s
      \]
Now as a word of caution, such equations don't always have solutions, and
if they do, there's no guarantee that the solution is unique, unless
we put some assumptions on the coefficients.  Respectively, the 
coefficients $a(x,s)$ and $b(x,s)$ are functions for the \emph{drift}
coefficient and the \emph{diffusion} coefficient.

\subsection{It\={o}'s Lemma}

\paragraph{Lemma} Let $f: \mathbb{R}\times [0,\infty) \rightarrow 
\mathbb{R}$ be a function that is twice differentiable in $x$, and
once differentiable in $t$. Also, let $X_t$ be a soltuion to the equation
   \[ dX_t = a(X_t,t)\; dt + b(X_t,t)\; dW_t.\]
Then $f(X_t,t)$ is a stochastic precess that satisfies the SDE
   \[ df(X_t,t) = \left[f_t(X_t,t) + a(X_t,t) \cdot f_x(X_t,t)
      + \frac{1}{2}\; b^2(X_t,t)\cdot f_{xx}(X_t,t)\right]\;dt \]
   \[ \qquad + b(X_t,t)\cdot f_x(X_t,t)\;dW_t. \]

\subsection{Solving SDE Problems}

Stochastic diffeernital equations offer two possibilities:
   \begin{enumerate}
      \item{Solving stochastic differential equations that are given 
	 by finding a function for the variable $X_t$ of interest.}
      \item{Solving stochastic integrals by transforming the problem
	 into an SDE and solving an easier expression}
   \end{enumerate}
We'll consider both of those now---albeit in the reverse order from
how they were stated.

\subsubsection{Solving Stochastic Integrals with SDE's}

Suppose that you're given a stochastic integral, $\int \phi_t dW_t$,
to compute. How can you simplify?
\begin{enumerate}
   \item{The ultimate goal is to find a function $f(x)$ such that 
      \[\phi_t dW_t = d(f(\phi_t)).\]
      If we have that function $f$, we can use the properties of the
      It\={o} Integral to compute easily
      \[ \int^b_a \phi_t\; dW_t =  \int^b_a d(f(\phi_t)) = 
	 f(\phi_b) - f(\phi_a).\]
	 }
   \item{But first, we have to find the SDE that this corresponds to. 
      Since we're applying It\={o}'s Lemma by finding $d(f(\phi_t))$, we
      should really find the parameters such that
      \begin{equation}
	 \label{implicit}
	 d\phi_t = a(\phi_t, t)\;dt + b(\phi_t, t) \; dW_t.
      \end{equation}
      Since we know $\phi$ and we $dt$ and $dW_t$ are already in there,
      this amounts to finding the values $a(\phi_t, t)$ and $b(\phi_t,t)$
      that make Equation \ref{implicit} hold.
	 } 
   \item{Once we have $a(\phi_t, t)$, $b(\phi_t, t)$, and a suitable 
      guess for $f(x)$, we can apply It\={o}'s Lemma.  Hopefully,
      the original integral, $\int \phi_t dW_t$, is in the derived 
      equation, and everything else is simple enough for us to solve.}
\end{enumerate}


\paragraph{Example 1} We hope to solve $\int^t_a W_s dW_s$. I will 
solve this problem in steps mirroring the numbered steps listed above.
\begin{enumerate}
   \item{Since our integral looks somewhat like $\int f(x) dx$, an
      educated guess would be the equation $f(x) = \frac{1}{2} x^2$, 
      which is the integral of the traditional calculus problem.
      }
   \item{Next, we realize that $\phi_t = W_t$ in our problem, and 
      so set up the corresponding, implicit SDE:
      \[ dW_t = a(W_t, t) dt + b(W_t, t) dW_t.\]
      The only values for $a$ and $b$ that make this hold are $a=0$
      and $b=1$.
      }
   \item{Next, we apply It\={o}'s Lemma, which simplifies to
	 \[d(f(W_t)) = \frac{1}{2}\; dt + W_t \; dW_t. \]
      From there, we can shift around to get our original problem
	 \[W_t \; dW_t= d(f(W_t)) -\frac{1}{2}\; dt \]
      and integrate to solve the SDE:
	 \[ \int^t_a W_t \; dW_t =\int^t_a d(f(W_t))-\frac{1}{2}\;dt\]
	 \[ \int^t_a W_t \; dW_t=\;\frac{1}{2}\;W^2_t-\frac{1}{2}\;W^2_a
	    - \frac{1}{2}\;(t-a).\]
      }
\end{enumerate}

\paragraph{Example 2} We hope to solve $\int^t_a W^2_s\;dW_s$. I will 
solve this problem in steps mirroring the numbered steps listed above.
\begin{enumerate}
   \item{Since our integral looks somewhat like $\int f^2 dx$, an
      educated guess would be the equation $f(x) = \frac{1}{3} x^3$, 
      which is the integral of the traditional calculus problem.
      }
   \item{Need help with this.}
   
\end{enumerate}

\subsubsection{Solving SDE's with Integrals}

The general method involves finding a suitable function to use in 
applying It{o}'s Lemma. Then solve this easier problem.


\subsection{Geometric Brownian Motion}

The basic stochastic differential equation representation is
   \[ dX_t = \mu X_t \; dt + \sigma X_t \;dW_t \]
We start by considering a function we can use in order to apply 
It\={o}'s Lemma:
   \[ f(x) = \ln(x) \]
   \[ f_t = 0, \;\; f_x = \frac{1}{x}, \;\; f_{xx} = -\frac{1}{x^2} \]
Using the Lemma, we ccan write
   \[ d(\ln(X_t)) = \left[ 0 + \mu X_t \cdot \frac{1}{X_t} - 
      \frac{1}{2} \; \sigma^2 X_t^2 \cdot \frac{1}{X_t^2} \right]\; dt 
      + \sigma \frac{1}{X_t} X_t\; dW_t \]
   \[ d(\ln(X_t)) = \left( \mu - \frac{1}{2}\sigma^2 \right) dt
      + \sigma dW_t \]
We can now solve this easily for $X_t$ since all terms on the righthand
side are known, and independentof $X_t$:
   \[ \int^t_0 d(\ln(X_s)) \; ds =  \int^t_0 \left( \mu - 
      \frac{1}{2}\sigma^2 \right) ds + \sigma dW_s \]
   \[ \ln(X_t) - ln(X_0) = \left( \mu - 
      \frac{1}{2}\sigma^2 \right) t - 0 + \sigma W_t - \sigma W_0 \]
   \[ \ln(X_t) = ln(X_0) + \left( \mu - 
      \frac{1}{2}\sigma^2 \right) t +  \sigma W_t \]
   \[X_t = X_0 \; e^{\left( \mu - 
      \frac{1}{2}\sigma^2 \right) t +  \sigma W_t} \]

\subsection{Ornstein-Uhlenbeck Process}

This process is defined by the SDE
   \[ dX_t = \mu X_t \; dt + \sigma \;dW_t \]
This case differs from geometric Brownian motion in that there is no
$X_t$ in front of $dW_t$.

For this case, choose $f(x,t) = xe^{-\mu t}$.  Applying It\={o}'s Lemma
gives us
   \[d(X_t e^{-\mu t})=\left(-X_t\;\mu e^{-\mu t} + X_t \mu e^{-\mu t}
      \right) + \sigma e^{-\mu t} dW_t \]
   \[ d(X_t e^{-\mu t})= \sigma e^{-\mu t} dW_t \]
Integrating this expression yields
   \[ \int^t_0 d(X_s e^{-\mu s}) = \int^t_0 \sigma e^{-\mu t} dW_t \]
   \[ X_t e^{-\mu t} - X_0 e^{-\mu 0} = \int^t_0 \sigma e^{-\mu t} dW_t
      \]
   \[ X_t = X_0 e^{\mu t} + \int^t_0 \sigma e^{-\mu (t-s)} dW_t
      \]

\subsection{Cox-Ingersoll-Ross Model}

This model, which exhhibits mean reversion and volatility that grows
with $r_t$ is defined
   \[ dr_t = a(b-r_t) \; dt + \sigma \sqrt{r_t} \; dW_t.\]

\subsection{Vasicek Model}

This interest rate model has mean reversion, where $b$ is the mean. It's
defined by the following SDE:
   \[ dr_t = a(b-r_t) \; dt + \sigma \; dW_t.\]

\end{document}














