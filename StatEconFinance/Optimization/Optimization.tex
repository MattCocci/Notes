\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Optimization}
\date{}
\usepackage{enumitem} %Has to do with enumeration
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{hyperref} % For internal/external linking.
				 % [hidelinks] removes boxes
\hypersetup{%
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black,
}
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section}
   % This labels the equations in relation to the sections
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim}

\begin{document}
\maketitle

\tableofcontents %adds it here

\clearpage
\section{Constrained Optmization}

\subsection{Equality Constraints}

\subsubsection{Single Constraint}
Suppose that for vector $x$ and scalar-valued differentiable functions
$f$ and $g$, we are faced with the problem
\begin{align}
  \max_x &f(x) \label{prob:eqconst}\\
  \text{s.t. } &g(x) = 0 \notag
\end{align}
If this were an \emph{unconstrained} optimization, we'd just find the
point(s) in the domain of $f$ where $f'(x)=0$, i.e.\ where $f$ doesn't
change in a small neighborhood about $x$.
\\
\\
But since this is a \emph{constrained} optimization problem, we can't do
quite that, but we will do something close: Find the point(s) where $f$
doesn't increase/change in a small neighborhood about $x$
\emph{restricted to the domain} where $g(x)=0$.
\\
\\
The intuition is quite simple. Whether the problem is constrained or
not, you want to find points where $f$ doesn't increase much in the
neighborhood of $x$---otherwise, you'd take a tiny step to an $x^*$ such
that $f(x^*) > f(x)$, and get an improvement.
\\
\\
So how do we find $x$ when constrained by $g(x)=0$?
\begin{enumerate}
  \item Walk along the contour line of $g$ such that $g(x)=0$. In other
    words, stay on the equality constraint.
  \item Look for points where $f$ does not change as we walk, which
    happens when either
    \begin{enumerate}
      \item There's a portion of your walk along $g(x)=0$ (i.e. there is
        a small neighborhood) that also has you walking along a contour
        of $f$. By definition, walking along a contour of $f$ means that
        $f$ is not changing. Since $f$ is not changing and you're still
        on $g(x)=0$, you have a candidate.

        Now in math, it must be the case that in this small
        neighborhood, $f$ and $g$ have the same directional gradient,
        since (1) gradients are always perpendicular to contour lines
        and (2) the contour $g(x)=0$ and some contour of $f$ are
        \emph{the same} (at least in this small neighborhood). That
        implies
        \begin{align}
          \nabla f = -\lambda \nabla g
          \label{eq:lag1}
        \end{align}
        where $\lambda$ is a constant that simply scales the gradient of
        $g$. We need to include that because while the gradients point
        in the same direction, there is no guarantee their magnitude is
        the same.

      \item You actually found an extremum of $f$ such that $g(x)=0$
        \emph{and} $f'(x)=0$. That's the best case scenario: even with
        the constraint, you can still hit an unconstrained extremum of
        $f$.

        In math, we can also use Equation~\ref{eq:lag1} since
        $\nabla f=0$ in this case, and we can just set $\lambda=0$.
    \end{enumerate}
  \end{enumerate}
Therefore, we can solve constrained optimization
Problem~\ref{prob:eqconst} by defining the Lagrangian $\mathscr{L}$
and finding $x$ such that
\begin{align}
  g(x) &= 0\label{lagrangeSingle}\\
  \nabla \mathscr{L}(x,\lambda) &= 0 \notag\\\notag\\
  \text{where} \quad
  \mathscr{L}(x,\lambda) &:= f(x) + \lambda g(x) \notag
\end{align}
We see that differentiating $\mathscr{L}$ with respect to $\lambda$ and
setting it equal to zero just gives us the constraint $g(x)=0$, while
differentiating with respect to elements of $x$ gives (\ref{eq:lag1}).
Hence, the maximization of $\mathscr{L}$ is equivalent to all of the
conditions and setup described above.

\subsubsection{Multiple Constraints}

In the case where we have multiple constraints $g_1(x)=\cdots=g_K(x)=0$,
solve
\begin{align}
  g_k(x) &= 0 \qquad\qquad k=1,\ldots,K\label{lagrangeMultiple}\\
  \nabla \mathscr{L}(x,\lambda) &= 0 \notag\\\notag\\
  \text{where} \quad
  \mathscr{L}(x,\lambda)
  &:= f(x) + \lambda \cdot g(x) \notag\\
  \lambda &:=
  \begin{pmatrix}\lambda_1 & \cdots & \lambda_k\end{pmatrix} \notag\\
  g(x) &=
  \begin{pmatrix}g_1(x) & \cdots & g_K(x)\end{pmatrix}^T \notag
\end{align}
Note that the gradient $\nabla \mathscr{L}$ is taken with respect to
\emph{all} arguments of $\mathscr{L}$: all elements of vector $x$ and
$\lambda_1,\ldots,\lambda_K$.

Much of the intuition from the last section holds, but now for each
$g_k$: We want to find points that are on $g_k(x)$---but now for
multiple $k$---where $f$ doesn't change very much.

\subsubsection{Interpretation of the Multipliers}

Suppose that our constraints take the form
\begin{align*}
  h_k(x) = c_k
  \qquad \Rightarrow \qquad
  g_k(x) := c_k-h_k(x) = 0
\end{align*}
where the $g_k$ match those above in
Representation~\ref{lagrangeMultiple}.

Well then by the definition of $\mathscr{L}$, we can differentiate with respect to the constraint $c_k$ to find
\begin{align*}
  \frac{\partial\mathscr{L}}{\partial c_k}
  &= \lambda_k
\end{align*}
Therefore, the Lagrange multiplier $\lambda_k$ represents the amount by
which the Lagrange function $\mathscr{L}$ changes when you relax
constraint constant $c_k$ a little bit.

Even more, since $g_k(x^*)=0$ at the optimum $x^*$, this is not just the
change in $\mathscr{L}$, but the change in the objective function $f$:
\begin{align*}
  \frac{\partial f(x^*)}{\partial c_k} = \lambda^*_k
\end{align*}
where $\lambda^*_k$ is the value of the multiplier at the extremum.

This is important in economics, since often $f$ is utility or profits,
in which case $\lambda^*_k$ tells you the amount by which profits or
utility would increase if you had a little more $c_k$ (often cash money
or consumption or something).

\subsection{Inequality Constraints}

Now suppose that for vector $x$ and scalar-valued differentiable
functions $f$, $g_k$ ($k~=~1,\ldots,K$), and $h_j$ ($j~=~1,\ldots,J$),
we are faced with the problem
\begin{align}
  \max_x &f(x) \label{prob:ineqconst}\\
  \text{s.t. } &g_k(x) \leq 0 \qquad k=1,\ldots,K\notag\\
               &h_j(x) =0 \qquad j=1,\ldots,J\notag
\end{align}
Then the optimum $x^*$ satisfies
\begin{align*}
\end{align*}




\clearpage
\section{Numerical Optimization}

Suppose that we want to minimize some function $f$ to find
\begin{align}
  \label{statement}
  x^* = \min_x f(x)
\end{align}
This note will focus on iterative algorithms that follow the same basic
steps:
\begin{enumerate}
  \item Start with a guess for $x^*$, and call it $x_n$.
  \item Use some update rule to get a better guess, $x_{n+1}$, and go
    there.
  \item Repeat Step 2 until some convergence criterion is met.
\end{enumerate}
The main difference is in the update step: How do we update so that
$f(x_{n+1}) < f(x_n)$ (i.e.\ we get some improvement) and so that we
converge \emph{quickly} to the minumum $x^*$.

Here are two possible update rules:
\begin{enumerate}
  \item Gradient Descent: The negative of the gradient $\nabla f(x_n)$
    points in the direction of steepest descent. To get $x_{n+1}$, just
    take a step in that direction.
  \item Newton's Method: Approximate $f$ about $x_n$ with a second-order
    Taylor expansion. Set $x_{n+1}$ to be the minimum of that approximation.
\end{enumerate}


\subsection{Gradient Descent}

\subsubsection{Single Dimension}

Suppose that $f$ is differentiable, and that we have some initial guess
$x_n$. The update according to
\begin{align*}
  x_{n+1} = x_n - \alpha f'(x_n)
\end{align*}
for some chosen scaling parameter $\alpha$.

\paragraph{Intuition}
If $f'(x_n)$ is large, then we can decrease $f$ a lot by moving in the
$-f'(x_n) = -\frac{df(x_n)}{dx_n}$ direction.  So let's update $x_n$ at
some rate $\alpha$ proportional to $f'(x_n)$.

\paragraph{Choice of $\alpha$}
Moreover, notice how this process self-adjusts in the
magnitude of the update.  In particular, as you get close to
the minimum, the derivatives approach zero.  So even with 
$\alpha$ constant, the shrinking derivative means that the gradient
descent algorithm will take smaller steps, reducing the likelihood that
you'll blow past the minimum on update.


\subsubsection{Multiple Dimensions}

To minimize differentiable objective function $f$, update according to
\begin{align*}
   x_{n+1}^{(1)} &= x_n^{(1)}
    - \alpha \frac{\partial f(x_n)}{\partial x_n^{(1)}}\\
   \vdots \quad & \qquad \vdots \\
   x_{n+1}^{(k)} &= x_n^{(k)}
    - \alpha \frac{\partial f(x_n)}{\partial x_n^{(k)}}
\end{align*}
where $x^{(i)}$ denotes the $i$th element of $k$-length vector $x$.

In matrix notation, we can write this more compactly as
\begin{align*}
  \label{eq:multigradesc}
  x_{n+1} = x_n - \alpha [\nabla f(x_n)]
\end{align*}
where $\nabla f(x)$ denotes the gradient of $f$ at $x$.

\begin{rmk}
At each iteration, we update all the components of $x$
\emph{simultaneously}.  We don't update each component, one at a time,
as one does in Gibbs Sampling.

So in one step, we update all parameters in the direction of their
respective largest changes. (That's just the definition of the
gradient.) We don't take account of any curvature in the objective
function (though Newton's method, detailed below, does).
\end{rmk}

\paragraph{Sensible Scaling}
If the entries of $x$ lie on vastly different scales, normalize so that
all entries of $x$ lie on common support, or adapt update
Equation~\ref{eq:multigradesc} to
\begin{align}
  x_{n+1} = x_n - \alpha w^T [\nabla f(x_n)]
\end{align}
where $w$ is another scaling vector whose $i$th entry is small when
$x_i$ has an especially large support.

To motivate, suppose $x_1$ lies on $[0,1]$, and if $x_2$ lies on
$[-1000, 1000]$. If we choose a reasonable $\alpha$ for convergence of
$x_1$, it will be \emph{way} too small for $x_2$. Convergence will take
forever. Normalizing the input vector $x$ or scaling elements of the
gradient vector (via $w$) avoids this issue.


\subsection{Newton's Method}

The basic idea behind Newton's Method is not to minimize the objective
function $f$ \emph{per~se}. Instead, minimize a second-order Taylor
approximation about some initial point $x_n$. Call that approximation
minimum $x_{n+1}$ and repeat the process until you converge.

You're simply solving a  sequence of easier optimization problems that
get successively closer to the true objective function you really want
to minimize.


\subsubsection{Single Dimension}

The basic recursive procedure proceeds as follows:
\begin{enumerate}
  \item Suppose that we have some initial guess $x_n$ for the minimum of
    $f$. Then write the second-order Taylor expansion of $f$ about
    $x_n$, and call that approximation $f_n$:
    \begin{align}
      f_n(x) &= f(x_n) + f'(x_n)(x-x_n) + \frac{1}{2} f''(x_n) (x-x_n)^2\\
      \text{where} \qquad
      f(x) &= f_n(x) + O(x^3)\notag
    \end{align}
  \item Do the easy thing and minimize this approximating function $f_n$
    rather than $f$. Then set that as your next guess, $x_{n+1}$. In
    other words:
    \begin{align*}
      x_{n+1} = \min_x f_n(x)
    \end{align*}
    This is equivalent to setting $x_{n+1}$ such that $f'_n(x_{n+1})=0$,
    so solve
    \begin{align*}
      0 &= f'_n(x_{n+1})\\
      &= f'(x_n) + f''(x_n)(x_{n+1}-x_n) \\\\
      \Rightarrow \quad
      x_{n+1} &= x_n - \frac{f'(x_n)}{f''(x_n)}
    \end{align*}
\end{enumerate}
Repeate that process until convergence.


\subsubsection{Newton's Method in Multiple Dimensions}

The procedure is identical, save some more general matrix and vector
notation:
\begin{enumerate}
  \item Start with an initial guess $x_n$ (now a vector), and write a
    second-order Taylor expansion of $f$ about $x_n$:
    \begin{align*}
      f_n(x) = f(x_n) + (x-x_n)^T \left[\nabla f(x_n)\right]
        + (x-x_n)^T \left[\nabla^2 f(x_n)\right](x-x_n)
    \end{align*}
    Letting $g_n$ denote gradient $\nabla f(x_n)$ and $H_n$ the Hessian
    $\nabla^2 f(x_n)$, we can rewrite this
    \begin{align}
      f_n(x) = f(x_n) + (x-x_n)^T g_n + (x-x_n)^T H_n(x-x_n)
      \label{eq:newtonapprox}
    \end{align}
  \item Set $x_{n+1}$ as the minimum of this approximating function
    $f_n$. In other words, set $x_{n+1}$ such that $f'_n(x_{n+1})=0$:
    \begin{align}
      0 &= f'_n(x_{n+1}) = \nabla f_n(x_{n+1})\notag\\
      &= g_n + H_n (x_{n+1}-x_n)\notag\\\notag\\
      \Rightarrow \quad
      x_{n+1} &= x_n - H_n^{-1} g_n \label{eq:newtonupdate1}
    \end{align}
\end{enumerate}
Note that the update step in Equation~\ref{eq:newtonupdate1} can be
modified to
\begin{align}
  x_{n+1} &= x_n - \alpha H_n^{-1} g_n \label{eq:newtonupdate2}
\end{align}
for an $\alpha$ such that $f(x_{n+1})$ is sufficiently smaller than
$f(x_n)$. Intuitively, we use the direction implied by the Hessian and
gradient, but scale that direction by $\alpha$ so that we get a big
enough improvement. To choose $\alpha$, we might start at 1 and keep
increasing until we reach an acceptable improvement.

\subsubsection{A Note on Finding Roots}

Sometimes, rather than finding a minimum of some objective function,
Newton's method is presented as a way to find zeros of some function
$f$--i.e.\ to find $x$ such that $f(x)=0$.

The basic idea is the same; just tweak the goal from minimizing a
sequence of approximations to finding zeros of a sequence of
approximations. In particular,
\begin{enumerate}
  \item Use a Taylor Series expansion to write an approximation $f(x)$
    about $x_n$:
    \begin{align*}
      f(x) \approx f(x_n) + f'(x_n)(x-x_n)
    \end{align*}

  \item Find the zero of this simpler, linear approximation, and set
    that as our new guess:
    \begin{align}
      0 &= f(x_n) + f'(x_n)(x_{n+1}-x_n)\notag\\
      \Rightarrow \quad x_{n+1} &= x_n - \frac{f(x_n)}{f'(x_n)}
    \end{align}
\end{enumerate}
What's more, this can all be tied back to the optimization problems
above, minimizing $f$ is equivalent to finding zeros of $f'$.

\section{Quasi-Newton Methods}

Newton's method is all well and good with a few parameters, but it's a
pain in many dimensions, which requires computing and storing a large
inverse Hessian matrix $H_n$ every time we update. What's more, we don't
even really care about the Hessian matrix \emph{itself} when we update.
Just look at Equation~\ref{eq:newtonupdate2}. We only care about the
\emph{product} of the Hessian and gradient $H_n^{-1} g_n$. That's a
sufficient statistic for the update.

That thinking brings us to Quasi-Newton methods, which try to find a
smart, efficient way to \emph{updating} the Hessian recursively (from
some starting value), rather than recomputing every update.

\subsection{Secant Conditions}

Suppose that we want to update from $x_n$ to $x_{n+1}$. We will now
replace the true Hessian in $H_n$ with a approximate Hessian $B_n$ to
rewrite Equation~\ref{eq:newtonapprox} and form a \emph{new}
approximating function
\begin{align}
  b_n(x) = f(x_n) + (x-x_n)^Tg_n + (x-x_n)^T B_n (x-x_n)
  \label{eq:quasinewtonapprox}
\end{align}
Now for the main question: How to choose $B_n$? Well, one reasonable
choice of $B_n$ would be one that preserves the gradient of $f$ at $x_n$
and $x_{n-1}$.

Therefore, we say that our choice of $B_n$ satsifies the \emph{secant
conditions} if
\begin{align}
  \nabla b_n(x_n) &= \nabla f(x_n) = g_n \label{eq:secant}\\
  \nabla b_n(x_{n-1}) &= \nabla f(x_{n-1}) = g_{n-1}\notag
\end{align}
We compute the gradient of $b_n$ from
Equation~\ref{eq:quasinewtonapprox} to be
\begin{align*}
  \nabla b_n &= g_n + B_n(x-x_n)
\end{align*}
Together with Equations~\ref{eq:secant}, this implies
\begin{align*}
  \nabla b_n(x_n) - \nabla b_n(x_{n-1})
    &= g_n - g_{n-1} \\
  (g_n + B_n(x_n-x_n)) - (g_n + B_n(x_{n-1}-x_n))
  &= g_n - g_{n-1} \\
  B_n(x_{n}-x_{n-1}) &= g_n - g_{n-1}
\end{align*}
Switching to more compact, standard notation and expressing the
conditions in terms of the Hessian \emph{inverse}, the secant conditions
become where $s_n$ and $y_n$ are standard notation
\begin{align}
  s_n &= B_n^{-1}y_n \label{eq:secant_compact}\\\notag\\
  \text{where} \qquad
  s_n &= (x_{n}-x_{n-1}) \notag \\
  y_n &= (g_{n}-g_{n-1}) \notag
\end{align}
We want this to hold for our choice of $B^{-1}_n$, the approximation to
the inverse Hessian.

\subsection{BFGS Recursive Update}

Suppose that we have a starting, symmetric Hessian approximation $B^{-1}_n$,
that satisfies the secant conditions above in
Equation~\ref{eq:secant_compact}. How can we update to get $B^{-1}_{n+1}$
such that
\begin{enumerate}
  \item $B^{-1}_{n+1}$ also satisfies the secant conditions.
  \item $B^{-1}_{n+1}$ is symmetric.
  \item $B^{-1}_{n+1}$ is as close to $B^{-1}_n$ as possible, where
    ``closeness'' is defined using the weighted Frobenius norm.
\end{enumerate}
In notation, we want
\begin{align*}
  \min_{B^{-1}} &\quad ||B^{-1} - B^{-1}_n||^2_W \\
  \text{s.t.} &\quad s_{n+1} = B^{-1}y_{n+1} \\
              &\quad B^{-1} \text{\;symmetric}
\end{align*}
After some math, we can show that the Broyden, Fletcher, Goldfarb,
Shanno (BFGS) update satisfies this condition:
\begin{align}
  B_{n+1}^{-1} = B^{-1}_n
  + \frac{(y_n^T s_n + y_n^T B^{-1}_n y_n)(s_n s_n^T)}{(y_n^T s_n)^2}
  - \frac{s_n (B^{-1}_n y_n)^T + B^{-1}_n y_n s_n^T}{y_n^T s_n}
  %H = H0 + (1+(dg'*Hdg)/dgdx)*(dx*dx')/dgdx - (dx*Hdg'+Hdg*dx')/dgdx;
  \label{eq:bfgs}
\end{align}
This update will also ensure that $B^{-1}_{n+1}$ is positive
semi-definite whenever $B^{-1}_n$ is. It's not a condition we need to
impose a priori, since it follows from this updating procedure.

Given this, we don't have to compute the true Hessians $H_n$ every time.
Instead, we have reasonable (i.e.\ satisfying the secant conditions)
approximations $B_n$ that are updated via recurrence relation. To choose
the initial approximate Hessian $B_0$, we have a few options:
\begin{enumerate}
  \item Compute the true Hessian $H_0$, and then just apply this
    updating scheme from there.
  \item Use a diagonal approximation to the true Hessian.
  \item Use a scalar multiple of the identity matrix.
\end{enumerate}


\section{Linear Programming}

\subsection{Introduction to Linear Optimization}

\subsubsection{Definition of a Linear Program}

Linear optimization, also known as \emph{linear programming},
consists of a linear \emph{objective function} and \emph{linear
inequalities}:
\begin{align*}
   \max \; \;& c_1 x_1 + \cdots + c_n x_n \\
   \text{s.t.} \; \;&a_{11} x_1 + \cdots + a_{1n} x_n \leq b_n \\
   & \qquad\qquad \vdots \\
   &a_{m1} x_1 + \cdots + a_{mn} x_n \leq b_m
\end{align*}
So we want to find the values of the \emph{variables}
$x_1, \ldots, x_n \in \mathbb{R}$ such that
the objective function is maximized while still satisfying the
linear inequalities. Using more convenient matrix notation,
our linear program above can be rewritten more compactly as
\begin{equation}
   \label{lp}
   \max \{ c^T x \; : \; x \in \mathbb{R}^n , \; Ax \leq b \}
\end{equation}
If we want to find the minimum instead of the max, we can easily
use the fact that the minimum of some set $S$ ($\min S$) is equivalent
to $-\max S$.

\subsubsection{Feasability, Optimality, Boundedness}

We call a solution $x \in \mathbb{R}^n$ \emph{feasible} if $x$
satisfies all the linear inequalities.  We call a linear program
feasible if it has a feasible solution.
\\
\\
A feasible solution $s \in \mathbb{R}^n$ is an \emph{optimal} solution
of a linear program if $c^T x \geq c^T y $ for all $y\in \mathbb{R}^n$.
\\
\\
A linear program is \emph{bounded} if there exists a constant $M\in
\mathbb{R}$ such that $c^T x \leq \mathbb{R}$ for all feasible
$x \in \mathbb{R}^n$.

\subsubsection{Linear Algebra Defintions}

Recall that for $A \in \mathbb{R}^{m\times n}$, the \emph{kernel}
of $A$, the \emph{image} of $A$, and the \emph{rowspace}
of $A$ are defined
\begin{align*}
   \text{ker}(A) &= \{ x \in \mathbb{R}^n \; | \; Ax =0 \} \subset
      \mathbb{R}^n\\
   \text{im}(A) &= \{ A x \; | \; x\in \mathbb{R}^n \} \subset
      \mathbb{R}^m\\
   \text{rowspace}(A) &= \{ y \in \mathbb{R}^n \; | \; \exists \lambda
   \in \mathbb{R}^m \text{ s.t. } A^T \lambda = y \}
\end{align*}
Note, the rowspace is just the set of all linear combinations of the
rows of $A$.
\\
\\
{\sl Result:}
With these definitions in hand, we can show that a solution to the
linear program in Equation \ref{lp} is feasible and unbounded if
$b \in \text{im}(A)$ and if $c \in \text{ker} A \setminus \{ 0 \}$.
\begin{proof}
   First, $b \in \text{im}(A)$ proves feasibility. It also implies that
   $\exists\; x^* \in \mathbb{R}^n$ such that $A x^* = b$. This
   and the fact that $c \in \text{ker}(A)\setminus \{0\}$
   allows us to write
   \begin{align*}
      A(x^* + \lambda c) &= Ax^* + \lambda A c \\
      &= b + 0 = b
   \end{align*}
   which proves $x^* + \lambda c$ is a feasible solution, satisfying
   all of the linear inequalities.
   \\\\
   So to finish up the unboundedness proof, we suppose the opposite:
   that the linear program is bounded:
      \[ c^T y \leq M \qquad \forall y \in \mathbb{R}^n \]
   Since $x^* + \lambda c$ is feasible, then it should also be bounded:
   \begin{align*}
      \Rightarrow \quad c^T (x^* + \lambda c ) &\leq M\\
      c^T x^* + \lambda c^T c \leq M
   \end{align*}
   Now since $c \neq 0$, we know that $c^T c$ will be greater than
   0. Now let's rearrange and choose
      \[ \lambda \geq \frac{M - c^T x^*}{c^T c} \]
   in which case $c^T ( x^* + \lambda c) > M$.
\end{proof}



\newpage
\subsection{The Geometry of Linear Programming}

\subsubsection{General Definitions}

{\sl Polyhedron:}
A set $P$ of vectors in $\mathbb{R}^n$ is a polyhedron
if $P=\{ x\in \mathbb{R}^n \; | \; Ax \leq b \}$ for some matrix $A$
and some vector $b$.  By our definition of a linear program in
Equation \ref{lp}, we see that the set of feasible solutions is
a polyhedron.\footnote{Just one quick note for generality: The empty
set, $\emptyset$, is also a polyhedron as $\emptyset = \{ x \in
\mathbb{R}^n \; |\; \mathbf{0}^T x \leq -1 \}$,
satisfying the definition of a polyhedron.}
\\
\\
{\sl Half space:} This is defined as $\{ x \in \mathbb{R}^n
   \; | \; a^T x \leq \beta \}$, where $a \in \mathbb{R}^n \setminus
   \mathbf{0}$.
\\\\
{\sl Hyperplane:} This is defined as $\{ x \in \mathbb{R}^n
   \; | \; a^T x = \beta \}$, where $a \in \mathbb{R}^n \setminus
   \mathbf{0}$.
\\\\
{\sl Valid:} An inequality $a^T x \leq \beta$ is valid for a
   polyhedron, $P$, if each $x^* \in P$ satisfies $a^T x^* \leq
   \beta$.
\\\\
{\sl Active:} An inequality $a^T x \leq \beta$ is active
   at $x^* \in \mathbb{R}^n$ if $a^T x^* = \beta$.


\subsubsection{Vertices}

We'll offer three equivalent definitions, each of which offers some
different intuition or an additional operational advantage:
\begin{enumerate}
   \item A point $x^* \in P$ is a \emph{vertex} of $P$ if
      there exists an inequality $a^T x \leq \beta$ such that
      \begin{enumerate}
	 \item $a^T x \leq \beta$ is valid for $P$.
	 \item $a^T x \leq \beta$ is active at $x^*$ and not active
	    at any other point in $P$.
      \end{enumerate}
   \item There's also an equivalent definition which says $x^* \in P$
      is a vertex if and only if there exists a vector $c \in
      \mathbb{R}^n$ such that $x^*$ is the unique optimal solution of
      the linear program $\max \{ c^T x \; | \; x \in P \}$.
   \item Suppose we have $x^* \in P = \{ x \in \mathbb{R}^n \; | \;
      Ax \leq b \}$ as a vertex of $P$.
      Then this vertex satisfies a smaller number of active contraints
      (relative to all of the constraints that define the linear
      program and $P$). Therefore, we express those active constraints
      that we want to enforce with $\bar{A}x \leq \bar{b}$.

      Now since $x^*$ is a vertex satisfying those
      certain active constraints, that means $x^*$ is the unique
      solution of $\bar{A} x = \bar{b}$. That's equivalent to the
      statement that $\text{rank}(\bar{A}) = n$, which is equivalent
      to the columns of $\bar{A}$ being linearly independent.

      So if we suspect that $x^*$ is a vertex of our linear program,
      then we examine the subsystem, $\bar{A} x \leq \bar{b}$,
      of our LP that $x^*$ satisfies with equality, and we look
      at the rank of $\bar{A}$. If $x^*$ really is a vertex of $P$,
      the the rank of $\bar{A}$ equals $n$ and the columns will
      be linearly independent.

\end{enumerate}





\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf}
%	 }
%      }
%   \end{figure}


%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
