\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Bellman Equation}
\date{\today}
\usepackage{fullpage}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{hyperref} 
\hypersetup{	
    colorlinks,		% This colors the links themselves, not boxes
    citecolor=black,	% Everything below changes the link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

%\tableofcontents %adds it here


\section{Shortest Path Problem}

To get the intuition for the Bellman Equation, we will consider the shortest path problem, which has the following features:
\begin{enumerate}

    \item You want to get from point $1$ to $N$.

    \item There are a number of possible nodes in between $1$ and $N$,
	permitting a number of different possible paths you might take.

    \item The paths from node to node have different costs.

\end{enumerate}

Therefore, we want a general solution to the problem that
provides us with the \emph{least cost} path to travel from
$1$ to $N$. 


\subsection{The Bellman Equation}

Suppose that at every single node, $i \in \{1, \ldots, N\}$, we \emph{knew}
the least-cost path to get from node $i$ to node $N$. Of course we don't,
unless we're trivially already at node $N$, but just suspend disbelief for
a second and suppose we do.

Now since we know the least-cost paths, we know the ``best-case'' cost to
get from node $i$ to $N$, for all $i$. Denote that ``best-case'' cost
by $J(i)$. Now it makes sense that this function
$J$, for each $i$, should also satisfy what's called the
\emph{Bellman Equation}:
\begin{equation}
    \label{bellman}
    J(i) = \min_{j \in F_i} \left\{ c(i, j) + J(j) \right\}
\end{equation}
In words, this equation says
\begin{enumerate}
    \item To find the best-case cost to go from $i$
	to $N$, consider the set of nodes that you can reach from node $i$,
	denoted $F_i$.

    \item Next, consder the cost of jumping from node $i$ to node $j$,
	denoted $c(i,j)$.

    \item Finally, since you know the best-case cost,
	$J(j)$ for $j \in F_i$, pick the minimum of $c(i,j)$ plus $J(j)$.
\end{enumerate} 

The best solution to our problem must, therefore, satisfy Equation
\ref{bellman}.

\subsection{Solving for the Minimum Cost Function, $J$}

We now detail the standard algorithm to find $J$.
\begin{enumerate}
    \item For some large $M$, set
	\begin{equation}
	    J_0(i) = \begin{cases} M & i \neq N \\ 
				    0 & \text{otherwise}
		    \end{cases}
	\end{equation}

    \item Next, set 
	$J_{n+1}(i) = \min_{j \in F_i} \left\{ c(i,j) + J_n(j)\right\}$
	for all $i$.

    \item If $J_{n+1} \neq J_n$, increment $n$ and return to step 2.

\end{enumerate}
The solution thus propagates back from the destination node, $N$.


\section{Infinite Horizon Dynamic Programming}

\subsection{The Problem and the Objective}

Suppose that an infinitely-lived agent gets utility in period
$t$ from consuming $y_t$. The variable $y_t$ is called the
\emph{control}; its value is chosen by the agent at time $t$
given some pre-determined value, $x_t$. Between periods,
$x_{t+1}$ is a deterministic function of $x_t$ and $y_t$, i.e.
\begin{align}
  \label{xevol}
  x_{t+1} = g(x_t, y_t)
\end{align}
subject to initial condition $x_0$.

Next, our problem is to maximize discounted lifetime utility: 
\begin{align}
  \label{toughproblem}
  \sum^\infty_{t=0} \beta^t u(y_t)
  \qquad \beta\in (0,1)
\end{align}
by choosing an feasible optimal path $\{ y_t \}_0^\infty$, again
subject to an initial condition $x_0$.  Here,
``optimal'' means utility maximizing. Typically, feasibility
will be determined by some sort of resource constraint or
restriction to non-negativity of the sequence(s) $\{ y_t \}$
and/or $\{x_t\}$. 

To make things simpler, we also define a bit of notation for feasibility. At time $t$, the set of all feasible $y_t$ is denoted $\mathcal{F}_t$. Therefore, $y_t$ is feasible if $y_t\in \mathcal{F}_t$. More generally, a sequence $\{y_t\}^\infty_0$, abbreviated $\{y_t\}$, is feasible if $\{y_t\} \subset \mathcal{F}$.

\subsection{Value Function and Bellman Equation}

For reasons that we'll see soon, we'll want to define the Bellman
Equation for our optimization problem. That first entails
defining the value function, $v^*$: 
\begin{align*}
  v^*(x_0) = \sup_{ \{y_t\} \subset \mathcal{F}}
  \sum^\infty_{t=0} \beta^t u(y_t)\; 
  \qquad
  \text{$x_0$ given}
\end{align*}
With the value function defined, we can now write the Bellman Equation, which looks very similar to the Shortest Path problem described above:
\begin{equation}
  v^*(x_t) = \max_{y_t \in \mathcal{F}_t} 
  \left\{ u(y_t) + \beta \; v^*\big(g(x_t, y_t)\big) \right\}
\end{equation}



\subsection{Policy Function Approach}

Choosing an optimal path $\{y_t\}_0^\infty$ directly is hard. It
amounts to selecting a point in infinite-dimensional space.
But in most cases, we can do something easier. Specifically,
we'll seek a \emph{policy function}---some function or
``decision rule'' $\sigma$ such that 
\begin{align}
  \label{markov}
  y_t = \sigma(x_t) = \sigma(g(x_{t-1}, y_{t-1}))
  \qquad \sigma \in \Sigma
\end{align}
where $\Sigma$ is the set of all feasible policies. 

Now a well-known result from dynamic programming says that, in
the types of problems we want to consider, an optimal
consumption sequence must be Markov just as it is above in
Equation \ref{markov}, so that a policy function $\sigma$ will,
indeed, exist and everything will work. This function $\sigma$
will also be time-homogeneous, so it will remain the same for
all $t$.

Now we can rewrite the problem as expressed in
(\ref{toughproblem}) and (\ref{xevol}) in terms of the policy
function:
\begin{align}
  \label{policyapproach}
  \max_{\sigma \in \Sigma} 
  &\left\{ \sum^\infty_{t=0} \beta^t u(\sigma(x_t)) \right\}\\
  \text{where} &\quad
  x_{t+1} = g(x_t, \sigma(x_t)) \notag
\end{align}
where the initial condition $x_0$ is given. Since $g$ is
assumed deterministic, we see that the only unknown in the
problem written above is the policy function, $\sigma$. We have
everything else we need.

So we see the crux of the policy function approach. Rather than
specifying a utility-maximizing infinite sequence
$\{y_t\}^\infty_0$, we find a single function $\sigma$ that
provides a map or decision rule to go from initial condition ($x_t$) to optimal choice for the control ($y_t$).
This choice then propagates to the next period via deterministic
$g$ to define the next initial condition.

\subsection{Dynamic Programming Solution}

We will solve for the optimal policy function using dynamic
programming. But first, for each feasible policy $\sigma\in
\Sigma$, define a \emph{policy value
function}:
\begin{align*}
  v_\sigma(x_0) :=
  \left\{ \sum^\infty_{t=0} \beta^t u(\sigma(x_t)) \right\}
\end{align*}
This gives the utility conditional on a policy and a initial
condition.  Our goal will be to choose the \emph{optimal}
policy, i.e. the policy function that yields the highest utility
over the set of all $\sigma \in \Sigma$.

Explicitly, we define \textbf{the} \emph{value function} for
our optimization problem as 
\begin{align}
  \label{valfn}
  v^*(x_0) := \sup_{\sigma\in\Sigma} \; v_\sigma(x_0)
\end{align}
A policy is called ``optimal'' if it attains the supremum in
(\ref{valfn}) for all $x_0$. 
\\
\\
We can better understand Our just-defined function $v^*$ if we
compare it to the minimum-cost function $J$ in the Shortest Path
problem above.
\begin{enumerate}
  \item Here, $v^*$ is a function of only the initial condition,
    as $J$ was a function only of the starting node.
  \item Moreover, $v^*$ does not depend upon a policy since we
    optimize over all policies. Similarly, $J$ didn't depend on
    the path we took since we optimized over all paths.
\end{enumerate}

Now with this definition of the value function in hand, we can
define a Bellman Equation for our optimization problem: 
\begin{align*}
  v^*(x) = \max_{y} \left\{ u(y) + \beta v^*( \right\}
\end{align*}








%%%% APPPENDIX %%%%%%%%%%%

% \appendix

%\cite{LabelInSourcesFile} 
%\citep{LabelInSourcesFile} Cites in parens
%\nocite{LabelInSourceFile} includes in refs w/o specific citation
%\bibliographystyle{apalike} 
%\bibliography{sources.bib} where sources.bib is file




\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
