\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Bellman Equation}
\date{\today}
\usepackage{fullpage}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{hyperref} 
\hypersetup{	
    colorlinks,		% This colors the links themselves, not boxes
    citecolor=black,	% Everything below changes the link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

%\tableofcontents %adds it here


\section{Shortest Path Problem}

To get the intuition for the Bellman Equation, we will consider the shortest path problem, which has the following features:
\begin{enumerate}

    \item You want to get from point $1$ to $N$.

    \item There are a number of possible nodes in between $1$ and $N$,
	permitting a number of different possible paths you might take.

    \item The paths from node to node have different costs.

\end{enumerate}

Therefore, we want a general solution to the problem that
provides us with the \emph{least cost} path to travel from
$1$ to $N$. 


\subsection{The Bellman Equation}

Suppose that at every single node, $i \in \{1, \ldots, N\}$, we \emph{knew}
the least-cost path to get from node $i$ to node $N$. Of course we don't,
unless we're trivially already at node $N$, but just suspend disbelief for
a second and suppose we do.

Now since we know the least-cost paths, we know the ``best-case'' cost to
get from node $i$ to $N$, for all $i$. Denote that ``best-case'' cost
by $J(i)$. Now it makes sense that this function
$J$, for each $i$, should also satisfy what's called the
\emph{Bellman Equation}:
\begin{equation}
    \label{bellman}
    J(i) = \min_{j \in F_i} \left\{ c(i, j) + J(j) \right\}
\end{equation}
In words, this equation says
\begin{enumerate}
    \item To find the best-case cost to go from $i$
	to $N$, consider the set of nodes that you can reach from node $i$,
	denoted $F_i$.

    \item Next, consder the cost of jumping from node $i$ to node $j$,
	denoted $c(i,j)$.

    \item Finally, since you know the best-case cost,
	$J(j)$ for $j \in F_i$, pick the minimum of $c(i,j)$ plus $J(j)$.
\end{enumerate} 

The best solution to our problem must, therefore, satisfy Equation
\ref{bellman}.

\subsection{Solving for the Minimum Cost Function, $J$}

We now detail the standard algorithm to find $J$.
\begin{enumerate}
    \item For some large $M$, set
	\begin{equation}
	    J_0(i) = \begin{cases} M & i \neq N \\ 
				    0 & \text{otherwise}
		    \end{cases}
	\end{equation}

    \item Next, set 
	$J_{n+1}(i) = \min_{j \in F_i} \left\{ c(i,j) + J_n(j)\right\}$
	for all $i$.

    \item If $J_{n+1} \neq J_n$, increment $n$ and return to step 2.

\end{enumerate}
The solution thus propagates back from the destination node, $N$.



\clearpage
\section{Infinite Horizon Dynamic Programming}


\subsection{The Problem and the Objective}

Suppose that an infinitely-lived agent gets utility in period
$t$ from consuming $y_t$. The variable $y_t$ is called the
\emph{control}; its value is chosen by the agent at time $t$
given some pre-determined value, $x_t$, called the \emph{state}.
Between periods, the future state $x_{t+1}$ is a deterministic function of $x_t$ and
$y_t$, i.e.
\begin{align}
  \label{xevol}
  x_{t+1} = g(x_t, y_t)
  \qquad
  \text{$x_0$ given}
\end{align}
Typically, the function $g$ will
involve some tradeoff between $y_t$ and $x_{t+1}$. So given
$x_t$, the more $y_t$ an agent chooses today, the less $x_{t+1}$ he
gets next period. 

Next, our problem is to maximize discounted lifetime utility: 
\begin{align}
  \label{toughproblem}
  \sum^\infty_{t=0} \beta^t u(y_t)
  \qquad \beta\in (0,1)
\end{align}
by choosing a feasible and optimal path $\{ y_t \}_0^\infty$, again
subject to an initial condition $x_0$.  Here,
``optimal'' means utility maximizing. Typically, feasibility
will be determined by some sort of resource constraint
(such as $y_t\leq x_t$ or $y_t\leq f(x_t)$) or some
restriction to non-negativity of the sequence(s) $\{ y_t
\}$ and/or $\{x_t\}$. 

To make things simpler, we will now define a bit of
notation for feasibility. At time $t$, the set of all
feasible $y_t$ is denoted $\mathcal{F}_t$. Therefore, $y_t$
is feasible if $y_t\in \mathcal{F}_t$. More generally, a
sequence $\{y_t\}^\infty_0$, abbreviated $\{y_t\}$, is
feasible if $\{y_t\} \subset \mathcal{F}$.


\subsection{Value Function and Bellman Equation}

For reasons that we'll see soon, we'll want to define the
Bellman Equation for our optimization problem. That first
entails defining the value function, $v^*$: 
\begin{align*}
  v^*(x_0) = \sup_{ \{y_t\} \subset \mathcal{F}}
  \sum^\infty_{t=0} \beta^t u(y_t)\; 
  \qquad
  \text{$x_0$ given}
\end{align*}
In words, the value function $v^*(x_0)$ describes that
maximum discounted value that can be obtained from initial
state $x_0$.

With the value function defined, we can now write the
Bellman Equation, which looks very similar to what we had
for the Shortest Path problem described above:
\begin{align}
  v^*(x_t) &= \max_{y_t \in \mathcal{F}_t} 
  \left\{ u(y_t) + \beta \; v^*\big(g(x_t, y_t)\big) \right\} \\
    \text{By (\ref{xevol})} \quad \Leftrightarrow \quad 
  &= \max_{y_t \in \mathcal{F}_t} 
    \left\{ u(y_t) + \beta v^*(x_{t+1}) \right\} \notag
\end{align}
Intuitively, this equation says that you maximize the value from the
current state, $x_t$, by trading off current reward, $u(y_t)$, in
exchange for the discounted future value of the resulting state,
$\beta v^*(x_{t+1})$.

Since we've formulated the problem in Bellman terms, it's
clear that we'll eventually want to apply dynamic
programming methods to solve for the value function $v^*$.
That's what we did in the Shortest Path problem above; it's
likely what we'll want to do again.  


\emph{However}, solving for the value function $v^*$ above
doesn't \emph{quite} give us what we want. We don't just
want to know the maximum lifetime utility of an agent; we
want to know what path they will choose for the control.
In other words, we want to know $\{y_t\}_0^\infty$. 

To solve for this sequence, we now discuss the policy function approach,
which allows us to answer this question while also redefining the
motivating problem and the Bellman Equation in simpler, more practical
terms. 


\subsection{The Policy Function Approach}

To start, note that even if we can solve for the value function,
$v^*(x_0)$ (and we can't yet), our real goal of directly specifying the
optimal path $\{y_t\}_0^\infty$ is hard. It amounts to selecting a point
in infinite-dimensional space.  So we'll do something easier.
Specifically, we'll seek a \emph{policy function}---some function or
``decision rule'' $\sigma$ that depends only upon the current state such
that 
\begin{align}
  \label{markov}
  y_t = \sigma(x_t) = \sigma(g(x_{t-1}, y_{t-1}))
  \qquad \sigma \in \Sigma
\end{align}
where $\Sigma$ is the set of all feasible policies. So rather than
trying to say \emph{what} an agent will choose, we'll describe
\emph{how} that agent will choose.

Now a well-known result from dynamic programming says that, in the types
of problems we want to consider, an optimal consumption sequence must be
Markov just as in Equation \ref{markov} above, so that a policy function
$\sigma$ will, indeed, exist.  This function $\sigma$ will also be
time-homogeneous, so it will remain the same for all $t$.  This is all
just to say that we're doing something reasonable that will allow us to
find the optimal path $\{y_t\}$.

\subsubsection{Adapting the Problem and Objective}

So first, we can rewrite our motivating problem as
expressed in (\ref{toughproblem}) and (\ref{xevol}) in
terms of the policy function:
\begin{align}
  \label{policyapproach}
  \max_{\sigma \in \Sigma} 
  &\left\{ \sum^\infty_{t=0} \beta^t u(\sigma(x_t)) \right\}\\
  \text{where} &\quad
  x_{t+1} = g(x_t, \sigma(x_t)) 
  \qquad \text{$x_0$ given}
  \notag
\end{align}
Since $g$ is assumed deterministic, we see that the only
unknown in the problem written above is the policy
function, $\sigma$. We have everything else we need.

This is the crux of the policy function approach.
Rather than having our unknown be a utility-maximizing
infinite sequence $\{y_t\}^\infty_0$, we find a single
function $\sigma$ that provides a map or decision rule to
go from the state ($x_t$) to the optimal choice for the
control ($y_t$).  This choice then propagates to the next
period via deterministic $g$ to define the state next
period.

\subsubsection{Adapting the Value Function}

Now that we've translated the basic problem into policy
function terms, let's see what that means for the value
function and Bellman equation we defined above.

So first, for each feasible policy $\sigma\in
\Sigma$, define a \emph{policy value
function}: \begin{align*}
  v_\sigma(x_0) :=
  \left\{ \sum^\infty_{t=0} \beta^t u(\sigma(x_t)) \right\}
\end{align*}
This gives the utility conditional on a policy and an initial
condition.  Our goal will be to choose an \emph{optimal} policy,
i.e. a policy function that yields the highest utility over the
set of all $\sigma \in \Sigma$.

Therefore, our definition of the \emph{value function} $v^*$ can
be rewritten as
\begin{align}
  \label{valfn}
  v^*(x_0) := \sup_{\sigma\in\Sigma} \; v_\sigma(x_0)
\end{align}
A policy is called ``optimal'' if it attains the supremum in
(\ref{valfn}) for all $x_0$. 

\subsubsection{Policies and the Bellman Equation}

Finally, let's finish off the last link in the chain and consider the
Bellman Equation in terms of a policy function. As we'll see in the next
section, we'll actually 

First, let's define a concept: Consider some continuous function $w$.
We say that a policy $\sigma\in\Sigma$ is \emph{$w$-greedy} if
$\sigma(x)$ solves 
\begin{align*}
  \max_{y_t \in F_t} 
  \left\{ u(y_t) + \beta w\big(g(x_t, y_t)\big)\right\}
\end{align*}
There might be many policies that solve this; therefore, there might be
a set of $w$-greedy policies given any initial state $x_t$ and functions
$u$, $g$, and $w$. 

Now this works for \emph{any} continuous function $w$. The function $w$ could even
be our value function $v^*$, provided it's continuous. We'll talk about
the conditions under which 

\subsection{Dynamic Programming Solution}

In this subsection, we now solve the problem as formulated in policy
function terms since, for the reasons given above, that will be
easiest. 







%%%% APPPENDIX %%%%%%%%%%%

% \appendix

%\cite{LabelInSourcesFile} 
%\citep{LabelInSourcesFile} Cites in parens
%\nocite{LabelInSourceFile} includes in refs w/o specific citation
%\bibliographystyle{apalike} 
%\bibliography{sources.bib} where sources.bib is file




\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
