\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Probability Theory}
\date{\today}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}

%% Formatting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=1in]{geometry}
    %   Adjust the margins with geometry package
%\usepackage{pdflscape}
    %   Allows landscape pages
%\usepackage{layout}
    %   Allows plotting of picture of formatting



%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %   Make the header bigger to avoid overlap

%\fancyhf{}
    %   Erase header settings

%\renewcommand{\headrulewidth}{0.3pt}
    %   Width of the line

%\setlength{\headsep}{0.2in}
    %   Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
%\numberwithin{equation}{section} % Number equations by section
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{courier}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{%
    colorlinks,
        %   This colors the links themselves, not boxes
    citecolor=black,
        %   Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\definecolor{codegreen}{RGB}{28,172,0}
\definecolor{codelilas}{RGB}{170,55,241}

% David4 color scheme
\definecolor{d4blue}{RGB}{100,191,255}
\definecolor{d4gray}{RGB}{175,175,175}
\definecolor{d4black}{RGB}{85,85,85}
\definecolor{d4orange}{RGB}{255,150,100}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %   For including verbatim code from files, no colors
\usepackage{listings}
    %   For including code snippets written directly in this doc

\lstdefinestyle{bash}{%
  language=bash,%
  basicstyle=\footnotesize\ttfamily,%
  showstringspaces=false,%
  commentstyle=\color{gray},%
  keywordstyle=\color{blue},%
  xleftmargin=0.25in,%
  xrightmargin=0.25in
}

\lstdefinestyle{matlab}{%
  language=Matlab,%
  basicstyle=\footnotesize\ttfamily,%
  breaklines=true,%
  morekeywords={matlab2tikz},%
  keywordstyle=\color{blue},%
  morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
  identifierstyle=\color{black},%
  stringstyle=\color{codelilas},%
  commentstyle=\color{codegreen},%
  showstringspaces=false,%
    %   Without this there will be a symbol in
    %   the places where there is a space
  numbers=left,%
  numberstyle={\tiny \color{black}},%
    %   Size of the numbers
  numbersep=9pt,%
    %   Defines how far the numbers are from the text
  emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
    %   Some words to emphasise
}

\newcommand{\matlabcode}[1]{%
    \lstset{style=matlab}%
    \lstinputlisting{#1}
}
    %   For including Matlab code from .m file with colors,
    %   line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %   Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %   For bibliographies
\usepackage{pdfpages}
    %   For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Misc Math
\newcommand{\Prb}{\mathrm{P}}
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\text{diag}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}
\newcommand{\one}{\boldsymbol{1}}

% Script
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sA}{\mathscr{A}}

% Mathcal
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}

% Blackboard
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rk}{\mathbb{R}^n}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}

\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

% Redefine real and imaginary from fraktur to plain text
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}

% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle
\tableofcontents

\clearpage

\section{Introduction}

This document aims to bring together the elements of probability,
measure theory, and stochastic calculus useful for the anlysis of
stochastic process that have particular relevance in finance and
economics.

\section{Finite Probability Spaces}

\subsection{Finite Probability Space}

\begin{defn}{(Sample Space)}
The \emph{sample space} $\Omega=\{\omega_1,\ldots,\omega_N\}$ is the set
of all \emph{elementary} outcomes of some statistical experiment. For a
given experiment, only one $\omega \in \Omega$ can occur when we conduct
and observe that experiment.
\end{defn}
\begin{ex}
If our experiment consists of flipping a single coin, the sample space
consisting of elementary events is $\{H,T\}$. If we flip two coins, the
sample space is $\{HH,HT,TH,TT\}$.
\end{ex}
\begin{rmk}
The previous example suggests that there's some subtlety in how you
define ``elementary'' event. In the case where we flipped two coins, it
might seem like a single flip is more ``elementary.'' However, if we define
our experiment to be the flipping of two coins, the set of outcomes from
that experiment really is $\Omega=\{HH,HT,TH,TT\}$. It's an unfortunate
bit of nomenclatural confusion.

Of course, we'll fix this quite soon when we encounter \emph{product
spaces}, which resolve a lot of this subtlety. In particular, product
spaces will let us define a narrow product space like $\Omega=\{H,T\}$
and then construct a richer probability space like
$\Omega\times \Omega = \{HH,HT,TH,TT\}$ or even $\Omega^N$, building up
from relatively simples ones.
\end{rmk}

\begin{defn}{(Finite Probability Space)}
A \emph{finite probability space} is a pair $(\Omega,P)$ consisting of a
finite sample space $\Omega=\{\omega_1,\ldots,\omega_N\}$ of elementary
events together with probabilities $\{p_1,\ldots,p_N\}$ satisfying
\begin{align*}
  \sumnN p_n = 1
  \quad\text{and}\quad
  p_n \in [0,1] \quad \forall n=1,\ldots,N
\end{align*}
where $p_n$ is the probability of elementary event $\omega_n\in\Omega$
occurring.
\end{defn}

\begin{defn}{(Event)}
An \emph{event} is any subset $A\subseteq \Omega$ of the sample space.
\end{defn}
\begin{ex}
Given sample space $\Omega=\{HH,HT,TH,TT\}$ for an experiment where we
toss two coins, we might consider the event $A=\{HH,HT\}$ which is
``heads tossed first.''
\end{ex}


\begin{defn}{(Set Algebra)}
A collection $\calF$ of subsets of $\Omega$ (i.e.\ a collection of
events) is called a \emph{set algebra}, or simply \emph{algebra}, if it
satisfies the following three properties:
\begin{enumerate}
  \item $\Omega\in\calF$
  \item $A,B\in\calF \implies A\cup B\in\calF$
  \item $A\in\calF\implies A^c\in\calF$.
\end{enumerate}
\end{defn}
\begin{rmk}
This is not to be confused with a $\sigma$-algebra, which we'll see
later. A set algebra is closed only under finite unions, which is not
going to be a problem in a finite probability space. Once we move to
infinite probability spaces, we will need $\sigma$-algebras, which are
closed under \emph{infinite} unions. But more on that later.

At any rate, the reason we care about algebras is that they collect and
contain the sets---the events---to which we will assign probabilities.
Our notion of a probability will be defined for elements in this
collection, as we see in the next definition.
\end{rmk}


\begin{defn}{(Probability Measure)}
\label{defn:finitemeasure}
Given a finite probability space, define the \emph{probability measure}
on $\Omega$ as the mapping
\begin{align*}
  \Prb&: 2^\Omega \rightarrow [0,1] \\
  \Prb[A] &:= \sum_{\{n|\omega_n\in A\}} p_n
\end{align*}
In this way, we use the probabilities $\{p_1,\ldots,p_n\}$ over the
elementary events to construct the probabilites of any event
$A\subseteq \Omega$ or $A\in\calF$.

Probability measures display the following properties
\begin{enumerate}
  \item $\Prb[\omega_n]=p_n$ for $\omega_n\in\Omega$
  \item $\Prb[\emptyset] = 0$
  \item $\Prb[\Omega] = 1$
  \item $\Prb[A\cup B] = \Prb[A] + \Prb[B] - \Prb[A\cap B]$ for any
    events $A,B$.
\end{enumerate}
\end{defn}


\begin{defn}{(Random Variable)}
A \emph{random variable} on finite probability space $\Omega$ is a
function
\begin{align*}
  X: \Omega\ra \R
\end{align*}
Since it's inconvenient to deal with sets and $\omega$'s in $\Omega$
directly, we use random variables that map these elements into the real
line. But in general, we only know how to assign probabilities to
events in $\Omega$. How can assign probabiilities to random variables?
That leads us to the next definition.
\end{defn}

\begin{defn}{($\calF$-Measurable)}
A random variable $X$ is $\calF$-measurable if
\begin{align*}
  \{X=x\} :=
  X^{-1}(x) =
  \{\omega\in\Omega \; | \; X(\omega)=x\}
  \in \calF
\end{align*}
for all $x\in\R$. The idea is that Definition~\ref{defn:finitemeasure}
tells us how to assign probabilities to events or ``measure'' events in
$\calF$. To ``measure'' or assign probabilities to random variables
taking values on the real line, we need to be able to map those values
back into the events in $\calF$ that we do know how to measure.

Not surprisingly, we differentiate between algebras and say
$\calF$-measurable because random variables might be measurable with
respect to some algebras and not others.
\end{defn}

\subsection{Product Spaces}

\begin{defn}{(Product Space)}
Given two finite probability spaces $(\Omega,P)$ and $(\Omega',P')$,
define a new probability space called the \emph{product space}
$(\Omega\times\Omega', P \otimes P')$ as
\begin{align*}
  \Omega\times\Omega'
  &= \{(\omega,\omega') \; |\; \omega\in\Omega, \; \omega'\in\Omega'\}
  \\
  (P \otimes P')(\omega, \omega')
  &= \Prb[\omega]\cdot \Prb[\omega']
\end{align*}
\end{defn}

\begin{defn}{(Lifting Random Variables to a Product Space)}
Suppose that we have events $A\subseteq \Omega$ and
$B\subseteq \Omega'$. We want to make sure that the same events are
defined on product space $\Omega\times \Omega'$. Then we can define new
sets
\begin{align*}
  \hat{A} &:= A\times \Omega' \\
  \hat{B} &:= \Omega \times B
\end{align*}
The probabilities of these events will be exactly the same on $\Omega
\times \Omega'$ as they were on just $\Omega$ or just $\Omega'$ because
notice
\begin{align*}
  (\Prb\otimes \Prb')[\hat{A}] &= \Prb(A) \cdot \Prb'(\Omega') = \Prb(A) \cdot 1\\
  (\Prb\otimes \Prb')[\hat{B}] &= \Prb(\Omega) \cdot \Prb'(B) = 1\cdot \Prb'(B)
\end{align*}
Moreover, $\hat{A}$ and $\hat{B}$ will be independent events, which
makes sense given that they started by having nothing to do with each
other, hanging out completely on their own in $\Omega$ or $\Omega'$.
\end{defn}

\begin{defn}{(Lifting Random Variables to a Product Space)}
We can easily ``lift'' random variables on $\Omega$ or $\Omega'$ into
the new product space $\Omega\times \Omega'$. To do so, suppose that we
have random variables $X:\Omega\ra\R$ and $Y:\Omega'\ra\R$. Then define
\begin{align*}
  \forall (\omega,\omega') \in \Omega\times \Omega' \qquad
  \begin{cases}
  \hat{X}(\omega, \omega') &= X(\omega) \\
  \hat{Y}(\omega, \omega') &= Y(\omega')
  \end{cases}
\end{align*}
Then $\hat{X}$ and $\hat{Y}$ are RVs $X$ and $Y$ suitably adapted to
live on probability space $(\Omega,\Omega')$.
\end{defn}

\clearpage
\section{Inequalities}

\begin{prop}{\emph{(Jensen's Inequality)}}
Let $g$ be a function and let $X$ be some random variable. Then
\begin{align*}
  \text{$g$ convex} &\implies \E[g(X)] \geq g\left(\E[X]\right) \\
  \text{$g$ concave} &\implies \E[g(X)] \leq g\left(\E[X]\right) \\
\end{align*}
\end{prop}
\begin{proof}
Since $g$ is a convex function, given any $x$, there is a constant $a$
such that
\begin{align*}
  g(y) \geq g(x) + a (y-x)
\end{align*}
Choose $x=\E[X]$. Then we can say that there is an $a$ such that
\begin{align*}
  g(y) &\geq g(\E[X]) + a (y-\E[X])
\end{align*}
But this must also be true for $y=X$, so
\begin{align*}
  g(X) &\geq g(\E[X]) + a (X-\E[X])
\end{align*}
Now take expectation
\begin{align*}
  \E[g(X)]
  &\geq \E[g(\E[X]) + a (X-\E[X])] \\
  &= \E[g(\E[X])] + a \E[(X-\E[X])] \\
  &= g(\E[X]) + a (\E[X]-\E[X]) \\\\
  \implies \quad
  \E[g(X)] &\geq g(\E[X])
\end{align*}
Concave proof is analogous.
\end{proof}

\begin{thm}{\emph{(Markov's Inequality)}}
\label{thm:markov}
Given random variable $X$,
\begin{equation}
    \label{markov}
    \Prb\left[
      \left\lvert X\right\rvert
      \geq \varepsilon\right]
    \leq \frac{\E\left[|X|^p\right]}{|\varepsilon|^p}
\end{equation}
\end{thm}

\begin{proof}
Write out the expectation in the numerator, where $F$ is the cdf of $X$:
\begin{align*}
  \E\left[|X|^p\right]
  &= \int^\infty_{-\infty} |X|^p \; dF \\
  &= \int^{\varepsilon}_{-\infty} |X|^p \; dF
    + \int^\infty_{\varepsilon} |X|^p \; dF
\end{align*}
Next, let's just throw out the first term in the sum above to get an
inequality. Since $|X|^p$ and the cdf are always positive, we know that
we're throwing out a positive portion, which gives us inequality
\begin{align*}
  \E\left[|X|^p\right]
  &\geq \int^\infty_{\varepsilon} |X|^p \; dF
\end{align*}
Next, notice that a lower bound for $|X|^p$ over this support will be
$|\varepsilon|^p$, so we can modify the inequality again to get
\begin{align*}
  \E\left[|X|^p\right]
  &\geq  |\varepsilon|^p \int^\infty_{\varepsilon} \; dF
  = |\varepsilon|^p \; \Prb[X\geq \varepsilon] \\
  \implies \quad
  \frac{\E\left[|X|^p\right]}{|\varepsilon|^p}
  &\geq \Prb(X\geq \varepsilon)
\end{align*}
\end{proof}

\begin{cor}{\emph{(Chebyshev's Inequality)}}
Given random variable $X$, we have
\begin{equation}
  \label{chebyshev1}
  \Prb\left[
    \left\lvert X\right\rvert
    \geq \varepsilon\right]
  \leq \frac{\E\left[X^2\right]}{\varepsilon^2}
\end{equation}
or equivalently
\begin{equation}
  \label{chebyshev2}
  \Prb\left[
    \big\lvert X-\E[X]\big\rvert
    \geq \varepsilon\right]
  \leq \frac{\Var(X)}{\varepsilon^2}
\end{equation}
\end{cor}
\begin{proof}
The proof of Statement~\ref{chebyshev1} follows by taking $p=2$ in
Markov's Inequality (Theorem~\ref{markov}).
To prove Statement~\ref{chebyshev2}, define $Z=X-\E[X]$ and use that in
Markov's inequality with $p=2$ on $Z$.
\end{proof}

\clearpage
\section{Large Sample Theory}

\subsection{Convergence Results}

Recall the usual Analysis definition of convergence of a sequence: A
sequence $\{x_n\}$ converges to limit $x$ if for all $\varepsilon> 0$,
there exists a finite $N$ such that
\begin{align*}
  n > N \implies ||x_n - x|| < \varepsilon
\end{align*}
But this, of course, is for a sequence of numbers $\{x_n\}$ in some
metric space. There's no randomness there. So what about a sequence of
random variables---something non-deterministic?

There are a few options when talking about convergence of random
variables, but all involve resolving the randomness somehow and checking
convergence of functions, probabilities, or expectations---all of which
are possible given the usual analysis machinary.
\begin{enumerate}
  \item \emph{Almost Sure Convergence}: Check if the sequence of
    functions $\{X_n\}$ on sample space $\Omega$ converge to some
    function $X$ on $\Omega$, i.e. $X_n(\omega)=X(\omega)$ for all
    $\omega\in\Omega$ that have nonzero probability.
  \item \emph{Convergence in Probability}: The sequence of probabilities
    that $X_n(\omega)$ is far from $X(\omega)$ over all $\omega$ goes to
    zero.
  \item \emph{Converge in $p$-Norm}: The sequence of expected distances
    $\E[|X_n-X|^p]$ goes to zero.
\end{enumerate}
In all of these cases, we've framed convergence of random variables in
terms of convergence of functions or numerical sequences. Now onto the
complete definitions.

\begin{defn}{(Almost Sure or Strong Convergence)}
Given a sequence of random variables $\{X_n\}$ and a random variable
$X$, we say that ``$X_n$ \emph{converges almost surely} to $X$'' written
\begin{align*}
  X_n\asto X
  \quad \iff \quad
  \limn \Prb[X_n = X]
  = \limn \Prb[\,\omega \,|\,X_n(\omega) = X(\omega)] = 1
\end{align*}
This checks that $X_n(\omega)\ra X(\omega)$ for each elementary event
$\omega$ in the sample space $\Omega$ that has nonzero probability.
(Though the functions can differ arbitrarily on elementary events that
will never occur.) If $X_n$ and $X$ are vectors, we need almost surely
convergence element-by-element.

This notion is called ``strong convergence'' because it it computes the
probability of the limit of a sequence of random variables, rather than
the limit of a sequence of probabilities. We're effectively asking that
the random variable \emph{itself}, which is a function on sample space
$\Omega$, converges pointwise to another function on $\Omega$---the
random variable $X$.
\end{defn}


\begin{defn}{(Convergence in Probability)}
We say that a sequence of random variables $\{ X_n \}$ on measure space
$(\Omega,\calF,\Prb)$ \emph{converges in probability} to $X$ if
\begin{equation}
  \label{plim}
  X_n\pto X
  \quad\iff\quad
  \limn p_n(\varepsilon) :=
  \limn
  \Prb(\left\lvert X_n - X \right\rvert > \varepsilon) = 0
  \qquad \forall  \varepsilon> 0
\end{equation}
This is someetimes written as $\plim X_n = X$.
 If $X_n$ and $X$ are vectors, we need convergence in probability
 element-by-element.


Convergence in probability is also known as \emph{weak convergence}
because it doesn't ask the random variables (functions) $X_n$ to
converge to the random variable (function) $X$, i.e.\ that
$X_n(\omega)=X(\omega)$ identically for all $\omega\in\Omega$.  Instead,
we just stipulate that the realizations are very close to $X$ as
$n\rightarrow\infty$.
\end{defn}

\begin{defn}{(Mean Square and $p$-Norm Convergence)}
We say that a sequence of random variables $\{ X_n \}$
\emph{converges in $p$-Norm} to $X$, written
\begin{align*}
  X_n \Lpto X
  \quad\iff\quad
  \limn \E\left[|X_n-X|^p\right] = 0
\end{align*}
The special case where $p=2$ is called \emph{mean square convergence} or
\emph{convergence in quadratic mean}, and is often written $X_n\msto X$.
If $X_n$ and $X$ are vectors, we need convergence in $p$-norm
element-by-element.
\end{defn}


\begin{defn}{(Convergence in Distribution)}
\label{defn:convergeInDistribution}
We say that a sequence of random variables $\{X_n\}$
\emph{converges in distribution} to random variable $X$, written
\begin{align*}
  X_n\dto X
  \quad\iff\quad
  \limn F_{X_n}(x) = F_X(x)
\end{align*}
for all values of $x$ where $F_x$ is continuous,
where $F_{X_n}$ and $F_X$ are the cdfs of $X_n$ and $X$, respectively.

If $X_n$ and $X$ are vectors, then the joint cdf of $X_n$ must converge
to the joint CDF of $X$. In that case, $a'X_n \dto a'X$ for any
non-stochastic vector $a$ (which is called the Cramer-Wold device).
\end{defn}
\begin{ex}
To see why we need to restrict convergence to points of continuity on
$F_X(x)$, consider the following example:
\begin{align*}
  F_{X_n}(x) &= \one_{\left\{x\geq \frac{1}{n}\right\}}\\
  F_{X}(x) &= \one_{\left\{x\geq 0\right\}}
\end{align*} In other words, random variable $X_n$ takes on value
$\frac{1}{n}$ with probability one, while $X$ takes on value zero with
probability one.  So if we check convergence at $x=0$, we have a
sequence $\{F_{X_n}(0)\}=\{0\}$, which clearly does not converge to
$F_X(0)=1$.

However, it is the case that $F_{X_n}$ is well-approximated by $F_X$ at
all other points, so we'd like to be able to say $X_n\dto X$. And we
can, if we simply restrict ourselves to points where $F_X$ is
continuous.
\end{ex}

\begin{prop}
Sequence of random variables $\{X_n\} \dto X$ if and only if
\begin{align*}
  \E[g(X_n)] = \E[g(X)]
\end{align*}
\end{prop}
\begin{rmk}
This is another useful way to characterize to characterize convergence
in distribution aside from Definiton~\ref{defn:convergeInDistribution}.
It's extremely handy for a lot of results.
\end{rmk}

\begin{prop}
Given a sequence of random variables $\{X_n\}$ and random variable $X$,
the various convergence concepts are related in the following way:
\begin{enumerate}
  \item $X_n\asto X \implies X_n\pto X$. Converse not true in general.
  \item $X_n\Lpto X \implies X_n\pto X$
  \item Any one of
    \begin{align*}
      \begin{rcases}
        X_n&\asto &X \\
        X_n&\pto &X \\
        X_n&\Lpto &X
      \end{rcases}
      \implies X_n\dto X
    \end{align*}
    We see here even more explicitly that converge in distribution is
    \textsc{super weak}.
  \item Almost Surely and $p$-norm, undecidable.
  \item $X_n\pto X \implies$ There's a deterministic subsequence such
    that $X_{n_k}\asto X$ (but does not imply almost sure convergence
    for the original sequence).
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn.
\begin{enumerate}
  \item
    %Suppose that we are given arbitrary $\delta,\varepsilon>0$. We
    %need to show that we can find an $N$ such that
    %\begin{align*}
      %n>N
      %\implies
      %P(|X_n-X|>\varepsilon) < \delta
    %\end{align*}
    %Since $X_n$ converges to $X$ almost surely, for any $\omega$, there
    %is an $N_\omega$ such that
    %\begin{align*}
      %n > N_\omega
      %\implies
      %|X_n(\omega)-X(\omega)| < \varepsilon
    %\end{align*}

  \item Define random variable $Z=X_n-X$. Suppose that $\varepsilon$ is
    given. From Markov's inequality,
    \begin{align*}
      P(|Z| \geq \varepsilon) \leq \frac{\E[|Z|^p]}{|\varepsilon|^p}
      \implies
      P(|X_n-X| \geq \varepsilon) \leq \frac{\E[|X_n-X|^p]}{|\varepsilon|^p}
    \end{align*}
    As $n\ra\infty$, the righthand side of the inequality goes to zero
    since $X_n$ converges to $X$ in $p$-norm. Hence, the lefthand side
    holds.

\end{enumerate}
\end{proof}


%Now for some final notes. Convergence in probability
%is \textbf{not} convergence in expectation. The former
%concerns a sequence of probabilities, while the latter a
%sequence of expectations. Finally, the probability limit
%$X$ must be free of all dependence upon the sample size $n$.



\section{Related Concepts}

\paragraph{Consistency} A sequence of estimators $\{ \hat{\theta}_n \}$
where $n=1,2,\ldots$ is \emph{consistent} for parameter $\theta$ if
$\hat{\theta}_n$ converges \emph{In Probability} to $\theta$.

\paragraph{Strongly Consistent} If convergence of $\hat{\theta}_n$
to $\theta$ holds with probability 1.



\section{Probability with Measure Theory}


\begin{rmk}
Because a $\sigma$-algebra is closed under countable unions,
interesections, and complements, it provides a natural way to think
about collections of elementary outcomes that we might care about. Said
another way, the definition of a $\sigma$-algebra results gives us
pretty much all of the sets we'd like to assign probabilities to.
\end{rmk}

\begin{ex}
For an even simpler example than die rolling of something that is
a perfectly valid $\sigma$-algebra, we
sometimes take the power set of $\Omega$---the collection of all
possible subsets of $\Omega$---to be our $\sigma$-algebra.  It will
surely satisfy the necessary properties, and it happens to be very easy
to denote.
\end{ex}


\subsection{Measure}

Once we have a $\sigma$-algebra, we can pair the sample space with it's
$\sigma$-algebra to form what's called a
\textbf{measurable space}, which is simply the ordered pair
$(\Omega, \mathcal{F})$.  From there, we can start assigning
probabilities to those elements in the $\sigma$-algebra which all have
some chance of occurring.  To do so, we will use a special
type of function, called a measure.

A \textbf{measure} is a function that maps sets into real numbers.
In our case, we will define a specific type of measure, a
\textbf{probability measure}, as a
function that has as it's domain a $\sigma$-algebra and maps measurable
sets in the $\sigma$-algebra into the
real line, subject to a few conditions. More precisely, it has the
following characteristics:
\begin{itemize}
   \item[i.] If $P$ is our probability measure and $\mathcal{F}$ is
      our $\sigma$-algebra, then
	 \[ P: \mathcal{F} \rightarrow \mathbb{R} \]
   \item[ii.] $ P(F) \in [0,1]$ for all measurable sets
      $F \in \mathcal{F}$. In addition, $P(\Omega) =1$.
   \item[iii.] Our probability measure satisfies the probability axioms.
      And if $F_i$ are all disjoint and members of the $\sigma$-algebra,
      then
	 \[ p( \cup F_i) = \sum P(F_i) \]
\end{itemize}

\paragraph{Definition} Now that we have our definition of a probability,
I just want to define a common term more precisely.  Specifically,
if a property is true \emph{except} for an event of probability 0,
then we say that property holds ``almost surely.''

\subsection{Probability Space}

So now we have a way of assigning probabilities to any arbitrary event in our measurable space, $(\Omega, \mathcal{F})$. If we
join the probability measure, $P$, to our measurable space, we obtain a proper \textbf{Probability Space}.  It is defined as
the ordered triplet
\[(\Omega, \mathcal{F}, P) \]

\subsection{Measurable Function and Random Variables}

One of the fundamental notions of probability is that of a random
variable; therefore, making the definition more rigorous
deserves some attention.

So let's start with the notion of a \textbf{measurable} function.
If $(\Omega_1, \mathcal{F}_1)$ and $(\Omega_2, \mathcal{F}_2)$ are two
measurable spaces then, a function
   \[ f: (\Omega_1, \mathcal{F}_1) \rightarrow (\Omega_2, \mathcal{F}_2)
      \]
is a measurable function if
   \[ f^{-1}(B) \in \mathcal{F}_1, \;\; \forall B \in\mathcal{F}_2 \]
In words, a function is measurable if the pre-image of an element in
the $\sigma$-algebra of the co-domain is in the
$\sigma$-algebra of the domain.  Simply, it preserves the structure
between two measurable spaces, sending measurable sets
in one measurable space to measurable sets in the other.

Turning to probability, suppose  that we have our probability space,
$(\Omega_1, \mathcal{F}_1, P)$, and another measurable space,
$(\Omega_2,\mathcal{F}_2)$.  Then an
$\mathbf{(\Omega_2, \mathcal{F}_2)}$\textbf{-valued random variable}
is a \emph{measurable function}
\[ X: (\Omega_1, \mathcal{F}_1) \rightarrow (\mathbb{R},
   \mathcal{F}_2) \]

Since $X$ is a measurable function, we know that
   \[ X^{-1}(B) \in \mathcal{F}_1, \;\; \forall B \in \mathcal{F}_2\]
in which case we say that $X$ is $\mathcal{F}_1$-measurable.
To clarify further
   \[ X^{-1}(B) = \left\{ \omega \in \Omega_1 | X(\omega)
      \in B \right\} \]

\paragraph{Note} Oftentimes, we'll just take the set
$\mathcal{F}_2$ to be some properly
defined $\sigma$-algebra on the real line, like the Borel Set
(defined below).

\subsection{Sigma Algebras Generated by Random Variables}

Above, we assumed that we knew the $\sigma$-algebra for
$\Omega_1$ already---that it was given. But we
could just as well \emph{generate} one from some random variable $X$,
and this generated $\sigma$-algebra could very well differ from
$\mathcal{F}_1$.

So let's assume that $X$ is a random variable
   \[X: (\Omega_1,\mathcal{F}_1) \rightarrow
      (\mathbb{R},\mathcal{F}_2).\]
Then the $\sigma$-algebra generated by $X$, denoted by $\sigma(X)$, is
defined as \emph{all} sets of the form
   \[ \sigma(X) := \{ \omega \in \Omega_1 | X(\omega) \in A \}, \;\;
      \forall \; A \subset \mathbb{R} \]
which can be written more compactly as
   \[\sigma(X)  = \{ X^{-1}(A) | A\subset \mathbb{R}  \} \]

\paragraph{Definition} Finally, if $\mathcal{G}$ is a
sub-$\sigma$-algebra of the set $\mathcal{F}$ defined above, then the
random variable $X$ is $\mathcal{G}$-measurable if
   \[ \sigma(X) \subset \mathcal{G} \]

Let's give a concrete example.  Suppose $\Omega$ consists of all the
possible combination of up and down moves in a binomial tree.  $X$ is
a random variable denoting stock price.  Then for a set of real numbers
representing the possible prices the stock could take on (this is
our $A \subset \mathbb{R}$), the set $\sigma(X)$ will be the sigma
algebra resulting from the set of all possible paths that that the
stock could have taken to get to those prices in $A$.

\subsection{Random Variables and Their Distributions}

A random variable is actually quite distinct from its distribution.  Recall that a Random Variable is just a mapping from the
sample space $\Omega$ into something more tractable, like the real line.  Therefore, we could discuss this mapping--the random variable--
without ever considering probabilities.

However, oftentimes we will want to discuss probabilities, but the sufficiently general definition of the random variable just given
offers a lot of flexibility.  So much, in fact, that a random variable could have more than one distribution, as we know occurs in
stock process which have traditional probabilities and also \emph{risk-neutral} probabilities (or \emph{pseudo-probabilities}).

So let's define a \textbf{distribution} as a measure that assigns probabilities to the real numbers that a random variable generates
(after being passed an element in the sample space).  The most natural distribution is the induced measure, $\mathcal{L}_X$ defined
as follows
\[ \mathcal{L}_X(A) := P \{ X \in A \}, \;\; A \subset \mathbb{R} \]

Let's unpack that. If $A$ is a subset of the real line, it the random variable may or may not take on values in that subset, so we want
a notion of probability.  Well, we can take the pre-image, $X^{-1}(A)$, and look at all those elements of the sample space that map to
$A \subset \mathbb{R}$ under the random variable $X$.  Then, once we have elements of the sample space, we can use our traditional notion
of the $\sigma$-algebra, $\mathcal{F}$, and the probability measure $P$ that are already defined on the probability space to assign
the probabilities.

Thus, we can associate probabilities with our random variable so long as there exists a distribution measure, like $\mathcal{L}_X$.
But recall that $\mathcal{L}_X$ was not unique.  It's simply a function to assign probabilities to values that the random variable can
take on.  We could consider other measures that also accomplish this, and that insight legitimizes the use of such things as
\emph{risk-neutral probabilities}.

\subsection{Lebesgue Measure and the Lebesgue Integral}

\subsection{Introduction}

The \textbf{Lebesgue Measure} is the standard way of assigning a measure to subsets of $n$-dimensional Euclidean space.  If we restrict
to $\mathbb{R}$, then the Lebesgue Measure of intervals is simply the length. But rather than consider all of $\mathbb{R}$, we'll
restrict further to \emph{Borel Sets}.

This will allow us to construct the \emph{Lebesgue Integral}, a generalization of the Riemann Integral.

\subsection{Borel Sets}

The \emph{Borel $\sigma$-algebra}, denoted $\mathcal{B}(\mathbb{R})$, is the smallest $\sigma$-algebra containing (and, in a sense,
generated by) all open intervals
in $\mathbb{R}$.

\paragraph{Examples} The following are a few samples of Borel Sets in $\mathcal{B}(\mathbb{R})$:
\begin{itemize}
   \item{Every open interval of the form $(a,b)$.}
   \item{The open rays $(a,+\infty)$ and $(-\infty,b)$.}
   \item{All unions of the form
	 \[ B_1 \cup B_2, \;\; B_1,B_2 \in \mathcal{B}(\mathbb{R}) \]
      }
   \item{All complements of sets in $\mathcal{B}(\mathbb{R})$, since it's a sigma algebra. Note that this implies all \emph{closed}
      intervals in $\mathbb{R}$ are Borel Sets as well, in addition to all half-open and half-closed intervals.}
   \item{All one point sets, $a\in\mathbb{R}$, as we see that it is in the intersection of other Borel Sets, implying inclusion in
	 $\mathcal{B}(\mathbb{R})$ since it's a $\sigma$-algebra
	 \[ \{a \} = \bigcap_{n=1}^{\infty} \left( a - \frac{1}{n}, a + \frac{1}{n} \right). \]
      }
   \item{The last item implies that all finite countable collections of points in $\mathbb{R}$ are Borel Sets too. Therefore, the set
      of all rational numbers is Borel since coutable, and the set of all irrational numbers is Borel since it's the complement of a
      set in the sigma algebra.}
\end{itemize}

However, it should be noted that not all sets of real numbers are Borel Sets.  In particular, any non-Borel set must be uncountable
(though the opposite is not ture, as shown above).

\subsection{Lebesgue Measure}

Let's start more generally and define a \emph{measure} on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ to be a function
\[ \mu: \mathcal{B} \rightarrow [0,\infty) \]
with the following properties
\begin{itemize}
   \item[i.]{$\mu(\emptyset) = \emptyset$.}
   \item[ii.]{If $A_1, A_2, \ldots$ are disjoing sets in $\mathcal{B}(\mathbb{R})$, then
	 \[ \mu\left(\bigcup_{k=1}^{\infty} A_k\right) =  \bigcup_{k=1}^{\infty} \mu(A_k). \]
      }
\end{itemize}

We define the \textbf{Lebesgue Measure} on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ to the measure $\mu_0$ that assigns the measure of
each interval to be its length.

\subsection{Functions in This World}

Let $f$ be a function
\[ f: \mathbb{R} \rightarrow \mathbb{R}. \]
We say that $f$ is \textbf{Borel-Measurable} if
\[ A \in \mathcal{B}(\mathbb{R}) \Rightarrow f^{-1}(A) \in \mathcal{B}(\mathbb{R}). \]
Or equivalently, we could say that we want the $\sigma$-algebra generated by $f$ to be contained in $\mathcal{B}(\mathbb{R})$.

\subsection{The Lebesgue Integral}

Let $I$ be the \emph{indicator function}.  We define
\[ A := \{ x \in \mathbb{R}| I(x) =1 \} \]
to be the set \emph{indicated by I}.

The \textbf{Lebesgue Integral} of $I$ is defined
\[ \int_{\mathbb{R}} I d\mu_0 = \mu_0(A). \]


\section{Stochastic Processes}

Throughout this section, we will explore stochastic processes building
from the simple case of a discrete state space in discrete time to the
most general case of an infinite state in continuous time, relaxing constraints through the subsections.

However, before proceeding, I here lay down the most generaly
definitions for stochastic process, which will we restrict to special
cases later on.

\subsection{General Definitions}

\begin{defn}{\citep{pavliotis}}
Let $T$ be an ordered set, $(\Omega,\mathscr{F},\mathbb{P})$ be a
probability space where $\Omega$ is the sample space, and
$(E,\mathscr{G})$ be a measurable space.

A \emph{stochastic process} is a collection of random variables $X =
\{X_t | t\in T\}$ such that for each $t\in T$, $X_t:
(\Omega,\mathscr{F},\mathbb{P})\mapsto (E,\mathscr{G})$ is a random
variable taking on values in its state space $E$.
\end{defn}

\begin{rmk}
Though it might not be obvious at first, a stochastic process dependends
upon two arguments: some $\omega\in\Omega$ and some $t\in T$. Therefore
the following representations are equivalent: $X_t$, $X_t(\omega)$,
$X(t,\omega)$ where the notation used will depend upon context.

Therefore, we might first think about $X(t,\omega)$ fixing $\omega$ and
letting $t$ vary. This corresponds to choosing a realization $\omega$ of
some experiment and looking at the resulting \emph{trajectory} of the
process.

On the other hand, we might fix $t$ and think about the different values
that $X(t,\omega)$ might take on depending upon the trajectory that is
realized in some experiment.
\end{rmk}

\begin{defn}
A process $\{X_t\}$ satisfies the \emph{Markov Property} if the
\begin{align*}
\mathbb{P}\left(X_{s+t} | \{X_{s-\varepsilon}\}_{\forall\varepsilon>0}\right)
= \mathbb{P}\left(X_{s+t} | X_{s}\right)
\end{align*}
In other words, only the latest state $X_s$ matters for the future distribution of $X_t$

\end{defn}

\subsection{Discrete State Space, Discrete Time}

\begin{defn}
A \emph{discrete space, discrete time Markov chain} is a stochastic
process $\{X_{t_i}\}_{t_i\in T}$ for some finite or countable set $T$
that satisfies
\begin{equation}
  P(X_n = x_n | X_{n-1} = x_{n-1}, \ldots
  X_0 = x_0) = P(X_n = x_n | X_{n-1}=x_{n-1})
\end{equation}
In words, only the most recent state matters for the future.
\end{defn}

\subsection{Infinite State Space, Discrete Time}
\subsection{Infinite State Space, Continuous Time}

\section{Stochastic Differential Equations}

Ordinary differential equations allow us to naturally relate rates of
change for some function to time and the value of the function itself:
\begin{align*}
  \frac{dy(t)}{dt} = b(t,y(t))
\end{align*}
Now, we want to modify this to allow random noise to influence the
change in the value of some stochastic process:
\begin{align*}
  \frac{dX_t}{dt} = b(t,X_t) + \sigma(t,X_t) \xi_t
\end{align*}
where $\xi_t$ is some random variable representing ``noise.'' (Though
because of some mathematical technicalities, we can't really define
$\xi_t$ this way since it is like a derivative, but one that cannot
exist. So we'll need to move to a definition with integrals that can be
properly defined.)




\clearpage
\bibliographystyle{apalike}  % Or some other style
\bibliography{biblio} % where sources.bib has all the citation info





\end{document}
