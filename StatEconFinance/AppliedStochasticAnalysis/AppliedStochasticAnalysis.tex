\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Applied Stochastic Analysis}
\date{\today}


%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt}
    %---Width of the line

%\setlength{\headsep}{0.2in}
    %---Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage{accents}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure}
    %---For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{
    colorlinks,
        %---This colors the links themselves, not boxes
    citecolor=black,
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},%
            %---Size of the numbers
        numbersep=9pt,%
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %---Has to do with enumeration
\usepackage{appendix}
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

\tableofcontents

\clearpage
\section{Elements of Probability Theory}

\begin{defn}
The \emph{sample space} $\Omega$ is the set of all elementary outcomes
of some statistical experiment.
\end{defn}
\begin{rmk}
The definition of an ``elementary outcome'' is rather loose and heavily
depedendent upon what you define the ``experiment'' to be. For example,
if you define the experiment to be a single coin toss, then the
elementary outcomes are heads (H) and tails (T), implying $\Omega =
\{H,T\}$. However, you could also define an experiement to be tossing
$N$ coins or tossing a coin $N$ times, in which case the elementary
outcomes are $N$-tuples with each element in $\{H,T\}$.
\end{rmk}

\subsection{References}
\begin{enumerate}
  \item Pavliotis Appendix B, p 303-319; Skip B.5.1 and B.4 is optional
  \item Lecture notes 1, 2 from 2014
  \item Homework 1,2 and Solutions from 2014
  \item Appendix A.4 from Durrett
  \item K \& S Ch 1-3 p 37
\end{enumerate}

\clearpage
\section{Markov Chains}

Intuitively, Markov Chains are ``forgetful processes.'' Their future
values dependent only upon their present value, not their history. More
precisely, conditional on the present, past values of a Markov Chain
give you no more information about its future path.
\begin{rmk}
  Throughout this section, I will denote discrete-time Markov Chains by
  $\{X_n\}$ rather than $\{X_t\}$ to suggest via notation that the time
  domain is $\mathbb{N}$,
not $\mathbb{R}$.
\end{rmk}

\subsection{Discrete Time, Discrete Space}

\begin{defn}
The process $\{X_n\}$ is a \emph{discrete time, discrete space Markov chain} if it takes on an at-most-countable set of values $S$ and if it
satisfies the \emph{Markov Property}:
\begin{align*}
  \mathbb{P}(X_n = x_n | X_{n-1}=x_{n-1}, \ldots, X_0 =x_0)
  =\mathbb{P}(X_n = x_n | X_{n-1}=x_{n-1})
\end{align*}
In words, the distribution for $X_{n}$ only depends upon its state at
time $n-1$. All history prior to time $n-1$ is Duncan-Hines irrelevant.
We can define the evolution of the states and probabilities of a Markov
Chain in terms of a transition matrix.
\end{defn}

\begin{defn}
A \emph{stochastic matrix} $P=(p_{ij})_{n\times n}$ is a matrix where
the sum of each row (across columns) equals 1, i.e.
\begin{align*}
  \forall i \qquad \sum_j p_{ij} = 1
\end{align*}
\end{defn}

\begin{defn}
A \emph{transition matrix} $P(n)$ is a non-negative stochastic matrix
where entry $p_{ij}(n)$ denotes the probability of
transitioning from state $i$ to state $j$ between time $n$ and $n+1$:
\begin{align*}
  p_{ij}(n) = \mathbb{P}(X_{n+1}=j | X_n=i)
\end{align*}
It is clear that $P(n)$ must be a stochastic matrix, because by summing
across columns, we're summing across the states $X_n$ could move to.
Since $X_N$ has to move somewhere and since probabilities sum to 1, the
row-sums of $P(n)$ must equal 1. Combining this with the non-negativity
of probabilities, our two conditions for a transition matrix are:
\begin{enumerate}
  \item $p_{ij}(n)>0$ for all $i,j$.
  \item $P(n)$ is stochastic.
\end{enumerate}
\end{defn}

\begin{defn}
The process $\{X_n\}$ is a \emph{homogenous Markov Chain} if
\begin{align*}
  P(n) = P(m) \qquad \forall m,n\geq 0 \qquad m\neq n
\end{align*}
In words, the distribution of \emph{where} you jump does not depend upon
the \emph{time} you jump. In such cases, we drop time from the notation
so that $P(n)$ becomes $P$ and $p_{ij}(n)$ becomes $p_{ij}$.
\end{defn}

\begin{rmk}
From here on, we will exclusively work with homogeneous Markov Chains.
\end{rmk}

\begin{thm}{\emph{(Chapman-Komogorov Equation, Discrete Time and Space)}}
For a homogeneous Markov Chain, we can compute $m$ step ahead transition
probabilities as follows:
\begin{align*}
  \mathbb{P}(X_{n+m}=j | X_{n}=i) = \sum_k \mathbb{P}(X_{n+m}=j| X_\ell=k)
    \mathbb{P}(X_\ell = k| X_n=i)
  \qquad
  n \leq \ell \leq n+m
\end{align*}
In words, break the probability of moving from $i$ to $j$ in $m$ steps
into a smaller problem: $i$ to $m$ then $m$ to $i$.
\end{thm}

\begin{thm}{\emph{(Forward Kolmorogov Equation)}}
This relationship tells us how to evolve distributions. In particular,
if $X_n\sim \mu^{(n)}$ where $\mu^{(n)}$ is a row vector or
probabilities summing to 1, then $\mu^{(n+1)}$ can be computed
\begin{align*}
  \mu^{(n+1)} = \mu^{(n)} P
\end{align*}
This also implies that we can compute the distribution of
time-homogeneous $X_n$ at any time if we know the transition matrix and
initial distribution:
\begin{align*}
  \mu^{(n)} = \mu^{(0)} P^n
\end{align*}
\end{thm}

\begin{thm}{\emph{(Backward Komogorov Equation)}}
This theorem tells us how to evolve moments and expected values. In
particular, for a function $f$ and Markov chain $\{X_n\}$ taking on any
of $m$ possible values, define column vector
\begin{align*}
  u_n =
  \begin{pmatrix}
    u^{(1)}_n & \cdots & u^{(k)}_n & \cdots & u^{(m)}_n
  \end{pmatrix}'
  \quad\text{where}\quad
  u^{(k)}_n := \mathbb{E}[f(X_n)|X_0=k]
\end{align*}
Then we have that
\begin{align*}
  u_{n+1} = P u_n
\end{align*} \end{thm}
\begin{proof}
Law of Iterated Expectations:
\begin{align*}
  u_{n+1}^{(i)} &= \mathbb{E}[f(X_{n+1})|X_0=i]\\
  \text{By definition}\qquad
  &= \sum_j f(j) \mathbb{P}(X_{n+1}=j |X_0=i)\\
  \text{Chapman-Komogorov}\qquad
  &= \sum_j f(j) \left(\sum_k \mathbb{P}(X_{n+1}=j |X_1 = k)
              \mathbb{P}(X_1=k|X_0=i)\right)\\
  &= \sum_k \left(\sum_j f(j) \mathbb{P}(X_{n+1}=j |X_1 = k)\right)
          \mathbb{P}(X_1=k|X_0=i)\\
  \text{By Homogeneity}\qquad
  &= \sum_k \left(\sum_j f(j) \mathbb{P}(X_{n}=j |X_0 = k)\right)
          \mathbb{P}(X_1=k|X_0=i)\\
  &= \sum_k \mathbb{E}[f(X_n)|X_0=k] \cdot \mathbb{P}(X_1=k|X_0=i)\\
  &= \sum_k u^{(k)}_n \cdot \mathbb{P}(X_1=k|X_0=i)\\
  &= P_{i,\cdot} u_n
\end{align*}
where $P_{i,\cdot}$ denotes the $i$th row. Stacking this result across
the $i$, we get $u_{n+1}=Pu_n$.
\end{proof}

\begin{thm}{\emph{(Alternative Backward Komogorov Equation)}}
For a function $f$, Markov chain $\{X_n\}$, and fixed time $N\geq n$,
define column vector
\begin{align*}
  v_n =
  \begin{pmatrix}
    v^{(1)}_n & \cdots & v^{(k)}_n & \cdots & v^{(m)}_n
  \end{pmatrix}'
  \quad\text{where}\quad
  v^{(k)}_n := \mathbb{E}[f(X_N)|X_n=k]
\end{align*}
Then we have that
\begin{align*}
  u_{n+1} = P u_n
\end{align*}
\end{thm}

\begin{rmk}
In both formulations of the Backward Equation, we have functions of time
and of a starting point, $u^{(k)}_n$ and $v^{(k)}_n$. However, for $u$,
the starting time is fixed, while we can vary the time \emph{from} or
\emph{after} the starting point. On the other hand, for $v$, the ending
time is fixed and $v$ varies as function of some time \emph{before} or
\emph{until} that terminal time. It is, therefore, from $v$ that the
name ``Backward Equation'' arises, as we can solve \emph{backwards}
moments and functions of $X_n$ prior to some terminal time.
\end{rmk}

\subsection{Continuous Time, Discrete Space}

\begin{defn}
The process $X=\{X_t\}_{t\geq 0}$ is a \emph{continuous time, discrete
space Markov Chain} if $X$ takes on only a discrete set of values in a
finite or countable state space $S$ and if
\begin{align*}
  \mathbb{P}\left(X_{t+h} = i_{t+h} | \left\{ X_{s}, s\leq t\right\}\right)
  =
  \mathbb{P}(X_{t+h} = i_{t+h} | X_{t} )
  \qquad \forall h \geq 0
\end{align*}
\end{defn}
\begin{ex}
The classic example is a time-homogeneous Poisson process with
$\lambda$, which has conditional probability distributions
\begin{align*}
  \mathbb{P}(X_{t+h}=j|X_t=i)
  &= e^{-\lambda h} \;
  \frac{(\lambda h)^{j-i}}{(j-i)!}
\end{align*}
\end{ex}

Now that time isn't neatly divided into discrete intervals, we can't
define a transition matrix since there's not necessarily a standard unit
of time between jumps. There are then two approaches:
\begin{enumerate}
  \item Consider the \emph{generator} $Q$, which is like a limit of the
    transition matrix as the time step goes to~0.
  \item Describe the distribution of \emph{jump times} of $X_t$, defined
    as $\tau_1, \tau_2,\ldots$, and then treat $X_t$ like a discrete
    time Markov Chain with the jump times given.
\end{enumerate}

\begin{defn}{(Transition Probabilities in Cts. Time)}
For a continuous time, discrete space Markov Chain, the \emph{transition
probability} $p_{ij}(s,t)$ at time $s$ for time step $t$ is defined
\begin{align*}
  p_{ij}(s,t) = \mathbb{P}(X_{t+s}=j|X_s=i)
\end{align*}
We can then collect these in a transition matrix $P(s,t) =
(p_{ij}(s,t))$. Note that $P(s,0) = I$ for all $s$ for obvious reasons.

If $X$ is a homogeneous Markov Chain, then we drop the argument $s$ and
simply have $p_{ij}(t)$, the probability of transitioning from $i$ to
$j$ in $t$ units of time. Similarly the matrix of transition
probabilities becomes simply $P(t)$.
\end{defn}

\begin{rmk}
Again, we'll consider only homogeneous Markov Chains from now on.
\end{rmk}

\begin{thm}{\emph{(Chapman-Kolmorogov)}}
Similar to the discrete-time case, we have
\begin{align}
  p_{ij}(t+s) = \sum_k p_{ik}(t) p_{kj}(s)
  \quad \Leftrightarrow \quad
  P(t+s) = P(t)P(s)
  \label{chapkolg2}
\end{align}
\end{thm}

\begin{defn}
We define the \emph{generator} of a discrete-space, continuous-time
homogeneous Markov Chain $X_t$ as
\begin{align*}
  G = \lim_{h\rightarrow0} \frac{P(h)-I}{h}
\end{align*}
This is like the derivative of the transition matrix. This is useful in
determining how $P(t)$ evolves in time:
\begin{align*}
  \frac{dP}{dt}
  &= \lim_{h\rightarrow0} \frac{P(t+h)-P(t)}{h}\\
  \text{By~(\ref{chapkolg2})}\qquad
  &= \lim_{h\rightarrow0} P(t) \frac{P(h)-I}{h}\\
  \Rightarrow\qquad
  \frac{dP}{dt}
  &= P(t) G
\end{align*}
Note also that we could have pulled $P(t)$ out on the righthand side,
implying that the transition matrix commutes with the generator:
\begin{align*}
  \frac{dP}{dt}
  = P(t) G
  = GP(t)
\end{align*}
which bring us to the forward and backward equations. In both equations,
we want to know how probabilities or moments change with time;
therefore, it's not surprising that the generator should factor into the
discussion.
\end{defn}

\begin{thm}{\emph{(Forward Kolmogorov Equation)}}
As in discrete time and space, this tells us how to evolve
probabilities, only now in continuous time:
\begin{align*}
  \frac{dP}{dt} = P_t G
\end{align*}
where $G$ is the generator. Elementwise, this is equivalent to
\begin{align*}
  \frac{dp_{ij}(t)}{dt} = \sum_k p_{ij}(t) g_{kj}
\end{align*}
\end{thm}

\begin{thm}{\emph{(Backward Kolmogorov Equation)}}
\end{thm}


\subsection{Continuous Time, Continuous Space}

\begin{defn}
The process $X=\{X_t\}_{t\geq 0}$ is a \emph{continuous time, continuous
space Markov Chain} if $X$ takes on values in $\mathbb{R}$ and if
\begin{align*}
  \mathbb{P}\left(X_{t+h} \in \Gamma | \left\{ X_{s}, s\leq t\right\}\right)
  =
  \mathbb{P}(X_{t+h} \in \Gamma | X_{t} )
  \qquad \forall h \geq 0
\end{align*}
\end{defn}



\subsection{References}
\begin{enumerate}
  \item Koralev and Sinai Ch 5.1-5.5
\end{enumerate}

\clearpage
\section{Spectral Representation of Stochastic Processes}

\subsection{Stationary Processes}

Strong stationarity imposes structure on the fdds. Weak stationarity
only imposes structure on the mean and covariance functions.

\subsection{Spectral Measure}

\begin{thm}\emph{(Bochner's Theorem)}
Suppose you have a covariance function $C(t)$ of a weakly stationary
stochastic process $X_t$ (weakly stationary implying that $C(t)$ is
positive semi-definite), then there exists a finite measure $F(\lambda)$
such that $C(t)$ can be represented as
\begin{align*}
  C(t) = \int_\mathbb{R} e^{i\lambda t} dF(\lambda)
\end{align*}
The measure $F$ is called the \emph{spectral measure} of $X(t)$.
\end{thm}

\begin{defn}
If the spectral measure $F$ is absolutely continuous w.r.t.\ the
Lebesgue measure on $\mathbb{R}$, then
\begin{align*}
  dF(\lambda) = S(\lambda)d\lambda
  \quad \Rightarrow\quad
  C(t)
    &= \int^\infty_{-\infty} e^{i\lambda t} S(\lambda) d\lambda \\
  \Leftrightarrow
  S(\lambda)
    &= \frac{1}{2\pi} \int^\infty_{-\infty} e^{-i\lambda t} C(t) dt
\end{align*}
Note that the function $S(\lambda)$ is non-negative. Moreover, if we
want to come up with covariance functions, we can just write down
spectral density functions $S(\lambda)$ then compute the Fourier
transform to recover the corresponding $C(t)$.
\end{defn}

\subsection{Spectral Representation}

We saw how to obtain the spectral measure for a \emph{covariance
function} of a weakly stationary process. Now we specify the spectral
representation of a stationary stochastic process \emph{itself}.

\begin{thm}\emph{(Spectral Theorem)}
Given a stationary mean-zero stochastic process $X_t$ with spectral
distribution function $F(\lambda)$, there exists a complex-valued
process $Z(\lambda)$ such that
\begin{align*}
  X_t = \int^\infty_{-\infty} e^{i\lambda t} dZ(\lambda)
\end{align*}
where $Z$ (call the \emph{spectral process}) has the following
properties:
\begin{enumerate}
  \item Orthogonal Increments: For disjoint intervals
    $[\lambda_1,\lambda_2]$ and $[\lambda_3,\lambda_4]$,
    \begin{align*}
      E\left[
        (Z(\lambda_2)-Z(\lambda_1))
        \overline{(Z(\lambda_4)-Z(\lambda_3))}
      \right] = 0
    \end{align*}
  \item Spectral Weight: For $\lambda_1\leq \lambda_2$,
    \begin{align*}
      E\left\lvert Z(\lambda_2)-Z(\lambda_1)\right\rvert^2
      = F(\lambda_2)-F(\lambda_2)
    \end{align*}
\end{enumerate}
\end{thm}
\begin{rmk}
This looks something like the Ito integral, except the measure of the
interval $[\lambda_1,\lambda_2]$ in the integral above is
$Z(\lambda_2)-Z(\lambda_1)$, not $W(\lambda_2)-W(\lambda_1)$.
\end{rmk}

\subsection{References}
\begin{enumerate}
  \item Pavliotis 1.1, 1.2
  \item Koralev and Sinai, Ch 12, 15.3, 15.7, 16
\end{enumerate}

\section{Brownian Motion}

\begin{defn}
Brownian motion is the real-valued continuous time, continuous space
stochastic process $W_t$ for $t\geq 0$ (with $W_0=0$) satisfying
\begin{enumerate}
  \item \emph{Independent Increments}: For non-overlapping itervals
    $[t_1,t_2]$ and $[t_3,t_4]$, we have
    $W_{t_2}~-~W_{t_1}~\perp~W_{t_4}~-~W_{t_3}$,
    which of course generalizes to multiple independent increments.
  \item \emph{Stationary Gaussian Increments}: For all $s,t\geq 0$, we
    have $W_{s+t}-W_s\sim N(0,t)$
  \item \emph{Continuous Sample Paths}.
\end{enumerate}
\end{defn}



%% APPPENDIX %%

% \appendix






%% APPPENDIX %%

% \appendix




\end{document}

- What is a diffusion, time homogeneous, dZ^t = dt
- Ito's Lemma: Do second order expansions, keep dW_t and dW_t^2 - dt terms
- Ito's Lemma puts diffusion terms in the drift: *Convex* functions of
some process will affect the drift. If $f$ is not convex, no impact
- For X_t GBM, log(X_t) will grow slower than mu. Volatility draws down
drift; drift adjustment
- For processes X_t, Y_t, we have d(X_tY_t) = Y_t dX_t + X_t dY_t + dX_t dY_t
- In discrete time, we can ``solve'' a process by expressing it as a
function of it's past values and random shocks. ``Solving SDEs'' does
precisely this
- BM, GBM, Ornstein Uhlenbeck (and the discrete time formula)
- Ito Isometry

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}  %Use in text; cites
        \citep{LabelInSourcesFile} %Use in text; cites in parens

        \nocite{LabelInSourceFile} % Includes in refs w/o specific citation
        \bibliographystyle{apalike}  % Or some other style

        % To ditch the ``References'' header
        \begingroup
        \renewcommand{\section}[2]{}
        \endgroup

        \bibliography{sources} % where sources.bib has all the citation info

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%

        \verbatiminput{file.ext}
            %---Includes verbatim text from the file
        \texttt{text}
            %---Renders text in courier, or code-like, font

        \matlabcode{file.m}
            %---Includes Matlab code with colors and line numbers


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}


        % Side by Side figures
            \begin{figure}[h!]
                \centering
                \mbox{\subfigure{
                    \includegraphics[scale=1]{file1.pdf}
                }\quad\subfigure{
                    \includegraphics[scale=1]{file2.pdf}
                }
                }
            \end{figure}


