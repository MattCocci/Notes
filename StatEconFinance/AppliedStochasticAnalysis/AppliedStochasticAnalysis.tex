\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Applied Stochastic Analysis}
\date{\today}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt}
    %---Width of the line

%\setlength{\headsep}{0.2in}
    %---Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage{accents}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure}
    %---For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{
    colorlinks,
        %---This colors the links themselves, not boxes
    citecolor=black,
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},%
            %---Size of the numbers
        numbersep=9pt,%
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %---Has to do with enumeration
\usepackage{appendix}
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

\tableofcontents

\clearpage
\section{Elements of Probability Theory}

\begin{defn}
The \emph{sample space} $\Omega$ is the set of all elementary outcomes
of some statistical experiment.
\end{defn}
\begin{rmk}
The definition of an ``elementary outcome'' is rather loose and heavily
depedendent upon what you define the ``experiment'' to be. For example,
if you define the experiment to be a single coin toss, then the
elementary outcomes are heads (H) and tails (T), implying $\Omega =
\{H,T\}$. However, you could also define an experiement to be tossing
$N$ coins or tossing a coin $N$ times, in which case the elementary
outcomes are $N$-tuples with each element in $\{H,T\}$.
\end{rmk}

\subsection{References}
\begin{enumerate}
  \item Pavliotis Appendix B, p 303-319; Skip B.5.1 and B.4 is optional
  \item Lecture notes 1, 2 from 2014
  \item Homework 1,2 and Solutions from 2014
  \item Appendix A.4 from Durrett
  \item K \& S Ch 1-3 p 37
\end{enumerate}

\clearpage
\section{Markov Chains}

Intuitively, Markov Chains are ``forgetful processes.'' Their future
values dependent only upon their present value, not their history. More
precisely, conditional on the present, past values of a Markov Chain
give you no more information about its future path.
\begin{rmk}
  Throughout this section, I will denote discrete-time Markov Chains by
  $\{X_n\}$ rather than $\{X_t\}$ to suggest via notation that the time
  domain is $\mathbb{N}$,
not $\mathbb{R}$.
\end{rmk}

\subsection{Discrete Time, Discrete Space}

\begin{defn}
The process $\{X_n\}$ is a \emph{discrete time, discrete space Markov chain} if it takes on an at-most-countable set of values $S$ and if it
satisfies the \emph{Markov Property}:
\begin{align*}
  \mathbb{P}(X_n = x_n | X_{n-1}=x_{n-1}, \ldots, X_0 =x_0)
  =\mathbb{P}(X_n = x_n | X_{n-1}=x_{n-1})
\end{align*}
In words, the distribution for $X_{n}$ only depends upon its state at
time $n-1$. All history prior to time $n-1$ is Duncan-Hines irrelevant.
We can define the evolution of the states and probabilities of a Markov
Chain in terms of transition matrix.
\end{defn}

\begin{defn}
A \emph{stochastic matrix} $P=(p_{ij})_{n\times n}$ is a matrix where
the sum of each row (across columns) equals 1, i.e.
\begin{align*}
  \forall i \qquad \sum_j p_{ij} = 1
\end{align*}
\end{defn}

\begin{defn}
A \emph{transition matrix} $P(n)$ is a non-negative stochastic matrix
where entry $p(n)_{ij}$ denotes the probability of transitioning from
state $i$ to state $j$ from time $n$ to $n+1$:
\begin{align*}
  p(n)_{ij} = P(X_{n+1}=j | X_n=i)
\end{align*}
It is clear that $P(n)$ must be a stochastic matrix, because by summing
across columns, we're summing across the states $X_n$ could move to.
Since it has to move somewhere and probabilities sum to 1, the row-sums
of $P(n)$ must equal 1. Combining this with the non-negativity of
probabilities, our two conditions for a transition matrix are:
\begin{enumerate}
  \item $p(n)_{ij}>0$ for all $i,j$.
  \item $P(n)$ is stochastic.
\end{enumerate}
\end{defn}

\begin{defn}
The process $\{X_n\}$ is a \emph{homogenous Markov Chain} if
\begin{align*}
  P(n) = P(m) \qquad \forall m,n\geq 0 \qquad m\neq n
\end{align*}
In words, transition probabilities do not depend upon the time of the
jump. In these cases, we drop time from the notation so that $P(n)$
becomes $P$ and $p(n)_{ij}$ becomes $p_{ij}$.
\end{defn}

\begin{rmk}
From here on, we will exclusively work with homogeneous Markov Chains.
\end{rmk}

\begin{thm}{\emph{(Chapman-Komogorov Equation, Discrete Time and Space)}}
For a homogeneous Markov Chain, we can compute $m$ step ahead transition
probabilities as follows:
\begin{align*}
  \mathbb{P}(X_{n+m}=j | X_{n}=i) = \sum_k \mathbb{P}(X_{n+m}=j| X_\ell=k)
    \mathbb{P}(X_\ell = k| X_n=i)
  \qquad
  n \leq \ell \leq n+m
\end{align*}
In words, break the probability of moving from $i$ to $j$ in $m$ steps
into a smaller problem: $i$ to $m$ then $m$ to $i$ for some intermediate
step $m$.
\end{thm}

\begin{thm}{\emph{(Forward Kolmorogov Equation)}}
This relationship tells us how to evolve distributions. In particular,
if $X_n\sim \mu^{(n)}$ where $\mu^{(n)}$ is a row vector or
probabilities summing to 1, then $\mu^{(n+1)}$, the distribution of
$X_{n+1}$ can be computed
\begin{align*}
  \mu^{(n+1)} = \mu^{(n)} P
\end{align*}
This also implies that we can compute the distribution of $X_n$ at any
time if we know the transition matrix and initial distribution:
\begin{align*}
  \mu^{(n)} = \mu^{(0)} P^n
\end{align*}
\end{thm}

\begin{thm}{\emph{(Backward Komogorov Equation)}}
This theorem tells us how to evolve moments and expected values. In
particular, for a function $f$ and Markov chain $\{X_n\}$ taking on any
of $m$ possible values, define column vector
\begin{align*}
  u_n =
  \begin{pmatrix}
    u^{(1)}_n & \cdots & u^{(k)}_n & \cdots & u^{(m)}_n
  \end{pmatrix}'
  \quad\text{where}\quad
  u^{(k)}_n := \mathbb{E}[f(X_n)|X_0=k]
\end{align*}
Then we have that
\begin{align*}
  u_{n+1} = P u_n
\end{align*} \end{thm}
\begin{proof}
Law of Iterated Expectations:
\begin{align*}
  u_{n+1}^{(i)} &= \mathbb{E}[f(X_{n+1})|X_0=i]\\
  \text{By definition}\qquad
  &= \sum_j f(j) P(X_{n+1}=j |X_0=i)\\
  \text{Chapman-Komogorov}\qquad
  &= \sum_j f(j) \left(\sum_k P(X_{n+1}=j |X_1 = k) P(X_1=k|X_0=i)\right)\\
  &= \sum_k \left(\sum_j f(j) P(X_{n+1}=j |X_1 = k)\right) P(X_1=k|X_0=i)\\
  \text{By Homogeneity}\qquad
  &= \sum_k \left(\sum_j f(j) P(X_{n}=j |X_0 = k)\right) P(X_1=k|X_0=i)\\
  &= \sum_k \mathbb{E}[f(X_n)|X_0=k] \cdot P(X_1=k|X_0=i)\\
  &= \sum_k u^{(k)}_n \cdot P(X_1=k|X_0=i)\\
  &= P_{i,\cdot} u_n
\end{align*}
where $P_{i,\cdot}$ denotes the $i$th row. Stacking this result across
the $i$, we get $u_{n+1}=Pu_n$.
\end{proof}

\begin{thm}{\emph{(Alternative Backward Komogorov Equation)}}
For a function $f$, Markov chain $\{X_n\}$, and fixed time $N\geq n$,
define column vector
\begin{align*}
  v_n =
  \begin{pmatrix}
    v^{(1)}_n & \cdots & v^{(k)}_n & \cdots & v^{(m)}_n
  \end{pmatrix}'
  \quad\text{where}\quad
  v^{(k)}_n := \mathbb{E}[f(X_N)|X_n=k]
\end{align*}
Then we have that
\begin{align*}
  u_{n+1} = P u_n
\end{align*}
\end{thm}

\begin{rmk}
In both formulations of the Backward Equation, we have functions of time
and of a starting point, $u^{(k)}_n$ and $v^{(k)}_n$. However, for $u$,
the starting time is fixed, while we can vary the time \emph{from} or
\emph{after} the starting point. On the other hand, for $v$, the ending
time is fixed and $v$ varies as function of some time \emph{before} or
\emph{until} that terminal time. It is, therefore, from $v$ that the
name ``Backward Equation'' arises, as we can solve \emph{backwards}
moments and functions of the state prior to some terminal time.
\end{rmk}

\subsection{Continuous Time, Discrete Space}

\begin{defn}
The process $X=\{X_t\}_{t\geq 0}$ is a \emph{continuous time, discrete
space Markov Chain} if $X$ takes on only a discrete set of values in a
finite or countable state space $S$ and if
\begin{align*}
  \mathbb{P}\left(X_{t+h} = i_{t+h} | \left\{ X_{s}, s\leq t\right\}\right)
  =
  \mathbb{P}(X_{t+h} = i_{t+h} | X_{t} )
  \qquad \forall h \geq 0
\end{align*}
\end{defn}
\begin{ex}
The classic example is a time-homogeneous Poisson process with
$\lambda$, which has conditional probability distributions
\begin{align*}
  \mathbb{P}(X_{t+h}=j|X_t=i)
  &=
  \frac{e^{-\lambda h}(\lambda h)^{j-i}}{(j-i)!}
\end{align*}
\end{ex}

Now that time isn't neatly divided into discrete intervals, we can't
define a transition matrix since there's not necessarily a standard unit
of time between jumps. There are then two approaches:
\begin{enumerate}
  \item Consider the \emph{generator} $Q$, which is like a limit of the
    transition matrix as the time step goes to~0.
  \item Describe the distribution of \emph{jump times} of $X_t$, defined
    as $\tau_1, \tau_2,\ldots$, and then treat $X_t$ like a discrete
    time Markov Chain, conditional on the jumping times.
\end{enumerate}

\begin{defn}{(Transition Probabilities in Cts. Time)}
For a continuous time, discrete space Markov Chain, the \emph{transition
probability} $p_{ij}(s,t)$ at time $s$ for time step $t$ is defined
\begin{align*}
  p_{ij}(s,t) = P(X_{t+s}=j|X_s=i)
\end{align*}
We can then collect these in a transition matrix $P(s,t) =
(p_{ij}(s,t))$. Note that $P(s,0) = I$ for all $s$ for obvious reasons.

If $X$ is a homogeneous Markov Chain, then we drop the argument $s$ and
simply have $p_{ij}(t)$, the probability of transitioning from $i$ to
$j$ in $t$ units of time. Similarly the matrix of transition
probabilities becomes simply $P(t)$.
\end{defn}

\begin{rmk}
Again, we'll consider only homogeneous Markov Chains from now on.
\end{rmk}

\begin{thm}{\emph{(Chapman-Kolmorogov)}}
Similar to the discrete-time case, we have
\begin{align}
  p_{ij}(t+s) = \sum_k p_{ik}(t) p_{kj}(s)
  \quad \Leftrightarrow \quad
  P(t+s) = P(t)P(s)
  \label{chapkolg2}
\end{align}
\end{thm}

\begin{defn}
We define the \emph{generator} of a discrete-space, continuous-time
homogeneous Markov Chain $X_t$ as
\begin{align*}
  G = \lim_{h\rightarrow0} \frac{P(h)-I}{h}
\end{align*}
This is like the derivative of the transition matrix. This is useful in
determining how $P(t)$ evolves in time:
\begin{align*}
  \frac{dP}{dt}
  &= \lim_{h\rightarrow0} \frac{P(t+h)-P(t)}{h}\\
  \text{By~(\ref{chapkolg2})}\qquad
  &= \lim_{h\rightarrow0} P(t) \frac{P(h)-I}{h}\\
  \Rightarrow\qquad
  \frac{dP}{dt}
  &= P(t) G
\end{align*}
Note also that we could have pulled $P(t)$ out on the righthand side,
implying that the transition matrix commutes with the generator:
\begin{align*}
  \frac{dP}{dt}
  = P(t) G
  = GP(t)
\end{align*}
which bring us to the forward and backward equations. In both equations,
we want to know how probabilities or moments change with time;
therefore, it's not surprising that the generator should factor into the
discussion.
\end{defn}

\begin{thm}{\emph{(Forward Kolmogorov Equation)}}
As in discrete time and space, this tells us how to evolve
probabilities, only now in continuous time:
\begin{align*}
  \frac{dP}{dt} = P_t G
\end{align*}
where $G$ is the generator. Elementwise, this is equivalent to
\begin{align*}
  \frac{dp_{ij}(t)}{dt} = \sum_k p_{ij}(t) g_{kj}
\end{align*}
\end{thm}

\begin{thm}{\emph{(Backward Kolmogorov Equation)}}
\end{thm}


\subsection{Continuous Time, Continuous Space}

\begin{defn}
The process $X=\{X_t\}_{t\geq 0}$ is a \emph{continuous time, continuous
space Markov Chain} if $X$ takes on values in $\mathbb{R}$ and if
\begin{align*}
  \mathbb{P}\left(X_{t+h} \in \Gamma | \left\{ X_{s}, s\leq t\right\}\right)
  =
  \mathbb{P}(X_{t+h} \in \Gamma | X_{t} )
  \qquad \forall h \geq 0
\end{align*}
\end{defn}



\subsection{References}
\begin{enumerate}
  \item Koralev and Sinai Ch 5.1-5.5
\end{enumerate}

\clearpage
\section{Spectral Representation of Stochastic Processes}

\subsection{Stationary Processes}

Strong stationarity imposes structure on the fdds. Weak stationarity
only imposes structure on the mean and covariance functions.

\subsection{Spectral Measure}

\begin{thm}\emph{(Bochner's Theorem)}
Suppose you have a covariance function $C(t)$ of a weakly stationary
stochastic process $X_t$ (weakly stationary implying that $C(t)$ is
positive semi-definite), then there exists a finite measure $F(\lambda)$
such that $C(t)$ can be represented as
\begin{align*}
  C(t) = \int_\mathbb{R} e^{i\lambda t} dF(\lambda)
\end{align*}
The measure $F$ is called the \emph{spectral measure} of $X(t)$.
\end{thm}

\begin{defn}
If the spectral measure $F$ is absolutely continuous w.r.t.\ the
Lebesgue measure on $\mathbb{R}$, then
\begin{align*}
  dF(\lambda) = S(\lambda)d\lambda
  \quad \Rightarrow\quad
  C(t)
    &= \int^\infty_{-\infty} e^{i\lambda t} S(\lambda) d\lambda \\
  \Leftrightarrow
  S(\lambda)
    &= \frac{1}{2\pi} \int^\infty_{-\infty} e^{-i\lambda t} C(t) dt
\end{align*}
Note that the function $S(\lambda)$ is non-negative. Moreover, if we
want to come up with covariance functions, we can just write down
spectral density functions $S(\lambda)$ then compute the Fourier
transform to recover the corresponding $C(t)$.
\end{defn}

\subsection{Spectral Representation}

We saw how to obtain the spectral measure for a \emph{covariance
function} of a weakly stationary process. Now we specify the spectral
representation of a stationary stochastic process \emph{itself}.

\begin{thm}\emph{(Spectral Theorem)}
Given a stationary mean-zero stochastic process $X_t$ with spectral
distribution function $F(\lambda)$, there exists a complex-valued
process $Z(\lambda)$ such that
\begin{align*}
  X_t = \int^\infty_{-\infty} e^{i\lambda t} dZ(\lambda)
\end{align*}
where $Z$ (call the \emph{spectral process}) has the following
properties:
\begin{enumerate}
  \item Orthogonal Increments: For disjoint intervals
    $[\lambda_1,\lambda_2]$ and $[\lambda_3,\lambda_4]$,
    \begin{align*}
      E\left[
        (Z(\lambda_2)-Z(\lambda_1))
        \overline{(Z(\lambda_4)-Z(\lambda_3))}
      \right] = 0
    \end{align*}
  \item Spectral Weight: For $\lambda_1\leq \lambda_2$,
    \begin{align*}
      E\left\lvert Z(\lambda_2)-Z(\lambda_1)\right\rvert^2
      = F(\lambda_2)-F(\lambda_2)
    \end{align*}
\end{enumerate}
\end{thm}
\begin{rmk}
This looks something like the Ito integral, except the measure of the
interval $[\lambda_1,\lambda_2]$ in the integral above is
$Z(\lambda_2)-Z(\lambda_1)$, not $W(\lambda_2)-W(\lambda_1)$.
\end{rmk}

\subsection{References}
\begin{enumerate}
  \item Pavliotis 1.1, 1.2
  \item Koralev and Sinai, Ch 12, 15.3, 15.7, 16
\end{enumerate}

\section{Forward and Backward Equations}

In the sections above, we considered SDEs of the form
\begin{align*}
  dX_t = b(X_t,t) dt + \sigma(X_t,t) dW_t
\end{align*}
We then tried to solve these to find expressions for the \emph{paths} of $X_t$ as a function of time and Brownian motion paths as follows:
\begin{align*}
  X_t = f(t,W_t)
\end{align*}
In this section, we try another approach. Here, we try instead to
describe $X_t$ by describing the evolution of either
\begin{enumerate}
  \item The probability distribution of $X_t$ (leads to the Forward
    Equation)
  \item Functions or moments of $X_t$ (leads to the Backard Equation)
\end{enumerate}
Both use PDE methods. Since $X_t$ is Markov (by the fact that it solves
the above SDE with particular smoothness and growth conditions on $b$
and $\sigma$), we will often deal with the transition density:
\begin{align*}
  p(x,t|y,s) := P(X_t\in [x,x+dx) \; | \; X_s=y)
\end{align*}

\begin{defn}
Given a function $f\in C_c^\infty(\mathbb{R}^d)$ and a time-homogeneous
process (multi-dimensional) stochastic process $X_t$, define the
\emph{generator} of $X_t$
\begin{align}
  (\mathscr{A}f)(x)=
  \lim_{t\rightarrow0} \frac{E[f(X_t)|X_0=x]-f(x)}{t}
  \label{genA}
\end{align}
In words, the generator of a time-homogeneous process is the
expected infinitesimal change of some function of that process.
\end{defn}

\begin{thm}
Given a time homogeneous process $X_t$ satisfying SDE
\begin{align*}
  dX_t = b(X_t) dt + \sigma(X_t) dW_t
\end{align*}
the generator $\mathscr{A}$ of $X_t$ can be written via linear operator
$\mathscr{L}$
\begin{align}
  \mathscr{A}f =
  \mathscr{L}f &= b \cdot \nabla f
  + \frac{1}{2} \left(\sigma \sigma^T:\nabla^2 f\right)\label{genL} \\
  \Leftrightarrow \qquad
  &= \sum_i b_i \frac{\partial f}{\partial x_i}
  + \frac{1}{2} \sum_i \sum_j \left(\sigma \sigma^T\right)
    \frac{\partial^2 f}{\partial x_i \partial x_j}\notag
\end{align}
where $\nabla^2 f$ is the Hessian matrix of $f$, and $A:B :=
\text{tr}(A^T B)$.
\end{thm}
\begin{rmk}
Though $\mathscr{A}f = \mathscr{L}f$, we use separate notation to
distinguish that $\mathscr{A}$ and $\mathscr{L}$ come from different
places: a limit as in (\ref{genA}) versus a simple lifting of
coefficients from an SDE as in (\ref{genL}), respectively.
\end{rmk}
\begin{proof}
The result follows directly from Ito's Lemma. Therefore apply
multi-dimensional Ito's Lemma to $f(X_t)$ for homogeneous process $X_t$:
\begin{align*}
  df(X_t) &=
  \left(b\cdot \nabla f + \frac{1}{2}\sigma\sigma^T : \nabla^2 f\right)
  dt
  + \nabla f\cdot \sigma \;dW_t\\
  f(X_t) - f(X_s) &=
  \int^t_s
  \left(b\cdot \nabla f + \frac{1}{2}\sigma\sigma^T : \nabla^2 f\right)
  d\tau
  +
  \int^t_s
  \nabla f\cdot \sigma \;dW_\tau
\end{align*}
Now take conditional expectation $E[\;\cdot\;|X_s=x]$ and use the fact
that the Ito Integral has expectation of zero:
\begin{align}
  E[f(X_t)|f(X_s)=x] - f(x) &=
  E\left[
  \int^t_s
  \left(b\cdot \nabla f + \frac{1}{2}\sigma\sigma^T : \nabla^2 f\right)
  d\tau
  \; \big|\; f(X_s)=x\right]\notag\\
  &\qquad
  +
  E\left[
  \int^t_s
  \nabla f\cdot \sigma \;dW_\tau
  \; \big|\;f(X_s)=x\right]
  \notag\\
  &=
  E\left[ \int^t_s (\mathscr{L} f)(X_\tau) d\tau
    \;\big|\:f(X_s)=x\right] + 0
  \label{genL.proof}
\end{align}
From there, form the limit that defines the generator:
\begin{align*}
  (\mathscr{A}f)(x)
  &=\lim_{t\rightarrow0} \frac{E[f(X_t)\;|\;f(X_s)=x] - f(x)}{t}\\
  \text{Sub in (\ref{genL.proof})} \Rightarrow\qquad
  &=\lim_{t\rightarrow0} \frac{1}{t}
  E\left[ \int^t_s (\mathscr{L} f)(X_\tau) d\tau \; \big|\:f(X_s)=x\right]\\
  &=(\mathscr{L}f)(x)
\end{align*}
where the last step follows from the Dominated Convergence Theorem since
$f$ and all of its derivatives are bounded (K\&S p 323).
\end{proof}

\begin{thm}{(Forward Kolmogorov Equation and Fokker-Planck Equation)}
There are two formulations, both of which make use of $\mathscr{L}^*$,
defined as
\begin{align*}
  \mathscr{L}^*f_x = -\nabla_x \cdot (b f)
  +\frac{1}{2}\nabla^2_x : (a f)
\end{align*}
The two formulations are as follows:
\begin{enumerate}
  \item The transition probability density $p(x,t|y,s)$ solves
    \begin{align*}
      \frac{\partial p}{\partial t} = \mathscr{L}^*_x p
      \qquad p(x,s|y,s) = \delta(x-y)
    \end{align*}

  \item If $\rho_0(x)$ is the initial probability desnity and
    $\rho(x,t)$ is the density at time $t$, then $\rho(x,t)$ solves
    \begin{align}
      \frac{\partial \rho}{\partial t} = \mathscr{L}^* \rho
      \qquad \text{where } \; \rho(x,0) = \rho_0(x)
      \label{fpeqn}
    \end{align}
\end{enumerate}
\begin{rmk}
$\mathscr{L}^*$ is the adjoint of $\mathscr{L}$ meaning that
on the Hilbert Space with inner product
$\langle\cdot,\cdot\rangle_{L^2}$,\footnote{%
  The inner product $\langle\cdot,\cdot\rangle_{L^2}$ is defined as
  \begin{align*}
    \langle f,g\rangle_{L^2}
    &= \int_X fg\;d\mu
  \end{align*}
}
we have that $\langle\mathscr{L}f,g\rangle_{L^2} =\langle
f,\mathscr{L}^*g\rangle_{L^2}$
\end{rmk}
\end{thm}
\begin{proof}
We take each case separately:
\begin{enumerate}
  \item By Ito's Lemma and results we saw above, we can write:
    \begin{align*}
      E[f(X_t) \;|\; X_s=y] -f(y)
      &= E\left[\int^t_s (\mathscr{L}f(X_\tau) d\tau \;\big|\;
          f(X_s)=y\right]
    \end{align*}
    Writing the expectation out explicitly,
    \begin{align*}
      \int_{\mathbb{R}^d} f(x)p(x,t|y,s)\;dx  -f(y)
      &= \int_{\mathbb{R}^d} \int^t_s
        (\mathscr{L}f)(x) \cdot p(x,\tau|y,s)\; d\tau dx
    \end{align*}
    Differentiating both sides with respect to $t$,
    \begin{align*}
      \int_{\mathbb{R}^d} f(x) \frac{\partial p}{\partial t}\;dx
      &= \int_{\mathbb{R}^d}
        (\mathscr{L}f)(x) \cdot p\; dx
    \end{align*}
    And this holds for all test functions $f$, so $p$ is a weak
    solution.
\end{enumerate}
\end{proof}

\begin{thm}{\emph{(Backward Komogorov Equation)}}
There are 3 formulations. For each, let $p(x,t|y,s)$ denote the
transition probability of $X_t$, and for a function $f\in
C^2(\mathbb{R}^d)$, Then it is the case that
\begin{enumerate}
  \item Define $u_t^{(x)} := \mathbb{E}[f(X_t)|X_0=x]$ for a
    time-homogeneous process $X_t$. Then given boundary condition
    $u^{(x)}_0=f(x)$,
    \begin{align*}
      \frac{\partial u^{(x)}_t}{\partial t} = \mathscr{L} u^{(x)}_t
      \qquad t>0
    \end{align*}
  \item For a possibly time-inhomogeneous SDE with solution $X_t$,
    define $v^{(y)}_t:=E[f(X_T)|X_t~=~y]$. Then given boundary
    condition $v^{(y)}_T=f(y)$, we have
    \begin{align*}
      \frac{\partial v^{(y)}_t}{\partial t}
      + \mathscr{L} v^{(y)}_t = 0
      \qquad t < T
    \end{align*}
  \item For a Markov process solving a time-inhomogeneous SDE
    and for boundary condition $p(y,t|x,s) = \delta(x-y)$,
    \begin{align*}
      \frac{\partial p}{\partial s} + \mathscr{L}_y p = 0
      \qquad s<t
    \end{align*}
    where $\mathscr{L}_y$ denotes that the derivatives in $\mathscr{L}$
    are with respect to variable $y$.
\end{enumerate}
\end{thm}
\begin{rmk}
The second and third formulations should be solved \emph{backward} in
time, which gives the theorem its name.
\end{rmk}
\begin{proof}
We prove each in turn:
  \begin{enumerate}
    \item Recall from Ito's Lemma that for some stochastic process $X_t$
      and some twice-differentiable function $g: \mathbb{R}^d \times
      [0,\infty) \rightarrow \mathbb{R}^d$ (a function of both $X_t$ and
      $t$), we have
      \begin{align*}
        d\left(g(X_t,t)\right)
        &=
        \left(
        \frac{\partial g}{\partial t}
        + b \cdot \nabla_x g
        + \frac{1}{2} \sigma \sigma^T : \nabla_x^2 g
        \right)dt
        +
        \frac{\partial g}{\partial x} \sigma dW_t
      \end{align*}
    \item Relabel time as $t' = t-s$ and consider the function
      $u(x,t')$.
  \end{enumerate}
\end{proof}

\subsection{Probability Flux}

Let's write out the Fokker-Planck Equation (\ref{fpeqn}) as follows:
\begin{align*}
  \frac{\partial\rho}{\partial t}
  &= \nabla \cdot \left( -b\rho + \frac{1}{2}\nabla \cdot (a\rho)\right)\\
  \Leftrightarrow\qquad
  0&=
  \frac{\partial\rho}{\partial t}
  + \nabla \cdot \underaccent{\bar}{j}
  \qquad
  \text{where}\;
  \underaccent{\bar}{j} = b\rho - \frac{1}{2}\nabla\cdot(a\rho)
\end{align*}
This is a conservation law for $\rho$, showing that any increase in
probability must be counterbalanced by $\nabla \cdot
\underaccent{\bar}{j}$ and vice versa.



%% APPPENDIX %%

% \appendix






%% APPPENDIX %%

% \appendix




\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}  %Use in text; cites
        \citep{LabelInSourcesFile} %Use in text; cites in parens

        \nocite{LabelInSourceFile} % Includes in refs w/o specific citation
        \bibliographystyle{apalike}  % Or some other style

        % To ditch the ``References'' header
        \begingroup
        \renewcommand{\section}[2]{}
        \endgroup

        \bibliography{sources} % where sources.bib has all the citation info

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%

        \verbatiminput{file.ext}
            %---Includes verbatim text from the file
        \texttt{text}
            %---Renders text in courier, or code-like, font

        \matlabcode{file.m}
            %---Includes Matlab code with colors and line numbers


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}


        % Side by Side figures
            \begin{figure}[h!]
                \centering
                \mbox{\subfigure{
                    \includegraphics[scale=1]{file1.pdf}
                }\quad\subfigure{
                    \includegraphics[scale=1]{file2.pdf}
                }
                }
            \end{figure}


