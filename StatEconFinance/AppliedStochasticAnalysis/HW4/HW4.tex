\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Homework 4}
\date{March 2, 2015}

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt}
    %---Width of the line

%\setlength{\headsep}{0.2in}
    %---Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure}
    %---For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{
    colorlinks,
        %---This colors the links themselves, not boxes
    citecolor=black,
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},%
            %---Size of the numbers
        numbersep=9pt,%
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %---Has to do with enumeration
\usepackage{appendix}
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

%\tableofcontents

\begin{enumerate}
  \item % Question 1
    To show that $B(s,t)=EX_sX_t$ is a positive semi-definite function,
    we need to show that the matrix $B =
    \left(B(t_i,t_j)\right)_{i,j=1}^k$ is positive semi-definite for all
    finite slices $\{t_i\}_{i=1}^k$.

    So let $\{t_i\}_{i=1}^k$ be any such finite slice. We want to show
    that for any vector $z\neq 0$
    \begin{align*}
      0 \leq z^T B z
      &= \sum^k_{j=1} \sum^k_{i=1} v_i v_j B(t_i,t_j)\\
      &= \sum^k_{j=1} \sum^k_{i=1} v_i v_j E[X_{t_i}X_{t_j}]\\
      \text{By linearity of expectations} \qquad
      &= E\left[\sum^k_{j=1} \sum^k_{i=1} v_i v_j X_{t_i}X_{t_j}\right]\\
      &= E\left[\sum^k_{j=1} v_j X_{t_j} \sum^k_{i=1} v_i
          X_{t_i}\right]\\
      &= E\left[\left(\sum^k_{j=1} v_j X_{t_j}\right)
          \left( \sum^k_{i=1} v_i X_{t_i}\right)\right]\\
      &= E\left[\left(\sum^k_{j=1} v_j X_{t_j}\right)^2 \right]
    \end{align*}
    which will certainly be greater than or equal to zero as the square
    of the sum will always be non-negative. Therefore $B(s,t)$ is
    positive semi-definite.

  \item % Question 2
    To begin, the distribution of a Poisson process follows
    \begin{equation}
      P(N_t = n) = e^{-\lambda t} \frac{(\lambda t)^n}{n!}
    \end{equation}
    \begin{enumerate}
      \item % Question 2a
        To calculate the mean
        \begin{align*}
          EN_t &=
            \sum^\infty_{n=0} n e^{-\lambda t} \frac{(\lambda t)^n}{n!}
            =\sum^\infty_{n=1} n e^{-\lambda t} \frac{(\lambda t)^n}{n!}\\
          &=e^{-\lambda t} (\lambda t) \sum^\infty_{n=1}
          \frac{(\lambda t)^{n-1}}{(n-1)!}\\
          &=e^{-\lambda t} (\lambda t) \sum^\infty_{n=0}
            \frac{(\lambda t)^{n}}{n!}\\
          \text{Simplifying the power series} \qquad
          &=e^{-\lambda t} (\lambda t) \cdot e^{\lambda t}\\
          &=\lambda t
        \end{align*}
        To calculate the covariance, I first calculate the second
        moment:
        \begin{align*}
          EN_t^2 &=
            \sum^\infty_{n=0} n^2 e^{-\lambda t} \frac{(\lambda t)^n}{n!}
            =\sum^\infty_{n=1} n^2 e^{-\lambda t} \frac{(\lambda t)^n}{n!}\\
          &=e^{-\lambda t} (\lambda t) \sum^\infty_{n=1} n
          \frac{(\lambda t)^{n-1}}{(n-1)!}\\
          &=e^{-\lambda t} (\lambda t) \sum^\infty_{n=0} (n+1)
            \frac{(\lambda t)^{n}}{n!}\\
          &=e^{-\lambda t} (\lambda t) \left\{\sum^\infty_{n=0} n
            \frac{(\lambda t)^{n}}{n!} + \sum^\infty_{n=0}
            \frac{(\lambda t)^{n}}{n!}\right\}\\
          &=(\lambda t) \sum^\infty_{n=0} ne^{-\lambda t}
            \frac{(\lambda t)^{n}}{n!}
            +  (\lambda t)\sum^\infty_{n=0}e^{-\lambda t}
            \frac{(\lambda t)^{n}}{n!}\\
          \text{From above} \qquad
          &=(\lambda t)(\lambda t)
            +  (\lambda t)\sum^\infty_{n=0}e^{-\lambda t}
            \frac{(\lambda t)^{n}}{n!}\\
          &=(\lambda t)^2 + (\lambda t) \cdot 1\\
          &=(\lambda t)^2 + \lambda t
        \end{align*}
        Now, to compute the covariance, I start by computing
        $E[N_{t+s}N_s]$. However, since $N_{t+s}$ depends upon $N_s$
        (and cannot be less than $N_s$), it will be easier to break
        things up into independent increments as follows:
        \begin{align*}
          E[N_{t+s} N_s]
            &= E[(N_{t+s} - N_s + N_s)N_s] \\
            &= E[(N_{t+s} - N_s)N_s] + E[N^2_s]
        \end{align*}
        Since the definition of the Poisson process includes independent
        increments, $(N_{t+s}~-~N_s) \perp N_s$, allowing us
        to apply $EXY = EX\cdot EY$ for $X\perp Y$, implying
        \begin{align*}
          E[N_{t+s} N_s]
            &= E[N_{t+s} - N_s]EN_s + EN^2_s \\
            &= (\lambda t) (\lambda s) + (\lambda s)^2 + \lambda s \\
        \end{align*}
        where this above simplification uses
        \begin{enumerate}
          \item The second moment result from above to get
            $EN_s^2=(\lambda s)^2 + \lambda s$.
        \item The result we will prove in part (c) that $E[N_{t+s}-N_s]
          = \lambda t$.
        \end{enumerate}
        Finally, this can be applied to compute the covariance function:
        \begin{align*}
          E[(N_{t+s}-EN_{t+s})(N_s-EN_s)]
            &= E[N_{t+s}N_{s}] - E[N_{t+s}]E[N_{s}]\\
            &= \left[(\lambda t) (\lambda s) + (\lambda s)^2 + \lambda s
              \right] - (\lambda(t+s))(\lambda s)\\
            &= \lambda s
        \end{align*}
        Since the covariance depends upon $s$, the absolute time, not
        the relative length $t$ of the time gap between $N_{t+s}$ and
        $N_s$, the process is not weakly stationary.  Therefore, it
        cannot be strongly stationary either, since strongly stationary
        implies weakly stationary.

      \item % Question 2b
        $X_t$ is strongly stationary because $X_t=N_{t+\alpha}-N_t$ is an
        increment of a Poisson Process. By the definition of the Poisson
        Process, these increments are independent and stationary, so
        $P(N_{t+\alpha}-N_t) = P(N_\alpha-N_0)$. Therefore $P(X_t) =
        P(X_0)$ for all $t$.

      \item % Question 2c
        \begin{enumerate}
        \item First, the mean
        \begin{align*}
          EX_t &= E[N_{t+\alpha} - N_t]
            = E[N_{t+\alpha}] - E[N_t]\\
            &= \lambda(t+\alpha) - \lambda t = \lambda \alpha
        \end{align*}

        \item
        Next, $E[X_{t+s} X_s]$, using some of the results from part (a)
        \begin{align}
          E[X_{t+s} X_s]
          &= E[(N_{t+s+\alpha}-N_{t+s})(N_{s+\alpha}-N_s)]\notag\\
          &= E[N_{t+s+\alpha} N_{s+\alpha}]
            - E[N_{t+s+\alpha}N_s]
            - E[N_{t+s}N_{s+\alpha}]
            + E[N_{t+s}N_s]\notag\\
          &= \left[(\lambda t)(\lambda(s+\alpha)) + (\lambda(s+\alpha))^2
            + \lambda (s+\alpha)\right]\notag\\
            & \qquad - \left[(\lambda(t+\alpha))(\lambda s)
               + (\lambda s)^2 + \lambda s\right]
            - E[N_{t+s}N_{s+\alpha}] \notag \\
          &\qquad + \left[(\lambda t)(\lambda s) +
              (\lambda s)^2 + \lambda s\right]\notag\\
          &= \lambda^2 ts + \lambda^2 t\alpha + \lambda^2 s^2
            + \lambda^2 s\alpha + \lambda^2 \alpha^2 + \lambda s
            +\lambda \alpha- E[N_{t+s}N_{s+\alpha}]
            \label{q2c.1}
        \end{align}
        Now suppose that $t>\alpha$. Again, we break up the interval
        into independent increments
        \begin{align*}
          E[N_{t+s}N_{s+\alpha}]
            &= E[(N_{t+s} - N_{s+\alpha} + N_{s+\alpha})N_{s+\alpha}]\\
            &= E[(N_{t+s} - N_{s+\alpha})N_{s+\alpha}]
              + E[N^2_{s+\alpha}]\\
            &= E[N_{t+s} - N_{s+\alpha}]E[N_{s+\alpha}]
              + E[N^2_{s+\alpha}]\\
            &= (\lambda(t-\alpha)) (\lambda (s+\alpha))
              + (\lambda(s+\alpha))^2 + \lambda (s+\alpha)
        \end{align*}
        In which case, after substituting in for
        $E[N_{t+s}N_{s+\alpha}]$, Expression~\ref{q2c.1} simplifies to
        \begin{align*}
          E[X_{t+s} X_s] = \lambda^2 \alpha^2
        \end{align*}
        As a result, we have the covariance
        \begin{align*}
          \text{Cov}(X_{t+s},X_s) &= EX_{t+s}X_s - (EX_{t+s})(EX_s)\\
          &= \lambda^2\alpha^2 -(\lambda \alpha)(\lambda \alpha) \\
          &= 0
        \end{align*}
        which follows from $t>\alpha$, since that means $X_{t+s}$ and
        $X_s$ have disjoint intervals.

        On the other hand, if $t<\alpha$,
        \begin{align*}
          E[N_{t+s}N_{s+\alpha}]
            &= (\lambda(\alpha-t)) (\lambda (s+t))
              + (\lambda(s+t))^2 + \lambda (s+t)
        \end{align*}
        In which case, after substituting in for
        $E[N_{t+s}N_{s+\alpha}]$, Expression~\ref{q2c.1} simplifies to
        \begin{align*}
          E[X_{t+s}X_{s+\alpha}]
            &= \lambda^2\alpha^2 + \lambda \alpha - \lambda t
        \end{align*}
        As a result, the covariance is
        \begin{align*}
          \text{Cov}(X_{t+s}, X_s)
            &= EX_{t+s}X_s - (EX_{t+s})(EX_s)\\
            &= \lambda^2\alpha^2 + \lambda \alpha - \lambda t - (\lambda\alpha)^2\\
            &= \lambda (\alpha - t)
        \end{align*}
        So in either case ($t\geq \alpha$ or $t<\alpha$), the covariance
        does not depend upon $s$, total length of time until $X_s$.
        Rather, it depends upon some fixed constant $\alpha$ and the
        time gap $t$ between $X_{s}$ and $X_{t+s}$.

        As a result, we can define
        \begin{align*}
        C(t) = \lambda(\alpha - |t|)
        \end{align*}
        where the absolute value follows since we must have $C(t) =
        C(-t)$.

        \item Now for the spectral density of $X(t)$ (which I couldn't
          get to converge when computing the integral)
        \begin{align*}
          S(\omega)
           &= \frac{\lambda}{2\pi} \int^\infty_{-\infty} (\alpha-|t|)e^{-it\omega} dt
           %\\
           %&= \frac{\lambda}{2\pi} \int^\infty_{0} (\alpha-t)e^{-it\omega} dt
              %+ \frac{\lambda}{2\pi}\int^0_{-\infty} (\alpha+t)e^{-it\omega} dt \\
           %&= \frac{\lambda}{2\pi}
              %\left( -\frac{1}{i \omega}(\alpha-t)e^{-it\omega} \big|^\infty_0
              %- \int^\infty_{0} \frac{1}{i \omega}e^{-it\omega} dt
              %\right) \\
            %&\qquad
             %+ \frac{\lambda}{2\pi}
              %\left(-\frac{1}{i\omega}(\alpha+t)e^{-it\omega} \big|^0_{-\infty}
              %+ \int^0_{-\infty} \frac{1}{i \omega}e^{-it\omega} dt
              %\right) \\
           %&= \frac{\lambda}{2\pi}
              %\left( \frac{\alpha}{i \omega}
              %+ \frac{1}{i^2\omega^2} e^{-it\omega}\big|^\infty_0
              %\right) \\
            %&\qquad
             %+ \frac{\lambda}{2\pi}
              %\left(-\frac{\alpha}{i\omega}
              %- \frac{1}{i^2 \omega^2}e^{-it\omega} \big|^0_{-\infty}
              %\right) \\
           %&= \frac{\lambda}{2\pi}
              %\left( \frac{\alpha }{i \omega }
              %+ \frac{1}{\omega^2}
              %\right) \\
            %&\qquad
             %+ \frac{\lambda}{2\pi}
              %\left(-\frac{\alpha}{i\omega}
              %- \frac{1}{i^2 \omega^2}e^{-it\omega} \big|^0_{-\infty}
              %\right) \\
           %&= \frac{\lambda}{2\pi}
              %\left(
                %\frac{\alpha i}{\omega} +
                %\frac{i^2}{\omega^2}e^{-it\omega}\big|_0^\infty
              %\right) \\
            %&\qquad
             %+ \frac{\lambda}{2\pi}
              %\left( \frac{i \alpha}{\omega}
              %-  \frac{i^2}{\omega^2}e^{-it\omega}\big|^0_{-\infty}
              %\right) \\
           %&= \frac{\lambda}{2\pi}
              %\left(
                %\frac{\alpha i}{\omega} -
                %\frac{1}{\omega^2}
              %\right)
             %+ \frac{\lambda}{2\pi}
              %\left( \frac{i \alpha}{\omega}
              %+  \frac{1}{\omega^2}
              %\right)
        \end{align*}
      \end{enumerate}
    \end{enumerate}

  \item % Question 3
    In this question $X_t = \xi_1 e^{i\lambda_1 t} + \xi_2
    e^{i\lambda_2 t}$ with $\lambda_1 \neq \lambda_2$ and $\xi_i$ random
    variables.
    \begin{enumerate}
      \item % Question 3a
        First we show that $E\xi_i=0$ and $E\xi_1\xi_2=0$ implies $X_t$
        is weakly stationary.

        First, the mean:
        \begin{align*}
          EX_t &= E[ \xi_1 e^{i\lambda_1 t} + \xi_2e^{i\lambda_2 t}] \\
          &=  e^{i\lambda_1 t}E[ \xi_1] + e^{i\lambda_2 t}E[\xi_2] \\
          \text{By assumption} \qquad
          &=  e^{i\lambda_1 t}\cdot 0 + e^{i\lambda_2 t}\cdot 0 = 0
        \end{align*}
        As a result, the mean of $X_t$ is constant at zero for all $t$.
        Now for the covariance:
        \begin{align*}
          \text{Cov}(X_{s+t},X_s) &= EX_{t+s}\overline{X_s} -(EX_{t+s})(EX_s) \\
          &=  EX_{t+s}\overline{X_s}-0 \\
          &= EX_{t+s}\overline{X_s}\\
          &= E[( \xi_1 e^{i\lambda_1 (t+s)} + \xi_2e^{i\lambda_2 (t+s)})
            \overline{( \xi_1 e^{i\lambda_1 s} + \xi_2e^{i\lambda_2 s})}]\\
          &= E[ (\xi_1 e^{i\lambda_1 (t+s)} + \xi_2e^{i\lambda_2 (t+s)})
            ( {\xi_1} e^{-i\lambda_1 s} + {\xi_2}e^{-i\lambda_2 s})]\\
          &= E[ \xi_1{\xi_1} e^{i\lambda_1 t}]
             + E[ \xi_1{\xi_2} e^{i\lambda_1 (t+s)-i\lambda_2 s}]\\
          &\qquad
             + E[ \xi_2{\xi_1} e^{i\lambda_2 (t+s)-i\lambda_1 s}]
            + E[ \xi_2{\xi_2} e^{i\lambda_2 t}] \\
          &= e^{i\lambda_1 t}E[ \xi_1{\xi_1} ]
             + e^{i\lambda_1 (t+s)-i\lambda_2 s}E[ \xi_1{\xi_2} ]\\
          &\qquad
             + e^{i\lambda_2 (t+s)-i\lambda_1 s}E[ \xi_2{\xi_1}]
            + e^{i\lambda_2 t}E[ \xi_2{\xi_2} ] \\
          \text{$E[\xi_1\xi_2] =0$ by assumption} \quad
          &= e^{i\lambda_1 t}E[ \xi_1{\xi_1} ]
             +0 +0
            + e^{i\lambda_2 t}E[ \xi_2{\xi_2} ]
        \end{align*}
        Clearly, $EX_{t+s}X_s$ does not depend upon $s$, only the time
        gap $t$ between $X_{s}$ and $X_{t+s}$.  Combined with the fact
        that $X_t$ has constant mean, $X_t$ is therefore second-order
        stationary.

        Next, we want to show that second order stationary implies
        $E\xi_i=0$ and $E\xi_1\xi_2=0$.
        %Starting with the mean, suppose without loss of generality that
        %$\lambda_2 >\lambda_1$. Then
        Recall that
        \begin{align*}
          EX_t &= E[ \xi_1 e^{i\lambda_1 t} + \xi_2e^{i\lambda_2 t}] \\
          &=  e^{i\lambda_1 t}E[ \xi_1] + e^{i\lambda_2 t}E[\xi_2]
          %\\
          %&=  e^{i\lambda_1 t}\left( E[ \xi_1] + e^{i(\lambda_2-\lambda_1) t}E[\xi_2]
          %\right)
        \end{align*}
        %Suppose that this equals a constant $C$ but $E\xi_i\neq 0$.
        %Then
        %\begin{align*}
          %C
          %&=  e^{i\lambda_1 t}E[ \xi_1] + e^{i\lambda_2 t}E[\xi_2]\\
          %\Rightarrow\qquad
          %\ln C
          %&=  {i\lambda_1 t} + \ln E[ \xi_1] + {i\lambda_2 t} + \ln E[\xi_2]\\
          %&=  i(\lambda_1 +\lambda_2)t + \ln E[ \xi_1] + \ln E[\xi_2]\\
          %%C
          %%&=  e^{i\lambda_2 t}
            %%\left( e^{i(\lambda_1-\lambda_2)t}E[ \xi_1] + E[\xi_2]
            %%\right)\\
        %\end{align*}
        For this to be constant for all $t$, we need $E\xi_i=0$,
        otherwise, for constants $E[\xi_1]$ and $E[\xi_2]$, the two
        terms won't balance out at a constant (assuming $\lambda_1\neq
        \lambda_2$) since the exponential will be growing at different
        rates with $t$.

        For the covariance, we use the fact that $E\xi_i=0$ and focus
        on $E\xi_1\xi_2$. From above, we know
        \begin{align*}
          EX_{t+s}\overline{X_s}
          &= e^{i\lambda_1 t}E[ \xi_1{\xi_1} ]
             + e^{i\lambda_1 (t+s)-i\lambda_2 s}E[ \xi_1{\xi_2} ]\\
          &\qquad
             + e^{i\lambda_2 (t+s)-i\lambda_1 s}E[ \xi_2{\xi_1}]
            + e^{i\lambda_2 t}E[ \xi_2{\xi_2} ]
        \end{align*}
        Since $\lambda_1\neq \lambda_2$, the terms $e^{i\lambda_1
        (t+s)-i\lambda_2 s}$ and $e^{i\lambda_2 (t+s)-i\lambda_1 s}$
        need to drop, otherwise the covariance will be a function of
        $s$, violating second-order stationarity. The only way this can
        happen is if $E\xi_1\xi_2=0$.

      \item % Question 3b
        Since $X_t = \sum^2_{i=1}\xi_i e^{i\lambda_i t}$,
        the spectral process of $X$ satisfies
        \begin{align*}
          dZ(\lambda) = \sum^2_{i=1} \xi_i \delta(\lambda -\lambda_i) d\lambda
        \end{align*}
        Therefore
        \begin{align*}
          dF(\lambda) = \sum^2_{j=1} a_j \delta(\lambda -\lambda_j) d\lambda
        \end{align*}
        where $E|\xi_j|^2 = a_j$.

        As a result
        \begin{align*}
          C(t) &= \int_\mathbb{R} e^{i\lambda t} dF(\lambda) \\
            &= \int_\mathbb{R} e^{i\lambda t}
              \sum^2_{j=1} a_j \delta(\lambda -\lambda_j) d\lambda \\
            &= \sum^2_{j=1} a_j e^{i\lambda_j t}
        \end{align*}

        Moreover, the spectral distribution function can be written
        \begin{align*}
         F(\lambda) =
          \begin{cases}
            0 & \lambda < \lambda_1 \\
            a_1 & \lambda \in [\lambda_1,\lambda_2)\\
            a_1+a_2 & \lambda \geq \lambda_2
          \end{cases}
        \end{align*}

      \item % Question 3c
      \item % Question 3d
        %By Euler's formula
        %\begin{align*}
          %X_t &= \xi_1 e^{i\lambda_1 t} + \xi_2 e^{i\lambda_2 t}\\
          %&= \xi_1 \left( \cos \lambda_1 t + i 
        %\end{align*}

    \end{enumerate}

  \item % Question 4
    \begin{enumerate}
      \item % Question 4a
        Because $X(0)\sim \pi$, we can evolve the probability
        distributions by right multiplying probability distributions by
        $P$. We also use the fact that $\pi$ is the stationary
        distribution (implying $\pi P = \pi$) to get
        \begin{align*}
          X_n &\sim \pi P^n = (\pi P)P^{n-1} = \pi P^{n-1} = \cdots = \pi
        \end{align*}
        Since the distribution of $X_n$ is the same for all $n$, $X$ is
        strongly stationary.

        $I$ is also strongly stationary because the distribution of
        $I_n$ is fully characterized by $P(I_n=1)=P(X_n=k)$, which does
        not change over time since $X_n$ is strongly stationary,
        implying $P(X_n=k)$ is constant. As a result $P(I_n=1)$ is
        constant as well.

      \item % Question 4b
        First, the mean:
        \begin{align*}
          EI_n &= 1\cdot P(I_n=1) + 0\cdot P(I_n=0)\\
          &= P(X_n=k)\\
          &= \pi_k
        \end{align*}
        Next for the covariance function, I compute $E[I_{n+m}I_n]$
        which will only be nonzero if $I_{n+m}=I_n=1$, allowing
        \begin{align*}
          E[I_{n+m}I_n] &= 1 \cdot 1\cdot P(I_{n+m}=1, I_n=1)\\
          &= P(X_{n+m}=k , X_n=k)\\
          &= P(X_{n+m}=k | X_n=k)\cdot P(X_n=k)\\
          &= (P^m)_{kk}  \pi_k
        \end{align*}
        where $(P^m)_{kk}$ is the $k$th element of matrix $P^m$.

      \item % Question 4c
        First, we need to argue that $\frac{1}{n}S_n$ is weakly stationary:
        \begin{align*}
          E\left[\frac{1}{n}S_n\right] = \frac{1}{n}E\left\{\sum^{n-1}_{j=0}I_j\right\}
            = \frac{1}{n}\sum^{n-1}_{j=0}E\left\{I_j\right\}
            = \frac{1}{n} (n \pi_k) = \pi_k
        \end{align*}
        which is constant for all $n$.

        In addition, the covariance function between
        $\frac{1}{n+m}S_{n+m}$ and $\frac{1}{n}S_n$ will only depend
        upon the gap $m$ since
        $E\left(\frac{1}{n+m}S_{n+m}\right)\left(\frac{1}{n}S_n\right)$
        is a sum of terms like $E[I_iI_j]$, which is just $(P^t)_kk$ for
        some $t$ (see above) and thus independent of $n$.

        Therefore, $\frac{1}{n}S_n$ is weakly stationary, and since the
        rate of approach of $(P^n)_{ij}$ to $\pi_j$ is exponential
        $C(n)\in L^1(0,\infty)$, implying that $\frac{1}{n}S_n$
        converges to $\pi_k$ in mean square.

        %Next, the covariance
        %\begin{align*}
          %E\left[\left(\frac{1}{n+m}S_{n+m} \right)\left(\frac{1}{n} S_n\right)\right]
          %= \frac{1}{n(n+m)}E\left[S_{n+m}S_n\right]\\
          %= \frac{1}{n(n+m)}E\left[(S_{n+m}-S_n + S_n)S_n\right]\\
        %\end{align*}

    \end{enumerate}

  \item % Question 5
    \begin{enumerate}
      \item Assuming that $C(t) = EX_{t+s}X_s$ has a continuous second
      derivative, we want to compute the limit of
      \begin{align*}
        E&\left[\frac{X_{t+h}-X_t}{h} - \frac{X_{t+h'}-X_t}{h'}\right]^2 \\
        &\qquad
          = E\left[\frac{\left(X_{t+h}-X_t\right)^2}{h^2}\right]
            - 2E\left[ \frac{X_{t+h}-X_t}{h} \frac{X_{t+h'}-X_t}{h'}\right]\\
          &\qquad\qquad + E\left[\frac{\left(X_{t+h'}-X_t\right)^2}{h'^2}\right]\\
        &\qquad
          =
            \frac{1}{h^2}E\left[X^2_{t+h}-2X_{t+h}X_t+X_t^2\right]\\
          & \qquad\qquad- \frac{2}{hh'}
            E\left[ X_{t+h}X_{t+h'} -X_{t+h} X_t -X_{t+h'}X_t + X_t^2\right]\\
          &\qquad\qquad + \frac{1}{h'^2} E\left[X_{t+h'}^2 - 2X_{t+h'}X_t + X_t^2\right]
      \end{align*}
    Breaking things up via linearity
    \begin{align*}
      E&\left[\frac{X_{t+h}-X_t}{h} - \frac{X_{t+h'}-X_t}{h'}\right]^2 \\
        &\qquad=
          \frac{1}{h^2}E\left[X^2_{t+h}\right]
          -\frac{2}{h^2}E\left[X_{t+h}X_t\right]
          +\frac{1}{h^2}E\left[X_t^2\right]\\
        & \qquad\qquad
          -\frac{2}{hh'}
          E\left[ X_{t+h}X_{t+h'}\right]
          +\frac{2}{hh'}
          E\left[ X_{t+h} X_t\right]
          +\frac{2}{hh'}
          E\left[ X_{t+h'}X_t \right]
          -\frac{2}{hh'}
          E\left[ X_t^2\right]\\
        &\qquad\qquad
          + \frac{1}{h'^2} E\left[X_{t+h'}^2\right]
          - \frac{2}{h'^2} E\left[X_{t+h'}X_t\right]
          + \frac{1}{h'^2} E\left[X_t^2\right]\\\\
    \end{align*}
    Now using the definition of the covariance function, we can
    substitute in and simplify:
    \begin{align*}
      E&\left[\frac{X_{t+h}-X_t}{h} - \frac{X_{t+h'}-X_t}{h'}\right]^2 \\
        & \quad =
          \frac{C(0) - 2C(h) +C(0)}{h^2}
          -\frac{2 (C(h-h') - C(h) - C(h') +C(0))}{hh'}\\
        &\qquad\qquad
          + \frac{(C(0) -2C(h') + C(0) }{h'^2}
    \end{align*}
    Since $C(t)$ has a continuous second derivative, it has a first
    derivative and we can apply the Mean Value Theorem
    \begin{align*}
      E&\left[\frac{X_{t+h}-X_t}{h} - \frac{X_{t+h'}-X_t}{h'}\right]^2 \\
        & \quad =
          -\frac{2}{h}\frac{C(h) - C(0)}{h}
          -\frac{2}{h} \frac{C(h-h') - C(h)}{h'}
          +\frac{2}{h} \frac{C(h') -C(0))}{h'}\\
        &\qquad\qquad
          -\frac{2}{h'}\frac{C(h') - C(0)}{h'}\\
        & \quad =
          -\frac{2}{h}C'(h_1)
          -\frac{2}{h} C'(h_2)
          +\frac{2}{h} C'(h_3)
          -\frac{2}{h'}C'(h_4)
    \end{align*}
    Taking the limit as $h,h'\rightarrow 0$, $h_i\rightarrow 0$ as well
    for $i=1,2,3,4$, and we get a sum of scalar multiples of $C'(0)$.
    Since $C(0)$ is the maximum of the covariance function, $C'(0)=0$,
    so the limit equals 0.

    \item % Question 5b
      To show this, we write out
      \begin{align*}
        E\left[ \frac{X_{s+t+h}-X_{s+t}}{h} \frac{X_{s+h}-X_s}{h}\right]
        &= \frac{1}{h^2}
        E\left[ (X_{s+t+h}-X_{s+t}) (X_{s+h}-X_s)\right]\\
        &= \frac{1}{h^2}\left\{
        E[ X_{s+t+h} X_{s+h}] -E[X_{s+t+h} X_s ] \right. \\
        &\left. \qquad -E[X_{s+t} X_{s+h}]+ E[X_{s+t}X_{s}]
        \right\}\\
        &= \frac{1}{h^2}\left\{ C(t) -C(t+h) -C(t-h)+ C(t) \right\}
      \end{align*}
      Rearranging and applying the Mean Value Theorem
      \begin{align*}
        \lim_{h\rightarrow 0}
        E\left[ \frac{X_{s+t+h}-X_{s+t}}{h} \frac{X_{s+h}-X_s}{h}\right]
        &= \lim_{h\rightarrow 0}
          \frac{1}{h^2}\left\{ C(t) -C(t+h) -C(t-h)+ C(t) \right\}\\
        &= -\lim_{h\rightarrow 0}
          \frac{1}{h}\left\{ \frac{C(t+h) -C(t)}{h} -\frac{C(t) -C(t-h)}{h} \right\}\\
        &= -\lim_{h\rightarrow 0}
          \frac{C'(h_1) - C'(h_2)}{h}
      \end{align*}
      for $h_1 \in [t,t+h]$ and $h_2\in [t-h,t]$. But this is
      \begin{align*}
        E\left[ \frac{X_{s+t+h}-X_{s+t}}{h} \frac{X_{s+h}-X_s}{h}\right]
        &= -C''(t)
      \end{align*}
  \end{enumerate}

  \item % Question 6
    \begin{enumerate}
      \item % Question 6a
        To calculate $X'_t$ via the spectral representation:
        \begin{align*}
          X'_t
            &=  \frac{d}{dt} \int^\infty_{-\infty} e^{i\lambda t} dZ(\lambda)\\
            &=  \int^\infty_{-\infty} \frac{d}{dt} e^{i\lambda t} dZ(\lambda)\\
            &=  \int^\infty_{-\infty} i\lambda e^{i\lambda t} dZ(\lambda)
        \end{align*}

      \item To compute the joint covariance:
        \begin{align*}
          C_{X,X'}(t) = EX_{t+s}\overline{X'_s}
            &= \int^\infty_{-\infty} \int^\infty_{-\infty}
            \left(e^{i\lambda (t+s)}\right)
            \overline{\left(i\tilde{\lambda} e^{i\tilde{\lambda} s}\right)}
            E(dZ(\lambda)\overline{dZ(\tilde{\lambda})}) \\
          &= \int^\infty_{-\infty} \int^\infty_{-\infty}
            -i\tilde{\lambda} e^{i(\lambda-\tilde{\lambda}) s +{i\lambda}t}
            E(dZ(\lambda)\overline{dZ(\tilde{\lambda})}) \\
          &= \int^\infty_{-\infty} \int^\infty_{-\infty}
            -i\tilde{\lambda} e^{i(\lambda-\tilde{\lambda}) s +{i\lambda}t}
            \delta(\lambda-\tilde{\lambda}) dF(\lambda)\tilde{\lambda}\\
          &= \int^\infty_{-\infty} 
            -i{\lambda} e^{{i\lambda}t}
            dF(\lambda)
        \end{align*}

    \end{enumerate}

\end{enumerate}


%% APPPENDIX %%

% \appendix




\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}  %Use in text; cites
        \citep{LabelInSourcesFile} %Use in text; cites in parens

        \nocite{LabelInSourceFile} % Includes in refs w/o specific citation
        \bibliographystyle{apalike}  % Or some other style

        % To ditch the ``References'' header
        \begingroup
        \renewcommand{\section}[2]{}
        \endgroup

        \bibliography{sources} % where sources.bib has all the citation info

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%

        \verbatiminput{file.ext}
            %---Includes verbatim text from the file
        \texttt{text}
            %---Renders text in courier, or code-like, font

        \matlabcode{file.m}
            %---Includes Matlab code with colors and line numbers


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}


        % Side by Side figures
            \begin{figure}[h!]
                \centering
                \mbox{\subfigure{
                    \includegraphics[scale=1]{file1.pdf}
                }\quad\subfigure{
                    \includegraphics[scale=1]{file2.pdf}
                }
                }
            \end{figure}


