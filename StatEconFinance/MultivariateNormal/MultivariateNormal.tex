\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Title}
\date{today}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage{hyperref}
%\numberwithin{equation}{section} 
%, This labels the equations in relation to the sections rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}


\begin{document}

\begin{center} \LARGE \bf
   The Multivariate Normal Distribution
\end{center}

The univariate normal distribution is an extremely familiar concept
where some random variable $X$ can take values along the real with 
probabilities that match the famouse bell-curve. Recall 
the probability density function of
   \[ f_Y(y) = \frac{1}{\sigma \sqrt{2\pi}} \; e^{-\frac{1}{2\sigma^2}
      (y - \mu)^2} \]
However, that's 
limited to only one dimension, and we would like to generalize to 
higher dimensions. In the next-simplest
2-dimensional case, we'd like a distribution
that actually looks like a bell---where potentional values can
range over the real plane, $\mathbb{R}^2$, with the density
clustered around some mean before tapering off in all directions, 
as below.
\begin{figure}[h!]
   \centering
   \includegraphics[scale=0.40]{multivariate.pdf}
\end{figure}
\\
This figure has mean zero for both $X_1$ and and $X_2$, and 
which are independent, implying $\sigma = I_2$, the identity matrix. 
It's easy to see that any vertical cuts parallel to $xz$ or $yz$ planes
will yield a traditional normal random variable. This of course 
generalizes to higher dimensions, although we can't display it so nicely.
\\
\\
This generalization will give us the 
\emph{Multivariate Normal (MVN) Distribution}. 
This turns out to be an extremely useful distribution. Let's highlight
a few applications here: 
\begin{enumerate}
    \item The MVN distribution has applications in Bayesian multivariate 
	regressions (i.e. Bayesian regressions with more than one 
	``independent,'' ``predictor,'' or ``$X$'' variable). 
	In such applications, we hope to find the posterior distribution
	of the $\beta$ regression vector, and it's common to assume
	a MVN prior on $\beta$ that results in an MVN posterior for
	$p(\beta|y)$.
    \item Next, we often use the MVN distribution to describe
	\emph{recurrence relations}, where we try to 
	forecast an MVN RV one period into the future, given
	current information about the RV, like it's current value
	and distribution. One example would include jointly forecasting 
	height \emph{and} weight in the future given current height
	and weight.
    \item Finally, the MVN distribution forms the basis for 
	VARs, which are used to forecast different economic variables 
	one period into the future, expressing each future variable
	as a function of lags of itself and other economic variables.
	However, VARs are too broad and interesting a subject to 
	elicit only a cursory summary in this note.  I'll write
	a more extensive summary of them elsewhere.
\end{enumerate}


\tableofcontents

\vskip 1in

\section{Notation}

In this note, the multivariate distribution will apply to a
$d$-dimensional random vector 
\[ X = \begin{pmatrix} X^{(1)} & X^{(2)} & \ldots & X^{(d)} \end{pmatrix}^T,
    \qquad X  \sim N_d(\mu, \Sigma) \]
where $\mu$ is the $n$-dimensional \emph{mean vector},
\[ \mu = \begin{pmatrix} EX^{(1)} & EX^{(2)} & \ldots & EX^{(d)} \end{pmatrix}^T,
      \]
and where $\Sigma$ is the $d\times d$ \emph{covariance matrix}, which
is defined and has in its $i,j$ entry
\[ \sigma^2 = E\left[ (X - \mu) (X-\mu)^T\right] \in
   \mathbb{R}^{N\times N} \] 
\[ \Sigma_{ij} = Cov(X^{(i)}, X^{(j)}), \qquad i,j = 1, \ldots, d\]
Superscripts with parentheses, like ``$X^{(i)}$,'' will denote
$i$th element of the MVN RV ``$X$,'' while ``$X_t$'' will denote
an MVN RV at time $t$.

\newpage
\section{Definition}

A random vector $X$ has a \emph{multivariate normal} 
distribution if every linear combination of its components,
\[ Y = a_1 X^{(1)} + \ldots + a_d X^{(d)} \] 
\[ \Leftrightarrow Y = {a} X, \qquad
    {a} \in \mathbb{R}^{d} \]
is \emph{univariate normally
distributed}, with a corresponding mean and variance. This gives a 
joint density function of 
\begin{equation}
    \label{pdf}
    f_{X}({x}) = \frac{1}{(2\pi)^{d/2}\lvert \Sigma \rvert^{1/2}} 
	\; e^{ -\frac{1}{2} ({x} - \mu)^T \;
	\Sigma^{-1} ({x} - \mu) }, \qquad \lvert\Sigma\rvert =
      \det\Sigma \qquad x \in \mathbb{R}^d
\end{equation}
Note, we impose the requirement that $\Sigma$ is symmetric and positive 
definite.  Symmetric because the correlation of $X$ and $Y$ equals the
correlation of $Y$ and $X$. 
\\
\\
{\sl Terminology}: I'll often us MVN to refer to the case where $X$ is
a vector with $d\geq2$; however, it should be clear that the univariate
normal distribution is just a special case where $d=1$. Therefore,
when speaking about an RV that could be either MVN or univariate normal
\emph{or} properties that apply equally well to either
type of RV, I'll often use the term \emph{Gaussian}
both for convenience and out of reverence to long-dead German-speaking
mathematicians. Stimmt?


\section{Linear Transformations of MVN Random Variables}

It's fairly common to consider linear transformations and functions
of a multivariate normally distributed random variable.  For instance,
we might have an economic or statistical model with a recurrence 
relation to describe the dynamics of some process:
\[ X_{t+1} = A X_{t} + {V}_{t} \]
where ${V}_t$ is some innovation or random noise vector.
Therefore, it would be useful to be able to derive the distributions
of \emph{functions} or \emph{linear transformation} of multivariate 
normal random variables.

\subsection{Transformation Theory Recap}

So let $X = A{Y}$. Suppose we have the distribution of
$X$, denoted $f_{{X}}$, and we want the distribution
of ${Y}$, $f_{{Y}}$. Then
\begin{align}
    f_{{Y}}(y) &= |\det(A)| \; f_{{X}}(Ay) \notag \\
    f_{{X}}(x) &= \frac{1}{|\det(A)|} \; f_{{Y}}(A^{-1} x) 
	\label{trans}
\end{align}

\newpage
\subsection{Derivation of Probability Distribution}

So let's find the probability distribution of a linear transforamtion
of an MVN RV. Begin by assuming 
\begin{align*}
    X &= A{Y}, \qquad {Y} \sim 
	\text{MVN}(\mu, \Sigma) \\
    \Rightarrow \quad f(y) &= k \; \exp\left\{-\frac{1}{2} (y - \mu)^T 
	\; \Sigma^{-1} (y-\mu)\right\}
\end{align*}
where $k$ is a constant.\footnote{The constant will come from the
definition of the distribution given above in Equation \ref{pdf}, but
it's boring algebra not that interesting, so I'll suppress the details.}
Assuming that $A$ is invertible, we substitute in, using Equation 
\ref{trans}, to get the distribution of $X$:
\begin{align}
    \label{midway}
    \Rightarrow \quad f_X(A^{-1}x) &= k' \exp\left\{-\frac{1}{2} (A^{-1}x - \mu)^T 
	\Sigma^{-1} (A^{-1}x-\mu)\right\} 
\end{align}
Next, since the expectation is a linear operator, we can use the fact
that 
\begin{align*}
    EX &= E[A{Y}] = A E{Y} = A \mu \\
    \Rightarrow \quad \mu_* &= A \mu \\
    \Leftrightarrow \quad \mu &= A^{-1} \mu_*
\end{align*}
where $\mu'$ is the mean vector of $X$.  With that, we
can substitute back into Equation \ref{midway} and simplify even
further, using convenient matrix manipulations like the distributivity
property, associativity, etc.:
\begin{align*}
    f_X(A^{-1}x) &= k' \exp\left\{-\frac{1}{2} (A^{-1}x - \mu)^T \;
	\Sigma^{-1} (A^{-1}x-\mu)\right\} \\
    &= k' \exp\left\{-\frac{1}{2} (A^{-1}x - A^{-1}\mu_*)^T \;
	\Sigma^{-1} (A^{-1}x-A^{-1}\mu_*)\right\} \\
    &= k' \exp\left\{-\frac{1}{2} [A^{-1}(x - \mu_*)]^T \;
	\Sigma^{-1} [A^{-1}(x-\mu_*)]\right\} \\
    &= k' \exp\left\{-\frac{1}{2} (x - \mu_*)^T \; [(A^{-1})^T \;
	\Sigma^{-1} A^{-1}] (x-\mu_*)\right\} \\
    &= k' \exp\left\{-\frac{1}{2} (x - \mu_*)^T \;  
	\Sigma_*^{-1} \; (x-\mu_*)\right\} \\
    \Rightarrow X &\sim  \text{MVN}(\mu_*, \Sigma_*), 
    \qquad \text{where $\mu_* = A\mu$ and $\Sigma_* = A\Sigma A^T $}
\end{align*}
This is huge! It means \emph{linear transformations of MVN
RV's are themselves MVN.}


\newpage
\section{From Standard to General MVN Random Variables}

If you're familiar with the standard univariate normal RV,
then you can probably guess what the standard MVN RV is:
\begin{equation}
    {Z} \sim \text{MVN}(0, \; I_d) 
\end{equation}
where $I_d$ is the $d\times d$ identity matrix.  This form for
the covariance matrix also suggests that the different components
of ${Z}$ (denoted $Z_1, Z_2, \ldots, Z_d$) are independent and
Gaussian.
\\
\\
Moreover, just like we can build from a standard univariate to
a general univariate.  To do so, we'll use the results from the 
last section, since building from a standard MVN
to general MVN simply involves linear transformations of the components.
Specifically, we can express the MVN RV ${X}$ as follows
\begin{equation}
    {X} = \mu + A{Z}, \qquad {X}\sim 
	\text{MVN}(\mu, AA^T)
\end{equation}
{\sl Computation}: Suppose we know that we want ${X}$ to
be MVN($\mu, \; \Sigma)$, and we can only generate ${Z}$.
How do we choose $A$ such that $AA^T = \Sigma$.  Typically, we'll
have to use something like a \emph{Cholesky Factorization}
algorithm to find the correct $A$ in the form of a lower triangular
matrix. And if $\Sigma$ is symmetric, positive definite, then 
$A$ is guaranteed to exist and this approach will work. 
The algorithm itself can be found in the appendix.


\section{Facts About Multivariate Normal Random Variables}

So to summarize, MVN (or Gaussian) RV's are particularly nice to work with because
of some convenient properties:
\begin{enumerate}
    \item Suppose that ${X}$ and ${Y}$ are \emph{univariate}
	normally distributed and independent. This then implies that they are 
	\emph{jointly normally distributed}. In other words, 
	$\begin{pmatrix} {X} & {Y} \end{pmatrix}$ must have a multivariate
	normal distribution. 
    \item Linear functions of Gaussians are Gaussian. So if $A$
	is a constant matrix and ${X}$ is MVN, then $A{X}$
	is also MVN.
    \item Uncorrelated Gaussian random variables are independent.
    \item Conditions Gaussian's are normal. So if $X_1$ and $X_2$
	are two components of a MVN RV, ${X}$, then 
	$X_1 | X_2$ is normal, and vice versa.
\end{enumerate}

\newpage 
\section{Linear Gaussian Recurrences}

So far, we've only looked at the properties of a multivariate
normal RV at one point in time. This is already useful for 
cross-section regressions, but it would 
it would also be relevant to explore
the MVN RV in in the context of time series and stochastic 
processes. For that, we turn
to \emph{recurrence relations}, which will conveniently characterize
a discrete-time, continuous space stochastic process, 
$\{{X}_t\}$.

\subsection{Single Lag, Order 1 Markov Chains}

Suppose we know that the stochastic process $\{{X}_t\}$
evolves according to the following recurrence relation,
where ${Z}_t$ is independent of ${X}_t$: 
\begin{equation}
    {X}_{t+1} = A {X}_t + B {Z}_t
    \qquad {Z}_t \sim \text{N}_d(0, \; I_d) 
\end{equation}
Because ${Z}_t$ is Gaussian, this implies that 
${X}_{t+1}$ will \emph{also} be Gaussian for reasons we saw  
in Section 3 above. So let's define
\begin{equation}
    {X}_t \sim \text{N}_d(\mu_t, \; \Sigma_t)
\end{equation}
{\sl Forward Mean}: 
Now our task is to compute $\mu_{t+1}$ and $\Sigma_{t+1}$
given the parameters $\mu_t$ and $\Sigma_t$.  Let's start with the
mean:
\begin{align*}
    \mu_{t+1} = E{X}_{t+1} &= E\left[A{X}_t +
	B {Z}_t\right] \\
    &= AE[{X}_t] + BE[{Z}_t] \\
    \Rightarrow \quad \mu_{t+1} &= A\mu_t
\end{align*}
{\sl Forward Covariance Matrix}:
Now, let's compute the covariance matrix, rewriting the recurrence 
relation somewhat suggestively:
\begin{align*}
    {X}_{t+1} &= A {X}_t + B {Z}_t \\
    \Leftrightarrow \qquad 
	{X}_{t+1} - \mu_{t+1} &= 
	A ({X}_t - \mu_t) + B {Z}_t \\
\end{align*}
Now, we move to the computation of $\Sigma_{t+1}$, which simplifies
nicely (omitting the explicit simplification and derivation process):
\begin{align*}
    \Sigma_{t+1} &= E\left\{ \left[{X}_{t+1} - \mu_{t+1} \right]
    \left[{X}_{t+1} - \mu_{t+1} \right]^T \right\} \\
    &= E\left\{ \left[A ({X}_t - \mu_t) + B {Z}_t\right]
	\left[A ({X}_t - \mu_t) + B {Z}_t\right]^T
	\right\} \\
    &= A\Sigma_t A^T + BB^T
\end{align*}
This is an example of a \emph{forward equation} because the 
distribution of ${X}_{t+1}$ is determined entirely by 
the distribution of ${X}_t$ and the recurrence relation.
\\
\\
Finally, note that in some cases, the noise vector won't necessarily
be of dimension $d$.  It might have dimension $m<d$, in which 
case $B$ will have dimension $d\times m$. You can think of 
$m$ correlated shocks being distributed over $d$ separate variables.


\newpage
\subsection{Multiple Lag, Higher Order Markov Chains}

Let's generalize a bit.  Suppose we want a particular RV,
${X}_{t+1}$, in a stochastic process, $\{{X}_t\}$, to
depend upon more than one lag, say ``$k$'' lags. Namely, suppose that
\begin{equation}
    \label{recgen}
    {X}_{t+1} = A_0 {X}_t + A_1 {X}_{t-1}
    + \cdots + A_{k-1}{X}_{t-k+1} + B {Z}_t
\end{equation}
We can use a \emph{state space expansion} to augment the matrices 
and write Equation \ref{recgen}'s more general recurrence relation 
as follows:
\begin{align*}
    \tilde{{X}}_{t+1} &= \tilde{A}_t
	\tilde{{X}}_t + \tilde{B}_t {Z}_t \\
    \Leftrightarrow \qquad
    \begin{pmatrix} {X}_{t+1} \\
	{X}_{t} \\ \vdots \\ {X}_{t-k+2} \end{pmatrix}
	&= 
	\begin{pmatrix} A_0 & A_1 & \cdots & & A_{k-1} \\
			1 & 0 & \cdots & & 0 \\
			0 & 1 & \cdots & & 0 \\
			\vdots & & \ddots & & \vdots \\
			0 & \cdots & & 1 & 0 
	\end{pmatrix} 
	\begin{pmatrix} {X}_{t} \\
	{X}_{t-1} \\ \vdots \\ {X}_{t-k+1} \end{pmatrix}
	+ \begin{pmatrix} B \\ 0 \\ \vdots \\ 0 \end{pmatrix}
	{Z}_t
\end{align*}
where $\tilde{A}$ is the \emph{companion matrix} of the
recurrence relation.
Note that ${Z}_t$ needs no augmenting because it is memoryless.
\\
\\
{\sl Forward Moments}: Using the results from the previous 
subsection, we immediately know that if the stochastic process
$\{{X}_t\}$ satisifes Equation \ref{recgen},
$E\tilde{{X}}_t = \tilde{\mu}_t$, and 
Cov$(\tilde{{X}}_t) = \tilde{\Sigma}_t$, then
\begin{align*}
    \tilde{\mu}_{t+1} &= \tilde{A}\tilde{\mu}_t \\
    \tilde{\Sigma}_{t+1} &= \tilde{A} \tilde{\Sigma}_t\tilde{A}^T +
	\tilde{B} \tilde{B}^T
\end{align*}
And if we want to find the covariance matrix $\Sigma_{t+1}$ for 
${X}_{t+1}$, then we can just find the $kd \times kd$ 
covariance matrix $\tilde{\Sigma}_{t+1}$ and look at the top left 
$d\times d$ block.
\\
\\
{\sl State Space Expansion}: Note what we did here.  We had a
stochastic process $\{{X}_{t}\}$ that was not a Markov 
Chain---future values depended upon more than one lag of 
${X}_t$, violating the Markov Property.  However, 
\emph{state space expansion} allowed us to form a new stochastic
process $\{\tilde{{X}}_t\}$ that was a Markov Chain, obeying
the Markov property.  Therefore, anytime a stochastic process 
does not display the Markov Property, blame that on the size of 
the state space.
\\
\\
Also note, state space expansion brings us to the sitaution we
mentioned in the previous subsection where the noise matrix,
$\tilde{B}$, has fewer sources of noise than state variables.


\newpage
\appendix
\section{Cholesky Decomposition}

Recall the motivation: we want to find a matrix $A$ such that
$AA^T = \Sigma$. This will be useful when we want to transform
a standard MVN RV into a general one via
\begin{equation}
    {X} = \mu + A{Z}, \qquad {X}\sim 
	\text{MVN}(\mu, AA^T)
\end{equation}



\end{document}













