\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Applied Linear Algebra}
\date{\today}

%% Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{fullpage}
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt}
    %---Width of the line

%\setlength{\headsep}{0.2in}
    %---Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother

%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure}
    %---For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{%
    colorlinks,
        %---This colors the links themselves, not boxes
    citecolor=black,
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},%
            %---Size of the numbers
        numbersep=9pt,%
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc.


%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %---Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %---For bibliographies
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Properties of Matrices}

Here are some words we use to describe matrices. Unless otherwise
specified, the matrices are assumed to be complex $m\times n$ matrices.
\begin{enumerate}
\item \emph{Hermitian}: $A=A^*$, where $A^*$ denotes the
  \emph{Hermitian transpose} of of $A$ (i.e.\ conjugate transpose of
  $A$).
\item \emph{Symmetric}: $A$ is a \emph{real} Hermitian matrix, i.e.\ $A
  = A^T$.
\item \emph{Invertible}: Matrix $A$ is invertible if there exists a matrix $A^{-1}$ such that $A A^{-1} = I$.
\item \emph{Unitary}: $A^{-1} = A^*$.
\item \emph{Orthogonal}: $A$ is a \emph{real} unitary matrix, i.e.\
  $A^{-1} = A^T$ so that $AA^T = A^T A = I$.
\item \emph{Normal}: $A^*A = AA^*$. Hermitian and unitary matrices are
  normal.
\item \emph{Sparse}: A matrix consisting of mostly zeros.
\item \emph{Tridiagonal}: A matrix with three diagonals, something like
\[
  A =
  \begin{bmatrix}[r]
   2 & -1 &  0 &  0 \\
  -1 & 2 & -1 &  0 \\
   0 & -1 &  2 & -1 \\
   0 &  0 & -1 &  2 \\
  \end{bmatrix}
\]
\item \emph{Positive Definite}: $x^T A x \geq 0$ for all $x\neq 0$ vectors.

\item \emph{Rank}: For an arbitrary $m\times n$ matrix $A$, the rank is the number of linearly independent columns or, alternatively, rows of $A$.\footnote{Whether you check rows or columns doesn't matter, those numbers will be equal.} The rank $r$ is such that $r \leq n$ and $r\leq m$.
\end{enumerate}
Now, one of the most important concepts for describing an $n\times n$
matrix $A$. The singular vs.\ nonsingular distinction entails a whole
host of consequences, detailed below:
\begin{table}[h!]
\centering
\begin{tabular}{lll}
\textbf{Nonsingular}                     && \textbf{Singular} \\
$A$ invertible                           && $A$ not invertible \\
Columns independent                      && Columns dependent \\
Rows independent                         && Rows dependent \\
$\det(A)\neq0$                           && $\det(A)=0$ \\
$Ax = 0$ has one solution, $x = 0$       && $Ax=0$ has infinitely many solutions\\
$Ax = b$ has one solution, $x = A^{-1}b$ && $Ax = b$ has no solution or infinitely many \\
$A$ has $n$ (nonzero) pivots             && $A$ has $r<n$ pivots \\
$A$ has full rank                        && $A$ has rank $r<n$ \\
Reduced row echelon form is $R = I$      && $R$ has at least one zero row \\
Column space is all of $\mathbb{R}^n$    && Column space has dimension $r<n$ \\
Row space is all of $\mathbb{R}^n$       && Row space has dimension $r<n$ \\
All eigenvalues are non-zero             && Zero is an eigenvalue of $A$ \\
$A^T A$ is symmetric positive definite   && $A^T A$ is only semidefinite \\
$A$ has $n$ (positive) singular values   && $A$ has $r<n$ singular values
\end{tabular}
\end{table}

\section{Matrix Multiplication and the Nullspace}

{\sl Matrix multiplication $Ax$ is the combination of the columns of
$A$. Each column scaled by the corresponding element of $x$.}\ This is,
hands down, one of the simplest, most important, and most practical ways
of thinking about matrix multiplication.

Being more explicit, and letting $A_{\cdot i}$ denote the $i$th column of $A$ (an $m\times n$ matrix), and letting $x_j$ denote the $j$th element of $n\times 1$ vector $x$, we can express
\[
  A x = x_1 A_{\cdot 1}  + \cdots + x_n A_{\cdot n}
\]
Since the result is a combination of the columns of $A$---each an $m\times 1$ vector---we can easily see that the result will also be an $m\times 1$ column vector.

This intuition also provides a convenient way to ``pick out'' the $i$th column of matrix $A$ via matrix multiplication: just right-multiply $A$ by an $n \times 1$ vector whose elements are all zero, except for the $i$th element, which equals 1. Example,
\[
  \begin{bmatrix}
  8 & 1 & 6 \\
  3 & 5 & 7 \\
  4 & 9 & 2
  \end{bmatrix}
  \begin{bmatrix}
  0 \\ 1 \\ 0
  \end{bmatrix}
  = 0
  \begin{bmatrix} 8 \\ 3 \\ 4 \\ \end{bmatrix}
  + 1  \begin{bmatrix} 1 \\ 5 \\ 9 \\ \end{bmatrix}
  + 0  \begin{bmatrix} 6 \\ 7 \\ 2 \\ \end{bmatrix}
  =
  \begin{bmatrix} 1 \\ 5 \\ 9 \\ \end{bmatrix}
\]


\begin{defn}
The \emph{nullspace} of a matrix $A$ ($m\times n$), which is denoted $N(A)$, is the set of all $x \in \mathbb{R}^n$ such that $Ax=0$. Intuitively, the nullspace consists of all vectors that can linearly combine the columns of $A$ to get the zero vector.

It is \emph{always} the case that the zero vector is in $N(A)$, so the set can never be empty.
\end{defn}


\section{Canonical Problem: The System of Linear Equations under the Row and Column Interpretations}

Linear algebra's founding problem is likely the task of solving the simple system of linear equations:
\begin{equation}
    \label{canonical}
    Ax = b
\end{equation}
For now, assume $A$ is $p\times p$. There are two ways to think of this equation:
\begin{enumerate}
    \item {\sl Row Interpretation}: We think of each row in the equation as defining a line in $p$ dimensional place. The solution is where those lines intersect.
    \item {\sl Column Interpretation}: A solution to the equation exists if and only if $b$ is in the column space of $A$. More explicitly, there is a solution if and only if $b$ can be written as a linear combination of the $p$ vectors defined by the columns of $A$.
\end{enumerate}
The latter interpretation is very powerful. We see that the column space of $A$---the set of all linear combinations of the columns of $A$---is crucial to characterizing the matrix $A$.

For example, if the columns of $A$ are linearly independent, then the matrix, in a sense, ``covers'' the entire $p$-dimensional space. We can thus solve \emph{any} equation like Equation \ref{canonical} because $b$ will certainly lie within the column space of $A$. Thus, we can find some $x$ such that $Ax=b$.

We only potentially run into problems if the columns aren't linearly independent. So if the rank of the matrix is $k<p$, only a $k$-dimensional hyperplance embedded within $\mathbb{R}^p$ is covered by the columns of $A$. And $b$ might not lie in that subspace. This very possibility manifests itself in the non-invertability of $A$, which prevents us from trivially solving the system via $x = A^{-1}b$, for the very reasons just discussed.



\section{The Eigenvalue Problem}

\begin{defn}{(Eigenvalue Problem)}
For a square $n\times n$ matrix $A$, a \emph{right eigenvector} (or,
more commonly, simply an \emph{eigenvector}) is a vector $x$ satisfying
\begin{equation}
  Ax = \lambda x
  \qquad \Leftrightarrow \qquad
  (A-\lambda I) x = 0
\end{equation}
where $\lambda$ is some scalar, called an \emph{eigenvalue}.

A \emph{left eigenvector} of matrix $A$ is a right eigenvector
of $A^*$.
\end{defn}

\begin{thm}\emph{(Solving the Eigenvalue Problem)}
For a matrix $A$, eigevalues are solutions to the \emph{characteristic
polynomial} defined
\begin{equation}
  \label{chareqn}
  \det(A-\lambda I) = 0
\end{equation}
Given an eigenvalue, its corresponding eigenvector(s) are elements in
the null space of the matrix $A-\lambda I$.
\end{thm}


\begin{defn}{(Multiplicities)}
The multiplicity of eigenvalue $\lambda$ as a solution to the
characteristic polynomial in Equation~\ref{chareqn} is called the
\emph{algebraic multiplicity of $\lambda$}, denoted $m(\lambda)$.

The number of eigenvectors corresponding to a given eigenvalue $\lambda$
is called the \emph{geometric multiplicty of $\lambda$}, denoted
$g(\lambda)$.
\end{defn}

\begin{defn}{(Eigenspace)}
Given an eigenvalue $\lambda$ of some matrix $A$, the set of all
eigenvectors corresponding to $\lambda$ and the zero vector form a
linear subspace called the \emph{eigenspace of $\lambda$}.
\end{defn}

\section{The Generalized Eigenvalue Problem}

\begin{defn}{(The Generalized Eigenvalue Problem)}
For square $n\times n$ matrices $A$ and $B$, an \emph{eigenvector of
$A$ relative to $B$} is a vector satisfying
\begin{equation}
  Ax = \lambda B x
  \qquad \Leftrightarrow \qquad
  (A - \lambda B) x = 0
\end{equation}
where $\lambda$ is some scalar, called an \emph{eigenvalue of $A$
relative to $B$}.

We see also that the Eigenvalue Problem introduced above is simply a
special case where $B=I_n$.
\end{defn}

\section{Matrix Decompositions}

Matrix decompositions are useful for a few key reasons
\begin{enumerate}
\item They express an arbitrary matrix $A$ in some canonical form, allowing
  the application of standard, more efficient algorithms and procedures
  to invert the matrix $A$, solve a system $Ax =b$, etc.
\item They more explicitly express the properties of $A$ 
\end{enumerate}

\subsection{Background Intuition}

Often, for any arbitrary matrix $A$, we want to consider the geometric
interpretation of $Ax$ for some vector $x$. because they often often
attempt to express the matrix $A$ as simpler matrices with more explicit
geometric intepretations. 

matrix multiplication
When considering matrix decompositions, it often helps to consider the
geometric interpretation of matrices and matrix multiplication. Often,
decompositions simply take arbitrary matrices and express them in a way
more amenable to direct geometric interpretation

\subsection{Schur and QZ Decomposition}

\begin{defn}{(Schur Decomposition)}
For any square matrix $A$, there exists a matrix $Q$ such that
\begin{align*}
  A = Q U Q^{-1} = Q U Q^*
\end{align*}
where $Q$ is a unitary matrix (hence $Q^{-1} = Q^*$) and $U$ is an upper
triangular matrix with the the eigenvalues of $A$ on the diagonal of
$U$. (This follows from $U$ being similar to $A$.)
\end{defn}

\begin{defn}{(QZ or Generalized Schur Decomposition)}
For square matrices $A$ and $B$, there exist unitary matrices $Q$ and
$Z$ such that
\begin{align*}
  A &= Q S Z^* = QSZ^{-1} \\
  B &= Q T Z^* = QTZ^{-1} \\
\end{align*}
where $S$ and $T$ are upper-triangular matrices.
\end{defn}

\begin{thm}
The $i$the generalized eigenvalue for matrices $A$ and $B$, denoted
satsfies $\lambda_i = S_{ii} / T_{ii}$ where $S$ and $T$ are the upper
triangular matrices from the QZ decomposition.
\end{thm}

\subsection{$LU$ Decomposition}

\subsection{Cholesky Decomposition}

The Cholesky Decomposition applies to all positive definite matrices;
therefore, it finds a number of applications in dealing with covariance
matrices.

\begin{thm}
Suppose that $A$ is a positive definite matrix. Then it
can be decomposed
\begin{equation}
  A = L L^T
\end{equation}
where $L$ is a lower triagular matrix with positive diagonal elements.
It represents something like the ``square root'' of $A$.
\end{thm}

\subsubsection{Applications}

\paragraph{Solving $Ax=b$} The Cholesky decomposition allows us to
quickly solve systems of equations because of the triangular
structure. Specifically, we can solve the problem as follows
\begin{align*}
  Ax = LL^Tx&=b\\
  \Leftrightarrow \quad \quad Lz&=b \qquad \text{where }z = L^T x
\end{align*}
Once we have it in this form, we have a system that is very easy to
solve by exploiting the structure using \emph{forward substitution}:
\begin{align*}
  \begin{pmatrix}
    l_{11} & 0      & 0 & \cdots & 0 \\
    l_{21} & l_{22} & 0 & \cdots & 0\\
    l_{31} & l_{32} & l_{33}& \cdots & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    l_{n1} & l_{n2} & l_{n3} &\cdots & l_{nn}\\
  \end{pmatrix}
  \begin{pmatrix}
    z_1 \\ z_2 \\ z_3 \\ \vdots \\ z_n
  \end{pmatrix}
  &=
  \begin{pmatrix}
    b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n
  \end{pmatrix}
\end{align*}
\begin{align*}
  \Rightarrow \qquad
  z_1 &= b_1/l_{11} \\
  z_2 &= (b_2 - l_{21}z_1)/l_{22} \\
  \vdots \; &= \quad \vdots\\
  z_n &= (b_n - l_{n1}z_1 - \cdots - l_{n,n-1}z_{n-1})/l_{nn}
\end{align*}
Then, to get from $z$ to $x$, we again solve a triangular system $L^Tx =
z$, doing something very similar, just in reverse, called \emph{backward
substitution}:
\begin{align*}
  \begin{pmatrix}
    l_{11} & l_{21}  & l_{31} & \cdots & l_{n1}  \\
    0 & l_{22} &  l_{32} & \cdots & l_{n2}\\
    0 & 0 & l_{33}& \cdots & l_{n3}\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 &\cdots & l_{nn}\\
  \end{pmatrix}
  \begin{pmatrix}
    x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
  \end{pmatrix}
  &=
  \begin{pmatrix}
    z_1 \\ z_2 \\ z_3 \\ \vdots \\ z_n
  \end{pmatrix}
\end{align*}
\begin{align*}
  \Rightarrow \qquad
  x_n &= z_n/l_{nn} \\
  x_{n-1} &= (z_{n-1} - l_{n,n-1}x_n)/l_{n-1,n-1} \\
  \vdots \; &= \quad \vdots\\
  x_1 &= (z_1 - l_{n1}x_n - \cdots - l_{21}x_{2})/l_{11}
\end{align*}

\paragraph{Generating Multivariate Random Normals}
We know how to draw the following multivariate random normal $Z$:
\begin{align*}
  Z \sim N(0,I)
\end{align*}
Just draw a bunch of iid $N(0,1)$ RV's and stack them into a vector.

But suppose we want to draw from more complicated MVN RV's with
arbitrary mean and covariance matrix
\begin{align*}
  X \sim N(\mu, \Sigma)
\end{align*}
We do this by the Cholesky decomposition and some simple results for
Normal RVs. Namely,
\begin{align}
  Z \sim N(0,I) \quad \Rightarrow \quad
  \mu+LZ \sim N(\mu, LL^T)
  \label{cholalg}
\end{align}
{\sl Algorithm}\: Result~\ref{cholalg} suggests a straightforward way
to draw $X\sim N(\mu,\Sigma)$.
\begin{enumerate}
  \item Since $\Sigma$ is a covariance matrix, it is positive definite.
    So we can compute the Cholesky decomposition $\Sigma=LL^T$.
  \item Draw $Z\sim N(0,I)$.
  \item Set $X=\mu+LZ$.
\end{enumerate}

\subsection{Singular Value Decomposition (SVD)}

The singular value decomposition is a useful decomposition that works
for \emph{any} matrix. Moreover, given the singular value decomposition
of some matrix $A$, many of properties of $A$ can be read off directly
from the decomposition.

\begin{thm}
For an arbitrary $m\times n$ matrix $A$, there exist orthogonal matrices
$U$ ($m\times m$) and $V$ ($n\times n)$ plus a diagonal matrix
$\Sigma=diag(\sigma_1, \ldots,\sigma_p)$ where $p=\min\{m,n\}$ such
that
\begin{align*}
  A = U\Sigma V^T
\end{align*}
Moreover, supposing that we order the singular values $\sigma_1 \geq
\sigma_2\geq\cdots\geq \sigma_r\geq \sigma_{r+1}=\cdots = \sigma_p=0$,
we have that
\begin{enumerate}
\item $rank(A)=r$
\item $null(A)=span\{v_{r+1},\ldots,v_n\}$
\item $range(A)=span\{u_{1},\ldots,u_r\}$
\item $\sigma_i = \sqrt{\lambda_i}$ for $i=1,\ldots,p$ where $\lambda_i$
  is the $i$th largest eigenvalue of $A^TA$.
\item $v_i$ are orthonormal basis for the domain of $A$.
\item $u_i$ are orthonormal basis for the range of $A$.
\end{enumerate}
\end{thm}

\subsubsection{SVD Intuition}

Geometrically, given an $n\times 1$ vector $x$, the SVD of a matrix $A$
says that the multiplication $Ax = U\Sigma V^T x$ is a sequence of three
operations on $x$:
\begin{enumerate}
\item Express $x$ in terms of the orthonormal basis defined by the
  columns of $V$. The multiplication $V^T x$ does this, resulting
  effectively in a rotation of $x$ from the standard basis for
  $\mathbb{R}^n$ onto the basis defined by $V$.
\item Stretch the result of the first step along the $i$th dimension or
  axis by $\sigma_i$, for $i=1,\ldots,p$. This is a stretching along the
  axes defined by $V$.
\item Rotate one final time by $U$, expressing $\Sigma V^Tx$ in terms of
  the orthonormal basis defined by columns $U$ (rather than $V$ or the
  standard basis).
\end{enumerate}


%% APPPENDIX %%

% \appendix




\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}
        \citep{LabelInSourcesFile} Cites in parens
        \nocite{LabelInSourceFile} includes in refs w/o specific citation
        \bibliographystyle{apalike}
        \bibliography{sources.bib} where sources.bib is file

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%
        \verbatiminput{file.ext}
            %---Includes verbatim text from the file
        \texttt{text}
            %---Renders text in courier, or code-like, font

        \matlabcode{file.m}
            %---Includes Matlab code with colors and line numbers


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}


        % Side by Side figures
            \begin{figure}[h!]
                \centering
                \mbox{\subfigure{
                    \includegraphics[scale=1]{file1.pdf}
                }\quad\subfigure{
                    \includegraphics[scale=1]{file2.pdf}
                }
                }
            \end{figure}


