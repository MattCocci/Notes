\documentclass[12pt]{article}

\author{Matthew Cocci}
\title{Markov Chains}
\date{\today}

%% Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{fullpage}
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\pagestyle{fancy} 
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt} 
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt} 
    %---Width of the line

%\setlength{\headsep}{0.2in}    
    %---Distance from line to text
            

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure} 
    %---For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref} 
\hypersetup{	
    colorlinks,		
        %---This colors the links themselves, not boxes
    citecolor=black,	
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{verbatim} 
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in 
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},% 
            %---Size of the numbers
        numbersep=9pt,% 
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc. 


%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{enumitem} 
    %---Has to do with enumeration	
\usepackage{appendix}
%\usepackage{natbib} 
    %---For bibliographies
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\begin{document}
\maketitle

\section{Introduction}

This note looks at Markov chains, which describe the evolution of some object, call it $X_t$, over time. The ``chain'' is the sequence of values it takes on, $\{X_t\}_{t\geq0}$. Almost always, we'll consider the case where $X_t$ evolves \emph{stochastically}, because a deterministic process is much more boring than a randomly evolving stochastic process.\footnote{Even if you don't agree, a deterministic process can be easily embedded as a special within the more general class of stochastic processes. Just specify zero variance in everything here.} 

So to preview, our very basic primitive will be the recurrence relation, or ``law of motion''
\begin{equation}
    X_{t+1} = AX_t + W_{t+1}
    \qquad W_{t+1} \sim \text{N}(0,\sigma)
\end{equation}
Within this framework, $X_t$ can take on scalar values in $\mathbb{R}$, or it can be a vector in $\mathbb{R}^N$ (where $A$ is a matrix). Hence the notation above allows sufficient generality. 

And if this still doesn't look general enough, note that we can even accommodate additional lags in this framework, as we'll see, by augmenting $X_t$, ``expanding the state space'', to include lagged values of itself.





\section{Markov Chain Definition}

Some of assumptions to make things more tractable:
\begin{enumerate}
   \item[i.]{Discrete time.}
   \item[ii.]{Finite number of states that the subject can be in.}
   \item[iii.]{History independence: the distribution for time $n+1$
      only considers, at most, the time $n$ and the state at time $n$.
      All prior states and additional information are superfluous. 
      This defining characteristic is called the {\sl Markov
      Property}.}
\end{enumerate}
The only condition that's really necessary to have a legitimate 
Markov Chain is condition (iii), which expresses the Markov Property.
Conditions (i) and (ii) are simplifications we could dispose of, if
we wish to consider continuous and infinite Markov Chains.

\paragraph{Definition} $\{X_t\}$ is a {\sl non-homogeneous Markov Chain}
when $\{X_t\}$ is an infinite sequence of random variables $X_0, X_1,\ldots$
which satisfy the following properties
\begin{enumerate}
   \item[i.]{$X_n$ denotes the \emph{state number} of a subject at time
      $t$.}
   \item[ii.]{Each $X_t$ is a discrete-type random variable over $n$
      possible discrete values.}
   \item[iii.]{The {\sl transition probabilities} are history
      independent when it comes to pre-$t$ states, 
      but the transition probabilities may vary with time, $t$:
      \[ Q_{ij}^{(t)} = P( X_{t+1}=j \; | \; X_t = i, \; X_{t-1}=k_{t-1},
	 \ldots,\; X_0 = k_0 ) \]
      \[ =P(X_{t+1} = j \; | \; X_t = i ) \]
      So $Q_{ij}^{(t)}$ is the probability of moving from
      state $i$ to state $j$ at time $t$.
      }
\end{enumerate}

\subsection{Homogeneous vs. Non-Homogeneous Markov Chains}
If the transition probabilities, $Q^{(t)}_{ij}$, do
not depend on $t$, then they are denoted by $Q_{ij}$ and we have
a {\sl homogeneous Markov Chain}. 
\\
\\
For this type of Markov Chain, 
only state or position, \emph{not} time, factors into the distribution for the
next period. This differs from the definition above, in that we explicity allow $Q_{ij}^{(t_1)} \neq Q_{ij}^{(t_2)}$ for 
$t_1\neq t_2$.


\subsection{Order of a Markov Chain}
We can generalize a bit and allow the Markov Chain to depend upon more
than one previously observed state.  In particular, we define an 
{\sl order}-${n}$ {\sl Markov Chain} to be a Markov Chain that 
depends upon the previous $n$ values.  
\\
\\
Above, we defined an order-1 Markov chain. If we want to consider 
higher-order Markov Chains, we'll have to do a bit of work when it
comes time to put the probabilities in the transition matrices that
we next define. Namely, we will have to convert them to order-1.

\newpage
\section{Transition Matrices}

Hopefully, the notation above clearly suggested that it will be
convenient to place the many transition probabilities into a
{\sl transition matrix}, whose $i,j$ entry is the transition
probability for moving from state $i$ to state $j$. So if 
there are $n$ states, define
\[\mathbf{Q}^{(t)}=\begin{pmatrix} Q_{ij}^{(t)} \end{pmatrix}_{ij}
    \quad \text{where $\mathbf{Q}^{(t)}$ is $n\times n$} \]
And just a word about the funky notation you see above: it says
``the matrix $\mathbf{Q}^{(t)}$ is a matrix populated
with the values/probabilities $Q_{ij}^{(t)}$ at the $ij$ 
entry.'' 
\\
\\
Now in a transition matrix, all the elements in a given row will add up
to 1, in which case we call $\mathbf{Q}^{(t)}$ a 
{\sl stochastic matrix}.\footnote{We will use 
``transition matrix'' and ``stochastic matrix interchangeably.}
If the elements of each column added up to 1 as well, we would call
the matrix {\sl doubly-stochastic}.
\\
\\
You should think about the row $Q_{(i\cdot)}^{(t)}$ as the 
conditional distribution of a Markov Chain, $X_{t+1}$, given
$X_t = i$.


\subsection{Longer Term Transition Probabilities and Matrices}

Often, we'll want to look further ahead than just the next period, which
is all that our current formulation allows. To do so, we'll define
the {\sl longer term transition probabilities} as 
\[ Q^{(t,k)}_{ij} = P(X_{t+k} = j \;|\; X_t = i ) \]
where $Q^{(t,k)}_{ij}$ is the probability of 
being in state $j$ in $k$ periods given that you are currently
at time $t$ and in state $i$.\footnote{It's important to note that the transition
    probability $Q^{(t,k)}_{ij}$ does \textbf{not} care what happens in between
    time $t$ and time $t+k$.  It's just the probability that you were in
    state $i$ at time $t$ and may be in state $j$ at time 
    $t+k$. In fact, it's even entirely possible 
    that you were \emph{already} in state $j$ sometime between
    time $t$ and $t+k$! So if we're considering $Q^{(t,k)}_{ii}$, 
    this is not the probability of staying of in state $i$
    from time $t$ to $t+k$.  You can drift away and then come back.}
The above sections dealt with the special case where $k=1$.
\\
\\
Quite naturally, the {\sl longer term transition matrix} is defined
\[\mathbf{Q^{(t,k)}} =
      \begin{pmatrix} Q^{(t,k)}_{ij} \end{pmatrix}_{ij}
      \]
\paragraph{Theorem} In non-homogeneous Markov Chains, the longer-term
probability $Q^{(t,k)}_{ij}$ can be computed as the $(i,j)$-entry of
the matrix 
   \[\mathbf{Q^{(t,k)}}= \mathbf{Q^{(t)}} \times \mathbf{Q^{(t+1)}} \times
      \cdots \times \mathbf{Q^{(t+k-1)}} 
            \]
And for a homogeneous Markov Chain, this matrix is just 
$\mathbf{Q^{(\cdot, k)}} = \left(\mathbf{Q}\right)^k$, dropping the
$t$ in the superscript since the transition probabilities do not 
change over time.

\section{Chapman-Kolmogorov Equations (Homogeneous Case)}

\paragraph{Theorem} If we restrict ourselves to the 
homogeneous case, 
the {\sl Chapman-Kolmogorov Equations} tell us that 
\[Q^{(\cdot, \; k+\ell)}_{ij} = \sum^n_{s=1} 
    Q^{(\cdot, \; k)}_{is} Q^{(\cdot,\; \ell)}_{sj} \]
This equation captures the idea of ``interposing'' some
intermediate time and state between now (time $t$, state
$i$) and the future (time $t+k+\ell$, and state $j$).
The logic goes as follows:
\begin{enumerate}
    \item Before you get to time $t+k+\ell$ and state $j$, you
	have to stop at time $t+k$.   
    \item At that time $t+k$, you'll be in any one of the states,
	call it $s$, where $s$ could equal $1, \cdots, n$ (where
	$n$ is the number of states.
    \item So if you sum over the probability
	of moving as follows: 
	\[  \text{state $i$ at $t$} \quad \Rightarrow \quad
	    \text{state $s$ at $t+k$} \quad \Rightarrow \quad 
	    \text{state $j$ at $t+k+\ell$} \]
	(where $s$ ranges over all possible intermediary states), 
	you'll get the desired probability.
\end{enumerate}
{\sl Matrix Representation:}
In matrix form, this result can be written more compactly as
    \[ \mathbf{Q}^{(\cdot,\; k+\ell)} = \mathbf{Q}^{(\cdot,\; k)} \times 
      \mathbf{Q}^{(\cdot,\; \ell)}\]
restricting ourselves to the homogeneous case.
Things are a little tougher if we want to consider the 
non-homogeneous case, as we'll have to multiply more matrices 
together and keep track of subscripts.


\newpage
\section{Marginal Distributions}

Suppose we have:
\begin{enumerate}
    \item A homogeneous Markov Chain, $\{X_t\}$ with a transition matrix,
	$\mathbf{Q}^{(\cdot)}$
    \item A marginal distribution for $X_t$, denoted by row-matrix $\psi^{(t)}$ 
	The $i$th entry of $\psi^{(t)}$ is
	\[ \psi^{(t)}_i = P\left\{ X_t = i \right\} \]
\end{enumerate}
First, suppose we want to get the marginal distribution of $X_{t+1}$.
This we can do by using the law of total probability:
\begin{align*}
     P\{X_{t+1} = j\} &= \sum_{i=1}^n 
	P\{X_{t+1} = j \; | \; X_t = i \} \cdot 
	P\{X_{t} = i\} \\
    &= \sum_{i=1}^n 
	Q^{(\cdot)}_{ij} \cdot 
	\psi_i^{(t)} \\
    \Rightarrow \qquad 
    \psi^{(t+1)} &= \psi^{(t)} \mathbf{Q}^{(\cdot)}
\end{align*}
More generally, and still restricting to the homogeneous case, 
this implies that for any arbitrary $m$, 
    \[ \psi^{(t+m)} = \psi^{(t)} \mathbf{Q}^{(\cdot,\; m)} \]

\paragraph{Note}: All of this math works whether we are 
\emph{certain} of the initial distribution, $X_t$, or
whether we aren't quite sure.  
\begin{itemize}
    \item[-] If we are \emph{certain} of the initial state $i$, 
	$X_t$, or if we want to assume an initial state $i$,
	$\psi^{(t)}$ will be a vector that is all zeros
	\emph{except} for $i$th position, which will have a 1.
    \item[-] If we want to think about things more probabilistically
	or if we are uncertain of the state $X_t$, then
	$\psi^{(t)}$ will be a vector that sums to one and
	has, in each position $i=1,\ldots, n$, a probability
	of being in state $i$ at time $t$.
\end{itemize}


\newpage
\section{Stationary Distributions}

In this section, we dispense with the non-homegeneous
case, assuming $\mathbf{Q}^{(t)}$ is homegeneous
over time. Therefore, we drop the superscript altogether
and simply write $\mathbf{Q}$.

\subsection{Definition}

A distribution $\psi^*$ is called {\sl stationary} or 
{\sl invariant} if $\psi^* = \psi^* \mathbf{Q}$,
which implies that $\psi^* = \psi^* \left(\mathbf{Q}\right)^m$
for all $m$ too.\footnote{Take note that a stationary 
    \emph{distribution} differs from a stationary \emph{process}.
    The former is the limiting distribution of a Markov Chain,
    or the marginal distribution of a stationary process. The
    latter is a stochastic process whose joint probability 
    distribution does not change when shifted through time or
    space.  Finally, a ``stationary process'' is \emph{not} the
    same as a ``process with a stationary distribution.''}
Mathematically, a stationary distribution
is a fixed point if we think of $\mathbf{Q}$ as a map:
\begin{align*}
    P: \; &\mathbb{R}^n \rightarrow \mathbb{R}^n \\
    &\psi \rightarrow \psi P
\end{align*}
At least one such distribution exists for each stochastic
matrix, $\mathbf{Q}$.\footnote{Use Brouwer's fixed point theorem.}
\\
\\
Importantly, if the distribution of $X_0$ is a stationary distribution,
then $X_t$ will have this same distribution for all $t$.
As a result stationary distributions have a natural
interpretation of \emph{stochastic steady states}.


\subsection{Solving for Stationary Distributions}

We saw above that a stationary distribution, $\psi^*$, must solve 
\begin{equation}
    \label{psicond1}
    \psi = \psi \mathbf{Q} \quad \Leftrightarrow \quad
    \psi \left(I_n - \mathbf{Q}\right) = 0
\end{equation}
But note that this does not require $\psi^*$ to be a probability
distribution since the zero vector happens to solve Equation
\ref{psicond1}. So we want to impose the additional constraint
\begin{align}
    \sum_{i=1}^n \psi_i = 1 \quad &\Leftrightarrow \quad
    \psi b = 1 \qquad \text{where $b_i \in \mathbb{R}^n$ and
				$b_i=1$ for $i=1,\ldots,n$.} \nonumber \\ 
    &\Leftrightarrow \quad \psi B = b 
    \qquad \text{where $B$ is an $n\times n$ matrix of 1s}
    \label{psicond2}
\end{align}
By adding together the two conditions, Equations \ref{psicond1}
and \ref{psicond2}, we see that $\psi$ must satisfy 
\begin{align*}
    \psi \left(I_n - \mathbf{Q} + B\right) &= b \\
    \Leftrightarrow \qquad
	\left(I_n - \mathbf{Q} + B\right)^T \psi^T &= b^T 
\end{align*}
In this way, we can solve for the stationary distribution
by inverting the matrix to get:
\begin{equation}
    \psi^* = \left[\left(I_n - \mathbf{Q} + B\right)^T\right]^{-1}
	b^T
\end{equation}


\subsection{Uniform Ergodicity} 

\paragraph{Definition}
A stochastic matrix, $\mathbf{Q}$,
is called uniformly ergodic if there is a positive
integer $m$ such that all elements of $\left(\mathbf{Q}\right)^m$
are \emph{strictly} positive.

\paragraph{Uniqueness of the Stationary Distribution}
Note that there may in fact be
many stationary distributions for the stochastic matrix,
$\mathbf{Q}$ (as in the case of the identity matrix).
But one sufficient condition for uniqueness is 
\emph{uniform ergodicity}.  

\paragraph{Convergence to Steady-State}
If uniform ergodicity holds, we also get the result that
for any non-negative row vector, $\psi$, summing to one (so a
proper distribution)
\[ \psi \mathbf{Q}^t \rightarrow \psi^* \quad \text{ as }
	t\rightarrow \infty \]
where $\psi^*$ is the unique stationary distribution. So
regardless of the distribution of $X_0$, the distribution 
of $X_t$ converges to $\psi^*$.

\paragraph{Time in States} To get another import interpretation
and result, assume $\{X_t\}$ is a Markov Chain with stochastic 
matrix $\mathbf{Q}$. Also assume that $\mathbf{Q}$ is 
uniformly ergodic with stationary distribution $\psi^*$. Then
\begin{equation}
    \lim_{s\rightarrow\infty}
    \frac{1}{s} \sum^s_{t=1} 1\{X_t = j\} \rightarrow
	\psi^*_j  \qquad
	\forall j \in \{1, \ldots, n\}
\end{equation}
This tells use that the fraction of time the Markov Chain
$\{X_t\}$ spends in state $j$ converges to $\psi^*_j$ as
time goes to infinity.  Therefore, if we consider many
Markov chains with the same stochastic matrix, $\mathbf{Q}$,
the long-run cross-sectional averages for a population will
equal time-series averages for individual chains.



\newpage
\section{Application: Page Rank}

Suppose we have a set of web-pages, $j \in \{1, 2, \dots,n \}$.
We want to find the fanking $r_j$ for each page, $j$. To do 
so, we'll set the ranking as
    \[ r_j = \sum_{i \in L_j} \frac{r_i}{\ell_i} \]
where $L_j$ is the set of pages linking to page $j$, and
where $\ell_i$ is the total number of outbound links from 
page $i$. 
\\
\\
But we can think of this another way.  Specifically, imagine
a jabroni on some webpage $i$ who is clicking links randomly
and with equal probability.  So what's his probability of 
landing on page $j$?  Well clearly, we can define this whole
``ranking'' business in terms of Markov Chains and transition
probabilities:
    \[ r_j = \sum_{\text{all $i$}} Q_{ij} r_i, \qquad
	\text{ where  } Q_{ij} = \frac{1\{i \rightarrow j\}}{\ell_i}
    \]
where $1\{i \rightarrow j\}$ is an indicator function that 
equals 1 if page $i$ links to page $j$, zero otherwise.
So $Q_{ij}$ is, as above, the transition probability for
moving from state--or in this case ``page''--$i$ to 
$j$.\footnote{We drop the superscript, since we'll assume 
we're in the homeogeneous case.}
\\
\\
Awesome, now let's write the ranking in vector and matrix 
notation so we can describe the problem for all pages,
$j \in \{1, \ldots, n\}$:
    \[ \mathbf{r} = \mathbf{r} \mathbf{Q} \]
where $\mathbf{r}$ is the vector of rankings and $\mathbf{Q}$
is the transition matrix.  Well that looks familiar.  
Specifically, $r$ is the stationary distribution of the
stochastic matrix $\mathbf{Q}$---and we know how to solve for
that!


\newpage
\section{Tauchen's Approximation Method}

\subsection{AR(1) Approximiation}
Often, we will approximate some continuous model with a 
discrete model that utilizes Markov Chains. Specifically, the
discrete model takes the form:
\begin{equation}
    y_{t+1} = \rho y_t + \varepsilon_{t+1} \qquad 
	\varepsilon_{t+1} \sim \text{N}(0, \sigma_\varepsilon^2)
\end{equation}
Now the variance of the stationary probability distribution 
of the stochastic process $\{y_t\}$ (which we hope to specify
as a Markov Chain) can be easily shown to be
    \[ \sigma_y^2 = \frac{\sigma^2_\varepsilon}{1-\rho^2} \]

\subsection{Constructing the State Space}
To discretize an otherwise continuous process, we choose
\begin{itemize}
    \item $n$, the number of states for the discrete 
	approximation that $y_t$ can be in.
    \item $m$, an integer that parameterizes the width of
	the space of values $y_t$ can take on.
\end{itemize}
From there, we create a space $\{x_1, \ldots, x_n\}$ and
transition matrix $\mathbf{Q}$ defined
by the parameters $n$ and $m$ such that
\begin{enumerate}
    \item $\{ x_1, x_n \} = \{ -m \sigma_y, \; m \sigma_y \}$.
	So we see how $m$ controls the width of the space.
    \item $x_{i+1} = x_i + s$ where
	\[ s = \frac{x_n - x_1}{n-1} \]
    \item $Q_{ij}$ represents the probability of transition
	from state $x_i$ to state $x_j$. 
\end{enumerate}

\subsection{Defining the Transition Probabilities}

We've essentially divided the values $y_t$ can take into buckets
that run from $[x_i, x_{i+1}]$.  So if we say that $y_t$
is in state $i$, we really mean that $y_t$ is in that bucket.
There are, of course, $n-1$ equal width buckets.


\newpage
\section{Dynamical Systems}

Now we look at an application of Markov Chains to the domain of {dynamical systems}.

\subsection{Definition and Main Ideas}

\begin{defn}
A \emph{dynamical system} can be defined as the couplet $(S,h)$, where $S$ is a metric space and $h$ is a map from $S$ onto itself. Hence, we have a space within which objects can move according to some law of motion, $h$.
\end{defn}



\end{document}
