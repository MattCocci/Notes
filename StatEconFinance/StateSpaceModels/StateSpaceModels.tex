\documentclass[a4paper,12pt]{article}

\author{Matthew Cocci}
\title{Kalman Filter}
\date{\today}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
%\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{fullpage}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
%\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Introduction}

State Space Models, the Kalman Filter, and simulation smoothers together
rank among those ``Hugely Important Things I Never Knew Existed''
following my undergrad education. 

They're hugely important because they embed as special cases many
concepts in time series and dynamics.  In fact, you can almost always
fall back on the toolkit developed here. It doesn't take much work to
cast ARMA models, dynamic factor models, structural VARs, and other
important concepts into state space model form, viewing those
applications as special cases within the broader state space framework.
Once you know the state space representation and methods for efficient
estimation and posterior sampling, you've done half the work for many
applications in time series econometrics.

So the first portion will consist of describing state space models and,
naturally, the very important concept of the ``state space.'' The second
portion will then give an overview of the subsequent sections which
discuss computing likelihood via the Kalman Filter and posterior
sampling to draw paths from the underlying state dynamics that 
model the data-generating process.


\section{State Space Models}

\subsection{Basic Building Blocks}

State space models quantify and link the evolution of observed and
(possibly) unobservated \emph{states} to series that we can directly
measure---the data---called \emph{observables}. 

State space models quantify how certain states evolve over time, where
\emph{states} consist of observed and/or unobserved variables of
interest. The evolution of these states are informed by  

Typically, we have some model that specifies how these states are
related. The dynamics of these states then generate the observed data.

The \emph{states} in our model (which we'll collect into a state vector)
might include GDP growth, inflation, unemployment, an unobserved
``business cycle'' component driving the dynamics of many series, a
synthetic index indicating investment prospects, or any number of
objects that we want to quantify, model, and forecast. With our states,
we then define the \emph{state space} as the set of all values our state
vector can assume. In other words, it's the multi-dimensional space
spanned by our state vector. 

To quantify and model the (possibly unobserved) states, we use
\emph{observables}---series we can directly measure and observe---to
inform our estimates of the states, which we cannot always directly
measure. So just to reiterate, the biggest distinction between states
and observables: observables \emph{always} have a directly measurable
series associated with them, while states might not.


\subsection{Linear State Space Models in Two Equations}

State space models essentially reduce to two fundamental equations:
\begin{align}
  \text{State Transition Equation:} \quad
    s_{t} &= C + T s_{t-1} + \varepsilon_{t} \label{ste}\
    \quad\quad E[\varepsilon_t] = 0
    \quad E[\varepsilon_t \varepsilon'_t] =  R \\
  \text{Measurement Equation:} \quad
    y_{t} &= D + M s_{t} + \eta_{t} \label{moe}
    \quad\qquad E[\eta_t] =  0 
    \quad E[\eta_t \eta'_t] =  Q 
\end{align}
Additionally, it is also assumed that $\varepsilon_t$ and $\eta_t$ are
uncorrelated, iid, and independent of past values. Also, by
construction, $E[\varepsilon_t s_t']=0$ and $E[\eta_t y_t']=0$.

Moreover, while $\varepsilon_t$ and $\eta_t$ are typically assumed to be
normal, we wrote them here in the more general (yet equally valid)
represenetation above. Later on, we will discuss the extra subtetly that
non-normal errors introduce into the interpretation.


\paragraph{State Transition Equation} Equation~\ref{ste} describes how
the state vector, $s_t$, evolves over time; it's a law of motion. In
words, the state vector next period is a function of the previous
period's state vector, plus some innovation or disturbance.  Since we
iterate on the state vector $s_{t-1}$ to get the next period $s_{t}$,
it's clear that $T$ must be square, while $C$ and $\varepsilon_t$ are
column vectors the same size as $s_t$.

But since the states might be unobserved (so that they do not have a
directly measurable series associated with them), we have to relate them
to the data somehow, which is where the next equation comes in.

\paragraph{Measurement Equation} Equation~\ref{moe}, sometimes called
the ``Observation Equation'', relates the states to the directly
measurable observables, allowing for potential ``measurement error''
captured by $\eta_t$. More succinctly, $M$ is a matrix that \emph{maps}
states to observables. Since the number of states and observables may
differ, $M$ need not be square. Clearly, $D$ and $\eta_t$ must be the
same size as $y_t$.

\paragraph{Note on Free Parameters} 
When we write down a state space model, this typically involves imposing
ones and zeros in certain entries of $M$, $R$, and $Q$, while leaving
other parameters free so that we can estimate them.  To pre-specify
certain parameters in both equations.  While our goal will be estimate
in Equation~\ref{ste} has to be estimated ($\{s_t\}$, $T$, $R$), the
matrix $M$ will need to be pre-specified by the econometrician. Matrix
$M$ makes explicit our idea of the relationship between the observables
we see and the deep underlying model dynamics that (we think) drive
what's observed. Of course, this relationship cannot be estimated; it
can only be imposed via $M$.

\newpage
\subsection{Expanding the State Space}

In this section, we show two examples of expanding the state space (introducing extra states), and why we might want to do that. 

\subsubsection{Introducing Extra Lags}

Equation~\ref{ste} only explicitly incorporates one lag of the state
vector. But suppose $s_t$ depends upon more than one lag. Not a problem;
the representation in Equation~\ref{ste} will still work if we expand
the state space---that is, incorporate additional variables into our
state vector. Specifically, we can rewrite the State Transition Equation
(in stacked vector and block matrix form) as 
\begin{align}
  \begin{bmatrix} s_{t} \\ s_{t-1}
  \end{bmatrix}
     &= 
  \begin{bmatrix} C \\ \mathbf{0} 
  \end{bmatrix}
  +\begin{bmatrix} T_1 & T_2 \\ I & \mathbf{0}
  \end{bmatrix}
  \begin{bmatrix} s_{t-1} \\ s_{t-2}
  \end{bmatrix}
  + \begin{bmatrix} 
      \varepsilon_{t}  \\ \mathbf{0}
    \end{bmatrix}
  \label{augmented.lags} 
\end{align}
So now our new state vector is no longer $s_t$, but the stacked $z_t:=(s_t \;\; s_{t-1})'$. In this formulation, $T_1$ captures the dependence of $s_t$ upon $s_{t-1}$, while $T_2$ captures the dependence upon $s_{t-2}$. The bottom half of the transition matrix above will enforce that $s_{t-1}=s_{t-1}$. The necessary changes to the measurement equation are obvious.

To incorporate still more lags, the extra modifications to the state
transition equation are similarly straightforward. In addition, it may
even be the case that we need to use extra lags for only a few elements
of $s_t$, rather than all elements of $s_t$. In such cases,
Equation~\ref{augmented.lags} can be simplified so that we augment the
state vector with only the necessary lags, rather than the vector
$s_{t-1}$ (on the lefthand side) in it's entirety. Only the most minor
of adjustments to the transition matrix would be necessary.


\subsubsection{Measurement Error as a State}

While the two-equation framework set up in (\ref{ste}) and (\ref{moe})
might be the most intuitive representation of our system, it might be
easier (for programming or computational reasons) to remove measurement
error from the measurement/observation equation. We can do so by, again,
enlarging the state space and incorporating measurement error as extra
states.

So we now rearrange the terms in Equation~\ref{ste} and
Equation~\ref{moe} into new State Transition and Measurement Equations.
Again, we use stacked vector and block matrix form, augmenting the state
vector with extra rows, $s'_t$, which capture the contribution from the
measurement errors:
\begin{align}
  \text{State Transition Equation:} \quad
    \begin{bmatrix} s_{t} \\ s'_t \end{bmatrix}
    &= \begin{bmatrix} C \\ \mathbf{0} \end{bmatrix}
    + \begin{bmatrix} T & \mathbf{0} \\ 
    \mathbf{0} & \mathbf{0} \end{bmatrix}
    \begin{bmatrix} s_{t-1} \\ s'_{t-1} \end{bmatrix}
    + \begin{bmatrix} \varepsilon_{t} \\ \eta_t \end{bmatrix}
    \label{ste.withmerr}
  \\ \notag \\
  \text{Measurement Equation:} \quad
    y_{t} &= D + 
    \begin{bmatrix} M & I \end{bmatrix}
    \begin{bmatrix} s_{t} \\ s'_t \end{bmatrix}
\end{align}
In this representation, $s'_t$ effectively ``carries'' the measurement error from the state transition equation down to the measurement equation. But any past contributions from measurement errors are decisively zeroed out going forward, as we see in the new transition matrix of Equation (\ref{ste.withmerr}).


\subsection{Examples within the State Space Framework}

A few examples can best illustrate how state space models work in practice. Rather than just listing equations and general forms, we will fully specify some typical examples in state space form.

By casting some familiar models into state space, we show that state space models really do unify and generalize many different ideas. In addition, we show that the state space framework can handle a larger class of problems, as the two layers of states and observables easily accommodate distinctions between ``true values'' we hope to discern and the noisy measures we actually observe---as in the GDP example below.\footnote{Regular time series analysis doesn't usually allow for such distinctions, treating the observed series as \emph{the} true value always.}

\subsubsection{AR Model}

Suppose that we have some variable $x_t$ that evolves according to an AR($p$) process, where $\mu$ denotes the mean of the series.\footnote{Both here and in the next subsection, $\mu$ is assumed to be known. Otherwise, it would need to be estimated, as we will describe in later sections.} In other words,
\[
  (x_t - \mu) = \sum^p_{i=1} \rho_i (x_{t-i} - \mu) 
  + \varepsilon_t
\]
Since the state transition equation relates past and present, this is where we will express the autoregressive relationship, using the approach in Subsection (Extra Lags) to augment the state vector to include extra lags:
\[
  s_t = 
  \begin{bmatrix} x_t - \mu \\ x_{t-1} - \mu \\ \vdots \\
    x_{t-p+1} - \mu \end{bmatrix}
  = 
  \begin{bmatrix}
    \rho_1 & \rho_2 & \cdots & \rho_{p+1} & \rho_p \\
    1 & 0 & \cdots & 0 & 0 \\
    0 & 1 & \cdots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \cdots & 1 & 0 \\
  \end{bmatrix}
  \begin{bmatrix} x_{t-1} - \mu \\ x_{t-2} - \mu \\ \vdots \\
    x_{t-p}- \mu \end{bmatrix}
  + \begin{bmatrix} \varepsilon_t \\ 0 \\ \vdots \\ 0 
  \end{bmatrix}
\]
Finally, we specify the measurement equation to account for the de-meaning of $x_t$ within the state transition equation. Since we don't consider the additional layer of measurement error within regular AR($p$) models, we don't add any term for the error:
\[
  y_t = \begin{bmatrix} x_t \end{bmatrix}
  = \mu + 
  \begin{bmatrix}
  1 & 0 & \cdots & 0 \\
  \end{bmatrix}
  \begin{bmatrix} x_t - \mu \\ x_{t-1} - \mu \\ \vdots \\
    x_{t-p+1} - \mu \end{bmatrix}
\]
Thus, we have now embedded the simple AR($p$) model within a state space framework.

\subsubsection{MA Model}

Now, let's consider an MA($q$) model: 
\[
  (x_t - \mu) = \varepsilon_t 
  + \sum^q_{i=1} \theta_i  \varepsilon_{t-i}
\]
Since we'll need to keep track of lagged \emph{innovations}, we'll incorporate those as elements of our state vector, rather than lagged values of $x_t$.\footnote{Here, we see a great example of the flexibility that state space models offer---the many different ways they allow the econometrician to think about and model the system.} This gives a state transition equation of 
\[
  s_t = 
  \begin{bmatrix}
  \varepsilon_t \\ \varepsilon_{t-1} \\
  \vdots \\ \varepsilon_{t-q}
  \end{bmatrix}
  = \begin{bmatrix}
  0 & 0 & \cdots & 0 & 0 \\
  1 & 0 & \cdots & 0 & 0\\
  0 & 1 & \cdots & 0 & 0\\
  \vdots & \vdots & \ddots & \vdots & \vdots\\
  0 & 0 & \cdots & 1 & 0\\ 
  \end{bmatrix}
  \begin{bmatrix}
  \varepsilon_{t-1} \\ \varepsilon_{t-2} \\
  \vdots \\ \varepsilon_{t-q-1}
  \end{bmatrix}  
  + \begin{bmatrix}
  \varepsilon_t \\ 0 \\ \vdots \\ 0 
  \end{bmatrix}
\]
Which then leads to the following observation equation. Again, no term for measurement error is necessary:
\[
  y_t = 
  \begin{bmatrix} x_t \end{bmatrix}
  = \mu +  
  \begin{bmatrix} 1 & \theta_1 & \theta_2 & \cdots &
    \theta_q \end{bmatrix}
  \begin{bmatrix}
  \varepsilon_t \\ \varepsilon_{t-1} \\
  \vdots \\ \varepsilon_{t-q}
  \end{bmatrix}
\]

\subsubsection{GDP: Income and Expenditure}
We also present an example where there is some unobserved ``true'' level
of GDP that we hope to capture, using two alternative measures that
ostensibly measure the same thing---the income and expenditure measures
of GDP. However, because of measurement error in both series, they
provide only a ``noisy'' measure. This is a situation ideally suited to
a state space representation.


\newpage
\section{Kalman Filter under Known State and Observation Equations}

Within the state space framework detailed above, we now face the task of estimating the sequence of (possibly unobserved) states. To do so, we typically use the Kalman filter, a recursive procedure to estimate and forecast states over time. 

Subsection 1 sketches the basic sequence of steps for deriving the distributions of the (possibly unobserved) states and then forecasting their future values. Next, Subsection 2 describes this recursive filtering algorithm in greater detail. Subsection 3 first defines the conditional distributions of normal random variables and then comments on the the consequence of assuming Gaussian errors in the Kalman Filter procedure. Subsection 4 then follows with a detailed examination of this special case of Gaussian errors.

Throughout this entire section, we will assume that the matrices defined in Equations \ref{ste} and \ref{moe} are entirely \emph{known}, postponing estimation (which entails constructing the full likelihood function) for a later section after first nailing down the intuition. 

\subsection{Outline of the Recursive Procedure}

Here's the basic Kalman Filter procedure to estimate and forecast the state vector, $s_t$, and it's distribution given data vector $y_t$ for which we have observations running from $t=1$ until $t=T$. The procedure specified here applies to all linear state space models as written in Equations \ref{ste} and \ref{moe}.
\begin{enumerate}
\item Start with a prior for the state vector at time $t$, call it, call it $p(s_t |\; \mathcal{I}_{t-1})$, where $\mathcal{I}_{t}$ denotes the set of information available at arbitrary time $t$, i.e. $y_1, \ldots, y_{t}$.\footnote{In the case where $t=1$, we form a true prior in that you haven't observed any $y_t$, so you don't have any data or information as a basis for the distribution of $s_1$.}
\item Observe the time $t$ realized data point, $y_t$. 
\item Use Bayes' Rule to ``filter'' out the noise and compute the filtering distribution, $p(s_t |\; \mathcal{I}_{t})$, where $\mathcal{I}_{t}$ reflects available information at time $t$.
\item Compute the one-step-ahead predictive distribution, $p(s_{t+1} | \; \mathcal{I}_{t})$, based on the filtering distribution from Step (3) and the your model's law of motion as specified in the State Transition Equation.  
\item Increment $t$ and return to step 1, taking
the predictive distribution $p(s_{t+1} |\; \mathcal{I}_{t})$ as your prior.
\end{enumerate} 
Note that many introductions to the Kalman Filter often frame the recursive process in terms of the evolution of only the \emph{mean} and \emph{variance} of $\{s_t\}$ over time. This is done because the Kalman Filter is often applied in the case where errors are normally distributed, in which case the mean and variance pin down the successive distributions {entirely}.\footnote{This is a feature of multivariate normal random variables.} However, the procedure outlined above is ``General'' in that it frames the recursive process in terms of the full \emph{distributions} for $\{s_t\}$ over time. Such an exposition then remains valid even when errors have a non-normal distribution.

Going forward, the remainder of this section will put some more structure on this basic procedure, filling in the mathematical details. It will also cover that special case of normal errors, and say why this assumption might not be as dangerous as it seems. For that discussion, follow a minor yet very important detour in two subsections.


\subsection{The General Kalman Filter}

Given a sequence of data points $\{y_t\}_{t=1}^T$ our goal is to estimate the sequence of states $\{s_t\}^T_{t=1}$. Of course, this will all be probabilistic. Unless the system is degenerate, we won't be able to know with certainty the true value of the states. We'll instead compute a sequence of probability distributions for the states at each point in time. Again, we assume throughout that the matrices of our linear state space model defined by Equations \ref{ste} and \ref{moe} are entirely known.


\paragraph{Prior Step} Suppose we're standing at time $t-1$, in full possession of all information up until then. Specifically, we know $y_1, \ldots, y_{t-1}$, denoted as the information set $\mathcal{I}_{t-1}$. 

Additionally, suppose that we have a prior for $s_t$, given $\mathcal{I}_{t-1}$,
\begin{equation}
  \text{Prior:} \quad p(s_t | \; \mathcal{I}_{t-1}) 
  \label{pri}
\end{equation}
In the case where $t=1$, then this is a true prior since $\mathcal{I}_{0}$ will be the empty set, and we will have no available observations as a basis for this prior. However, when $t>1$, the Kalman Filter procedure will furnish a prior, which we will derive in full below.

\paragraph{Filtering Step}
With the above prior in hand, when we get to time $t$, we observe $y_t$ and should immediately update our estimate of the distribution for $s_t$, incorporating the new time $t$ information. This step is called the ``Filtering Step'' because our combination of both prior and data allows us to \emph{filter out} the noise in our estimates for $s_t$. 

But how exactly do we do this? Bayes' rule, of course, along with the fact that $\mathcal{I}_{t} = \{y_1, \ldots, y_t\}=\mathcal{I}_{t-1} \cup y_t$. More specifically, we compute the filtering distribution 
\begin{align*}
  p(s_t | \mathcal{I}_{t}) = 
  p(s_t | \; y_t, \mathcal{I}_{t-1}) = 
  \frac{
    p(y_t | s_t, \mathcal{I}_{t-1}) 
    \cdot p(s_t|\; \mathcal{I}_{t-1})
  }{p(y_t | \; \mathcal{I}_{t-1})}
\end{align*}
This doesn't look like we've gotten anywhere, but note that we can simplify a bit. By the setup and assumptions in Equations \ref{ste} and \ref{moe}, we know that past values $y_1, \ldots, y_{t-1}$ have no bearing upon $y_t$ if we know the state, $s_t$. Hence $p(y_t|s_t, \mathcal{I}_{t-1}) = p(y_t|s_t)$. In addition, since the denominator is just a normalizing constant, we can simplify the expression further to  
\begin{equation}
  \label{filt}
  \text{Filtering Distribution:} \quad
  p(s_t | \mathcal{I}_{t}) \propto p(y_t | s_t) \cdot 
  p(s_t |\mathcal{I}_{t-1})
\end{equation}
This is a very friendly expression since we already have $p(s_t | \mathcal{I}_{t-1})$ from our prior and, given the setup in Equations \ref{ste} and \ref{moe}, the likelihood $p(y_t|s_t)$ is very easy to compute once we condition on the state $s_t$.

In fact, the measurement equation determines the distribution $p(y_t|s_t)$ entirely. Since the $\eta_t$ errors are mean zero with covariance matrix, $Q$, we know by construction that
\[
  E[y_t|s_t] = D + M s_t 
  \qquad 
  \text{Var}(y_t|s_t) = Q
\]
Beyond the mean and variance, the full distribution is characterized by the distribution we assume for the measurement errors.

\paragraph{Forecast Step} Now that we have our filtering distribution, we compute the \emph{forecast distribution}, which the one-step ahead distribution of $s_{t+1}$ given $\mathcal{I}_{t}$. Again, Bayes' Rule is useful
\begin{align*}
  p(s_{t+1} | \; \mathcal{I}_{t}) &= 
    \int p(s_{t+1}, s_t |\; \mathcal{I}_{t}) \; ds_t \\
  &= \int p(s_{t+1} |  s_t, \mathcal{I}_{t}) 
    p(s_t |  \mathcal{I}_{t}) \; ds_t 
\end{align*}
But note that $p(s_{t+1} | s_t, \mathcal{I}_{t}) = p(s_{t+1}|s_t)$ since according to the State Transition Equation, if we know $s_{t}$, the observations $y_1, \ldots, y_t$ provoide no additional information. This yields the final expression
\begin{align}
  \text{Forecasting Distribution:} \quad
  p(s_{t+1} | \; \mathcal{I}_{t}) 
  &= \int p(s_{t+1} |  s_t) 
    p(s_t |  \mathcal{I}_{t}) \; ds_t 
\end{align}
Again, this doesn't look much simpler, but it upon closer inspection, we can easily recognize the components. Namely, $p(s_{t+1}|s_t)$ is characterized by the assumption about the errors in the State Transition Equation. By construction,
\[
  E[s_{t+1}|s_t] = C + Ts_t
  \qquad 
  \text{Var}(s_{t+1}|s_t) = R
\]
Beyond just the mean and variance, the full distribution is
determined by the assumed distribution of the errors.

Finally, we see that $p(s_t|\mathcal{I}_{t})$ is just the
filtering distribution we computed in the last step. 

\paragraph{Recursive Step} What exactly makes this whole procedure recursive? Increment $t$, take the distribution $p(s_{t+1}|\;\mathcal{I}_{t-1})$ that we just computed, and let that be our prior for the next period. Repeat until we get to time $T$.
\\
\\
All of these steps simplify considerably and become much more concrete if we're willing to assume normal errors. But how big of an assumption is that? What are the practical consequences of such a choice? To answer these questions, we will take a minor detour for the sake of intuition and interpretation before delving into the particular case of the Kalman Filter under normal errors.

\clearpage
\subsection{Detour: Conditional Distributions and Interpretation in the Case of Normal Errors}

Suppose that there is a multivariate normal random variable $X$---a vector that can be partitioned into two pieces and written
\[
  X \sim N(\mu, \Sigma) \quad\Leftrightarrow\quad
  \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}
  \sim 
  N\left(\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix},
  \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\
  \Sigma_{21} & \Sigma_{22} \end{bmatrix} 
  \right)
\]
Then the distribution of $X_1$ given $X_2$ (i.e. after having observed $X_2$) is 
\begin{align}
  X_1 | X_2 \sim &N(\hat{\mu}, \hat{\Sigma})  \notag\\
  \label{reg} \\
  \text{where} \quad
  \hat{\mu} = \mu_1 + \Sigma_{12} \Sigma^{-1}_{22} 
  (X_2-\mu_2)  & \quad \quad
  \hat{\Sigma} = \Sigma_{11} - \Sigma_{12} \Sigma^{-1}_{22} 
    \Sigma_{21}
  \notag
\end{align}
See Appendix {link} for the derivation.

\paragraph{Intuition}
Consider the case where $X_1$ and $X_2$ are scalars, not vectors. Then we see in the second term of the sum for $\hat{\mu}$ that an above-average realization of $X_2$ will lead us to \emph{upwardly} revise a purely unconditional forecast of $X_1$ above the mean $\mu_1$ (provided $\Sigma_{12}>0$, otherwise we revise down). 

In addition, provided that the correlation $\Sigma_{12} = \Sigma_{21}$ is non-zero, we will revise \emph{down} the variance of $X_1$ relative to the baseline of $\Sigma_{11}$. This follows because the observation of $X_2$ has provided valuable information about $X_1$. 


\paragraph{The Normality Assumption} 
Consider your basic regression with 
\begin{align}
  \label{regex}
  Y_i = \alpha + \beta X_i + \varepsilon_t
\end{align}
In Intro Stat, any discussion of regression analysis starts by assuming that the errors, $\varepsilon_t$, are normally distributed, which is equivalent to assuming that $Y_i$ and the regressors in $X_i$ are, together, jointly normally distributed---i.e.  $\begin{bmatrix} Y_i & X^T_i \end{bmatrix}^T$ is  multivariate normal. 

Then, if you get to intermediate Stat, your professor starts to retreat on the normality assumptions. In particular, you learn that it doesn't have to be the case that the variables are, in fact, jointly normally distributed. It could just be that you want the best \emph{linear predictor} of $Y_i$ given $X_i$. In that case, you care about regression estimates only because they provide the best linear approximation of the conditional expectation function, $E[Y_i|X_i]$. Alternatively, you might also hear that linear regression gives the best linear ``projection'' of $Y_i$ onto $X_i$. 

Well, that's our ticket. If you look at Result \ref{reg}, you'll notice that we're getting a kind of \emph{regression estimate} of $X_1$ given $X_2$. To be even more explicit, we might write
\[
  E[X_1|X_2] = \mu_1 + \beta (X_2 - \mu_2) 
\]
where $\beta= \Sigma_{12}\Sigma^{-1}_{22}$. This is just like Regression \ref{regex} above, only here, we regress \emph{vector} (not scalar) $X_1$ on vector $X_2$. However, the intuition remains the same.

Therefore, Kalman Filter estimates that assume normally distributed errors are not as nefarious as they seem. Since Kalman Filter estimates of the states under the normality assumption will follow directly from Result \ref{reg} above (as we will see in the next subsection), we can motive the Kalman Filter with normal errors by framing it as a recursive procedure to find the best \emph{linear estimates} or \emph{projection estimates} of the states. 


\subsection{Kalman Filter Under Normal Errors}

We now derive the Kalman Filter procedure in the case of normally distributed errors, $\varepsilon_t$ and $\eta_t$. Since the normal distribution is pinned down entirely by the mean and variance, we need only characterize the evolution of these two objects to characterize the evolution of the full distribution of the states. For simpler notation, we will ignore the constant vectors, $C$ and $D$ throughout this section.

\paragraph{Notation}
Taking advantage of this, we will adopt the following standard notation, letting $\mu$ and $\Sigma$ denote the mean and variance, respctively: 
\begin{align*}
  E[s_t|\;\mathcal{I}_{t-1}] &= \mu_{t|t-1} 
  \qquad
  \qquad
  E[s_t|\;\mathcal{I}_{t}] = \mu_{t|t}  \\
  \text{Var}(s_t|\;\mathcal{I}_{t-1}) &= \Sigma_{t|t-1}  
  \qquad
  \quad
  \text{Var}(s_t|\;\mathcal{I}_{t}) = \Sigma_{t|t} 
\end{align*}
So rather than directly specifying the sequence of distributions (prior, filtering, and forecast), we need only specify the sequence of the determinants of those distributions: the prior, filtering, and forecast means and variances.
%\[
%  p(s_t|\;\mathcal{I}_{t-1})
%  \qquad 
%  p(s_t|\;\mathcal{I}_{t})
%  \qquad
%  p(s_{t+1}|\;\mathcal{I}_{t})
%\] 
%we need only specify the sequence of the determinants of those distributions, 
%\[
%  \begin{pmatrix} \mu_{t|t-1} & \Sigma_{t|t-1}  
%  \end{pmatrix}
%  \qquad
%  \begin{pmatrix} \mu_{t|t} & \Sigma_{t|t} \end{pmatrix}
%  \qquad 
%  \begin{pmatrix} \mu_{t+1|t} & \Sigma_{t+1|t} \end{pmatrix}
%\]

\paragraph{Prior Step}
Suppose that we have a Guassian prior for $s_t$, given $\mathcal{I}_{t-1}$,
\[
  s_t | \; \mathcal{I}_{t-1} 
  \sim N(\mu_{t|t-1}, \Sigma_{t|t-1})
\]
In the case where $t=1$, then this is a true prior, and $\mu_{1|0}$ and $\Sigma_{1|0}$ reflect an initial guess. When $t>1$, the Kalman Filter procedure will furnish a Gaussian prior, which we will derive in full below.

\paragraph{Filtering Step}
Recall that the goal of the filtering step is to compute the
filtering distribution, $p(s_t|\mathcal{I}_{t})=p(s_t|y_t, \mathcal{I}_{t-1})$. Now because we're assuming both $s_t$ and $y_t$ are normally distributed (since the error terms are), we can write 
\begin{align*}
  \begin{bmatrix} s_t \\ y_t \end{bmatrix} | \; 
  \mathcal{I}_{t-1}
  \sim 
  N\left(\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix},
  \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\
  \Sigma_{21} & \Sigma_{22} \end{bmatrix} 
  \right)
\end{align*}
for some value of the terms $\mu_{i}$, and $\Sigma_{jk}$, where $i, j, k\in\{1,2\}$. Now with this setup, it's clear how to compute the filtering distribution: apply the result from the last subsection to get $p(s_t|y_t, \mathcal{I}_{t-1}) = p(s_t | \;\mathcal{I}_{t})$. We just need to pin down those terms.
\\
\\
{\sl Defining the Terms:} Throughout, well use the notation that
$E_{t-1}[\,\cdot\,] = E[\,\cdot\,|\mathcal{I}_{t-1}]$ to make things more
readable.
\begin{itemize}
  \item $\mu_1$ and $\Sigma_{11}$: These terms correspond
    to the mean and covariance matrix for $s_t |\;
    \mathcal{I}_{t-1}$. But we know these. We defined them
    in the prior step to be $\mu_{t|t-1}$ and
    $\Sigma_{t|t-1}$, respectively. 

  \item $\mu_2$ and $\Sigma_{22}$: First, use Measurement
    Equation \ref{moe} to get the mean
    \begin{align*}
      \mu_2 := E_{t-1}[y_t ] &= 
        E_{t-1}[Ms_t + \eta_t ] =
        ME_{t-1}[s_t] 
        + E_{t-1}[\eta_t ]\\
        &= M \mu_{t|t-1} + 0 = M\mu_{t|t-1}
    \end{align*}
    Now that we have the mean, let's get the variance of $y_t$: 
    \begin{alignat*}{2}
      \text{From Measurement Equation} &\qquad
        \text{Var}_{t-1}(y_t) 
        &&= \text{Var}_{t-1}(Ms_t + \eta_t) \\
      \text{Since $s_t\perp\eta_t$} &\qquad 
        &&= \text{Var}_{t-1}(Ms_t) 
        + \text{Var}_{t-1}(\eta_t)\\
      \text{Property of covariance matrices} &\qquad 
        &&= M [\text{Var}_{t-1}(s_t)] M' 
        + \text{Var}(\eta_t) \\
      \text{From Equations \ref{ste} and \ref{moe}} &\qquad 
        &&= M\Sigma_{t|t-1} M' + Q =: \Sigma_{22}
    \end{alignat*}
    
  \item Finally, let's define $\Sigma_{12} =
    (\Sigma_{21})^T$, using some of the results just
    obtained and again substituting in using the
    Measurement Equation.
    \begin{align*}
      \Sigma_{21} := E_{t-1}\left[(y_t-\mu_2)(s_t - \mu_1)'
        \right] &= 
        E_{t-1}\left[(Ms_t+\eta_t-M\mu_{t|t-1})(s_t-\mu_{t|t-1})'
        \right]  \\
      &= E_{t-1}\left[\left\{\eta_t+ M(s_t-\mu_{t|t-1})\right\}
        (s_t-\mu_{t|t-1})' \right]  \\
      \text{Distribute}
      \qquad
      &= E_{t-1}\left[\eta_t(s_t-\mu_{t|t-1})'\right]]\\
      &\quad + E_{t-1}\left[M(s_t-\mu_{t|t-1})
        (s_t-\mu_{t|t-1})'\right]  \\
      \text{By definition of $\Sigma_{11}=\Sigma_{t|t-1}$}
      \qquad
      &= E_{t-1}\left[\eta_t\varepsilon'_t\right]
        + M\Sigma_{t|t-1} \\
      \text{Since $\varepsilon_t\perp \eta_t$} \qquad
      &= 0 + M\Sigma_{t|t-1}  = M\Sigma_{t|t-1} 
    \end{align*}
\end{itemize}
Now that all the components have been specified, we can
fill in, apply the result in the previous subsection, and
conclude. Also remember that
$\mathcal{I}_{t}=\mathcal{I}_{t-1} \cup \{y_t\}$. Here we
are:
\begin{alignat}{3}
  &&\begin{bmatrix} s_t \\ y_t \end{bmatrix} | \; 
  \mathcal{I}_{t-1}
  &\sim 
  N\left(\begin{bmatrix} \mu_{t|t-1} \\ M\mu_{t|t-1} 
    \end{bmatrix},
  \begin{bmatrix} 
    \Sigma_{t|t-1} & \Sigma_{t|t-1} M'
    \notag\\
    M\Sigma_{t|t-1} & M\Sigma_{t|t-1}M' + Q 
  \end{bmatrix} 
  \right) \notag\\\label{normfilt}\\
  \Rightarrow \qquad
  \mu_{t|t} =&& E[s_t|y_t, \mathcal{I}_{t-1}]
    &= \mu_{t|t-1} + \Sigma_{t|t-1} M' 
    \left(M\Sigma_{t|t-1}M' + Q\right)^{-1} (y_t-M\mu_{t|t-1}) \notag\\
  \Sigma_{t|t} =&& 
    \text{Var}(s_t|y_t, \mathcal{I}_{t-1})
    &= \Sigma_{t|t-1} - \Sigma_{t|t-1}M' 
    \left(M\Sigma_{t|t-1}M' + Q\right)^{-1} M\Sigma_{t|t-1} \notag
\end{alignat}
Echoing the last subsections intuition section, this
result has a very nice interpretation. We see that our
estimate of the filtered mean $\mu_{t|t}$ is the sum of
the prior mean $\mu_{t|t-1}$ plus the scaled error for
$y_t$ relative to it's prior expectation $M\mu_{t|t-1}$.
In addition, the information provided by $y_t$ allows us
to reduce our the variance in our estimate of $s_t$ to
$\Sigma_{t|t}$ relative to the prior variance
$\Sigma_{t|t-1}$.

\paragraph{Forecast Step} Now that we have our filtered
mean and variance estimates, we can apply the law of
motion as written in State Transition Equation \ref{ste} to get the mean and variance of the forecast for
the states:
\begin{alignat*}{3}
  \mu_{t+1|t} =&& E[s_{t+1}|\;\mathcal{I}_{t}] &= 
    E[Ts_t + \varepsilon_t |\;\mathcal{I}_{t}] = 
    TE[s_t | \mathcal{I}_{t}] 
    + E[\varepsilon_t |\;\mathcal{I}_{t}]  \\
    && &= T\mu_{t|t} + 0 = T\mu_{t|t} \\
  \Sigma_{t+1|t} = &&\text{Var}(s_{t+1}|\;\mathcal{I}_{t})
  &= \text{Var}(Ts_t + \varepsilon_t |\;\mathcal{I}_{t})
  = T[\text{Var}(s_t|\;\mathcal{I}_{t})]T'
  + \text{Var}(\varepsilon_t|\;\mathcal{I}_{t}) \\
  && &= T\Sigma_{t|t}T' + R
\end{alignat*}
A similarly straightforward application and writing-out of
the measurement equation will yield the following forecast
mean and variance for the observables:
\begin{align*}
  E[y_{t+1}|\;\mathcal{I}_{t}] &= T\mu_{t|t} \\
  \text{Var}(y_{t+1}|\;\mathcal{I}_{t})
    &= M\left(T\Sigma_{t|t}T' + R\right)M' + Q
\end{align*}

\subsection{Long-Horizon Forecasts}

The section has so far only discussed the consruction of one step ahead forecast distributions for states and observables. However, once we reach the end of the sample and derive $p(s_T | \mathcal{I}_{T})$, we will often want to forecast $s_{T+1}$, $s_{T+2}$, etc. along with $y_{T+1}$, $y_{T+2}$, etc. So let's now characterize the distributions of our forecasts. 

For simplicity, we ignore the constant terms $C$ and $D$. We will also construct the $m$-step ahead distributions for any $t=1,\ldots,T$, not just $t=T$ since the results are the same.

\paragraph{State Forecasts} Start by applying the transition matrix to get the mean. All of the error terms will drop out:
\[
  E[s_{t+m} | \mathcal{I}_{t}] = 
  T^m \mu_{t|t} =: \mu_{t+m|t}
\]
Now for the covariance matrix, we first construct the error:
\begin{align*}
  s_{t+m} - \mu_{t+m|t} &= 
  \left(T^m s_t + T^{m-1} \varepsilon_{t+1}
  + T^{m-2} \varepsilon_{t+2} + \ldots
  + T\varepsilon_{t+m-1} + \varepsilon_{t+m}\right) 
  - T^m \mu_{t|t}\\
  &= T^m (s_t-\mu_{t|t}) + T^{m-1} \varepsilon_{t+1}
  + T^{m-2} \varepsilon_{t+2} + \ldots
  + T\varepsilon_{t+m-1} + \varepsilon_{t+m}
\end{align*}
Now take the expectation of the squared error above to get the covariance matrix:
\begin{align*}
  \Sigma_{t+m|t} &:=
    E[(s_{t+m} - \mu_{t+m|t})'(s_{t+m} - \mu_{t+m|t})]\\
    &= T^m \Sigma_{t|t} (T^m)' + T^{m-1}R(T^{m-1})'
      + \cdots + TRT' + R
\end{align*}

\paragraph{Observable Forecasts} From there, it's easy to see how we get the forecast distribution of the observables. To get the mean, recall the measurement equation:
\begin{align*}
  E[y_{t+m}|\;\mathcal{I}_{t}] &= 
    E[Ms_{t+m} + \eta_{t+m}|\;\mathcal{I}_{t}]
    = ME[s_{t+m}|\;\mathcal{I}_{t}]
    + E[\eta_{t+m}|\;\mathcal{I}_{t}]\\
  &= M\mu_{t+m|t} + 0 = M T^m \mu_{t|t}
\end{align*}
Next, we construct the error of the forecast, which we then square to get the covariance matrix:
\begin{align*}
  y_{t+m} - E[y_{t+m}|\;\mathcal{I}_{t}] &= 
    \left(Ms_{t+m} + \eta_{t+m}\right) 
    - M\mu_{t+m|t}\\
  &= M\left(s_{t+m}-\mu_{t+m|t}\right) + \eta_{t+m}\\
  \Rightarrow\qquad
  E\left\{y_{t+m} - E[y_{t+m}|\;\mathcal{I}_{t-1}]
  \right\} &= M\Sigma_{t+m|t}M' + Q
\end{align*}


\clearpage
\section{Kalman Smoother under Known State and Observation Equations}

The last section laid out in detail the recursive Kalman Filter procedure for deriving the distributions $\{s_t\}_1^T$ given observations $\{y_t\}_1^T$. If you only care about forecasting $s_{t+i}$ and $y_{t+i}$ for $i>0$, then you're done. The Kalman Filter gave you an estimate of the distribution of $s_T$. Forecast forward from there.

However, if you're also interested in the states themselves---that is, the sequence $\{s_t\}_1^T$ actually matters to you---then you can do better. This is because we're working with time series and $y_{t+1}$ can give you useful information about $s_1, \ldots, s_t$, not just $s_{t+1}$. But you'll notice that the Kalman Filter ignores this information. It only goes from $t=1$ to $t=T$ without ever looking back and revising early estimates in light of later information. 

This is where the Kalman Smoother comes in: it is another recursive procedure to improve our estimates of the state distributions by conditioning each estimate on the \emph{entire} information set, $\mathcal{I}_{T}$. 

\subsection{Outline of the Recursive Procedure}

Again, we seek the distribution of $s_t$ given data $\{y_t\}^T_1$. This applies to all linear state space models as written in Equations \ref{ste} and \ref{moe}:
\begin{enumerate}
  \item Run the Kalman Filter, storing the filtered
    probability distributions (or relevant parameters
    defining those distributions),
    $p(s_t|\mathcal{I}_{t-1})$ from $t=1,\ldots,T$.
  \item 
\end{enumerate}


\clearpage
\section{Time-Varying State Space Models}

Suppose we want to consider time series models that
include time-varying coefficients. We can easily extend
the state space framework developed above to accommodate.




%\section{Filtering}
%
%Suppose we want to measure some latent state variable $x$. We will 
%assume a \emph{prior} that is multivariate normal such that
%    \[ s_t \sim \text{N}(\hat{s}_t, \Sigma) \]
%Next, we ``measure'' $x$ by matching it to an observable in
%a \emph{measurement equation}:
%    \[ y = G x + v \qquad v\sim \text{N}(0, R) \]
%where $R$ is positive definite, while $G$ and $R$ are both 
%$2 \times 2$. This forms the \emph{likelihood}.
%\\
%\\
%We then ``filter'' out the noise, updating our view of $x$ in
%light of the data in the filtering step using Bayes' Rule.
%Note, this is called ``filtering'' because we don't use the prior
%and likelihood to forecast into
%the future. We combine the prior with the likelihood only to filter
%out noise and get closer to the true value of $x$ based on
%the data, summarized in the posterior, or the
%\emph{filtering distribution}:
%\begin{align}
%    p(x \; | \; y) &= \frac{p(y \; | \; x) \cdot p(x)}{p(y)} 
%    \propto p(y \; | \; x) \cdot p(x) \label{xgiveny} \\
%    &\propto \exp\left\{-\frac{1}{2}
%	\left( y - Gx \right)' R^{-1} \left(y-Gx\right)
%	\right\} \exp\left\{-\frac{1}{2}( x - \hat{x} )' 
%	\Sigma^{-1} (x-\hat{x})\right\} \notag
%\end{align}
%Now let's expand the term in the lefthand exponential:
%\begin{align*}
%    A = \left( y - Gx \right)' R^{-1} \left(y-Gx\right) &= 
%	\left( y' - x'G' \right) R^{-1} \left(y-Gx\right) 
%    = \left( y'R^{-1} - x'G'R^{-1} \right)  \left(y-Gx\right) \\
%    &= \left( y'R^{-1}y - y'R^{-1} Gx - x'G'R^{-1} y 
%	+ x'G'R^{-1} Gx\right)
%\end{align*}
%And now the same for the righthand exponential:
%\begin{align}
%     B = ( x - \hat{x} )' \Sigma^{-1} (x-\hat{x}) 
%	&= ( x' - \hat{x}' ) \Sigma^{-1} (x-\hat{x}) \notag\\
%    &=  (x'\Sigma^{-1} - \hat{x}'\Sigma^{-1})   
%	(x-\hat{x}) \notag \\
%    &=  x'\Sigma^{-1} x - x'\Sigma^{-1} \hat{x} - 
%	\hat{x}'\Sigma^{-1} x
%	+ \hat{x}'\Sigma^{-1} \hat{x} \label{B}
%\end{align}
%Adding the two exponentials, we get:
%\begin{align*}
%    C = A + B &= x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
%	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
%	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \\
%	& \qquad + \hat{x}' \Sigma^{-1} \hat{x} + y' R^{-1} y 
%\end{align*}
%Now notice that Expression \ref{xgiveny} is the probability
%distribution of $x$ \emph{conditional} on $y$ and pretty
%much anything else that isn't $x$.  And because of the
%wonderful properties of the exponential function and the
%black-hole powers of the proportionality constant, we'll be
%able to simplify things nicely (and we'll worry that the
%distribution $p(x|y)$ integrates to one later on).  
%
%So in the expression for $C$, the two terms
%in the second row \emph{don't} depend upon $x$. Therefore,
%letting $C(x)$ be the portion of $C$ that depends upon
%$x$, and letting $C(\lnot x)$ bet the additive terms which
%don't depend upon $x$, we can simplify
%\begin{align*}
%    p(x\; | \; y) &\propto \exp\left\{ -\frac{1}{2}
%	C \right\} = \exp\left\{ -\frac{1}{2}
%	\left[C(x) + C(\lnot x) \right]\right\}  \\
%    &\propto \exp\left\{ -\frac{1}{2}
%	C(x)\right\} +  \exp\left\{ -\frac{1}{2} C(\lnot x) 
%	\right\}  \\
%    &\propto \exp\left\{ -\frac{1}{2}
%	C(x)\right\}
%\end{align*}
%We just absorb the portion not relevant to $p(x|y)$ into
%the proportionality constant.  This means our the work we
%did above to get $C$ simplifies our target expression to
%\begin{align}
%    p(x\;|\;y) &\propto \exp\left\{-\frac{1}{2} \left[
%	x' \left( \Sigma^{-1} + G'R^{-1}G\right) x 
%	- x' (\Sigma^{-1} \hat{x} + G'R^{-1}y)
%	- (\hat{x}' \Sigma^{-1} + y' R^{-1}G) x  \right]\right\}
%	\label{tosimp}
%\end{align}
%\paragraph{Goal}
%Now this doesn't look too helpful, but with a little bit of 
%work, we can turn this into the probability distribution
%for a multivariate normal random variable. 
%In fact, the rest of the section may look complicated, but
%keep in mind the big picture: the likelihood and prior
%were both multivariate normal, so 
%the posterior $p(x|y)$ is going to be normal. We just want
%to identify the mean vector and variance-covariance matrix;
%then we're home.
%
%\paragraph{Variance} So first, the {variance} of the 
%normal distribution corresponding to $p(x|y)$ can
%be derived by examining Equation \ref{tosimp} and likening
%it to Equation \ref{B} (which gives the contents of the 
%exponential in the prior MVN distribution of $x$).  
%
%Namely, the inverse of the new variance,
%which we'll denote as $\Sigma^F$ will be sandwiched in 
%between $x'$ and $x$ in Equation \ref{tosimp}, just as it 
%was sandwiched between $x'$ and $x$ in Equation \ref{B}.
%We use this fact, along with the 
%the Woodbury matrix identity (stated in 
%the appendix) to derive:
%\begin{align}
%    \Sigma^{F} &= \left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
%	\notag \\
%    \text{Woodbury Identity} \Rightarrow
%	\qquad &=  \Sigma - \Sigma G'(R 
%	    + G\Sigma G')^{-1}
%	    G\Sigma
%	    \label{covar}
%\end{align}
%\paragraph{Mean} 
%Next, we want to get the {mean} of the distribution of 
%$p(x|y)$, which we'll denote by $\hat{x}^F$.  Again, once we
%take a second and compare Expression \ref{tosimp}
%to Expression \ref{B}, it's becomes clear from inspection 
%that we must have 
%\begin{equation}
%    \label{notobvious}
%    (\Sigma^{-1} \hat{x} + G'R^{-1}y) 
%	= \left( \Sigma^{-1} + G'R^{-1}G\right) Z
%\end{equation}
%To see this, 
%liken the lefthand side of Equation \ref{notobvious} (which 
%itself comes from Expression \ref{tosimp}) to
%the result of the matrix multiplication $\Sigma^{-1}\hat{x}$ in
%Equation \ref{B}. To get the righthand side, use the fact
%that we \emph{know} the Equation \ref{notobvious} analogue
%to Equation \ref{B}'s $\Sigma^{-1}$, which we just 
%derived in the variance section and called $\Sigma^F$.
%\\
%\\
%So all that's left to do is solve for $Z$ in Equation
%\ref{notobvious}.  The result will turn out to be our mean
%vector for the posterior, $\hat{x}_F$:
%\begin{align}
%    \label{mean}
%    \hat{x}^F = Z 
%        &= \hat{x} 
%        + \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
%        \left( y - G \hat{x} \right)  
%\end{align}
%If you want to see the nasty linear algebra that gets you
%to this result, you can check out the appendix. Or you can 
%you just take this result as given and save yourself an
%hour of painstaking derivation and eye-crossing complications,
%unlike myself.\footnote{But if you \emph{do} look at the appendix,
%you might just give my semi-wasted hour some meaning, in which 
%case---thank you.}
%\\
%\\
%Putting together the expressions for the mean and variance
%(see Equations \ref{mean} and \ref{covar}, respectively)
%of the posterior estimate for $x|y$ (i.e. the ``filtering
%distribution''), we get that
%\begin{align}
%    x | y &\sim \text{N} \left(\hat{x}^F, \; \Sigma^F\right) 
%    \label{filtered} \\
%    \notag\\
%    \text{where} \quad \hat{x}^F &= \hat{x} 
%	+ \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
%	\left( y - G \hat{x} \right)  \notag \\
%    \Sigma^F &= \Sigma - \Sigma G'(R 
%	    + G\Sigma G')^{-1}
%	    G\Sigma \notag
%\end{align}
%Notice that our new ``filtered'' mean is simply a 
%combination of our prior mean, $\hat{x}$, and 
%a transformation of the ``error'' between our
%observed value and the prior guess for that
%observable ($y - G\hat{x}$).
%
%\newpage
%\paragraph{Recap} Okay, so what did we just do? 
%\begin{enumerate} 
%    \item We took a Multivariate Normal (MVN) prior to 
%	summarize our beliefs about a latent, imperfectly 
%    observable state variable $x$. 
%    \item Knowing that we'll observe some data, $y$, which
%	provides a ``noisy'' measure of $x$, we postulated a 
%	likelihood $p(y|x)$ that is also MVN. 
%    \item Then, using Bayes' Rule, we combine the information
%	contained in our prior $p(x)$ and the data (via
%	the likelihhod $p(y|x)$) to get a ``filtered'' 
%	distribution of $x$, $p(x|y)$, given the data and our
%	prior.
%\end{enumerate} 
%Why might this long, tortuous, painful process help us in
%economics?  Well, imagine that in our model, there's some 
%state for the ``natural rate of unemployment,'' denoted by $x$.  
%Now of course, we can't observe that value.  But we'll have
%economic statistics, like measurements of unemployment itself along 
%with other informative statistics such as GDP
%and hours, which might provide information about the natural
%rate of unemployment.
%However, those statistics are imperfect and noisy.  The
%Kalman Filter gives us a way to combine those noisy estimates
%in with our beliefs in a principled, sensible manner.
%
%
%\section{Forecasting Step}
%
%Now let's make our model a little more dynamic and consider 
%forecasting ahead. To do so, we specify a model
%of how the state, $x$, evolves.  To make it easy on ourselves,
%let's assume everything's Gaussian (woohoo! that's easy):
%\begin{equation}
%    \label{lom}
%    x_{t+1} = Ax_t + w_{t+1} \qquad w_t \sim \text{N}(0, Q)
%\end{equation}
%Now, we want to come up with a \emph{predictive distribution}
%given our prior and the current information encapsulated in 
%our filtering distribution, $p(x|y)$. Since we're assuming
%everything is normal, we need only pin down the
%mean and variance of the forecast, since linear combinations
%of Gaussian variables are Gaussian.  
%
%Of course, these kinds of things are well known for  
%MVN random variables, which has the nice properties
%\begin{align*}
%    E[AX] &= A E[X] = A \mu \\
%    \text{Var}(AX) &= A \text{Var}(X) A' = A\Sigma A' \\
%    \text{where} \quad X &\sim \text{N}(\mu, \Sigma)
%\end{align*}
%Now let's use these facts, along with the assumption
%that we're predicting $x_{t+1}$ by starting with a 
%\emph{filtered} $x_t$ (denoted $x^F_t$ which has the 
%distribution in Equation \ref{filtered}), and 
%assuming $x^F_t$ is uncorrelated with $w_{t+1}$:
%\begin{align}
%    E[x_{t+1}] &= E\left[A{x}^F_{t} + w_{t+1} \right] 
%        = AE\left[{x}^F_{t}\right] + E\left[w_{t+1} 
%            \right] \notag\\
%        &= A \hat{x}^F_{t} + 0 = A \hat{x}^{F}_t
%            \label{intfore_mean}\\
%    \text{Var}({x}_{t+1}) &= 
%        \text{Var}(A{x}^F_{t} + w_{t+1}) 
%        = A\text{Var}\left({x}^F_{t}\right)A' 
%            +\text{Var}(w_{t+1})\notag \\
%        &= A\Sigma^F_t A' + Q \label{intfore_var}
%\end{align}
%where $\hat{x}^F_t$ and $\Sigma_t^F$ are as above in Equation 
%\ref{filtered}. This characterizes the distribution for the
%one step ahead forecasting distribution.
%
%\newpage
%Now, we can simplify what we have a bit more by defining 
%the \emph{Kalman Gain}:
%\begin{equation}
%    \label{kalgain}
%    K_\Sigma = A\Sigma G' (R + G\Sigma G')^{-1}
%\end{equation}
%We see the practical use of this by subbing in the full
%expressions for $\hat{x}_t^F$ and $\Sigma_t^F$ into Equations 
%\ref{intfore_mean} and \ref{intfore_var} above,
%and then simplifying our expressions for the
%mean and variance as a function of the Kalman Gain:
%\begin{align}
%    \hat{x}_{t+1} &= E[x_{t+1}] = 
%        A\left\{ \hat{x}_t
%	    + \left[\Sigma_t G' (R + G \Sigma_t G')^{-1}  \right]
%        \left( y - G \hat{x}_t \right) \right\} \notag \\
%        &= A\hat{x}_t + K_{\Sigma_t} (y - G \hat{x}_t) 
%        \label{foremean} \\
%    \Sigma_{t+1} &= \text{Var}(x_{t+1}) = 
%        A \left\{ \Sigma_t - \Sigma_t G'(R 
%	    + G\Sigma_t G')^{-1}
%        G\Sigma_t \right\} A' + Q \notag \\
%    &= A\Sigma_t A' - K_{\Sigma_t} G \Sigma_t A' + Q
%        \label{forevar}
%\end{align}
%
%
%\section{Full Recursive Procedure}
%
%Now we need to build up the recursive algorithm to forecast 
%further into the future. Let $t$ be the current time period, 
%and we proceed as follows:
%\begin{enumerate}
%    \item Start with a prior in the current period at 
%        time $t$, as given above
%        \[ p_{t}(x) \sim \text{N}\left(\hat{x}_t, \; 
%            \Sigma_t\right) \]
%    \item Observe the current measurement, $y_t$.
%    \item Update and filter out the noise as we did in
%        the Filtering Section to get, you guessed it,
%        the filtering distribution: $p_t(x | y) = 
%        \text{N}(\hat{x}^F_t, \Sigma^F_t)$.
%    \item Compute the predictive distribution 
%        $p_{t+1}(x) = \text{N}(\hat{x}_{t+1}, \Sigma_{t+1})$
%        from the filtering distribution, $p_t(x|y)$, and
%        the law of motion in Equation \ref{lom}.
%    \item Increment $t$. Return to step 1, taking
%        the predictive distribution, $p_{t+1}(x)$, as
%        the prior.
%\end{enumerate}
%
%
%\section{Convergence}
%
%Now since $x_{t}$ is random from the perspective
%of time $t-1$ (i.e. there is some irreducable uncertainty
%resulting from shocks at time $t$), we know that $\Sigma_t$
%will never be zero, unless $w_t$ in Equation \ref{lom} is 
%degenerate. However, we might ask whether $\Sigma_t$, our
%measure of uncertainty for our prediction $\hat{x}_t$
%of $x_t$, will ever converge to a \emph{constant} matrix over
%time. 
%
%Recall how $\Sigma_t$ evolves, as specified in Equation 
%\ref{forevar} (substituting in for the Kalman Gain, 
%$K_\Sigma$), which gives the non-linear difference equation:
%\begin{equation}
%    \label{sig_euler}
%    \Sigma_{t+1} = A\Sigma_t A' - K_{\Sigma_t} G \Sigma_t A' + Q
%\end{equation}
%If it were the case that $\Sigma_t$ converges to some fixed
%matrix, then there would be a fixed point, $\Sigma^*$, satisfying 
%Equation \ref{sig_euler} as follows:
%\begin{equation}
%    \label{riccati}
%    \Sigma^* =
%        A\Sigma^* A' - A\Sigma^* G' (R + G\Sigma^* G')^{-1} G \Sigma^* A' + Q
%\end{equation}
%This is known as the \emph{Discrete Time Algebraic Riccati Equation}.
%A sufficient condition for convergence is that all the eigenvalues of
%$A$, $\lambda_i$, satisfy $|\lambda_i|<1$. 
%
%
%
%
%
%
%
%
%%%%% APPPENDIX %%%%%%%%%%%
%
%\newpage
%\appendix
%
\section{Woodbury Matrix Identity}

For matrices $A$, $U$, $C$, and $V$:
\begin{equation}
    (A+UCV)^{-1} = A^{-1} - A^{-1} U(C^{-1} + VA^{-1}U)^{-1}
	VA^{-1}
\end{equation}
Now consider the special case we have above with the
Kalman filter:
\begin{equation}
    \label{special}
    (A + V'CV)^{-1} = A^{-1} - A^{-1} V'(C^{-1} + VA^{-1}V')^{-1}
	VA^{-1}
\end{equation}

\section{Derivation of the Mean}

Recall what we want to show.  For $\hat{x}_F \equiv Z$, we want
to show that
\begin{align*}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) 
	&= \left( \Sigma^{-1} + G'R^{-1}G\right) Z \\
    \Rightarrow 
    \hat{x}_F = Z 
        &= \hat{x} 
        + \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
        \left( y - G \hat{x} \right)  
\end{align*}
And so we solve this equation by using the Woodbury matrix
identity representation from above:
\begin{align*}
    (\Sigma^{-1} \hat{x} + G'R^{-1}y) &= 
	\left( \Sigma^{-1} + G'R^{-1}G\right) Z \\
    \Rightarrow \quad Z &=
	\left( \Sigma^{-1} + G'R^{-1}G\right)^{-1} 
	(\Sigma^{-1} \hat{x} + G'R^{-1}y) \\
    Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right)
\end{align*}
Now let's simplify $\hat{x}_F = Z$ a bit, 
expanding out the multiplication:
\begin{align*}
    \hat{x}_F = Z &= 
	\left( \Sigma - \Sigma G'(R 
	+ G\Sigma G')^{-1}
	G\Sigma\right) \left(\Sigma^{-1} \hat{x} 
	+ G'R^{-1}y\right) \\
    \text{FOIL} \quad &= \hat{x} + \Sigma G' R^{-1} y - 
	\left[\Sigma G' (R + G \Sigma G')^{-1} G \Sigma\right]
	\left[ \Sigma^{-1} \hat{x} \right]  \\
    &\qquad - \left[\Sigma G' (R+ G \Sigma G')^{-1}G\Sigma \right]
	\left[G' R^{-1} y \right] \\
    \text{Simpify} \quad &= \hat{x} + \Sigma G' R^{-1} y - 
	\left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad - \Sigma G' (R+ G \Sigma G')^{-1}G\Sigma 
	G' R^{-1} y  \\
    \text{Change Order} \quad &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad 
	+ \Sigma G'  R^{-1} y 
	- \Sigma G' (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} y \\
    \text{Regroup} \quad &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  \\
    &\qquad 
	+ \Sigma G'  \left\{ R^{-1}  
	-  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \right\} y 
\end{align*}
Okay, now let's take a breather.  We'll make this a bit
easier on ourselves, and just consider simplifying the guy
in the brackets, $\{\}$ by using 
$A^{-1} A = I$ with a very special choice of $A$:
\begin{align*}
    \left\{ R^{-1}  -  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \right\}   &=   
	(R+ G \Sigma G')^{-1} (R+ G \Sigma G') R^{-1}  \\
    &\qquad -  (R+ G \Sigma G')^{-1}G\Sigma  
	G' R^{-1} \\
    \text{Group} \quad &=   (R+ G \Sigma G')^{-1} \left[ (R+ G \Sigma G') R^{-1}  
	-  G\Sigma   G' R^{-1}\right] \\
    \text{Distribute} \quad &=   (R+ G \Sigma G')^{-1} \left[ RR^{-1} 
	+ G \Sigma G'R^{-1} -  G\Sigma   G' R^{-1}\right] \\
    \text{Simplify} \quad &=   (R+ G \Sigma G')^{-1} \left[ I 
	+ 0\right] \\
    &=   (R+ G \Sigma G')^{-1} 
\end{align*}
Substituting back in above for the term in braces, 
$\{\}$, we get the following expression
for $Z = \hat{x}_F$:
\begin{align}
    \hat{x}_F = Z &= \hat{x} 
	- \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( G \hat{x} \right)  
	+ \Sigma G'  (R+ G \Sigma G')^{-1} y \notag \\
    &= \hat{x} 
	+ \left[\Sigma G' (R + G \Sigma G')^{-1}  \right]
	\left( y - G \hat{x} \right)  
\end{align}

\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
