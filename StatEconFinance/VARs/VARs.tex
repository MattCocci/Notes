\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Vector Autoregressions \\ (VARs)}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Intuition}

Let's build up to the intuition of VARs by starting of its
relatives, the zero-mean AR(1):
    \[ y_t = \varphi y_{t-1} + \varepsilon_t \]
The AR(1) tries to predict \emph{this} period's value 
of some variable, $y$, given the \emph{last} value
of that variable. But we don't need to stop with just
last period's lags.  We can include as many as we want:
    \[ y_t = \varphi_1 y_{t-1} + \varphi_2 y_{t-2} + \cdots
	+ \varphi_p y_{t-p} + \varepsilon_t \]
This gives us the AR(p) model, with $p$ lags. But again,
we don't need to stop there. Suppose that we include
the lagged values of \emph{other} variables that might
help predict $y$, like say $x$:
\begin{equation}
    \label{ary}
     y_t = a_1 y_{t-1}  + \cdots +
	a_p y_{t-p} + b_1 x_{t-1} +  \cdots
	+ b_p x_{t-p} + \varepsilon_t 
\end{equation}
And presumably, if $x$ helps predict $y$, why not use 
$y$ to predict $x$? So let's write
\begin{equation}
    \label{arx}
    x_t = c_1 x_{t-1}  + \cdots +
	c_p x_{t-p} + d_1 y_{t-1} +  \cdots
	+ d_p y_{t-p} + \eta_t 
\end{equation}
As you can see, we can simplify this a lot. In fact,
we might toss Equations \ref{ary} and \ref{arx} into
matrices and vectors to simplify notation.  This
will be particularly useful when we have lots and
lots of variables, parameters, and lags. So we might
have
\begin{equation}
    \begin{pmatrix} x_t \\ y_t \end{pmatrix} = 
	\begin{pmatrix} c_1 & d_1 & \cdots & c_p & d_p \\
	    b_1 & a_1 & \cdots & b_p & a_p \\
	\end{pmatrix}
	\begin{pmatrix} x_{t-1} \\ y_{t-1} \\
	    \vdots \\ 
	    x_{t-p} \\ y_{t-p} \\
	\end{pmatrix}  
	+ \begin{pmatrix} \varepsilon_t \\ \eta_t
	 \end{pmatrix}
\end{equation}
And viola. We have a bona fide \emph{vector autoregression}, 
or VAR. 

We arrived at it through successive generalizations
of \emph{very} simple intuitive ideas. So if you know
time series, and if you know linear algebra, you're home.
You know this stuff already. And had you done this 40 years
ago and pushed the technique and consequences
to their natural implications, you would've gotten a Nobel 
Prize. 


\newpage
\section{Definition and Notation}

Now, we'll generalize to non-zero mean process, and 
we'll fix our notation, which was a bit
sloppy above.  So let $y_t$ denote
an $n\times 1$ vector of observables we
want to predict.  
\\
\\
Now define the VAR(p) model as follows:
\begin{align}
    \label{varp}
    y_t &= \phi_0 + \phi_1 y_{t-1} + \cdots + 
	\phi_p y_{t-p} + u_{t} 
	\qquad u_t \sim \text{N}_n(0, \Sigma)
\end{align}
where $\phi_0$ is $n\times 1$ and where $\Sigma$ and
$\phi_i$ ($i = 1, \ldots, p$) are $n\times n$.
So clearly, $y_t$ can be quite a complicated linear
function of it's previous lags and it's components.
\\
\\
Next, let's write Equation \ref{varp} in more
compact, matrix notation.  Specifically, concatenate
the $\phi_i$ column vectors horizontally, and stack the lags 
of $y$ into one big column vector:
\begin{align} 
    y_t &= \underbrace{\begin{pmatrix} \phi_0 & \phi_1 & \cdots 
	& \phi_p \end{pmatrix}}_{\Phi}
	\underbrace{\begin{pmatrix} 
	    1 \\ y_{t-1} \\ \vdots \\ y_{t-p}
	\end{pmatrix}}_{x_t} + u_t \qquad u_t \sim \text{N}_n(0, \Sigma) 
	 \notag \\
    \Leftrightarrow \qquad y_t &= \Phi x_t + u_t 
    \label{varvec}
\end{align} 
where $\Phi$ is $n\times (np +1)$ and $x_t$ is 
$(np+1) \times 1$.


\section{OLS Estimator}

Now that we have the model, let's find the OLS for 
the parameters in $\Phi$. To do so, we'll take one element (or
component) of the vector $y_t$ at a time, so one row of $y_t$
and $\Phi$ at a time. So let's minimize the sum of
squared errors for row/component $i$: 
\begin{equation}
    \min_{\Phi^{(i)}} \sum^T_{i=1} 
	\left(y_t^{(i)} - \Phi^{(i)} x_t\right)^2
\end{equation}
Now this estimator looks just like standard multivariate OLS 
regression, resulting in the ``the normal equation'':
\begin{align*}
    0 &= \frac{d}{d\Phi^{(i)}}\left\{ \sum^T_{i=1} 
	\left(y_t^{(i)} - \Phi^{(i)} x_t\right)^2\right\} 
	= -2 \sum^T_{i=1} x'_t
	\left(y_t^{(i)} - \Phi^{(i)} x_t\right) \\
    \Rightarrow \quad \hat{\Phi}^{(i)}  &= 
	\frac{\sum^T_{t=1} x'_t y_t^{(i)} }{\sum^T_{t=1} x'_t x_t}
	= \left[(X' X)^{-1} X' Y^{(i)}\right]'
\end{align*}
where $X$ is $T \times (np+1)$ and $Y^{(i)}$ is $T \times 1$
where
\begin{equation}
    \label{xy}
     X = \begin{pmatrix} x'_1 \\ \vdots \\ x'_T \end{pmatrix}
	\qquad Y^{(i)} = \begin{pmatrix} y_1^{(i)}  \\
	\vdots \\ y_T^{(i)} \end{pmatrix} 
\end{equation}
So now we have the estimator for each \emph{row}
of $\Phi$, which allows us to write
\begin{equation}
    \label{normaleqn}
    \hat{\Phi} = \begin{pmatrix} \hat{\Phi}^{(1)} \\ \vdots \\
	\hat{\Phi}^{(n)} \end{pmatrix}  
	= \left[(X' X)^{-1} X' Y\right]'
\end{equation}
where $X$ is as above and $Y$ is $T \times n$, which is 
a matix with $y'_t$ stacked on top of each other for
$t = 1, \ldots, T$.  So it's the $Y^{(i)}$ defined
above placed next to each other in a row.
\\
\\
This notation we've developed also gives us a convenient
way to define the \emph{sum of squared OLS residuals}
matrix:
\begin{equation}
    \hat{S} = (Y - X\hat{\Phi})'(Y - X\hat{\Phi})
\end{equation}
Eventually, when we start doing the Bayesian computations, 
we'll use this in the multivariate analog of a result
we recall from univariate mathematical statistics, 
both of which are written below:
\begin{align}
    \sum^m_{i=1} (y_i - \mu)^2 &= \sum^m_{i=1} (y_i - \hat{\mu})^2
	+ m(\hat{\mu}-\mu)^2 \notag \\
    (Y - X\Phi)'(Y - X\Phi) &= \hat{S} + (\Phi - \hat{\Phi})'
	X'X(\Phi - \hat{\Phi})' \label{sumsq}
\end{align}
where anything with ``$\;\hat{\;}\;$'' denotes the sample
estimate/analog.










\newpage
\section{Likelihood Function}

Now that we have an easy representation of the our
VAR(p) model, we move to the likelihood function.
By our definition,  $y_t$, conditional on $x_t$ 
(the lags $y_{t-1}, \ldots, y_{t-p}$), happens
to be normally distributed:
\begin{align}
    p(y_t \; | \; x_t, \Phi, \Sigma) &\sim
	\text{N}_{n}(\Phi x_t, \; \Sigma) \notag \\
    \Rightarrow \quad p(y_t \; | \; x_t, \Phi, \Sigma) 
	&\propto |\Sigma|^{-1/2} 
	\exp\left\{ -\frac{1}{2} \left(y_t - \Phi x_t
	\right)' \Sigma^{-1} (y_t - \Phi x_t)\right\}
\end{align}
Now we use the trick from the trace section in the 
appendix to rewrite the previous line
\begin{align}
    \Rightarrow \quad p(y_t \; | \; x_t, \Phi, \Sigma) 
	&\propto |\Sigma|^{-1/2} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(y_t - \Phi x_t)\left(y_t - \Phi x_t\right)'
	\right]\right\}
\end{align}





\newpage
\section{Joint Density Function}

Now, we construct the \emph{joint density function} for
the entire series, $Y_{1:T}$, where 
    \[ Y_{t_0:t_1} = (y_{t_0}, \ldots, y_{t_1}) \]
Therefore, conditional on a presample $y_{-p+1}, \ldots, y_0$, 
the jdf is written
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
	&= \prod_{t=1}^T p(y_t \; | \; Y_{-p+1:t-1}, \Phi, \Sigma) 
	\label{jump1} \\
    &= \prod_{t=1}^T p(y_t \; | \; Y_{t-p:t-1}, \Phi, \Sigma) 
	\label{jump2} \\
    &\propto \prod_{t=1}^T
	|\Sigma|^{-\frac{1}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(y_t - \Phi x_t)\left(y_t - \Phi x_t\right)'
	\right]\right\} \notag
\end{align}
where we could jump from Equation \ref{jump1} to \ref{jump2}
since the density of $y_t$ depends only on the previous
$p$ lags, not the entire history up until $t$. 
And given that the trace of a sum of two matrices is the
sum of the traces, we can simplify the jdf
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
	&\propto 
	|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	\sum^T_{t=1} (y_t - \Phi x_t)\left(y_t - \Phi x_t\right)'
	\right]\right\} \notag \\
    &\propto 
	|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(Y - X\Phi)'(Y-X\Phi)\right]\right\} \notag
\end{align}
where $X$ and $Y$ are defined as above in Equation \ref{xy}
and the subsequent conclusion to the section.





\newpage
\appendix
\section{Trace}

{\sl Definition}: If $A$ is an $n\times n$ matrix, then 
\begin{equation}
    \text{tr}[A] = \sum^n_{i=1} a_{ii} 
\end{equation}
which is the sum of diagnoal elements.
\\
\\
{\sl Trace Fact}: If $X$ is $m\times n$ and $Y$
is $n\times m$, then
\begin{equation}
    \label{tr}
    \text{tr}[XY] = \text{tr}[YX]
\end{equation}
This isn't dificult to prove, just tedious. 
\\
\\
{\sl Useful Trick}: Suppose that $a$ is an 
$n\times 1$ vector and $B$ is a symmetric
positive definite $n\times n$ matrix. Then 
    \[ a' B a \text{ is a scalar} \]
Then, since the trace of a scalar is just eqal
to that scalar, we can rewrite
\begin{align*}
    a' B a &= tr\left[a' B a \right] \\
    &= tr\left[a' (B a) \right] 
\end{align*}
Now if we use Equation \ref{tr}, taking $X = a'$ and
$Y = Ba$, we can carry on from the simplification
above to write
\begin{align*}
    a' B a &= tr\left[a' (B a) \right]  
	= tr\left[(B a) a' \right]  \\
	&= tr\left[B a a' \right]
\end{align*}


\newpage
\section{Kronecker Product and Vec Operator}

\subsection{Definitions}
Suppose we have two matrices, $A$ which is $m \times n$ and $B$
which is $p\times q$. Then the {\sl Kronecker Product} of $A$ and $B$ is 
    \[ A \otimes B = \begin{pmatrix} a_{11} B & \cdots & a_{1n} B \\
			    \vdots & \ddots & \vdots \\
			    a_{m1} B & \cdots & a_{mn}B \end{pmatrix}
    \]
which implies that the new matrix is $(mp) \times (nq)$. 
\\
\\
Next, the {\sl vec operator} takes any matrix $A$ that is $m \times n$
and stacks to columns on top of each other (left to right) to 
form a column vector of length $mn$.  Supposing that $a_i$ are column
vectors to simplify notation:
\begin{align*}
    \text{if } A &= \begin{pmatrix} a_1 & \cdots & a_n \end{pmatrix}
	\qquad a_i \in \mathbb{R}^{n\times 1} \\
    \text{then } \text{vec}\; A &= 
	\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
\end{align*}


\subsection{Properties}

\paragraph{Property 1} Let $A$ be $m\times n$, $B$ be $p \times q$,
$C$ be $n\times r$, and $D$ be $q \times s$. Then
\begin{equation}
    (A \otimes B)(C \otimes D) = AC \otimes BD
\end{equation}

%\begin{proof} We start by writing:
%
%\begin{align*}
%    (A \otimes B)(C \otimes D) = 
%    \begin{pmatrix} a_{11} B & \cdots & a_{1n} B \\
%	\vdots & \ddots & \vdots \\
%	a_{m1} B & \cdots & a_{mn}B \end{pmatrix}
%    \begin{pmatrix} c_{11} D & \cdots & c_{1r} D \\
%	\vdots & \ddots & \vdots \\
%	c_{n1} D & \cdots & c_{nr}D \end{pmatrix}
%\end{align*}
%Since the matrix $D$ has the same number of rows as $B$ has columns,
%we can carry out the multiplication to get
%
%\end{proof}


\paragraph{Property 2} $(A\otimes B)' = (A' \otimes B')$.

\paragraph{Property 3} $(A\otimes B)^{-1} = (A^{-1} \otimes B^{-1})$.

\paragraph{Property 4} 
tr$\left[A'BCD'\right] = \text{vec}(A)' (D \otimes B) \text{vec}(C)$.
\\
\\
How about some proofs?


\newpage
\subsection{Proofs of Property}

Of all the properties, this one strikes me the most of 
``Now what the hell?''  There is zero intuition for this that
I can see, so let's try to derive it.
\begin{equation}
    \label{vecder}
    \left[A'BCD'\right] = \text{vec}(A)' (D \otimes B)		 
	\text{vec}(C)
\end{equation}

\begin{proof} We know that the matrix that results from 
$A'BCD'$ must be square to
apply the trace operator to it.  So let's call that the beast 
inside the trace operator $E$ and suppose that it is
$q \times q$. Now what does that imply about the size of our
other matrices? 
\\
\\
Well to allow the matrix multiplication to 
be carried out (i.e. we can only multiply matrix $X$ and
matrix $Y$ if the columns of $X$ equal the rows of $Y$), 
and to have $E$ be $q\times q$, this forces
\begin{align*}
    A \text{ is } m \times q \qquad 
	 \qquad B \text{ is } m \times n \qquad
    C \text{ is } n \times p \qquad 
	& \qquad
    D \text{ is } q \times p
\end{align*}
where $m$, $n$, $p$, and $q$ are all left unspecified.
\\
\\
Okay then, let's get a move on
\begin{align*}
    \text{tr}[A'BCD'] = \text{tr}[E] &= 
	\sum^q_{i=1} e_{ii} \\
\end{align*}
Now suppose define $X = A'B$ and $Y = CD'$, implying
that $X$ is $q\times n$ and $Y$ is $n\times q$.
We can rewrite the diagonal components of $E$ in 
terms of the rows of $X$, the $x_{i\cdot}$, and 
the colums of $Y$, $y_{\cdot i}$.
\begin{align*}
    \text{tr}[E] &= 
	\sum^q_{i=1} e_{ii} 
	= \sum^q_{i=1} x_{i\cdot} y_{\cdot i}
\end{align*}
Now let's not forget where the rows and columns of 
$X$ and $Y$ come from. The $i$th row of $X$ is 
the product of the $i$th row of $A'$ (or the $i$th
column of $A$) with 
each subsequent column of $B$. Similarly, each
column of $Y$ is the dot product of each 
successive row of $C$ with the $i$th column of $D'$
(or the $i$th row of $D$).
Mathematically, 
\begin{align*}
    \text{tr}[E] &= 
	\sum^q_{i=1} e_{ii} 
	= \sum^q_{i=1} x_{i\cdot} y_{\cdot i} \\
    &= \sum^q_{i=1} a'_{\cdot i} 
	\begin{pmatrix} b_{\cdot 1} & \cdots & b_{\cdot n}
	\end{pmatrix}
	\begin{pmatrix} c_{1\cdot} \\ \vdots \\ c_{n \cdot}
	\end{pmatrix} 
	d'_{i \cdot}
\end{align*}
where $a'_{\cdot i}$ is the $i$th column of $A$,
transposed, and $d'{i \cdot}$ is the $i$th row of $D$,
transposed.

\newpage
Alright, where did that get us? Well, let's look at 
that last expression. If we group the last 
three components (with $b$'s and $c$'s and
$d$'s in them) into some abstract term $z_i$, 
we notice that we multiply some $z_i$ by $a'_{\cdot i}$,
add to the sum, increment $i$, multiply the next $z_i$ by 
the next $a'_{\cdot i}$, add to the sum, increment $i$, and so 
on. Mathematically:
\begin{align}
    \text{tr}[E] 
    &= \sum^q_{i=1} a'_{\cdot i} 
	\begin{pmatrix} b_{\cdot 1} & \cdots & b_{\cdot n}
	\end{pmatrix}
	\begin{pmatrix} c_{1\cdot} \\ \vdots \\ c_{n \cdot}
	\end{pmatrix} 
	d'_{i \cdot} \notag \\
    &= \sum^q_{i=1} a'_{\cdot i}  z_i \label{goback}
\end{align}
where $z_i$ is of size $m\times 1$.
\\
\\
But wouldn't it be nice if we could write that as 
a dot product---a product of scalars.  
We could use simple matrix notation to 
and just write
\begin{align}
    \text{tr}[E] 
	&= \sum^q_{i=1} a'_{\cdot i}  z_i \notag \\
	&= \begin{pmatrix} a'_{\cdot 1}  
	    & \cdots & a'_{\cdot q} \end{pmatrix}
	    \begin{pmatrix} z_{1}  
	    \\ \vdots \\ z_{q} \end{pmatrix}
	= \text{vec} (A)'
	    \begin{pmatrix} z_{1}  
	    \\ \vdots \\ z_{q} \end{pmatrix}
	    \label{zrep}
\end{align}
So clearly you noticed what that guy on the left is! It's 
the transpose of the columns of $A$ lined up
alongside to each other.  In other words, this is
vec$(A)'\;$!! 
Whew, so now we at least have \emph{something}
from Equation \ref{vecder}.
Now let's try to pin down that $z_i$ terms that
helped us get here.
\\
\\
Equation \ref{zrep} prompts us to think
a bit differently. Namely, imagine the
big column vector of $z$'s in Equation \ref{zrep}.
Since each $z_i$ term is, itself, a vector
of size $m\times 1$, let's go to a more
granular level.  Namely, if $z^*_{ij}$ represents
the $j$ component of $z_i$, then we can write
\begin{align*}
    z_i = \begin{pmatrix}
	z^*_{i1} \\ \vdots \\ z^*_{im} \end{pmatrix}
	&= 
	\begin{pmatrix} b_{\cdot 1} & \cdots & b_{\cdot n}
	\end{pmatrix}
	\begin{pmatrix} c_{1\cdot} \\ \vdots \\ c_{n \cdot}
	\end{pmatrix} 
	d'_{i \cdot} \notag \\
    &= 	\begin{pmatrix} c_{1\cdot} \\ \vdots \\ c_{n \cdot}
	\end{pmatrix} 
	d'_{i \cdot} \notag \\
\end{align*}
    





\end{proof}




%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font

\end{document}

