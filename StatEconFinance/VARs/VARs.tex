\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Vector Autoregressions \\ (VARs)}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Intuition}

Let's build up to the intuition of VARs by starting of its
relatives, the zero-mean AR(1):
    \[ y_t = \varphi y_{t-1} + \varepsilon_t \]
The AR(1) tries to predict \emph{this} period's value 
of some variable, $y$, given the \emph{last} value
of that variable. But we don't need to stop with just
last period's lags.  We can include as many as we want:
    \[ y_t = \varphi_1 y_{t-1} + \varphi_2 y_{t-2} + \cdots
	+ \varphi_p y_{t-p} + \varepsilon_t \]
This gives us the AR(p) model, with $p$ lags. But again,
we don't need to stop there. Suppose that we include
the lagged values of \emph{other} variables that might
help predict $y$, like say $x$:
\begin{equation}
    \label{ary}
     y_t = a_1 y_{t-1}  + \cdots +
	a_p y_{t-p} + b_1 x_{t-1} +  \cdots
	+ b_p x_{t-p} + \varepsilon_t 
\end{equation}
And presumably, if $x$ helps predict $y$, why not use 
$y$ to predict $x$? So let's write
\begin{equation}
    \label{arx}
    x_t = c_1 x_{t-1}  + \cdots +
	c_p x_{t-p} + d_1 y_{t-1} +  \cdots
	+ d_p y_{t-p} + \eta_t 
\end{equation}
As you can see, we can simplify this a lot. In fact,
we might toss Equations \ref{ary} and \ref{arx} into
matrices and vectors to simplify notation.  This
will be particularly useful when we have lots and
lots of variables, parameters, and lags. So we might
have
\begin{equation}
    \begin{pmatrix} x_t \\ y_t \end{pmatrix} = 
	\begin{pmatrix} c_1 & d_1 & \cdots & c_p & d_p \\
	    b_1 & a_1 & \cdots & b_p & a_p \\
	\end{pmatrix}
	\begin{pmatrix} x_{t-1} \\ y_{t-1} \\
	    \vdots \\ 
	    x_{t-p} \\ y_{t-p} \\
	\end{pmatrix}  
	+ \begin{pmatrix} \varepsilon_t \\ \eta_t
	 \end{pmatrix}
\end{equation}
And viola. We have a bona fide \emph{vector autoregression}, 
or VAR. 

We arrived at it through successive generalizations
of \emph{very} simple intuitive ideas. So if you know
time series, and if you know linear algebra, you're home.
You know this stuff already. And had you done this 40 years
ago and pushed the technique and consequences
to their natural implications, you would've gotten a Nobel 
Prize. 


\newpage
\section{Definition and Notation}

Now, we'll generalize to non-zero mean process, and 
we'll fix our notation, which was a bit
sloppy above.  So let $y_t$ denote
an $n\times 1$ vector of observables we
want to predict.  
\\
\\
Now define the VAR(p) model as follows:
\begin{align}
    \label{varp}
    y_t &= \phi_0 + \phi_1 y_{t-1} + \cdots + 
	\phi_p y_{t-p} + u_{t} 
	\qquad u_t \sim \text{N}_n(0, \Sigma)
\end{align}
where $\phi_0$ is $n\times 1$ and where $\Sigma$ and
$\phi_i$ ($i = 1, \ldots, p$) are $n\times n$.
So clearly, $y_t$ can be quite a complicated linear
function of it's previous lags and it's components.
\\
\\
Next, let's write Equation \ref{varp} in more
compact, matrix notation.  Specifically, concatenate
the $\phi_i$ column vectors horizontally, and stack the lags 
of $y$ into one big column vector:
\begin{align} 
    y_t &= \underbrace{\begin{pmatrix} \phi_0 & \phi_1 & \cdots 
	& \phi_p \end{pmatrix}}_{\Phi'}
	\underbrace{\begin{pmatrix} 
	    1 \\ y_{t-1} \\ \vdots \\ y_{t-p}
	\end{pmatrix}}_{x_t} + u_t \qquad u_t \sim \text{N}_n(0, \Sigma) 
	 \notag \\
    \Leftrightarrow \qquad y_t &= \Phi' x_t + u_t 
    \label{varvec}
\end{align} 
where $\Phi$ is $(np +1) \times n$ and $x_t$ is 
$(np+1) \times 1$.


\section{OLS Estimator}

Now that we have the model, let's find the OLS for 
the parameters in $\Phi$. To do so, we'll take one element (or
component) of the vector $y_t$ at a time, so one row of $y_t$
and $\Phi$ at a time. So let's minimize the sum of
squared errors for column/component $i$: 
\begin{equation}
    \min_{\Phi'_{\cdot i}} \sum^T_{i=1} 
	\left(y_t^{(i)} - \Phi'_{\cdot i} x_t \right)^2
\end{equation}
Now this estimator looks just like standard multivariate OLS 
regression, resulting in the ``the normal equation'':
\begin{align*}
    0 &= \frac{d}{d\Phi'_{\cdot i}}\left\{ \sum^T_{i=1} 
	\left(y_t^{(i)} - \Phi'_{\cdot i} x_t \right)^2\right\} 
	= -2 \sum^T_{i=1} x'_t
	\left(y_t^{(i)} - \Phi'_{\cdot i} x_t\right) \\
    \Rightarrow \quad \hat{\Phi}_{\cdot i}   &= 
	\frac{\sum^T_{t=1} x'_t y_t^{(i)} }{\sum^T_{t=1} x'_t x_t}
	= (X' X)^{-1} X' Y^{(i)}
\end{align*}
where $X$ is $T \times (np+1)$ and $Y^{(i)}$ is $T \times 1$
where
\begin{equation}
    \label{xy}
     X = \begin{pmatrix} x'_1 \\ \vdots \\ x'_T \end{pmatrix}
	\qquad Y^{(i)} = \begin{pmatrix} y_1^{(i)}  \\
	\vdots \\ y_T^{(i)} \end{pmatrix} 
\end{equation}
So now we have the estimator for each \emph{column}
of $\Phi$, which allows us to write
\begin{equation}
    \label{normaleqn}
    \hat{\Phi} = \begin{pmatrix} \hat{\Phi}_{\cdot 1} & \cdots &
	\hat{\Phi}_{\cdot n} \end{pmatrix}  
	= (X' X)^{-1} X' Y
\end{equation}
where $X$ is as above and $Y$ is $T \times n$, which is 
a matix with $y'_t$ stacked on top of each other for
$t = 1, \ldots, T$.  So it's the $Y^{(i)}$ defined
above placed next to each other in a row.
\\
\\
This notation we've developed also gives us a convenient
way to define the \emph{sum of squared OLS residuals}
matrix:
\begin{equation}
    \hat{S} = (Y - X\hat{\Phi})'(Y - X\hat{\Phi})
\end{equation}
Eventually, when we start doing the Bayesian computations, 
we'll use this in the multivariate analog of a result
we recall from univariate mathematical statistics, 
both of which are written below:
\begin{align}
    \sum^m_{i=1} (y_i - \mu)^2 &= \sum^m_{i=1} (y_i - \hat{\mu})^2
	+ m(\hat{\mu}-\mu)^2 \notag \\
    (Y - X\Phi)'(Y - X\Phi) &= \hat{S} + (\Phi - \hat{\Phi})'
	X'X(\Phi - \hat{\Phi}) \label{sumsq}
\end{align}
where anything with ``$\;\hat{\;}\;$'' denotes the sample
estimate/analog.


\section{Likelihood Function}

Now that we have an easy representation of the our
VAR(p) model, we move to the likelihood function.
By our definition,  $y_t$, conditional on $x_t$ 
(the lags $y_{t-1}, \ldots, y_{t-p}$), happens
to be normally distributed:
\begin{align}
    p(y_t \; | \; x_t, \Phi, \Sigma) &\sim
	\text{N}_{n}(\Phi x_t, \; \Sigma) \notag \\
    \Rightarrow \quad p(y_t \; | \; x_t, \Phi, \Sigma) 
	&\propto |\Sigma|^{-1/2} 
	\exp\left\{ -\frac{1}{2} \left(y_t - \Phi' x_t
	\right)' \Sigma^{-1} (y_t - \Phi' x_t)\right\}
\end{align}
Now we use the trick from the trace section in the 
appendix to rewrite the previous line
\begin{align}
    \Rightarrow \quad p(y_t \; | \; x_t, \Phi, \Sigma) 
	&\propto |\Sigma|^{-1/2} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(y_t - \Phi' x_t)\left(y_t - \Phi' x_t\right)'
	\right]\right\}
\end{align}



\newpage
\section{Joint Density Function}

Now, we construct the \emph{joint density function} for
the entire series, $Y_{1:T}$, where 
    \[ Y_{t_0:t_1} = (y_{t_0}, \ldots, y_{t_1}) \]
Therefore, conditional on a presample $y_{-p+1}, \ldots, y_0$, 
the jdf is written
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
	&= \prod_{t=1}^T p(y_t \; | \; Y_{-p+1:t-1}, \Phi, 
	\Sigma) 
	= \prod_{t=1}^T p(y_t \; | \; Y_{t-p:t-1}, \Phi, \Sigma) 
	\label{jump1} \\
    &\propto \prod_{t=1}^T
	|\Sigma|^{-\frac{1}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(y_t - \Phi' x_t)\left(y_t - \Phi' x_t\right)'
	\right]\right\} \notag
\end{align}
where we could make the jump in Equation \ref{jump1}  
because the density of $y_t$ depends only on the previous
$p$ lags, not the entire history up until $t$. 
And given that the trace of a sum of two matrices is the
sum of the traces, we can simplify the jdf
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
	&\propto 
	|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	\sum^T_{t=1} (y_t - \Phi' x_t)\left(y_t -\Phi' x_t\right)'
	\right]\right\} \notag \\
    &\propto 
	|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(Y - X\Phi)'(Y-X\Phi)\right]\right\} \label{jdf}
\end{align}
where $X$ and $Y$ are defined as above in Equation \ref{xy}
and the subsequent section discussion.
\\
\\
Now, we use a few tricks to rewrite the jdf representation
in Equation \ref{jdf}. Namely, we'll want to take care
of the nastiness in the trace operator, so let's simplify 
that straight away (letting $\hat{\beta} = 
\text{vec}(\hat{\Phi})$, a vector of coefficients):
\begin{align*}
    \text{By Eqn \ref{sumsq}:} \qquad 
	\text{tr} & \left[\Sigma^{-1} 
	(Y - X\Phi)'(Y-X\Phi)\right] = 
	\text{tr} \left[\Sigma^{-1} \left\{
	\hat{S} + (\Phi - \hat{\Phi})'
	X'X(\Phi - \hat{\Phi}) \right\}\right]  \\
    &\qquad\qquad= \text{tr} \left[\Sigma^{-1} 
	\hat{S} \right] + \text{tr} \left[ \Sigma^{-1}
	(\Phi - \hat{\Phi})'X'X(\Phi - \hat{\Phi}) \right] \\
    \text{By Eqn \ref{tr}} \qquad \quad
	&\qquad\qquad= \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] + \text{tr} \left[ 
	(\Phi - \hat{\Phi})'X'X(\Phi - \hat{\Phi})
	\Sigma^{-1} \right] \\
    \text{By Eqn \ref{vecder}} \qquad \quad
	&\qquad\qquad= \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] + 
	(\beta - \hat{\beta})'
	\left[\Sigma^{-1} \otimes (X'X)\right] 
	(\beta - \hat{\beta}) \\
    \text{By Kronecker Prop. 3}  
	&\qquad\qquad= \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] + 
	(\beta - \hat{\beta})'
	\left[\Sigma \otimes (X'X)^{-1}\right]^{-1} 
	(\beta - \hat{\beta}) 
\end{align*}
Now substitute this back into the jdf to get
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
    \propto 
	&\;|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] \right\} \notag \\
    &\times
	\exp\left\{ -\frac{1}{2} 
	(\beta - \hat{\beta})'
	\left[\Sigma \otimes (X'X)^{-1}\right]^{-1} 
	(\beta - \hat{\beta}) 
	\right\}  \label{bjdf}
\end{align}


\newpage
\section{Bayesian Analysis}

So we saw above how to arrive at a new representation
for the jdf in Equation \ref{bjdf}, and in this section 
we'll see why that was useful.

\subsection{Flat Prior, $\Sigma$ Given}

This is perhaps the easiest case, where we want to find
the distributions for $\Phi$ assuming a flat prior for
$\Phi$ and that $\Sigma$ is given. This gives us
a posterior of
\begin{align*}
    p(\Phi \; | \; Y, \Sigma) &\propto p( Y \; | \; \Phi, \Sigma)
	p(\Phi \; | \; \Sigma) \propto p(Y \; |\; \Phi, \Sigma)\\
    &\propto 	
\end{align*}



\newpage
\appendix
\section{Trace}

{\sl Definition}: If $A$ is an $n\times n$ matrix, then 
\begin{equation}
    \text{tr}[A] = \sum^n_{i=1} a_{ii} 
\end{equation}
which is the sum of diagonal elements.
\\
\\
{\sl Trace Fact}: If $X$ is $m\times n$ and $Y$
is $n\times m$, then
\begin{equation}
    \label{tr}
    \text{tr}[XY] = \text{tr}[YX]
\end{equation}
This isn't dificult to prove, just tedious. 
\\
\\
{\sl Useful Trick}: Suppose that $a$ is an 
$n\times 1$ vector and $B$ is a symmetric
positive definite $n\times n$ matrix. Then 
    \[ a' B a \text{ is a scalar} \]
Then, since the trace of a scalar is just eqal
to that scalar, we can rewrite
\begin{align*}
    a' B a &= tr\left[a' B a \right] \\
    &= tr\left[a' (B a) \right] 
\end{align*}
Now if we use Equation \ref{tr}, taking $X = a'$ and
$Y = Ba$, we can carry on from the simplification
above to write
\begin{align*}
    a' B a &= tr\left[a' (B a) \right]  
	= tr\left[(B a) a' \right]  \\
	&= tr\left[B a a' \right]
\end{align*}


\newpage
\section{Kronecker Product and Vec Operator}

\subsection{Definitions}
Suppose we have two matrices, $A$ which is $m \times n$ and $B$
which is $p\times q$. Then the {\sl Kronecker Product} of $A$ and $B$ is 
    \[ A \otimes B = \begin{pmatrix} a_{11} B & \cdots & a_{1n} B \\
			    \vdots & \ddots & \vdots \\
			    a_{m1} B & \cdots & a_{mn}B \end{pmatrix}
    \]
which implies that the new matrix is $(mp) \times (nq)$. 
\\
\\
Next, the {\sl vec operator} takes any matrix $A$ that is $m \times n$
and stacks to columns on top of each other (left to right) to 
form a column vector of length $mn$.  Supposing that $a_i$ are column
vectors to simplify notation:
\begin{align*}
    \text{if } A &= \begin{pmatrix} a_1 & \cdots & a_n \end{pmatrix}
	\qquad a_i \in \mathbb{R}^{n\times 1} \\
    \text{then } \text{vec}\; A &= 
	\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
\end{align*}


\subsection{Properties}

\paragraph{Property 1} Let $A$ be $m\times n$, $B$ be $p \times q$,
$C$ be $n\times r$, and $D$ be $q \times s$. Then
\begin{equation}
    (A \otimes B)(C \otimes D) = AC \otimes BD
\end{equation}

%\begin{proof} We start by writing:
%
%\begin{align*}
%    (A \otimes B)(C \otimes D) = 
%    \begin{pmatrix} a_{11} B & \cdots & a_{1n} B \\
%	\vdots & \ddots & \vdots \\
%	a_{m1} B & \cdots & a_{mn}B \end{pmatrix}
%    \begin{pmatrix} c_{11} D & \cdots & c_{1r} D \\
%	\vdots & \ddots & \vdots \\
%	c_{n1} D & \cdots & c_{nr}D \end{pmatrix}
%\end{align*}
%Since the matrix $D$ has the same number of rows as $B$ has columns,
%we can carry out the multiplication to get
%
%\end{proof}


\paragraph{Property 2} $(A\otimes B)' = (A' \otimes B')$.

\paragraph{Property 3} $(A\otimes B)^{-1} = (A^{-1} \otimes B^{-1})$.

\paragraph{Property 4} 
tr$\left[A'BCD'\right] = \text{vec}(A)' (D \otimes B) \text{vec}(C)$.
\\
\\
How about some proofs?


\newpage
\subsection{Proof of Property 4}

Of all the properties, this one strikes me the most of 
``Now what the hell?''  There is zero intuition for this that
I can see, so let's try to derive it.
\begin{equation}
    \label{vecder}
    \text{tr}\left[A'BCD'\right] 
	= \text{vec}(A)' (D \otimes B) \text{vec}(C)
\end{equation}

\begin{proof} We know that the matrix that results from 
$A'BCD'$ must be square to
apply the trace operator to it.  So let's call that the beast 
inside the trace operator $E$ and suppose that it is
$q \times q$. Now what does that imply about the size of our
other matrices? 
\\
\\
Well to allow the matrix multiplication to 
be carried out (i.e. we can only multiply matrix $X$ and
matrix $Y$ if the columns of $X$ equal the rows of $Y$), 
and to have $E$ be $q\times q$, this forces
\begin{align*}
    A \text{ is } m \times q \qquad 
	 \qquad B \text{ is } m \times n \qquad
    C \text{ is } n \times p \qquad 
	& \qquad
    D \text{ is } q \times p
\end{align*}
where $m$, $n$, $p$, and $q$ are all left unspecified.
\\
\\
Okay then, let's get a move on
\begin{align*}
    \text{tr}[A'BCD'] = \text{tr}[E] &= 
	\sum^q_{i=1} e_{ii} \\
\end{align*}
Now suppose define $X = A'B$ and $Y = CD'$, implying
that $X$ is $q\times n$ and $Y$ is $n\times q$.
We can rewrite the diagonal components of $E$ in 
terms of the rows of $X$, the $x_{i\cdot}$, and 
the colums of $Y$, $y_{\cdot i}$.
\begin{align*}
    \text{tr}[E] &= 
	\sum^q_{i=1} e_{ii} 
	= \sum^q_{i=1} x_{i\cdot} y_{\cdot i}
\end{align*}
Now let's not forget where the rows and columns of 
$X$ and $Y$ come from. The $i$th row of $X$ is 
the product of the $i$th row of $A'$ (or the $i$th
column of $A$) with 
each subsequent column of $B$. Similarly, each
column of $Y$ is the dot product of each 
successive row of $C$ with the $i$th column of $D'$
(or the $i$th row of $D$).
Mathematically, 
\begin{align}
    \text{tr}[E] &= 
	\sum^q_{i=1} e_{ii} 
	= \sum^q_{i=1} x_{i\cdot} y_{\cdot i} \notag \\
    &= \sum^q_{i=1} a'_{\cdot i} 
	\begin{pmatrix} b_{\cdot 1} & \cdots & b_{\cdot n}
	\end{pmatrix}
	\begin{pmatrix} c_{1\cdot} \\ \vdots \\ c_{n \cdot}
	\end{pmatrix} 
	d'_{i \cdot} \label{fullrep} \\
    &= \sum^q_{i=1} a'_{\cdot i} 
	BC
	d'_{i \cdot} \label{simprep} 
\end{align}
where $a'_{\cdot i}$ is the $i$th column of $A$,
transposed, and $d'_{i \cdot}$ is the $i$th row of $D$,
transposed.

\newpage
Alright, where did that get us? Well, let's look at 
that last expression. Let's suppress the three
components to the write of $a'_{\cdot i}$---those nasty
vectors and matrices with $b$'s, $c$'s, and $d$'s in
them. We'll do so by just grouping those three
terms into a ``back-box'' term called ``$z_i$,'' which
we'll postpone computing for now.  This let's us
write
\begin{align}
    \text{tr}[E] 
    &= \sum^q_{i=1} a'_{\cdot i}  z_i \label{goback}
\end{align}
where $z_i$ is of size $m\times 1$.
\\
\\
But hey, that's a regular old dot product, so let's
rewrite it as such with simple matrix notation:
\begin{align}
    \text{tr}[E] 
	&= \sum^q_{i=1} a'_{\cdot i}  z_i \notag \\
	&= \begin{pmatrix} a'_{\cdot 1}  
	    & \cdots & a'_{\cdot q} \end{pmatrix}
	    \begin{pmatrix} z_{1}  
	    \\ \vdots \\ z_{q} \end{pmatrix}
	= \text{vec}(A)' \; Z
    \label{zrep}
\end{align}
Now hopefully you were able to make the jump to 
Equation \ref{zrep} yourself once you saw what
we were doing.
And now we at least have \emph{something}
from Equation \ref{vecder}.
Now let's try to pin down those $z_i$ terms that
helped us get here.
\\
\\
Now, let's pin down that $Z$ matrix and show
that, in fact, 
\begin{equation}
    Z = (D\otimes B) \text{vec}(C)
\end{equation}
We do so by first using Equations 
\ref{simprep}, \ref{goback}, and \ref{zrep} to infer the
following: 
\begin{align}
    Z &= \begin{pmatrix} z_1 \\ \vdots \\ z_q 
	\end{pmatrix} = 
	\begin{pmatrix} BCd'_{1\cdot} \\ \vdots \\ BCd'_{q\cdot} 
	\end{pmatrix} \label{bigz} 
\end{align}
Next, let's consider the $j$th row of the last vector, then
rewrite $C$, expanding it into a different but equivalent
representation---a row of column vectors all lined up:
\begin{align*}
    BCd'_{j\cdot} &= B 
	\begin{pmatrix} c_{\cdot 1} & \cdots & c_{\cdot p}
	\end{pmatrix} d'_{j \cdot} \notag \\
    \text{expanding $d'_{j \cdot}$} \quad 
	&= B 
	\begin{pmatrix} c_{\cdot 1} & \cdots & c_{\cdot p}
	\end{pmatrix}  
	\begin{pmatrix} d_{j 1} \\ \vdots \\ d_{j p}
	\end{pmatrix}  
\end{align*}
Now distribute $B$ and dot product with the $d'_{j\cdot}$:
\begin{align}
    BCd'_{j\cdot} &= 
	\begin{pmatrix} Bc_{\cdot 1} d_{j 1} + \cdots 
	    + Bc_{\cdot p} d_{j p}
	\end{pmatrix}   \notag \\
    \text{Since the $d_{jm}$ are scalars} \quad &= 
	\begin{pmatrix}  d_{j 1} Bc_{\cdot 1} + \cdots 
	    +  d_{j p}Bc_{\cdot p}
	\end{pmatrix}   \notag 
\end{align}
Amost there, I promise.

\newpage
Now stack these $BCd'_{j\cdot}$ terms
to get $Z$ as in Equation \ref{bigz}:
\begin{align}
    Z &= \begin{pmatrix} z_1 \\ \vdots \\ z_q 
	\end{pmatrix} = 
	\begin{pmatrix} BCd'_{1\cdot} \\ \vdots \\ BCd'_{q\cdot} 
	\end{pmatrix} 
    = \begin{pmatrix} 
	    d_{1 1} Bc_{\cdot 1} + \cdots 
	    +  d_{1 p}Bc_{\cdot p} \\ 
	    \vdots \\
	    d_{q 1} Bc_{\cdot 1} + \cdots 
	    +  d_{q p}Bc_{\cdot p}  
	\end{pmatrix} 
\end{align}
Finally, take out the expanded $C$ term from
each row and rewrite the sums in each row
as a result of matrix multiplication
\begin{align}
    Z &=\begin{pmatrix} 
	    d_{1 1} Bc_{\cdot 1} + \cdots 
	    +  d_{1 p}Bc_{\cdot p} \\ 
	    \vdots \\
	    d_{q 1} Bc_{\cdot 1} + \cdots 
	    +  d_{q p}Bc_{\cdot p} 
	\end{pmatrix} 
    = \begin{pmatrix} 
	    d_{1 1} B & \cdots 
	    &  d_{1 p}B \\ 
	    \vdots & \ddots & \vdots \\
	    d_{q 1} B & \cdots 
	    &  d_{q p}B 
	\end{pmatrix} 
    \begin{pmatrix}
	c_{\cdot 1} \\ \vdots \\ c_{\cdot p}
    \end{pmatrix}
\end{align}
Or in other words, if you look at the
last line closely
\begin{align*}
    Z = (D\otimes B) \text{vec}(C)
\end{align*}
QED (drops mic).


    





\end{proof}




%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font

\end{document}

