\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Vector Autoregressions \\ (VARs)}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim} % for including verbatim code from a file
\usepackage{natbib} % for bibliographies

\begin{document}
\maketitle

% \tableofcontents %adds it here

\section{Intuition}

Let's build up to the intuition of VARs by starting of its
relatives, the zero-mean AR(1):
    \[ x_t = \varphi x_{t-1} + \varepsilon_t \]
The AR(1) forecasts \emph{this} period's value 
of some variable, $x$, given the \emph{last} period. 
But we don't need to stop with just
last period's lags.  We can include arbitrarily many:
    \[ x_t = a_1 x_{t-1} + a_2 x_{t-2} + \cdots
	+ a_p x_{t-p} + \varepsilon_t \]
This gives us the AR(p) model. But again,
we don't need to stop there. Suppose that we include
the lagged values of \emph{other} variables that might
help predict $x$, like say $y$:
\begin{equation}
    \label{ary}
     x_t = a_1 x_{t-1}  + \cdots +
	a_p x_{t-p} + b_1 y_{t-1} +  \cdots
	+ b_p y_{t-p} + \varepsilon_t 
\end{equation}
And presumably, if $x$ helps predict $y$, why not use 
$y$ to forecast $x$? Perhaps they're jointly determined, 
as we might expect in economic equilibrium or any situation
with endogeneity. So let's write
\begin{equation}
    \label{arx}
    y_t = c_1 x_{t-1} +  \cdots + c_p x_{t-p} 
	+ d_1 y_{t-1}  + \cdots +
	d_p y_{t-p} + \eta_t 
\end{equation}
As you can guess, we can simplify this a lot. In fact,
we might toss Equations \ref{ary} and \ref{arx} into
matrices and vectors to simplify notation.  This
will be particularly useful when we have lots and
lots of variables, parameters, and lags. So we might
write
\begin{equation}
    \begin{pmatrix} x_t \\ y_t \end{pmatrix} = 
	\begin{pmatrix} a_1 & b_1 & \cdots & a_p & b_p \\
	    c_1 & d_1 & \cdots & c_p & d_p \\
	\end{pmatrix}
	\begin{pmatrix} x_{t-1} \\ y_{t-1} \\
	    \vdots \\ 
	    x_{t-p} \\ y_{t-p} \\
	\end{pmatrix}  
	+ \begin{pmatrix} \varepsilon_t \\ \eta_t
	 \end{pmatrix}
\end{equation}
And viola. We have a bona fide \emph{vector autoregression}, 
or VAR. 

We arrived at it through successive generalizations
of {very} natural intuitive ideas. So if you know
time series, and if you know linear algebra, you're home.
You know this stuff already. And had you done this 40 years
ago and pushed the technique and consequences
to their natural implications, you might have even won
a Nobel Prize. 


\newpage
\section{Definition and Notation}

Now, we'll generalize to non-zero mean process, and 
we'll fix our notation, which was a bit
sloppy above.  So let $y_t$ denote
an $n\times 1$ vector of observables we
want to predict.  
\\
\\
Now define the VAR(p) model as follows:
\begin{align}
    \label{varp}
    y_t &= \phi_0 + \phi_1 y_{t-1} + \cdots + 
	\phi_p y_{t-p} + u_{t} 
	\qquad u_t \sim \text{N}_n(0, \Sigma)
\end{align}
where $\phi_0$ is $n\times 1$ and where $\Sigma$ and
$\phi_i$ ($i = 1, \ldots, p$) are $n\times n$.
So clearly, $y_t$ can be quite a complicated linear
function of its previous lags and its components.
\\
\\
Next, let's write Equation \ref{varp} in more
compact, matrix notation.  Specifically, concatenate
the $\phi_i$ column vectors horizontally, and stack the lags 
of $y$ into one big column vector:
\begin{align} 
    y_t &= \underbrace{\begin{pmatrix} \phi_1 & \cdots 
	& \phi_p & \phi_0 \end{pmatrix}}_{\Phi'}
	\underbrace{\begin{pmatrix} 
	     y_{t-1} \\ \vdots \\ y_{t-p} \\ 1
	\end{pmatrix}}_{x_t} + u_t \qquad u_t \sim \text{N}_n(0, \Sigma) 
	 \notag \\
    \Leftrightarrow \qquad y_t &= \Phi' x_t + u_t 
    \label{varvec}
\end{align} 
where $\Phi$ is $(np +1) \times n$ and $x_t$ is 
$(np+1) \times 1$.


\section{OLS Estimator}

Now that we have the model, let's find the OLS for 
the parameters in $\Phi$. To do so, we'll take one element (or
component) of the vector $y_t$ at a time, so one row of $y_t$
and $\Phi$ at a time. So let's minimize the sum of
squared errors for column/component $i$: 
\begin{equation}
    \min_{\Phi'_{\cdot i}} \sum^T_{i=1} 
	\left(y_t^{(i)} - \Phi'_{\cdot i} x_t \right)^2
\end{equation}
Now this estimator looks just like standard multivariate OLS 
regression, resulting in the ``the normal equation'':
\begin{align*}
    0 &= \frac{d}{d\Phi'_{\cdot i}}\left\{ \sum^T_{i=1} 
	\left(y_t^{(i)} - \Phi'_{\cdot i} x_t \right)^2\right\} 
	= -2 \sum^T_{i=1} x'_t
	\left(y_t^{(i)} - \Phi'_{\cdot i} x_t\right) \\
    \Rightarrow \quad \hat{\Phi}_{\cdot i}   &= 
	\frac{\sum^T_{t=1} x'_t y_t^{(i)} }{\sum^T_{t=1} x'_t x_t}
	= (X' X)^{-1} X' Y^{(i)}
\end{align*}
where $X$ is $T \times (np+1)$ and $Y^{(i)}$ is $T \times 1$
where
\begin{equation}
    \label{xy}
     X = \begin{pmatrix} x'_1 \\ \vdots \\ x'_T \end{pmatrix}
	\qquad Y^{(i)} = \begin{pmatrix} y_1^{(i)}  \\
	\vdots \\ y_T^{(i)} \end{pmatrix} 
\end{equation}
So now we have the estimator for each \emph{column}
of $\Phi$, which allows us to write
\begin{equation}
    \label{normaleqn}
    \hat{\Phi} = \begin{pmatrix} \hat{\Phi}_{\cdot 1} & \cdots &
	\hat{\Phi}_{\cdot n} \end{pmatrix}  
	= (X' X)^{-1} X' Y
\end{equation}
where $X$ is as above and $Y$ is $T \times n$, which is 
a matix with $y'_t$ stacked on top of each other for
$t = 1, \ldots, T$.  So it's the $Y^{(i)}$ defined
above placed next to each other in a row.
\\
\\
This notation we've developed also gives us a convenient
way to define the \emph{sum of squared OLS residuals}
matrix:
\begin{equation}
    \hat{S} = (Y - X\hat{\Phi})'(Y - X\hat{\Phi})
\end{equation}
Eventually, when we start doing the Bayesian computations, 
we'll use this in the multivariate analog of a result
we recall from univariate mathematical statistics, 
both of which are written below:
\begin{align}
    \sum^m_{i=1} (y_i - \mu)^2 &= \sum^m_{i=1} (y_i - \hat{\mu})^2
	+ m(\hat{\mu}-\mu)^2 \notag \\
    (Y - X\Phi)'(Y - X\Phi) &= \hat{S} + (\Phi - \hat{\Phi})'
	X'X(\Phi - \hat{\Phi}) \label{sumsq}
\end{align}
where anything with ``$\;\hat{\;}\;$'' denotes the sample
estimate/analog.


\section{Likelihood Function}

Now that we have an easy representation of the our
VAR(p) model, we move to the likelihood function.
By our definition,  $y_t$, conditional on $x_t$ 
(the lags $y_{t-1}, \ldots, y_{t-p}$), happens
to be normally distributed:
\begin{align}
    p(y_t \; | \; x_t, \Phi, \Sigma) &\sim
	\text{N}_{n}(\Phi x_t, \; \Sigma) \notag \\
    \Rightarrow \quad p(y_t \; | \; x_t, \Phi, \Sigma) 
	&\propto |\Sigma|^{-1/2} 
	\exp\left\{ -\frac{1}{2} \left(y_t - \Phi' x_t
	\right)' \Sigma^{-1} (y_t - \Phi' x_t)\right\}
\end{align}
Now we use the trick from the trace section in the 
appendix to rewrite the previous line
\begin{align}
    \Rightarrow \quad p(y_t \; | \; x_t, \Phi, \Sigma) 
	&\propto |\Sigma|^{-1/2} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(y_t - \Phi' x_t)\left(y_t - \Phi' x_t\right)'
	\right]\right\}
\end{align}



\newpage
\section{Joint Density Function}

Now, we construct the \emph{joint density function} for
the entire series, $Y_{1:T}$, where 
    \[ Y_{t_0:t_1} = (y_{t_0}, \ldots, y_{t_1}) \]
Therefore, conditional on a presample $y_{-p+1}, \ldots, y_0$, 
the jdf is written
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
	&= \prod_{t=1}^T p(y_t \; | \; Y_{-p+1:t-1}, \Phi, 
	\Sigma) 
	= \prod_{t=1}^T p(y_t \; | \; Y_{t-p:t-1}, \Phi, \Sigma) 
	\label{jump1} \\
    &\propto \prod_{t=1}^T
	|\Sigma|^{-\frac{1}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(y_t - \Phi' x_t)\left(y_t - \Phi' x_t\right)'
	\right]\right\} \notag
\end{align}
where we could make the jump in Equation \ref{jump1}  
because the density of $y_t$ depends only on the previous
$p$ lags, not the entire history up until $t$. 
And given that the trace of a sum of two matrices is the
sum of the traces, we can simplify the jdf
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
	&\propto 
	|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	\sum^T_{t=1} (y_t - \Phi' x_t)\left(y_t -\Phi' x_t\right)'
	\right]\right\} \notag \\
    &\propto 
	|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[ \Sigma^{-1}  
	(Y - X\Phi)'(Y-X\Phi)\right]\right\} \label{jdf}
\end{align}
where $X$ and $Y$ are defined as above in Equation \ref{xy}
and the subsequent section discussion.

\paragraph{Alternative Representation}
Now, we use a few tricks to rewrite the jdf representation
in Equation \ref{jdf}. (This will aid in posterior inference.)
Namely, we'll want to take care
of the nastiness in the trace operator, so let's simplify 
that straight away (letting $\hat{\beta} = 
\text{vec}(\hat{\Phi})$, a vector of coefficients):
\begin{align*}
    \text{By Eqn \ref{sumsq}:} \qquad 
	\text{tr} & \left[\Sigma^{-1} 
	(Y - X\Phi)'(Y-X\Phi)\right] = 
	\text{tr} \left[\Sigma^{-1} \left\{
	\hat{S} + (\Phi - \hat{\Phi})'
	X'X(\Phi - \hat{\Phi}) \right\}\right]  \\
    &\qquad\qquad= \text{tr} \left[\Sigma^{-1} 
	\hat{S} \right] + \text{tr} \left[ \Sigma^{-1}
	(\Phi - \hat{\Phi})'X'X(\Phi - \hat{\Phi}) \right] \\
    \text{By trace trick} \qquad \quad
	&\qquad\qquad= \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] + \text{tr} \left[ 
	(\Phi - \hat{\Phi})'X'X(\Phi - \hat{\Phi})
	\Sigma^{-1} \right] \\
    \text{By vec trick} \qquad \quad
	&\qquad\qquad= \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] + 
	(\beta - \hat{\beta})'
	\left[\Sigma^{-1} \otimes (X'X)\right] 
	(\beta - \hat{\beta}) \\
    \text{By Kronecker Prop. 3}  
	&\qquad\qquad= \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] + 
	(\beta - \hat{\beta})'
	\left[\Sigma \otimes (X'X)^{-1}\right]^{-1} 
	(\beta - \hat{\beta}) 
\end{align*}
Now substitute this back into the jdf to get
\begin{align}
    p(Y_{1:T} \; | \; Y_{-p+1:0}, \Phi, \Sigma)
    \propto 
	&\;|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] \right\} \notag \\
    &\times
	\exp\left\{ -\frac{1}{2} 
	(\beta - \hat{\beta})'
	\left[\Sigma \otimes (X'X)^{-1}\right]^{-1} 
	(\beta - \hat{\beta}) 
	\right\}  \label{bjdf}
\end{align}


\newpage
\section{Bayesian Analysis}

So we saw above how to arrive at a new representation
for the jdf in Equation \ref{bjdf}, and in this section 
we'll see why that was useful.

\subsection{Flat Prior, Known Variance}

This is perhaps the easiest case, where we want to find
the distributions for $\Phi$ assuming a flat prior for
$\Phi$ and that $\Sigma$ is given. This gives us
a posterior proportional to the likelihood, which we
pull straight from equation \ref{bjdf}:
\begin{align*}
    p(\Phi \; | \; Y, \Sigma) &\propto p( Y \; | \; \Phi, \Sigma)
	p(\Phi \; | \; \Sigma) \propto p(Y \; |\; \Phi, \Sigma) \\
    &\propto 	
	\;|\Sigma|^{-\frac{T}{2}} 
	\exp\left\{ -\frac{1}{2} \text{tr} \left[\Sigma^{-1}
	\hat{S} \right] \right\} \notag 
	\exp\left\{ -\frac{1}{2} 
	(\beta - \hat{\beta})'
	\left[\Sigma \otimes (X'X)^{-1}\right]^{-1} 
	(\beta - \hat{\beta}) 
	\right\} \notag
\end{align*}






\newpage
%\appendix






%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font

\end{document}

