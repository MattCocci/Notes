\documentclass[a4paper,12pt]{scrartcl}

\author{Matthew Cocci}
\title{Algorithms}
\date{}
\usepackage{enumitem} %Has to do with enumeration	
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm} %allows for labeling of theorems
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{hyperref} % For internal/external linking. 
				 % [hidelinks] removes boxes
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black,
}
% \usepackage{url} % allows for url display, non-clickable
%\numberwithin{equation}{section} 
   % This labels the equations in relation to the sections 
      % rather than other equations
%\numberwithin{equation}{subsection} %This labels relative to subsections
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\setkomafont{disposition}{\normalfont\bfseries}
\usepackage{appendix}
\usepackage{subfigure} % For plotting multiple figures at once
\usepackage{verbatim}

\begin{document}
\maketitle

%\tableofcontents %adds it here

\section{Guiding Principles}

We adopt the following guiding principles for analyzing
algorithms:
\begin{enumerate}
    \item \emph{Worst case analysis}: We analyze the worst
	case running time of an algorithm given the most
	difficult input the algorithm will half to handle. 
	It makes \textbf{no} assumptions about the input.
	This is in contrast to \emph{average case analysis},
	which considers the running time for the ``average''
	case, taking account of the relative frequencies of 
	certain inputs.
    \item Don't pay attention to constant factors and
	lower order terms. Different programming languages
	and compiling schemes will lead varying constant
	factors by a small amount. It's just inappropriate
	to worry about it unless the code is crucial to
	your task.
    \item Asymptotic Analysis: Focus on running time for 
	large input sizes $n$.
\end{enumerate}
Putting these three guiding principles together, we can
define a ``fast'' algorithm as one whose worst-case 
running time grows slowly with the input size,
where ``slowly'' in the best case scenario is typically
\emph{linear} growth in running time with increasing size, $n$.
In effect, that's the Holy Grail, best-case running time
that we'll shoot for.


\section{Divide and Conquer Algorithms}

Divide the problem into smaller sub-problems, solve recursively,
where a \emph{recursive algorithm} is an algorithm that calls itself.


\subsection{Merge Sort}

Merge sort solves the \emph{sort problem}, where you want to 
order a unsorted list, by a divide-and-conquer, recursive
approach.

\subsubsection{Basic Structure}
Merge Sort works as follows:
\begin{enumerate}
    \item Split the list in half into two sub-lists.
    \item Recursively sort the first half. Then, 
	recursively sort the second half.
    \item Merge/recombine the smaller sorted lists.
	\begin{enumerate}
	    \item You step through the output array, 
		populating it with the minimum 
		number from the two sorted sublists you're
		trying to recombine.
	    \item Implicit in the process, the minimum 
		element of each sublist is at the
		beginning of each sublist, so you don't
		have to waste time searching through them.
	\end{enumerate}
\end{enumerate}
Why is this better? Well, it runs with an upper bound
of 
    \[ \text{operations} \leq 6n \log_2 n + 6n \]
while Insertion Sort, Bubble Sort, selection sort, and 
other's require $n^2$ operations. This running time
is not quite linear since it's $6n \log_2 n$, although
it's pretty good.

\subsubsection{Runtime Analysis}
{\sl Simplifying Assumptions}:
    Suppose we have a list of length
    $n$ where, for simplicity, we assume $n$ is a
    power of 2 such that $j = \log_2 n$. 
    \\
    \\
{\sl Recursive Structure Tree}:
    We will make $\log_2 n = j$ divisions of 
    the original list into sub-lists: dividing the original
    list in half, dividing the first level of two sublists in
    half again for a total of four sublists at the second
    level, etc. This will generate a tree with $j$ levels.
    \\
    \\
    Within our tree, at each specific level, $k$, we have 
    $2^k$ sub-problems, with each problem of size
    $n/2^k$.  There are $2^k$ sub-problems because
    we divide by two at each step down the tree.
    On top of that, the sub-problems are
    of size $n/2^k$ because with each division of a 
    parent problem, we reduce the size of the problem 
    by one half.
    \\
    \\
{\sl Merging Sublists}:
    Now, when the Merge Sort has to merge two sub-lists 
    to form a list of size $m$, it will use at most
    $6m$ steps (by a rough approximation that you
    can figure out by looking at the code or pseudocode
    for merging).
    \\
    \\
{\sl Operations at Level $j$}:
    Combining previous results, we see that the number of 
    steps at level $j$ turns out to be:
    \[ \text{operations at level $j$} \leq 
	\underbrace{2^j}_{\text{No. of level $j$ sub-probs}} 
	\times \underbrace{6}_{\text{Merge steps}}
	\underbrace{\left(n/2^j\right)}_{\text{Sub-prob size
	at level $j$}} = 6n \]
    This is awesome because the number of operations
    required at level $j$ is \emph{independent} of the level!
    \\
    \\
{\sl Total Runtime}:
    This means that we take the work per level just computed
    $6n$, times the number of levels:
    \[ \text{total operations} \leq \underbrace{6n}_{\text{work per level}}
	\times \underbrace{\left( \log_2 n +1\right)}_{\text{number of levels}}
	\]


    
\section{Asymptotic Notation}

\subsection{Big-Oh Notation}

This is a type of asymptotic notation concerning functions
defined on the positive integers. In particular, 
we'll consider a function $T(n)$, $n=1,2,\ldots$.
\\
\\
{\sl Definition}: A function $T(n) = O(f(n))$ if
and only if
\begin{equation} 
    \exists \; c, n_0 >0 \quad \text{s.t.} \quad 
    T(n) \leq c \; f(n) \quad \forall n \geq n_0
\end{equation} 
In words, $T(n)$ is $O(f(n))$ if $T(n)$ is 
bounded by a constant multiple of $f(n)$ 
for sufficiently large $n$.

\subsubsection{Polynomial Functions}

We want to prove that Big-Oh notation really does
supress constant and lower order terms:
\begin{align}
    \text{if} \quad T(n) &= a_k n^k + \cdots + a_1 n + a_0 \notag\\
    \Rightarrow \quad T(n) &= O(n^k) \label{polynom}
\end{align}

\begin{proof} To prove, we need to find a $c$ and $n_0$
so that Equation \ref{polynom} satisfies the definition. 
So let's go.
\begin{align*}
    T(n) &= a_k n^k + \cdots + a_1 n + a_0 \\
    &\leq |a_k| n^k + \cdots + |a_1 | n + |a_0| \\
    &\leq |a_k| n^k + \cdots + |a_1 | n^k + |a_0|n^k \\
    &\leq \sum_{i=0}^k |a_i| n^k \\
    \Rightarrow \quad n_0=1 \quad & \quad c = \sum_{i=0}^k |a_i| 
\end{align*}
This satisfies the definition for $T(n)=O(n^k)$.
\end{proof}

\subsection{Omega Notation}

{\sl Definition}: We say that $T(n)=\Omega(f(n))$ if 
and only if there exist constants $c$ and $n_0$
such that 
    \[ T(n) \geq c\cdot f(n) \quad \forall n\geq n_0 \]

\subsection{Theta Notation}

{\sl Definition}: We say that $T(n) =\Theta(f(n))$ if
and only if $T(n)=O(f(n))$ AND $T(n) = \Omega(f(n))$.
Visually, this means $T(n)$ is ``sandwiched''
between two different constant multiples of $f(n)$.
\\
\\
Equivalently, $T(n) =\Theta(f(n))$ if and only if
\[ \exists c_1, c_2, n_0 \quad \text{s.t.}
    \quad c_1 \cdot f(n) \leq T(n) \leq c_2 \cdot f(n)
    \qquad \forall n\geq n_0 \]
Although, we typically only care about Big-Oh notation,
since we care abot upper bounds.

\subsection{Little-Oh Notation}

Similar to Big-Oh, but a little bit stronger. 
Formally, $T(n) = o(f(n))$ if and only
if for \emph{all} constants $c>0$, there exists 
a constant $n_0$ such that
    \[ T(n) \leq c\cdot f(n) \qquad \forall n \geq n_0 \]

\end{document}




%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
