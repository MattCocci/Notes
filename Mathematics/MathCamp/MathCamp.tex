\documentclass[12pt]{article}

\author{Matthew D. Cocci}
\title{Math Camp}
\date{\today}
\listfiles

%% Formatting & Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % most detailed page formatting control
\usepackage{fullpage} % Simpler than using the geometry package; std effect
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}

%% Formatting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=1in]{geometry}
    %   Adjust the margins with geometry package
%\usepackage{pdflscape}
    %   Allows landscape pages
%\usepackage{layout}
    %   Allows plotting of picture of formatting



%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt}
    %   Make the header bigger to avoid overlap

%\fancyhf{}
    %   Erase header settings

%\renewcommand{\headrulewidth}{0.3pt}
    %   Width of the line

%\setlength{\headsep}{0.2in}
    %   Distance from line to text


%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\numberwithin{equation}{section} % Number equations by section
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{ax}[thm]{Axiom}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}
\newtheorem{assump}[thm]{Assumption}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

% Below supports left-right alignment in matrices so the negative
% signs don't look bad
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother

% Make it impossible to break inline math
\relpenalty=10000
\binoppenalty=10000

%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
%\usepackage{blindtext}
\usepackage{courier}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{pgfplots}
\usetikzlibrary{decorations.pathreplacing,arrows}
\usepackage{graphicx}
\usepackage{subfigure}
    %   For plotting multiple figures at once
%\graphicspath{ {Directory/} }
    %   Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{%
    colorlinks,
        %   This colors the links themselves, not boxes
    citecolor=black,
        %   Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\definecolor{codegreen}{RGB}{28,172,0}
\definecolor{codelilas}{RGB}{170,55,241}

% David4 color scheme
\definecolor{d4blue}{RGB}{100,191,255}
\definecolor{d4gray}{RGB}{175,175,175}
\definecolor{d4black}{RGB}{85,85,85}
\definecolor{d4orange}{RGB}{255,150,100}


%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}
    %   For including verbatim code from files, no colors
\usepackage{listings}
    %   For including code snippets written directly in this doc

\lstdefinestyle{bash}{%
  language=bash,%
  basicstyle=\footnotesize\ttfamily,%
  showstringspaces=false,%
  commentstyle=\color{gray},%
  keywordstyle=\color{blue},%
  xleftmargin=0.25in,%
  xrightmargin=0.25in
}

\lstdefinestyle{matlab}{%
  language=Matlab,%
  basicstyle=\footnotesize\ttfamily,%
  breaklines=true,%
  morekeywords={matlab2tikz},%
  keywordstyle=\color{blue},%
  morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
  identifierstyle=\color{black},%
  stringstyle=\color{codelilas},%
  commentstyle=\color{codegreen},%
  showstringspaces=false,%
    %   Without this there will be a symbol in
    %   the places where there is a space
  numbers=left,%
  numberstyle={\tiny \color{black}},%
    %   Size of the numbers
  numbersep=9pt,%
    %   Defines how far the numbers are from the text
  emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
    %   Some words to emphasise
}

\newcommand{\matlabcode}[1]{%
    \lstset{style=matlab}%
    \lstinputlisting{#1}
}
    %   For including Matlab code from .m file with colors,
    %   line numbering, etc.

%% Bibliographies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{natbib}
    %---For bibliographies
%\setlength{\bibsep}{3pt} % Set how far apart bibentries are

%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
    %   Has to do with enumeration
\usepackage{appendix}
%\usepackage{natbib}
    %   For bibliographies
\usepackage{pdfpages}
    %   For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi
    %   Bigger Chi

% In math mode, Use this instead of \munderbar, since that changes the
% font from math to regular
\makeatletter
\def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
\makeatother

% Limits
\newcommand{\limN}{\lim_{N\rightarrow\infty}}
\newcommand{\limn}{\lim_{n\rightarrow\infty}}
\newcommand{\limt}{\lim_{t\rightarrow\infty}}
\newcommand{\limT}{\lim_{T\rightarrow\infty}}
\newcommand{\limhz}{\lim_{h\rightarrow 0}}

% Misc Math
\newcommand{\Prb}{\mathrm{P}}
\newcommand{\ra}{\rightarrow}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dom}{\text{dom}}

% Script
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sA}{\mathscr{A}}

% Mathcal
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}

% Dot over
\newcommand{\dx}{\dot{x}}
\newcommand{\ddx}{\ddot{x}}
\newcommand{\dy}{\dot{y}}
\newcommand{\ddy}{\ddot{y}}
\newcommand{\dz}{\dot{z}}
\newcommand{\ddz}{\ddot{z}}

% Blackboard
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Rk}{\mathbb{R}^k}
\newcommand{\Rmn}{\mathbb{R}^{m\times n}}
\newcommand{\Rmm}{\mathbb{R}^{m\times m}}
\newcommand{\Rnn}{\mathbb{R}^{n\times n}}
\newcommand{\Rkn}{\mathbb{R}^{k\times n}}
\newcommand{\Rnk}{\mathbb{R}^{n\times k}}
\newcommand{\Rkk}{\mathbb{R}^{k\times k}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Cn}{\mathbb{C}^n}
\newcommand{\Cnn}{\mathbb{C}^{n\times n}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}

\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

% Various probability and statistics commands
\newcommand{\nul}{\operatorname{null}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\spann}{\operatorname{span}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\msto}{\xrightarrow{m.s.}}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\Lpto}{\xrightarrow{L_p}}
\newcommand{\plim}{\text{plim}_{n\rightarrow\infty}}

% Redefine real and imaginary from fraktur to plain text
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Shorter sums: ``Sum from X to Y''
% - sumXY  is equivalent to \sum^Y_{X=1}
% - sumXYz is equivalent to \sum^Y_{X=0}
\newcommand{\sumnN}{\sum^N_{n=1}}
\newcommand{\sumin}{\sum^n_{i=1}}
\newcommand{\sumkn}{\sum^n_{k=1}}
\newcommand{\sumtT}{\sum^T_{t=1}}
\newcommand{\sumninf}{\sum^\infty_{n=1}}
\newcommand{\sumtinf}{\sum^\infty_{t=1}}
\newcommand{\sumnNz}{\sum^N_{n=0}}
\newcommand{\suminz}{\sum^n_{i=0}}
\newcommand{\sumknz}{\sum^n_{k=0}}
\newcommand{\sumtTz}{\sum^T_{t=0}}
\newcommand{\sumninfz}{\sum^\infty_{n=0}}
\newcommand{\sumtinfz}{\sum^\infty_{t=0}}

% Bounds/Limits
\newcommand{\atob}{_a^b}
\newcommand{\ztoinf}{_0^\infty}
\newcommand{\kinf}{_{k=1}^\infty}
\newcommand{\ninf}{_{n=1}^\infty}
\newcommand{\minf}{_{m=1}^\infty}
\newcommand{\tinf}{_{t=1}^\infty}
\newcommand{\kinfz}{_{k=0}^\infty}
\newcommand{\ninfz}{_{n=0}^\infty}
\newcommand{\minfz}{_{m=0}^\infty}
\newcommand{\tinfz}{_{t=0}^\infty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

\tableofcontents

\clearpage
\section{Introduction}

These notes were taken to accompany Princeton's Math Camp for PhD
students in Economics, Summer 2015. The course was taught by Juan
Pablo-Xandri; therefore, a decent portion of document reflects his own
(excellent) notes for the course, as well as many intuitive in-class
explanations.

However, one key difference is the order and grouping of topics, which
differs from that of the class. First, I've organized the sections so
that you can progress (more or less) linearly through the document.
Second, I typically do examples and applications only once. I could have
revisited each multiple times and over many sections, adding additional
complexity and explanations as I hit each new topic. In general, I tried
to avoid that. Instead, I only work out an example after covering all
topics and results necessary to understand the most general case.
That's why you see optimization and dynamic programming at the end.
That's also why I use this document more like a reference book---you
only have to look in one place for each application or example.

Lastly, I've included things not covered during Math Camp. This seemed
like a great opportunity to consolidate notes that I had spread over
multiple documents on topics such as Linear Algebra, Numerical
Optimization, differential equations, etc. In some cases, the result is
disjointed. For example, the linear algebra definitions often cover
$\Cn$, while I otherwise deal mostly with $\Rn$. These things should be
smoothed out eventually.

\clearpage
\section{Linear Algebra}

In this section, we will study vector spaces in $\Rn$ and $\Cn$.  That
means that we will have space $\Rn$ or $\Cn$ along with some definition
of sums and products.

\subsection{Solving Linear Systems}

Linear algebra's founding problem is essentially the task of solving a
linear system of equations:
\begin{equation}
    \label{canonical}
    Ax = b
    \qquad x,b\in \Rn
    \quad A \in \Rnn
\end{equation}
There are two ways to think of this problem:
\begin{enumerate}
  \item {\sl Row Interpretation}:
    Each row/equation in the system defines a line/hyperplane in $n$
    dimensional place. The solution is where those lines intersect.
  \item {\sl Column Interpretation}:
    A solution to the system exists if and only if $b$ is in the
    column space of $A$. More explicitly, there is a solution if and
    only if $b$ can be written as a linear combination of the $n$
    vectors defined by the columns of $A$.
    The system solution $x$ specifies how exactly to combine the columns
    of $A$ to produce $b$.
\end{enumerate}
The latter interpretation is especially powerful. We see that the column
space of $A$---the set of all linear combinations of the columns of
$A$---is crucial to characterizing the matrix $A$.

For example, if the columns of $A$ are linearly independent, then the
matrix ``spans'' the entire $n$-dimensional space. We can thus solve
\emph{any} equation like Equation~\ref{canonical} because $b$ will
certainly lie within the column space of $A$. Thus, we can find some $x$
that combines the columns of $A$ in such a way that $Ax=b$.

We only potentially run into problems if the columns aren't linearly
independent. So if the rank of the matrix is $k<n$, only a
$k$-dimensional hyperplance embedded within $\mathbb{R}^n$ is covered by
the columns of $A$. And $b$ might not lie in that subspace. This very
possibility manifests itself in the non-invertability of $A$, which
prevents us from trivially solving the system via $x = A^{-1}b$, for the
very reasons just discussed.


%\begin{defn}(Matrix Multiplication and Column Space)
%\end{defn}
%Matrix multiplication $Ax$ is the combination of the columns of $A$,
%with each column scaled by the corresponding element of $x$. This is,
%hands down, one of the simplest, most important, and most practical ways
%of thinking about matrix multiplication.

%Being more explicit, let $A_{\cdot i}$ denote the $i$th column of $A \in
%\C^{m\times n}$ and let $x_j$ denote the $j$th element of $n\times 1$
%vector $x$. We can express
%\begin{align*}
  %A x = x_1 A_{\cdot 1}  + \cdots + x_n A_{\cdot n}
%\end{align*}
%Since the result is a combination of the columns of $A$---each an
%$m\times 1$ vector---we can easily see that the result will also be an
%$m\times 1$ column vector.

%This intuition also provides a convenient way to ``pick out'' the $i$th
%column of matrix $A$ via matrix multiplication: just right-multiply $A$
%by an $n \times 1$ vector whose elements are all zero, except for the
%$i$th element, which equals 1. Example,
%\begin{align*}
  %\begin{bmatrix}
  %8 & 1 & 6 \\
  %3 & 5 & 7 \\
  %4 & 9 & 2
  %\end{bmatrix}
  %\begin{bmatrix}
  %0 \\ 1 \\ 0
  %\end{bmatrix}
  %= 0
  %\begin{bmatrix} 8 \\ 3 \\ 4 \\ \end{bmatrix}
  %+ 1  \begin{bmatrix} 1 \\ 5 \\ 9 \\ \end{bmatrix}
  %+ 0  \begin{bmatrix} 6 \\ 7 \\ 2 \\ \end{bmatrix}
  %=
  %\begin{bmatrix} 1 \\ 5 \\ 9 \\ \end{bmatrix}
%\end{align*}


%\clearpage
\subsection{Definitions and Vocabulary}

Here are some terms we use to describe matrices and operations with
matrices. Unless otherwise specified, the matrices are assumed to be
complex $m\times n$ matrices so that we can capture the most general
cases.

\begin{defn}{(Subspace)}
$V\subseteq \Rn$ is a \emph{vector subspace} of $\Rn$ if and only if
\begin{enumerate}
  \item $0 \in V$
  \item $x,y\in V$ implies that $\alpha x + \beta y \in V$ for all
    $\alpha,\beta\in \R$.
\end{enumerate}
\end{defn}

\begin{defn}{(Base)}
A \emph{base} for vector space $\Cn$ is a set of vectors
$\mathcal{B}=\{b_1,\ldots,b_n\}$ such that for any $x\in \Cn$, there
exist $\{\alpha_i\}_{i=1}^n \subset \C$ such that we can express $x$ as
\begin{align}
  x = \sum^n_{i=1} \alpha_i b_i
  \label{defn:basis}
\end{align}
Of course, we can always restrict everything from $\Cn$ and $\C$ to
simply $\Rn$ and $\R$.

To determine if $\mathcal{B}$ is a base for $\Cn$, arrange the vectors
as columns in a matrix
\begin{align*}
  V_\mathcal{B} = \begin{pmatrix} b_1 & b_2 &\cdots & b_n \end{pmatrix}
\end{align*}
The vectors in $\mathcal{B}$ form a basis for $\Cn$ if and only if
$V_\mathcal{B}$ is invertible. If that's the case, then we can always
solve
\begin{align*}
  V_\mathcal{B} \;\alpha = x
  \qquad\Leftrightarrow\qquad
  \alpha = V_\mathcal{B}^{-1} x
  \qquad \forall x\in \Rn
\end{align*}
and determine the $\alpha_i$ terms as in Equation~\ref{defn:basis} such
that $x \in \Rn$ can always be expressed as a linear combination of
elements in $\mathcal{B}$.
\end{defn}

\begin{defn}{(Canonical Base)}
For $\Rn$, we define the \emph{canonical base}
$\mathcal{B}=\{e_1,\ldots,e_n\}$ as
\begin{align*}
  e_1 =
  \begin{pmatrix}
  1 \\ 0 \\ \vdots \\ 0 \\ 0
  \end{pmatrix}
  \qquad
  e_2 =
  \begin{pmatrix}
  0 \\ 1 \\ \vdots \\ 0 \\ 0
  \end{pmatrix}
  \quad \cdots \quad
  e_{n-1} =
  \begin{pmatrix}
  0 \\ 0 \\ \vdots \\ 1 \\ 0
  \end{pmatrix}
  \qquad
  e_n =
  \begin{pmatrix}
  0 \\ 0 \\ \vdots \\ 0 \\ 1
  \end{pmatrix}
\end{align*}
That is, base vector $e_i$ is zero everywhere except for the $i$the
element, which equals unity.
\end{defn}

\begin{defn}(Span)
Given set of vectors $X=(x_1,\ldots,x_n)$ each in $\Rm$, the
\emph{span} of $X$ is the set of all vectors in $\Rm$ that can be
written as a linear combination of vectors in $X$.
\end{defn}

\begin{defn}(Range)
Given $A\in\Rmn$, the \emph{range} is the set of all vectors in $\Rm$
that you can create from linear combinations of columns of $A$:
\begin{align*}
  \range(A)
  =\{Ax\in\Rm\;|\; x\in\Rn\}
\end{align*}
\end{defn}

\begin{defn}(Nullspace or Kernel)
The \emph{nullspace} (sometimes called the \emph{kernel}) of a matrix
$A$ ($m\times n$), denoted $\nul(A)$, is defined
\begin{align*}
  \nul(A)
  = \{x\in\Rn\;|\; Ax = 0\}
\end{align*}
In words, the nullspace consists of all vectors that can linearly
combine the columns of $A$ to get the zero vector.
Since the zero vector is \emph{always} in $\nul(A)$, the set can never
be empty.
\end{defn}

\begin{defn}(Orthogonal Complement)
The \emph{orthogonal complement} of subspace $V$ is
\begin{align*}
  V^\perp = \{ y\; | \; \forall x\in V,\; x\cdot y=0\}
\end{align*}
\end{defn}

%\begin{prop}
%\emph{(Orthogonal Complement of Range is Null Space of Transpose)}
%The \emph{orthogonal complement of the range} is the null space of the
%transpose
%\begin{align*}
  %\range(A)^{\perp}
  %&= \nul(A^T)
%\end{align*}
%\end{prop}
%\begin{proof}
%Write out the definition, writing the dot product as $x\cdot y=y^Tx$:
%\begin{align*}
  %\range(A)^{\perp}
  %&= \{y \in\Rm\;|\; \forall x\in \range(A), \; y^T x = 0\}
  %\\
  %&= \{y \in\Rm\;|\; \forall z\in\Rn, \; y^T (Az) = 0\}
%\end{align*}
%Since $Az$ isn't necessarily zero, each $y\in\range(A)^\perp$ must
%linearly combine the rows of $A$ to be zero, so we're looking for those
%$y\in\R^m$ in the nullspace of $A^T$, implying
%$\range(A)^{\perp} = \nul(A^T)$
%\end{proof}

%\begin{prop}
%\emph{(Orthogonal Complement of Null Space is )}
%The \emph{orthogonal complement of the null space} is the range of the
%transpose or, equivalently, the span of the rows:
%\begin{align*}
  %\nul(A)^{\perp}
  %&= \range(A^T)
%\end{align*}
%\end{prop}
%\begin{proof}
%Write out the definition, writing the dot product $x\cdot y$ as $x^Ty$:
%\begin{align*}
  %\nul(A)^{\perp}
  %&= \{y \in\Rn\;|\; \forall x\in \nul(A), \; x^Ty = 0\}
  %\\
  %&= \{A^Tz \in\Rn\;|\; \forall x\in \nul(A), \; x^T (A^Tz) = 0\}
  %\\
  %&\qquad\quad
  %\cup \{y\not\in \range(A^T)\;|\;\forall x\in \nul(A), \; x^Ty = 0\}
%\end{align*}
%Now for all $x\in\nul(A)$, we know $Ax=0$, or $x^TA^T=0^T$. Hence, in
%the first set of the union, \emph{any} $z$ works, i.e. any vector in
%$\range(A^T)$ works.
%The second set is also empty somehow.
%\end{proof}

\begin{defn}{(Hermitian Transpose)}
Given matrix $A$, the \emph{Hermitian transpose} is the conjugate
transpose of $A$ denoted $A^*$.
\end{defn}

\begin{defn}{(Hermitian)}
Matrix $A$ is a \emph{Hermitian matrix} if $A=A^*$.
\end{defn}

\begin{defn}{(Symmetric)}
$A$ is \emph{symmetric} if it is a real Hermitian matrix:
$A=A^T$.
\end{defn}

\begin{defn}{(Invertible)}
Matrix $A$ is \emph{invertible} if there exists a matrix $A^{-1}$ such
that $A A^{-1} = A^{-1}A=I$
\end{defn}

\begin{defn}{(Unitary)}
Matrix $A$ is \emph{unitary} if $A^{-1} = A^*$.
\end{defn}

\begin{defn}{(Orthogonal)}
Matrix $A$ is \emph{orthogonal} if $A$ is a \emph{real} unitary matrix:
$A^{-1} = A^T$ so that $AA^T = A^T A = I$.
\end{defn}

\begin{defn}{(Normal)}
$A$ is \emph{normal} if $A^* A = A^*$. Hermitian plus unitary
implies normal.
\end{defn}

\begin{defn}{(Sparse)}
Matrix $A$ is \emph{sparse} if its entries are mostly zeroes.
\end{defn}

\begin{defn}(Idempotent)
Matrix $A$ is \emph{idempotent} if $A = AA$.
\end{defn}

\begin{defn}{(Diagonal)}
Matrix $D=(d_{ij})_{n\times n}$ is a \emph{diagonal} matrix if and only
if $d_{ij}=0$ for all $i\neq j$. That is, the only positive entries are
on the diagonal, $d_{ii}$ for all $i$.

Diagonal matrices are exceptionally nice to work with since they permit
us to calculate higher powers of the matrix very easily. In particular,
\begin{align*}
  D =
  \begin{pmatrix}
    d_1 & 0 & \cdots & 0 \\
      0 & d_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & d_n \\
  \end{pmatrix}
  \quad\Rightarrow\quad
  D^k =
  \begin{pmatrix}
    d_1^k & 0 & \cdots & 0 \\
      0 & d_2^k & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & d_n^k \\
  \end{pmatrix}
  \qquad
  \forall k \in \R
\end{align*}
This even allows us to define $\sqrt{D}$ for $k=1/2$.

Moreover, although matrix multiplication does \emph{not} commute in
general, it does when dealing with diagonal matrices:
\begin{align*}
  C =
  \begin{pmatrix}
    c_1 & \cdots & 0 \\
    \vdots & \ddots & \vdots \\
    0 & \cdots & c_n \\
  \end{pmatrix}
  \quad
  D =
  \begin{pmatrix}
    d_1 & \cdots & 0 \\
    \vdots & \ddots & \vdots \\
    0 & \cdots & d_n \\
  \end{pmatrix}
  \quad\Rightarrow\quad
  CD =
  \begin{pmatrix}
    c_1d_1 & \cdots & 0 \\
    \vdots & \ddots & \vdots \\
    0 & \cdots & c_n d_n \\
  \end{pmatrix}
\end{align*}
\end{defn}

\begin{defn}{(Tridiagonal)}
Matrix $A$ is \emph{tridiagonal} if it has three diagnoals, for example
\begin{align*}
  A =
  \begin{pmatrix}[r]
   2 & -1 &  0 &  0 \\
  -1 & 2 & -1 &  0 \\
   0 & -1 &  2 & -1 \\
   0 &  0 & -1 &  2 \\
  \end{pmatrix}
\end{align*}
\end{defn}

\begin{defn}{(Similar)}
\label{defn:similar}
Matrices $A,B \in \Rnn$ are \emph{similar}, denoted $A\sim B$, if there
exists an invertible matrix $V\in \Rnn$ such that
\begin{align*}
  B = V^{-1} A V
\end{align*}
Calculating higher powers of matrix $B$ also have a nice relationship to
higher powers of $A$:
\begin{align*}
  B^k &= \left(V^{-1} A V\right)^k
      = \underbrace{\left(V^{-1} A V\right)\left(V^{-1} A V\right)
      \cdots  \left(V^{-1} A V\right)}_{k \;\text{times}}
      = V^{-1} A^k V
\end{align*}
This will prove extremely useful in the next definition.
\end{defn}

\begin{defn}{(Diagonalizable)}
$A$ is \emph{diagonalizable} if similar to some diagonal matrix $D$.
\end{defn}
\begin{rmk}
This is hugely important for calculating powers of matrices. We saw that
powers of diagonal matrices are extremely easy to compute, while
Definition~\ref{defn:similar} also showed a nice and easy way to relate
powers of $A$ to powers of $D$. So we can bypass the tougher problem of
computing $A^k$ and simply calculate $V^{-1} D^k V$.\footnote{%
The question of whether matrices $V$ and $D$ exist for arbitrary $A$
will be answered in Section~\ref{subsec:eigenvaluedecomp}, which covers
the Eigenvalue Decomposition.
}
\end{rmk}

Next, we want to generalize the concepts of ``positive'' and
``negative'' to matrices. We could go element by element and require a
matrix to have all positive elements. But that's a bit too restrictive.
So let's look at what it means for a scalar to be positive and work from
there. Specifically, scalar $a$ is positive if
\begin{align*}
  a \geq 0
  \quad\Leftrightarrow\quad
  \forall x\in \R
  \quad
  x^2 a = x a x\geq 0
\end{align*}
Now for matrices, we'll employ a similar idea, suitably modified to
handle matrices.

\begin{defn}{(Positive- and Negative-(Semi)definite Matrices)}
Suppose we have Hermitian $A \in \C^{n\times n}$ or simply
symmetric if $A \in \R^{n\times n}$. We classify it as follows, with
each classification defined for $A$ complex or real
\begin{itemize}
  \item \emph{Positive-Semidefinite}
    \begin{itemize}
      \item $A \in \C^{n\times n}$:
          $\qquad x^* A x \geq 0
          \qquad \forall x\in \C^n
          \quad \text{and} \quad x^* A x\in \R$
      \item $A \in \R^{n\times n}$:
          $\qquad x^T A x\geq 0
          \qquad \forall x\in \R^n$
    \end{itemize}
  \item \emph{Positive-Definite}: Inequalities in the
    positive-semidefinite case are strict.
  \item \emph{Negative-Semidefinite}
    \begin{itemize}
      \item $A \in \C^{n\times n}$:
          $\qquad x^* A x \leq 0
          \qquad \forall x\in \C^n
          \quad \text{and} \quad x^* A x\in \R$
      \item $A \in \R^{n\times n}$:
          $\qquad x^T A x\leq 0
          \qquad \forall x\in \R^n$
    \end{itemize}
  \item \emph{Negative-Definite}: Inequalities in the
    negative-semidefinite case are strict.
\end{itemize}
\end{defn}

\begin{defn}{(Rank)}
For matrix $A\in \R^{m \times n}$, the rank is the number of linearly
independent columns or, alternatively, rows of $A$.\footnote{Whether you
check rows or columns doesn't matter, those numbers will be equal.} The
rank $r$ is such that $r \leq n$ and $r\leq m$.
\end{defn}

\begin{defn}(Moore-Penrose Pseudoinverse)
This is defined $A^\dagger = (A^TA)^{-1}A^T$.
\end{defn}

\subsection{The Determinant}

\begin{defn}{(Multilinear Function)}
A function $f:\R^n\rightarrow \R$ is \emph{multilinear} if the following
functions are linear:
\begin{align*}
  f_1 &:= f(x,x_2,\ldots,x_n)\\
  f_2 &:= f(x_1,x,\ldots,x_n)\\
  \vdots\; & \qquad \vdots\\
  f_n &:= f(x_1,x_2,\ldots,x)
\end{align*}
That is, to construct function $f_i$, fix all but the $i$th argument of
$f$. If the $f_i$ are linear for all $i = 1,\ldots, n$, then $f$ is
multilinear.
\end{defn}

Next, I present a rather unusual definition of the determinant relative
to Linear Algebra 101. Rather than simply describe how to compute the
determinant, this definition specifies the properties that the
determinant must satisfy. Surprisingly, that's enough to generate the
usual determinant that arrives at the same number for each matrix
compared to the usual computational definition. The determinant will in
fact be the \emph{only} function satisfying the following properties.

\begin{defn}{(Determinant)}
The determinant is a real-valued function mapping $n$ vectors of size
$n\times 1$
\begin{align*}
  \det: \underbrace{\R^n \times \cdots \times \R^n}_{n \;\text{times}}
  \rightarrow \R
\end{align*}
and satisfying the following properties
\begin{enumerate}
  \item Multilinearity
  \item Asymmetry so that swapping input arguments $i$ and $j$ for any
    $i\neq j$ in $\{1,\ldots,n\}$ changes the sign of the determinant.
    Example for $n=3$ and $x,y,z\in \R^3$:
    \begin{align*}
      \det(x,y,z) = -\det(y,x,z) = - \det(x,z,y)
      = \det(z,x,y) = -\det(z,y,x)
    \end{align*}
  \item $\det(e_1,e_2,\ldots,e_n)=1$ where the set
    $\mathcal{B}=\{e_1,\ldots,e_n\}$ represents the canonical base for
    $\R^n$. This states how we normalize the determinant.
\end{enumerate}
\end{defn}


\begin{prop}{\emph{(Properties of Determinants)}}
Given square matrices $A,B\in \R^{n\times n}$, it follows that
\begin{enumerate}
  \item $\det(AB) = \det(A) \det(B)$
  \item Matrix $A$ is invertible if and only if $\det(A) \neq 0$.
  \item $\det(A^{-1}) = \frac{1}{\det(A)}$
  \item $\det(A) = \det(A^T)$
  \item For similar matrices $A\sim B$, we have $\det(A) = \det(B)$.
  \item For diagonal matrix $D=\diag\{\lambda_1,\ldots,\lambda_n\}$,
    $\det(D) = \prod^n_{i=1} \lambda_i$.
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn
\begin{enumerate}
  \item
  \item
  \item
  \item
  \item Since $A\sim B$, there exists an invertible $V$ such that
    \begin{align*}
      B &= V^{-1} A V\\
      \Rightarrow\quad
      \det(B) &= \det(V^{-1} A V)\\
      \text{By (1)} \quad\quad
      \det(B) &= \det(V^{-1}) \det(A) \det(V)\\
      \text{By (3)} \quad\quad
      \det(B) &= \frac{1}{\det(V)} \det(A) \det(V)\\
      \Rightarrow\quad
      \det(B) &= \det(A)
    \end{align*}
\end{enumerate}
\end{proof}

\clearpage
\subsection{Orthogonal Matrices}

We now give an alternate definition to the one above for orthogonal
matrices, show the earlier definition is implied by this one, and then
derive various properties of orthogonal matrices.

\begin{defn}(Orthogonal Matrix and Transformation)
Square matrix $Q\in\Rnn$ with columns $(q_1,\ldots,q_n)$ is
\emph{orthogonal} if its columns form an an \emph{orthonormal basis} for
$\Rn$, i.e. if
\begin{itemize}
  \item All columns unit length, $\lVert q_i\rVert = 1$
  \item All columns mutually perpendicular,
    $\langle q_i, q_j\rangle =0$ for all $i\neq j$
\end{itemize}
Multiplication by an orthogonal matrix corresponds geometrically to
\emph{reflection} and \emph{rotation}. So if we have vector $x$, the
orthogonal transformation $Qx$ returns another vector that is a
reflection and rotation of the original $x$ about some axes.
\end{defn}

\begin{defn}(Changing Coordinates with an Orthogonal Matrix and its Basis)
Orthogonal matrices make it very easy to change coordinates.
To see this, recall how we change coordinates.
Given point $b\in\Rn$ expressed in the standard basis, we can reexpress
point $b$ in some other basis $(a_1,\ldots,a_n)$ by placing column
vectors $a_i\in\Rn$ in matrix $A=(a_1,\ldots,a_n)$ and solving $Ax=b$.
Then $x=A^{-1}b$ will express point $b$ in terms of basis
$(a_1,\ldots,a_n)$.

Now suppose that $Q=(q_1,\ldots,q_n)$ is an
orthogonal matrix, implying that the columns represent an orthonormal
basis. Then we can reexpress point $b$ as $x=Q^{-1}b=Q^Tb$, taking
advantage of the result $Q^{-1}=Q^T$ that we will prove below.
Therefore, we see that we can \emph{easily} change coordinates from the
standard basis to $(q_1,\ldots,q_n)$ by simple dot products $x=Q^Tb$,
rather than computing an expensive matrix inverse $x=A^{-1}b$.
\end{defn}

\begin{prop}\emph{(Properties of Orthogonal Matrices)}
If $Q\in\Rnn$ is orthogonal, then
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $Q^TQ=I_n$ and $QQ^T=I_n$
  \item $Q^{-1}=Q^T$. This is a hugely important property. It means
    computing inverses of orthogonal matrices are really, really cheap
    and easy numerically
  \item $\langle Qx, Qy \rangle=\langle x, y \rangle$. In words,
    the inner product of $x$ and $y$ equals the inner product of $x$ and
    $y$ rotated by $Q$. Hence, $Q$ rotates in a way that preserves inner
    products
  \item $\lVert Qx \rVert =\lVert x\rVert$.
    In words, transforming $x$ to $Qx$ preserves the length in terms of
    the standard Euclidean norm.
  \item $\det Q = \pm 1$
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn, letting $(q_1,\ldots,q_n)=Q$ denote the columns
of $Q$
\begin{enumerate}[label=(\roman*)]
  \item Let $A=Q^TQ$. Then the $i,j$ element is given by
    \begin{align*}
      A_{ij} = \langle q_i, q_j\rangle
      = \begin{cases}
          1 & i=j \\
          0 & i\neq j \\
        \end{cases}
    \end{align*}
    Therefore $A=I_n$. Moreover,
    \begin{align*}
      Q^TQ = I_n
      \quad\implies\quad
      QQ^TQ = Q I_n
      \quad\implies\quad
      (QQ^T)Q = Q
      \quad\implies\quad
      QQ^T=I_n
    \end{align*}

  \item
    Start from a result in (i): $QQ^T = I_n$. Then clearly
    $Q^T = Q^{-1}$
  \item
    We simply use properties of the inner product, along with result
    (i):
    \begin{align*}
      \langle Qx, Qy \rangle
      =
      (Qx)^T Qy
      = x^T Q^TQ y = x^T I_n y = \langle x, y\rangle
    \end{align*}
  \item
    We can apply result (iii) directly:
    \begin{align*}
      \lVert Qx\rVert
      =
      \sqrt{%
      \langle Qx,Qx\rangle
      }
      =
      \sqrt{%
      \langle x,x\rangle
      }
      = \lVert x\rVert
    \end{align*}
\end{enumerate}
\end{proof}

\begin{prop}\emph{(Reflecting Vectors across a Hyperplane)}
\label{prop:reflect}
Suppose that we have a hyperplane in $\Rn$ defined by perpendicular
unit vector $v$. Then we can reflect $x$ about the hyperplane by
computing
\begin{align*}
  x - 2\langle x,v\rangle v
\end{align*}
\end{prop}
\begin{proof}
First, $\langle x,v\rangle$ gives the projection of $x$ onto $v$.
Subtract off two of these projects from $x$ to get to the hyperplane,
then to the other side.
\end{proof}

\begin{defn}(Householder Matrix)
Given vector $u\in\R^n$, we define the associated
\emph{Householder matrix}
\begin{align*}
  H = I_n - 2\frac{uu^T}{\lVert u\rVert^2}
\end{align*}
Note that every Householder matrix is associated with a vector $u$; they
are tied together.
\end{defn}

\begin{prop}\emph{(Householder Matrices Reflect Vectors)}
Given vector $u\in\Rn$, we form the associated Householder matrix $H$.
The vector $u$ is also perpendicular to an implicit hyperplane, and
we can reflect any $x\in\Rn$ about this hyperplane by simple matrix
multiplication, $Hx$.
\end{prop}
\begin{proof}
First, $u$ is any arbitrary vector, and it's perpendicular to some
implicit hyperplane. We can compute the unit vector perpendicular to
that same hyperplane by standardizing: $v=u/\lVert u\rVert$.
Then, by Proposition~\ref{prop:reflect}, we can reflect $x\in\Rn$ about
the hyperplane perpendicular to unit-length $v$ (in direction of $u$) by
using $\langle x,v\rangle=x^Tv=v^Tx\in\R$ to write
\begin{align*}
  x - 2\langle x,v\rangle v
  =
  x - 2 v(v^Tx)
  =
  \big( I_n - 2 vv^T\big)x
\end{align*}
The above computation reflects vector $x$; therefore, we just define
\begin{align*}
  H
  = I_n - 2 vv^T
  = I_n
  - 2 \left(\frac{u}{\lVert u\rVert}\right)
  \left(\frac{u}{\lVert u\rVert}\right)^T
  = I_n - 2 \frac{uu^T}{\lVert u\rVert^2}
\end{align*}
which is exactly the definition of the Householder matrix associated
with $u$ and, by construction, makes $Hx$ a reflection about the
hyperplane perpendicular to $u$ and $v$.
\end{proof}

\clearpage
Below, we plot an example.
We want to reflect vector $x$ (blue) about the dotted line.
(In higher dimensions, the dotted line would instead be a hyperplane.)
That dotted line is defined/pinned down by an orthogonal vector $u$
(black), with $v$ (gray) representing the unit-length version of $u$.
To start, the dashed blue line travels from $x$ to
$\langle x, v\rangle v$, the projection of $x$ onto $v$ (orange).  The
two dashed orange lines show $x-2\langle x,v\rangle v$, i.e.  adding two
negative copies of the projection vector (orange) to $x$ (blue) to
arrive at reflected $x$ in Quadrant IV.

\begin{figure}[htbp!]
\centering
\begin{tikzpicture}[xscale=1.5, yscale=1.5, shorten >= -3pt, shorten <= -3pt]
  % Axes
  \draw[d4black, <->] (0,-0.5) -- (0,0) -- (0,2.5);
  \draw[d4black, <->] (-3,0) -- (0,0) -- (3,0);

  % Dashed line giving line/hyperplane to reflected about
  \draw[d4black, thick, dotted] (-1,2) -- (0.25,-0.5);
  \node[d4black] at (-2.5,2.2) {Reflection Line/Plane};

  % Vector u orthogonal to the line/hyperplane
  \draw[d4black, ultra thick, ->] (0.00,0.00) -- (2,1);
  \node[d4black] at (2.2,1.2) {$u$};

  % -<x,v> v, the two negative orange vectors
  \draw[d4orange, ultra thick, dashed, ->] (1,2) -- (-0.55,1.25); % (-0.6,1.2)
  \draw[d4orange, ultra thick, dashed, ->] (-0.55,1.25) -- (-2.15,0.45); % (-2.2, 0.4)

  % Vector x
  \draw[d4blue, ultra thick, ->] (0.00,0.00) -- (1,2);
  \node[d4gray] at (1.3,2.0) {$x$};
  \node[d4blue] at (1.3,2.0) {$x$};

  % <x,v> v, i.e. projecting x onto v
  \draw[d4blue, thick, dashed, ->] (1,2) -- (1.5,0.9);
  \draw[d4orange, ultra thick, ->] (0,0) -- (1.50,0.75); % (1.6,0.8)
  \node[d4gray, thick] at (2.05,0.6) {$\langle x,v\rangle v$};
  \node[d4orange, thick] at (2.05,0.6) {$\langle x,v\rangle v$};

  % Vector v, the unit-length version of u
  \draw[d4gray, ultra thick, ->] (0.00,0.00) -- (0.894427,0.447214);
  \node[d4black, thick] at (1.3,0.2) {$v=u/\lVert u\rVert$};
  \node[d4gray, thick] at (1.3,0.2) {$v=u/\lVert u\rVert$};

  % Reflected x
  \draw[d4blue, ultra thick, ->] (0,0) -- (-2.15,0.4);
  \node[d4gray] at (-3,0.5) {Reflected $x$};
  \node[d4blue] at (-3,0.5) {Reflected $x$};
\end{tikzpicture}
\end{figure}


\begin{prop}\emph{(Householder Matrix Properties)}
For any $u\in\Rn$, associated $H$ satisfies
\begin{enumerate}[label=\emph{(\roman*)}]
    \item If $u,w\in\Rn$ point in the same direction, the associated
      Householder matrices are equal.
  \item $H$ symmetric \& orthogonal.
    The latter is unsurprising. As stated, orthogonal matrices
    encode reflection and rotation, and (as shown) Householder matrices
    reflect vectors.
  \item $\rank(H)=1$, hence multiplication $HA$ is referred to as a
    \emph{``rank-1 update of $A$''}
\end{enumerate}
\end{prop}
\begin{proof}
We prove each in turn
\begin{enumerate}[label=(\roman*)]
  \item
    Since $u,w$ point in the same direction, the corresponding unit
    vectors are equal: $u/\lVert u\rVert=w/\lVert w\rVert$.  So the
    implicit perpendicular hyperplane we reflect about is the same.
  \item
    Symmetry:
    Write out the formula, using $v=u/\lVert u\rVert$,
      $H^T
      = \big(I_n- 2vv^T\big)^T
      %= I_n^T - 2(vv^T)^T
      = I_n - 2vv^T = H$

    To prove $H$ is orthogonal, we will show $H^TH=I_n$, i.e.
    $(H^TH)_{ij}=\mathbf{1}\{i=j\}$.
    So let $H=(h_1,\ldots,h_n)$ where $h_i$ denotes the $i$th column of
    $H$, let $e_i$ denote the $i$th standard basis element, and let
    $v=u/\lVert u\rVert$ so that $H=I_n-2vv^T$ and
    $\langle v,v\rangle=v^Tv=1$. Then
    \begin{align*}
      \big(H^TH\big)_{ij}
      =
      \langle h_i, h_j\rangle
      = h_i^Th_j
      &=
      \big(e_i - 2v_iv\big)^T
      \big(e_j - 2v_jv\big)
      \\
      &=
      e_i^Te_j - 2v_je_i^T v
      - 2v_i v^Te_j + 4v_iv_j v^Tv
      \\
      &=
      \mathbf{1}\{i=j\}
      - 2v_jv_i
      - 2v_iv_j + 4v_iv_j\cdot 1
      \\
      \implies\quad
      \big(H^TH\big)_{ij}
      &=
      \mathbf{1}\{i=j\}
    \end{align*}

  \item
    Recall $H=I_n-2vv^T$, where $v=u/\lVert u\rVert$.
    Clearly, $vv^T$ is a matrix with all columns parallel to $v$. Hence,
    $\rank(vv^T)=1$. Consequently, $H=I_n-2vv^T$ also has $\rank=1$.
\end{enumerate}
\end{proof}

\clearpage
\subsection{The Eigenvalue Problem}

%\subsubsection{Basic Eigenvalue Problem}

\begin{defn}{(Eigenvalue Problem)}
For a square matrix $A \in \Rmm$, a \emph{right eigenvector}
(or, more commonly, simply an \emph{eigenvector}) is a vector $x\neq 0$
satisfying
\begin{equation}
  Ax = \lambda x
  \qquad \Leftrightarrow \qquad
  (A-\lambda I) x = 0
\end{equation}
where $\lambda$ is some scalar, called an \emph{eigenvalue}.  A
\emph{left eigenvector} of matrix $A$ is a right eigenvector of $A^*$.
The restriction that $x\neq 0$ is important since zero is always a
solution, but it's a dumb trivial one we don't really care about.  In
other words, eigenvectors are those non-zero vectors that are simply
\emph{rescaled} by matrix $A$. They're kind of like fixed points where
you don't care about changes in proportionality.
\end{defn}

\begin{defn}
Given a matrix $A\in \Rmm$, define the characteristic
polynomial $P_A(\lambda)$ as
\begin{align}
  \label{chareqn}
  P_A(\lambda) = \det(A-\lambda I)
\end{align}
This polynomial has at most $m$ distinct roots.
\end{defn}

\begin{thm}\emph{(Solving the Eigenvalue Problem)}
The scalar $\lambda$ is an eigenvalue of square matrix $A$ if and only
if it is a root of the characteristic polynomial, i.e.\ if
$P_A(\lambda)=0$.  Given an eigenvalue, its corresponding eigenvector(s)
is (are) elements in the null space of the matrix $A-\lambda I$.
\end{thm}
\begin{proof}
By definition, an eigenvector $x$ satisfies
\begin{align}
  A x &= \lambda x\notag\\
  (A- \lambda I)x &= 0
  \label{eq:eigprob}
\end{align}
We want this to have a non-trivial solution. That is, we want
Equation~\ref{eq:eigprob} to have a non-trivial solution. But that only
happens if $(A-\lambda I)$ is not invertible. And a matrix is not
invertible if and only if it's determinant is zero. Therefore we need to
solve for $\lambda$ that imply
\begin{align*}
  \det(A-\lambda I)=0
\end{align*}
\end{proof}


\begin{defn}{(Multiplicities)}
The multiplicity of eigenvalue $\lambda$ as a solution to the
characteristic polynomial is called the \emph{algebraic multiplicity of
$\lambda$}, denoted $m(\lambda)$.

The number of eigenvectors corresponding to a given eigenvalue $\lambda$
is called the \emph{geometric multiplicty of $\lambda$}, denoted
$g(\lambda)$.
\end{defn}

\begin{prop}{\emph{(Eigenspace)}}
Given eigenvalue $\lambda$ of some matrix $A\in\Rmm$, define the set of
all eigenvectors corresponding to $\lambda$:
\begin{align*}
  V_\lambda = \{ x\in \Rm \; | \; Ax = \lambda x \}
\end{align*}
Then $V_\lambda$ is a linear subspace called the
\emph{eigenspace of $\lambda$}. Hence, linear combinations of
eigenvectors are also eigenvectors.
\end{prop}
\begin{proof}
First, we know that $0\in V_\lambda$ since it is always the case that
$A\cdot 0=\lambda \cdot 0$ for any $\lambda$.

Next, suppose $x,y \in V_\lambda$ and $\alpha,\beta\in\R$. Then
\begin{align*}
  A(\alpha x + \beta y)
  &= \alpha Ax + \beta Ay \\
  &= \alpha \lambda x + \beta \lambda y
  \qquad \text{since $x,y\in V_\lambda$} \\
  \Rightarrow\quad
  A(\alpha x + \beta y)
  &= \lambda (\alpha x + \beta y)
\end{align*}
which implies that $\alpha x + \beta y \in V_\lambda$ so that
$V_\lambda$ is a subspace of $\Rn$.
\end{proof}


%\subsubsection{The Generalized Eigenvalue Problem}

\begin{defn}{(The Generalized Eigenvalue Problem)}
For square matrices $A,B\in\Rmm$, an \emph{eigenvector of $A$ relative
to $B$} is a vector satisfying
\begin{equation}
  Ax = \lambda B x
  \qquad \Leftrightarrow \qquad
  (A - \lambda B) x = 0
\end{equation}
where $\lambda$ is some scalar, called an
\emph{eigenvalue of $A$ relative to $B$}.

We see also that the Eigenvalue Problem introduced above is simply a
special case where $B=I_m$.
\end{defn}

\subsection{Singular vs. Nonsingular Reference Chart}

Suppose we have an $n\times n$ matrix $A$. The singular vs.\ nonsingular
distinction entails a whole host of consequences, summarized below:
\begin{table}[h!]
\centering
\begin{tabular}{lll}
\textbf{Nonsingular}                     && \textbf{Singular} \\
$A$ invertible                           && $A$ not invertible \\
Columns independent                      && Columns dependent \\
Rows independent                         && Rows dependent \\
$\det(A)\neq0$                           && $\det(A)=0$ \\
$Ax = 0$ has one solution, $x = 0$       && $Ax=0$ has infinitely many solutions\\
$Ax = b$ has one solution, $x = A^{-1}b$ && $Ax = b$ has no solution or infinitely many \\
$A$ has $n$ (nonzero) pivots             && $A$ has $r<n$ pivots \\
$A$ has full rank                        && $A$ has rank $r<n$ \\
Reduced row echelon form is $R = I$      && $R$ has at least one zero row \\
Column space is all of $\mathbb{R}^n$    && Column space has dimension $r<n$ \\
Row space is all of $\mathbb{R}^n$       && Row space has dimension $r<n$ \\
All eigenvalues are non-zero             && Zero is an eigenvalue of $A$ \\
$A^T A$ is symmetric positive definite   && $A^T A$ is only semidefinite \\
$A$ has $n$ (positive) singular values   && $A$ has $r<n$ singular values
\end{tabular}
\end{table}


\clearpage
\subsection{Matrix Decompositions}
\label{subsec:decomp}

Matrix decompositions that express arbitrary matrix $A$ in some
canonical form are very useful for the following key reasons:
\begin{enumerate}
  \item They more explicitly express properties of $A$
  \item They often suggest a geometric intepretation of matrix
    properties or operations.
  \item They allow the application of standard, more efficient
    algorithms to invert a matrix or solve a system $Ax=b$.
\end{enumerate}

\subsubsection{$LU$ Decomposition}

First, a result that will be useful below: If we want to swap rows of a
matrix, we can express that as matrix multiplication. More specifically,
given $A\in\R^{m\times n}$, we can swap rows $i$ and $j$ via
matrix multiplication $PA$, where $P$ is simply the identity matrix
$I_m$ with its rows $i$ and $j$ swapped. This generalizes, so if
we want to rearrange more rows of $A$, simply reorder the rows of the
identity matrix $I_m$ according to how you want the rows of $A$ ordered.

\begin{thm}\emph{(LU Decomposition)}
Given any square matrix $A\in\Rmm$, we can express
\begin{align}
  PA = LU
  \label{ludecomp}
\end{align}
where $L$ is lower triangular, $U$ is upper triangular, and $P$ is a
permutation matrix that reorders rows of $A$ for greater numerical
stability in the implementation.
\end{thm}
\begin{proof}%(How to Construct $LU$ Decomposition)
Main idea:
construct $L$ and $U$ through simple Gaussian elimination.
$U$ is the upper-triangular result of elimination, while
$L$ encodes the sequence of elimination steps.
(Ignore permutation matrix $P$ for now---assume $P=I_m$).

Most generally, let $B^{(k)}$ denote the matrix that results after $k$
Gaussian elimination steps applied to $B^{(0)}=A$. We can write
$B^{(k-1)}$ in full/explicit notation then a more compact block form
that ignores some superscripts, respectively, as:
\begin{align*}
  B^{(k-1)}
  &=
  \begin{pmatrix}
    b_{11}^{(k-1)} & \cdots & b_{1k}^{(k-1)} & \cdots & b_{1m}^{(k-1)}\\
    & \ddots & \vdots  & & \vdots\\
    & & b_{kk}^{(k-1)} & \cdots & b_{km}^{(k-1)} \\
    & & \vdots & & \vdots \\
    & & b_{mk}^{(k-1)} & \cdots & b_{mm}^{(k-1)}
  \end{pmatrix}
  =
  \begin{pmatrix}
    b_{11} & \\
    & \ddots &  & \\
    & & b_{kk} & \beta_{k2} \\
    & & \beta_{2k} & \beta_{22}
  \end{pmatrix}
\end{align*}
Then we can express Gaussian elimination step $k$ as simple matrix
multiplication. Letting $M_k$ denote the matrix that implements
elimination step $k$, we can write
\begin{align*}
  B^{(k)} = M_k B^{(k-1)}
  &=
  \begin{pmatrix}
    1 & \\
      & \ddots & & \\
      &  & 1 & 0 \\
      &  & -\frac{\beta_{2k}}{b_{kk}} & I_{m-k}\\
  \end{pmatrix}
  \begin{pmatrix}
    b_{11} & \\
    & \ddots &  & \\
    & & b_{kk} & \beta_{k2} \\
    & & \beta_{2k} & \beta_{22}
  \end{pmatrix}
  =
  \begin{pmatrix}
    b_{11} & \\
    & \ddots &  & \\
    & & b_{kk} & \beta_{k2}  \\
    & & 0 & \beta_{22}
      - \frac{\beta_{21}\beta_{12}}{a_{11}}
  \end{pmatrix}
\end{align*}
In this way, we can write the entire sequence of Gaussian elimination
steps as a sequence of matrix multiplications until we get an
upper-triangular matrix:
\begin{align*}
  U = B^{(m-1)} = M_{m-1}\cdots M_1 A
  \qquad
  \text{where}\quad
  M_k
  =
  \begin{pmatrix}
    1 & &\\
    & \ddots &\\
    & & 1 & 0\\
    & & m_k & I_{m-k} \\
  \end{pmatrix}
  \qquad m_k \in\R^{m-k}
\end{align*}
with $m_k = -\beta_{2k}/b_{kk}$.
This then suggests that
\begin{align*}
  U = \big(M_{m-1}\cdots M_1\big)A
  = L^{-1} A
  \quad\implies\quad
  L = M_1^{-1}\cdots M_{m-1}^{-1}
\end{align*}
We can verify that $L$ is lower triangular by computing the product of
inverses of $M_k$. First, the special form of each $M_k$ implies that
\begin{align*}
  M_k^{-1} =
  \begin{pmatrix}
    1 & &\\
    & \ddots &\\
    & & 1 & 0\\
    & & -m_k &I_{m-k} \\
  \end{pmatrix}
  \qquad m_k \in\R^{m-k}
\end{align*}
That is, just negate $M_k$ below the diagonal to get $M_{k}^{-1}$;
therefore, each $M_k^{-1}$ is also lower diagonal. Since the product of
lower diagonal matrices is lower diagonal,
$L=M_1^{-1}\cdots M_{m-1}^{-1}$ is lower diagonal. What's more, the
special form implies that
\begin{align*}
  L=
  \begin{pmatrix}
    1 & &\\
    \uparrow& \ddots &\\
     & & 1 \\
    -m_1 & \cdots & -m_k &\ddots \\
    \downarrow & & \downarrow & & 1 \\
  \end{pmatrix}
\end{align*}
The permuation matrix arises if, during the course of Gaussian
elimination, we swap rows to prevent the multipliers from becoming huge.
In particular, when we compute $-\beta_{2k}/b_{kk}$, we run the risk of
$b_{kk}$ being very small and the resulting vector of multipliers being
huge. That will lead to numerical precision issues. Therefore, we
sometimes swap rows, called ``pivoting.'' So in Gaussian elimination
step $k$, we swap the $k$th row with a later row $i>k$ that has
$b_{ik}>b_{kk}$. We just need to track the swaps and provide a
permutation matrix $P$ at the end that reorders things correctly.
\end{proof}


\clearpage
\subsubsection{Cholesky Decomposition}

The Cholesky Decomposition applies to all \emph{symmetric}
\emph{positive definite} matrices; therefore, it finds a number of
applications in dealing with covariance matrices.

\begin{thm}\emph{(Cholesky Decomposition)}
Any symmetric positive definite matrix $A\in\Rmm$ can be
decomposed
\begin{equation}
  A = L L^T
\end{equation}
where $L$ is lower triagular with positive diagonal elements.
\end{thm}

\begin{rmk}
The Cholesky represents something like the ``square root'' of $A$.
What's more, it's more numerically stable that the $LU$ (provided the
matrix is positive definite so that the Cholesky exists). Therefore, we
don't worry about pivoting/row-swapping as with the $LU$.
\end{rmk}

\begin{ex}(Generating Multivariate Random Normals)
We know how to draw the following multivariate random normal $Z$:
\begin{align*}
  Z \sim N(0,I)
\end{align*}
Just draw a bunch of iid $N(0,1)$ RV's and stack them into a vector.
But suppose we want to draw from more complicated MVN RV's with
arbitrary mean and covariance matrix
\begin{align*}
  X \sim N(\mu, \Sigma)
\end{align*}
We do this by the Cholesky decomposition. Namely, since $\Sigma$ is a
covariance matrix, it is positive definite and admits a Cholesky
decomposition $\Sigma=LL^T$. Therefore, to draw $X\sim N(\mu,\Sigma)$
\begin{enumerate}
  \item Draw $Z\sim N(0,I)$ (again, draw iid $N(0,1)$ RVs and stack)
  \item Set $X=\mu+LZ$, which implies $X\sim N(\mu,LL^T)=N(\mu,\Sigma)$.
\end{enumerate}
\end{ex}


\clearpage
\subsubsection{QR Decomposition}


\begin{thm}\emph{(QR decomposition)}
We can express matrix $A\in\Rmn$ as the product of upper-triangular $R$
and orthogonal matrix $Q$ (i.e. $Q^TQ=I_m$):
\begin{align*}
  \underset{(m\times n)}{A\vphantom{Q}}
  =
  \underset{(m\times m)}{Q}
  \underset{(m\times n)}{R\vphantom{Q}}
\end{align*}
\end{thm}

\begin{rmk}
As always, orthogonal matrices have an easy inverse $Q^{-1}=Q^T$, making
it easy to convert coordinate systems. This will be particular useful
for solving a system of equations and the least-squares problem. That's
predominantly why we care about the QR.

On top of that, the QR decomposition also provides useful information
about the range (column space) of $A$. In particular, suppose
we have a tall-matrix, $m>n$, with full-column rank, $\rank(A)=n$. In
this case, the columns of $A$ do not span all of $\Rm$. Rather, they
span subspace $\range(A)\subset \Rm$. Now, write the orthogonal matrix
$Q\in\R^{m\times m}$ as
\begin{align*}
  \underset{(m\times m)}{Q} =
  \begin{pmatrix}
    \underset{(m\times n)}{Q_1}
    & \underset{(m\times m-n)}{Q_2}
  \end{pmatrix}
\end{align*}
By construction, the columns of $Q_1$ serve as a basis for
$\range(A)\subset\Rm$, while the columns of $Q_2$ serve as a basis for
the rest of $\Rm$ that the columns of $A$ do not span. Thus, the QR
decomposition furnishes a set of bases for important subspaces of
$\Rm$ related to $A$.\footnote{%
  Note that, for a tall matrix, the row space isn't really interesting
  since full column rank, $\rank(A)=n$, implies $\Rn$ is spanned
  completely. (The Singular Value Decomposition below handles cases
  where this is not true.) Also, if the matrix is wide $n>m$, everything
  flips: we run the decomposition on $A^T$, $\Rn$ is not spanned by the
  rows of $A$, and the column space $\Rm$ is spanned, hence
  uninteresting.
}
\end{rmk}


\begin{proof}
Main idea: Multiply by a sequence of Householder matrices to produce
upper triangular matrix $R$. The product of the Householder matrices
will also serve as our orthogonal $Q$.
Letting $H_i$ denote the $i$th Householder matrix update and
$p=\min\{m,n\}$, that means
\begin{align}
  R &= H_{n} \cdots H_1 A \label{qrform} \\
  \implies\quad
  Q &= \big(H_{n} \cdots H_1\big)^{-1}
  = H_{1}^{-1} \cdots H_{p}^{-1}
  = H_{1} \cdots H_{p}
  \notag
\end{align}
where we used that $H^{-1}_i=H_i'=H_i$ since Householder matrix $H_i$ is
orthogonal and symmetric.

We just need to construct the correct Householder matrices. To do so,
start with $A$.  We can get all zeros under the diagonal in the
first column if we multiply by Householder matrix
\begin{align*}
  H_1 = I_m - \frac{2}{\lVert u_1\lVert^2}u_1u_1^T
  \qquad \text{where}\quad
  \begin{cases}
    u_1 = a_1 -\alpha e_1 \\
    \text{$a_1$ is first column of $A$} \\
    \alpha = -\text{sign}(A_{11})\cdot \lVert a_1\rVert
  \end{cases}
\end{align*}
To prove that $u_1$ is the correct vector for constructing the Householder
matrix, first compute
\begin{align*}
  \lVert u_1\rVert^2
  =
  \langle u_1, u_1\rangle
  = u_1^T u_1
  &=
  \big(a_1 - \alpha e_1\big)^T
  \big(a_1 - \alpha e_1\big)
  =
  a_1^Ta_1 - \alpha a_1^Te_1 - \alpha e_1^Ta_1 +\alpha^2e_1^Te_1
  \\
  &=
  \lVert a_1\rVert^2
  - \alpha A_{11} - \alpha A_{11} +\alpha^2\cdot 1
  \\
  \implies\quad
  \lVert u_1\rVert^2
  &=
  2\alpha(\alpha- A_{11})
\end{align*}
where I used $\alpha^2 =\lVert a_1\rVert^2$.
Next, we compute the Householder transformation of the first column of
$A$, and verify that it is all zero below the diagonal:
\begin{align*}
  H_1a_1
  = \left(I_m - \frac{2}{\lVert u_1\rVert^2}u_1u_1^T\right)a_1
  &= a_1 - \frac{2}{\lVert u_1\rVert^2}u_1u_1^Ta_1
  = a_1
  - \frac{2}{\lVert u_1\rVert^2}(a_1-\alpha e_1)(a_1-\alpha e_1)^Ta_1
  %\\
  %&= a_1
    %- \frac{2}{\lVert u_1\rVert^2}
    %\big(
      %a_1a_1^T - \alpha a_1e_1^T - \alpha e_1a_1^T
      %+ \alpha^2e_1e_1^T
    %\big)
    %a_1
  \\
  &= a_1
    - \frac{2}{\lVert u_1\rVert^2}
    \big(
      a_1a_1^Ta_1 - \alpha a_1e_1^Ta_1 - \alpha e_1a_1^Ta_1
      + \alpha^2e_1e_1^Ta_1
    \big)
  \\
  &= a_1
    - \frac{2}{\lVert u_1\rVert^2}
    \big(
      \alpha^2a_1
      - \alpha A_{11}a_1
      -\alpha^3 e_1
      + \alpha^2A_{11}e_1
    \big)
  \\
  \implies\quad H_1a_1
  &= a_1
    -
    \big(
      a_1
      - \alpha e_1
    \big)
  =\alpha e_1
\end{align*}
Thus, $H_1A$ turns the first column into $\alpha e_1$, i.e. all zeros
below the diagonal as desired.
This takes care of the first column. After that, we can think of
reducing the remaining lower-right $(m-1)\times (n-1)$ matrix block in a
completely analogous way. In particular, if $A^{(i)}$ represents the
matrix that results after $i$ Householder elimination steps, let
\begin{align}
  a_i =
  \big(
  \underbrace{0,\cdots, 0}_{\text{$i-1$ of them}},
  A_{ii}^{(i-1)}, ,\ldots,A_{mi}^{(i-1)}
  \big)'
  \label{qrai}
\end{align}
That is, take column $i$ of $A^{(i-1)}$ (the matrix after $i-1$
Householder elimination steps) and zero everything out above the $i$th
element. Then construct
\begin{align*}
  H_i = I_m - \frac{2}{\lVert u_i\lVert^2}u_iu_i^T
  \qquad \text{where}\quad
  \begin{cases}
    u_i = a_i -\alpha e_i \\
    \text{$a_i$ as in \ref{qrai}} \\
    \alpha = -\text{sign}\big(A_{ii}^{(i-1)}\big)\cdot \lVert a_1\rVert
  \end{cases}
\end{align*}
It can be shown that applying this $H_i$ as the $i$th Householder step
will zero out everything below the diagonal in column $i$. This is easy
to see since everything in $a_i$ from element $i$ down is exactly as in
the first step above, but just working on the remaining non-reduced
$(m-i+1)\times (n-i+1)$ matrix in the lower right. At the same time, the
zeros above element $i$ in $a_i$ ensure that the resulting Householder
matrix just has a $I_{i-1}$ identity matrix in the upper left corner to
preserve the upper diagonal structure from the previous $i-1$
elimination steps. Do this and then just define $Q$ and $R$ as in the
Expressions~\ref{qrform}.
\end{proof}

\clearpage
\subsubsection{Singular Value Decomposition (SVD)}

The SVD is  useful because it works for \emph{any} matrix, including and
especially rank deficient matrices. Moreover, many properties of a
matrix can be read off directly from its SVD.

\begin{thm}
For any arbitrary $A\in\Rmn$, there exist orthogonal matrices $U\in\Rmm$
and $V\in\Rnn$ plus non-square ``diagonal'' $\Sigma\in\Rmn$ such that
\begin{align*}
  A
  =
  \underset{(m\times m)}{U}
  \underset{(m\times n)}{\Sigma}
  \underset{(n\times n)}{V^T}
\end{align*}
Letting $r=\rank(A)$, we also define
\begin{itemize}
  \item \emph{Left Singular Vectors}: $u_1,\ldots,u_m$, columns of
    orthogonal $U$, form basis for range $\Rm$
  \item \emph{Right Singular Vectors}: $v_1,\ldots,v_n$, columns of
    orthogonal $V$, form basis for domain $\Rn$
  \item \emph{Singular Values}: $\sigma_i=\Sigma_{ii}$ for
    $i=1,\ldots,\min\{m,n\}$. Zero $\forall i> r$, nonzero
    otherwise
\end{itemize}
For $m>n$, and rank $r$ possibly less than $n$, these matrices look like
\begin{align}
  \underset{(m\times n)}{A}
  =
  \underbrace{%
    \begin{pmatrix}
      \underset{(m\times r)}{U_1}
      &
      \underset{(m\times m-r)}{U_2}
    \end{pmatrix}
    \vphantom{%
    \begin{pmatrix}
      \underset{(r\times n)}{V_1^T}
      \\
      \underset{(n-r\times n)}{V^T_2}
    \end{pmatrix}
    }
  }_{(m\times m)}
  \underbrace{%
    \begin{pmatrix}
      \underset{(r\times r)}{\Sigma_1}
      &
      \underset{(r\times n-r)}{0}
      \\
      \underset{(m-r\times r)}{0}
      &
      \underset{(m-r\times n-r)}{0}
    \end{pmatrix}
    \vphantom{%
    \begin{pmatrix}
      \underset{(r\times n)}{V_1^T}
      \\
      \underset{(n-r\times n)}{V^T_2}
    \end{pmatrix}
    }
  }_{(m\times n)}
  \underbrace{%
    \begin{pmatrix}
      \underset{(r\times n)}{V_1^T}
      \\
      \underset{(n-r\times n)}{V^T_2}
    \end{pmatrix}
  }_{(n\times n)}
  =
  \underset{(m\times r)}{U_1}
  \underset{(r\times r)}{\Sigma_1}
  \underset{(r\times n)}{V_1^T}
  \label{svd}
\end{align}
\end{thm}

\begin{rmk}(Intuition)
The SVD makes explicit the geometry of linear transformations. In
particular, if we start with the unit circle in $\Rn$ and multiply every
point of it by $A$, we get an ellipse with axes
$\sigma_1u_1,\ldots,\sigma_mu_m\in\Rm$. Correspondingly, vector
$v_i\in\Rn$ is the vector on the unit circle in domain $\Rn$ that gets
mapped to $\sigma_iu_i\in\Rm$ in the range. To see this, write
\begin{align*}
  Av_i = U\Sigma V^T v_i = U\Sigma e_i
  = U\sigma_i e_i = \sigma_iu_i
\end{align*}
More generally, the SVD breaks up the multiplication of any point $Ax$
into three steps:
\begin{enumerate}
  \item
    Rotation from ``input'' coordinates in $\Rn$ to more
    convenient coordinate system in $\Rn$.

    Since $V\in\Rnn$ orthogonal, we know $V^{-1}=V^T$, multiplication by
    $V$ or $V^T$ induces rotation, and $v_1,\ldots,v_n$ are an
    orthonormal basis for $\Rn$. Therefore, $V^Tx$ (the solution to
    $Vy=x$) converts $x$ from coordinates in the standard basis to basis
    $v_1,\ldots,v_n$.

  \item Stretching and dimension adjustments in this new, more
    convenient coordinate system.

    First, dim. adjustments. If $m>n$ so $A$ and $\Sigma$ tall,
    multiplication $\Sigma V^Tx$ takes $V^Tx\in\Rn$ and spits out
    $\Sigma V^Tx$ in larger $\Rm$ with zeros tacked onto the bottom (to
    account for the space enlargement). If $m<n$ so $A$ and $\Sigma$
    wide, we take $V^Tx\in\Rn$ and spit out $\Sigma V^Tx$ in smaller
    $\Rm$, shutting down some dimensions, projecting onto subspace
    $\Rm\subset\Rn$.

    Next, stretching. Given $V^Tx$ and ``diagonal'' $\Sigma$, aside from
    shutting down dimensions or tacking them on, multiplication $\Sigma
    V^Tx$ will stretch column $i$ of $V^Tx$ by $\Sigma_{ii}$.

  \item Convert to ``output'' coordinates in $\Rm$.
    $U\Sigma V^Tx\in\Rm $ expresses $Ax$ as a linear combination of
    columns $u_1,\ldots,u_m\in\Rm$, with weights given by $\Sigma V^Tx$.
\end{enumerate}
\end{rmk}

\clearpage
\begin{cor}\emph{(SVD Properties)}
Given SVD of matrix $A\in\Rmn$ as in Expression~\ref{svd} with $n\leq
m$, $\rank(A)=r\leq n$, and singular values ordered from largest to
smallest in $\Sigma$
\begin{itemize}
  \item As mentioned, the columns $u_1,\ldots,u_m$ are an orthonormal
    basis for $\Rm$, while the columns $v_1,\ldots,v_n$ are an
    orthonormal basis for $\Rn$
  \item $\range(A)=\range(U_1)=\spann\{u_{1},\ldots,u_r\}$. In words,
    the first $r$ left singular vectors form an orthonormal basis for
    subspace $\range(A)$ of $\Rm$
  \item
    $\range(A)^\perp=\range(U_2)=\spann\{u_{r+1},\ldots,u_m\}$.
    Obviously, this is only relevant if $r<m$.
    In words, if the matrix $A$ is rank-deficient, the last $m-r$ left
    singular vectors form an orthonormal basis for $\range(A)^\perp$,
    the part of $\Rm$ \emph{not} spanned by the columns of $A$.
  \item $\nul(A)=\range(V_2)=\spann\{v_{r+1},\ldots,v_n\}$
    Obviously only relevant if $r<n$.
    In words, any vector $x$ in the span of the last $n-r$ right
    singular vectors is mapped to zero by $Ax$
  \item $\range(A^T)=\range(V_1)=\spann\{v_1,\ldots,v_r\}$.
    In words, the first $r$ right singular vectors form an orthonormal
    basis for the subspace $\range(A^T)$ of $\Rn$.
  \item $\sigma_i = \sqrt{\lambda_i}$ for $i=1,\ldots,r$ where
    $\lambda_i$ is the $i$th largest eigenvalue of $A^TA$.
\end{itemize}
\end{cor}

\begin{defn}(Pseudoinverse from SVD)
Suppose $A\in\Rnn$ is deficient $\rank(A)=r<n$. Then we can
still form a pseudoinverse from its SVD as in Expression~\ref{svd}:
\begin{align*}
  A^+ =
  \big(
  U_1\Sigma_1 V^T_1
  \big)^{-1}
  =
  V_1\Sigma_1^{-1}U_1^T
\end{align*}
This is essentially just the inverse of $A$, throwing out the parts
($U_2$, $V_2$, zero blocks) that pose some trouble because of rank
defficiency.
\end{defn}

\clearpage
\subsubsection{Eigenvalue Decomposition}
\label{subsec:eigenvaluedecomp}

\begin{thm}
\label{thm:eigendecomp}
Matrix $A\in \Rmm$ is diagonalizable if and only if there exist $n$
linearly independent eigenvectors of $A$. Then, we can decompose matrix
$A$ as follows:
\begin{align*}
  A =
  V \Lambda V^{-1}
\end{align*}
where $\Lambda$ is a diagonal matrix of the eigenvalues and $V$ is a matrix
whose columns are the corresponding eigenvectors of $A$.
\end{thm}
\begin{note}
The matrix $\Lambda$ is allowed to have repeated eigenvalues. It's only matrix
$V$ where we require ``distinctness'' by having the eigenvectors be
linearly independent.
\end{note}

\begin{proof}
Suppose that we have eigenvalues $\{\lambda_1,\ldots,\lambda_m\}$ and
corresponding eigenvectors $\{v_1,\ldots,v_m\}$.
Construct $\Lambda$ and $V$ as
\begin{align*}
  \Lambda &= \diag\{\lambda_1, \ldots, \lambda_m\} \\
  V &=
  \begin{pmatrix}
    v_1 & \cdots & v_m
  \end{pmatrix}
\end{align*}
where the eigenvectors of $A$ are used as separate columns in matrix
$V$.  Since we assumed that the eigenvectors are linearly independent,
$V$ is full rank and invertible. Therefore, $V^{-1}$ is well-defined.

So we are left with the task of proving
\begin{align*}
  \Lambda = V^{-1} A V
  \quad\Leftrightarrow\quad
  V\Lambda = A V
\end{align*}
To prove, simply write things out
\begin{align*}
  AV = A
  \begin{pmatrix}
    v_1 & \cdots & v_m
  \end{pmatrix}
  &=
  \begin{pmatrix}
    Av_1 & \cdots & Av_m
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \lambda_1 v_1 & \cdots & \lambda_m v_m
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    v_1 & \cdots & v_m
  \end{pmatrix}
  \begin{pmatrix}
    \lambda_1 & \cdots & 0 \\
    \vdots & \ddots & \vdots \\
    0 & \cdots & \lambda_m
  \end{pmatrix}\\
  \Rightarrow\quad
  AV &= V\Lambda
\end{align*}
\end{proof}

\begin{cor}
$A$ is diagonalizable if and only if, for all $\lambda$ eigenvalues,
\begin{align}
  \label{cond:dimmult}
  \dim(V_\lambda)=m(\lambda)
\end{align}
Repeating condition~\ref{cond:dimmult} in words: if eigenvalue $\lambda$
is a $k$-time repeated root of characteristic polynomial $P_A(\lambda)$,
there are $k$ eigenvectors. Only if this is true for all eigenvalues
will $A$ be diagonalizable.
\end{cor}
\begin{proof}
If this is true, there will be $m$ linearly independent eigenvectors for
$A\in \Rmm$. Then we can diagonalize $A$ according to
Theorem~\ref{thm:eigendecomp}.
\end{proof}


\subsubsection{Schur and QZ Decomposition}

\begin{defn}{(Schur Decomposition)}
For any square matrix $A\in\Rmm$, there exists a matrix $Q$ such that
\begin{align*}
  A = Q U Q^{-1} = Q U Q^*
\end{align*}
where $Q$ is a unitary matrix (hence $Q^{-1} = Q^*$) and $U$ is an upper
triangular matrix with the the eigenvalues of $A$ on the diagonal of
$U$. (This follows from $U$ being similar to $A$.)
\end{defn}

\begin{defn}{(QZ or Generalized Schur Decomposition)}
For square matrices $A$ and $B$, there exist unitary matrices $Q$ and
$Z$ such that
\begin{align*}
  A &= Q S Z^* = QSZ^{-1} \\
  B &= Q T Z^* = QTZ^{-1} \\
\end{align*}
where $S$ and $T$ are upper-triangular matrices.
\end{defn}

\begin{thm}
The $i$th generalized eigenvalue for matrices $A$ and $B$, denoted
satsfies $\lambda_i = S_{ii} / T_{ii}$ where $S$ and $T$ are the upper
triangular matrices from the QZ decomposition.
\end{thm}


\clearpage
\subsubsection{Solving System $Ax=b$ with Matrix Decompositions}

In this subsection, we solve system $Ax=b$ by first doing a matrix
decomposition on $A$ to re-express the system in a form that's more easy
to solve.
Geometrically, we are doing a change of coordinates to a friendlier
basis, solving an easy system (on this friendly basis), then
changing coordinates back again to the original basis.
Moreover, this approach generally avoids forming matrix inverse $A^{-1}$
which could be ill-conditioned.

In this subsection, we assume \emph{square}, \emph{full rank} $A$
so that a unique solution does indeed exist. In the next
subsection, we consider nonsquare or square-but-rank-deficient $A$
matrices.

\begin{prop}\emph{(Solving via LU Decomposition)}
Suppose matrix $A\in\Rmm$ permits $LU$ decomposition $PA = LU$ as in
Expression~\ref{ludecomp}.
Then we can rewrite the system
\begin{align*}
  Ax = b
  \quad\iff\quad
  PAx = LUx = Pb
\end{align*}
We can easily solve this sytem by defining $c=Ux$ and then solving
triangular system (i) forward for $c$, then triangular system (ii)
backward for $x$:
\begin{align*}
  \text{(i)}\quad Lc = Pb
  \qquad
  \text{(ii)}\quad Ux = c
\end{align*}
\end{prop}
\begin{rmk}
This is what Matlab does for mldivide, \texttt{A\textbackslash b}.
The fact that, after the LU decomposition, everything is triangular,
means that the system can be solved very easily.

Geometrically, we should think of this as follows. Vector $c$ is just
vector $x$, but reexpressed in new coordinates after rotation/stretching
by $U$ since $c=Ux$. This convenient change of coordinates gives us a
triangular system $Lc = Pb$ that we can solve very quickly. Then we just
have to solve $Ux = c$ to change back to the original coordinate system.
\end{rmk}

\begin{prop}\emph{(Solving via Cholesky Decomposition)}
Suppose $A\in\Rmm$ symmetric positive definite so that it permits a
Cholesky decomposition. Then we can write the system
\begin{align*}
  Ax = LL^Tx&=b
\end{align*}
We can easily solve this sytem by defining $c=L^Tx$ and then solving
lower triangular system (i) forward for $c$, then upper triangular
system (ii) backward for $x$:
\begin{align*}
  \text{(i)}\quad Lc = b
  \qquad
  \text{(ii)}\quad L^Tx = c
\end{align*}
\end{prop}

\begin{prop}\emph{(Solving via QR Decomposition)}
Given $A\in\Rmm$, can rewrite system
\begin{align*}
  Ax = QRx = b
\end{align*}
Can easily solve system by defining $c=Rx$,
solving system (i) as $c=Q^{-1}b=Q^Tb$ (taking advantage of
$Q$ orthogonal) and then solving backward upper triangular system
(ii) for $x$:
\begin{align*}
  \text{(i)}\quad
  Qc = b
  \qquad
  \text{(ii)}\quad
  Rx = c
\end{align*}
%The easy inverse $Q^{-1}=Q^T$ of (i) and upper triangular form of (ii)
%makes this easy to solve.
\end{prop}

\begin{prop}
\emph{(Solving via SVD)}
Given any $A\in\Rmm$, can rewrite system as
\begin{align*}
  Ax
  = U\Sigma V^Tx
  %= U_1\Sigma_1 V_1^Tx
  = b
\end{align*}
%Since we're assuming $A$ has full column rank in this subsection,
%this implies $U_1=U$,
%$\Sigma_1=\Sigma$, and $V_1=V$ with $\sigma_i\neq 0$ $\forall i$.
Therefore, we can easily solve the system by matrix multiplication
since since $U$, $V$ are orthogonal and
$\Sigma=\diag\{\sigma_1,\ldots,\sigma_m\}$ (hence has simple inverse):
%$\Sigma^{-1}=\diag\{\sigma_1^{-1},\ldots,\sigma_n^{-1}\}$):
\begin{align*}
  x
  = (U\Sigma V^T)^{-1}b
  = V\Sigma^{-1}U^Tb
  %= V_1\Sigma_1^{-1}U_1^Tb
\end{align*}
\end{prop}
\begin{rmk}
This sequence of multiplications used in computing $x$ does three
things:
(1) $U^Tb$ to convert $b$ to basis given by $U$ which spans $\Rm$, (2)
$\Sigma_1^{-1}$ to undo scaling, (3) $V$ to transform out of orthonormal
basis given by $V$ which spans $\Rm$.
\end{rmk}


\clearpage
\subsubsection{Solving System $Ax\approx b$ with Matrix Decompositions}

We now consider solving approximate systems $Ax\approx b$.
Such a system could arise because
\begin{itemize}
  \item \emph{System Overdetermined}: $A\in\Rmn$ is tall, $m>n$.
    For simplicity, we will assume full column rank in the case of tall
    matrices, i.e.  $\rank(A)=n<m$.
  \item \emph{System Square but Rank Deficient}:
    $A\in\Rmm$ has $\rank(A)=r<m$.
\end{itemize}
However in either case, the problem is fundamentally the same:
\begin{align*}
  r < m
  \quad\text{i.e.}\quad
  \rank(A)<\dim(b)
\end{align*}
Consequently, $b\in\Rm$ is not necessarily in the column-space
$\range(A)\subset\Rm$, which is a $r$-dimensional proper subspace of
$\Rm$.  So we instead must find a vector $x$ that gives a linear
combination of columns $Ax\in\range(A)\subset\Rm$ that is as close as
possible $Ax\approx b$, though we generally can't get equality.

Let's take an example. Suppose $A$ tall with $m=2$ and $n=1$. Then we
have two data points, with one independent variable. Letting $a_1$
denote the first (and only) column, we have something like the picture
below.  The black vector is the first (and only) column of independent
variable data, $a_1$. The dashed line behind $a_1$ is $\range(A)$, i.e.
it captures all the points we can get by stretching the single column
$a_1$. Our goal is to choose $x$ so that $Ax\approx b$, i.e. the point
$Ax=a_1x$ on that dashed line as close as possible to dependent
variable $b$.

As an example fitted value, we plot the orange vector.
This particularly attractive choice is the least-squares choice---the
projection of $b$ onto $\range(A)$. The gray vector is the residual that
separates the dependent variable data $b$ from the fitted value
$Ax=a_1x$, the tip of the orange vector. As we see the
residual is orthogonal to $\range(A)$.

\begin{figure}[htbp!]
\centering
\begin{tikzpicture}[xscale=1.5, yscale=1.5, shorten >= -3pt, shorten <= -3pt]
  % Axes
  \draw[d4black, <->] (0,-0.5) -- (0,0) -- (0,2.5);
  \draw[d4black, <->] (-2,0) -- (0,0) -- (3.5,0);

  % a_1 vector and dashed line
  \draw[d4black, thick, dashed] (-1,-0.5) -- (3,1.5);
  \draw[d4black, ultra thick, ->] (0.05,0.05) -- (2,1);
  \node[d4black] at (2.5,0.85) {$a_1$};
  \node[d4black] at (3.7,1.6) {$\range(A)$};

  % Projection of b onto a_1
  \draw[d4orange, ultra thick, ->] (0.05,0.05) -- (1.5,0.75);
  \node[d4orange, thick] at (1.7,0.35) {$\frac{\langle b, a_1\rangle}{\lVert a_1\rVert}a_1$};

  % Residual
  \draw[d4gray, ultra thick, <-] (1.05,1.95) -- (1.55,0.85);
  \node[d4gray] at (1.5,1.5) {$r$};

  % Vector b
  \draw[d4blue, ultra thick, ->] (0.05,0.05) -- (1,2);
  \node[d4gray] at (1,2.3) {$b$};
  \node[d4blue] at (1,2.3) {$b$};

  %\draw[d4blue, ultra thick, domain=0.05:3] plot (\x, {-pow(\x,2)+3.5*\x+1});
  %\draw[d4gray, thick, domain=0.05:2] plot (\x, {1.5*\x+2});
  %\draw[d4gray, thick, dashed] (1,0.1) -- (1,3.4);
\end{tikzpicture}
\end{figure}

From this picture, we see the standard two-step approach
to solving overdetermined or rank defficient systems,
$Ax\approx b$ or (equivalently) $Ax+r=b$:
\begin{enumerate}[label=(\roman*)]
  \item Project $b$ (blue vector) onto the column space
    $\range(A)$\footnote{%
        i.e. the points in $\Rm$ you \emph{can actually} produce from
        linear combinations of columns of $A$
    }
    (dashed line) to get fitted value $Ax$ (orange vector)
  \item Solve for the coefficient vector $x$ that gives this
    projection/fitted value
\end{enumerate}
Notice that the orthogonal residual vector $r$ (gray line) falls right
out of this approach.

\clearpage
Now, we consider what happens if we change $m$ and $n$. First, if we
increase $n$ to $n=m=2$, then we add another column of data to $A$.
Provided that $a_2$ is not parallel to $a_1$, we have a proper basis for
$\R^2$ and we can find coefficients $x$ such that we \emph{perfectly}
fit $Ax=b$. This should be no surprise because $m=n$ with full-rank $A$
implies a square system.

Next, suppose we increase $m=3$ keeping $n=1$. Then that 2-dimensional
picture above must be lifted to 3-dimensions. Again, $\range(A)$ is a
line, and we project $b\in\R^3$ onto it.

Lastly, suppose we introduce another (non-parallel) column to $A$ so
that $m=3$ and $n=2$. In the figure below, we plot an example.
Vectors $a_1$ and $a_2$ represent the $n=2$ columns of $A$, while the
gray triangle defined by those two vectors is part of $\range(A)$.
(Only part because the entire $\range(A)$ is a plane extending
infinitely in each direction out from the triangle. We plot only the
triangle to reduce clutter.)
Therefore, $\range(A)$ is a 2-dimensional subspace of $\R^3$---i.e. it's
a hyperplane. This hyperplane represents the set of all points we could
construct by linearly combining columns of $A$.  Again, $b\in\R^3$ is
likely not on this plane (as shown), in which case we project $b$ onto
that hyperplane $\range(A)$ to find the fitted value, $Ax$, the solve
for the coefficient vector $x$ that linearly combines the columns to get
as close to $b$ as possible while keeping in $\range(A)$.

\begin{figure}[htbp!]
\centering
\tdplotsetmaincoords{70}{15}
\begin{tikzpicture}[scale=2.5,tdplot_main_coords]
    % Define axes
    \draw[d4black,<->] (-0.5,0,0) -- (3,0,0) node[anchor=north west]{$x$};
    \draw[d4black,<->] (0,-0.5,0) node[anchor=north east]{$y$} -- (0,3,0);
    \draw[d4black,<->] (0,0,-0.5) -- (0,0,2.5) node[anchor=south]{$z$};

    \def\x{.5}

    % Draw the vectors a1=(1,3,1) and a2=(3,1,1)
    \draw[ultra thick,->] (0,0,0) -- (1,3,1) node[anchor=south west]{$a_1$};
    \draw[ultra thick,->] (0,0,0) -- (3,1,1) node[anchor=south west]{$a_2$};

    % Draw fitted value
    \draw[d4orange, ultra thick,->] (0,0,0) -- (1.33333,1.33333,0.666666);
    \node[d4orange] at (1.10,1.10,0.40) {$Ax$};

    % Draw vector b=(1,1,2)
    \draw[d4blue, ultra thick,->] (0,0,0) -- (1,1,2);
    \node[d4gray] at (0.7,0.7,1.8) {$b$};
    \node[d4blue] at (0.7,0.7,1.8) {$b$};

    % Draw residual
    \draw[d4gray, ultra thick,->] (1.33333,1.33333,0.666666) -- (1,1,2);
    \node[d4gray] at (1.15,1.15,1.7) {$r$};

    % Draw square to denote right angle
    \draw[d4black] (1.2,1.2,0.60) -- (1.15,1.15,0.80) -- (1.2833333, 1.2833333, 0.8666667);


    % Draw the plane
    \node[d4black] at (1.7,1.7,1) {$\range(A)$};
    \fill[
        %draw=d4gray,%
        fill=d4gray,%
        fill opacity=0.3,%
    ] (0,0,0)
            -- (1,3,1)
            -- (3,1,1)
            -- cycle;
\end{tikzpicture}
\end{figure}

We now introduce a solution method for the least squares problem that
does not entail using the normal equation formula $(A^TA)^{-1}A^Tb$.
Rather than form $A^TA$ (and square things), we will instead just
row-reduce and solve an upper triangular system. This is much more
numerically stable.

\clearpage
\begin{prop}
\emph{(Solving Overdetermined/Least-Squares Problems with QR)}
Suppose we have $A\in\Rmn$ and $b\in\Rm$ for $m>n$.
Then we have an overdetermined system that we can only solve
approximately, $Ax\approx b$. To solve for $x$,
define the residual $r=b-Ax$ and choose
\begin{align}
  x = \argmin_x \;\lVert r\rVert^2 =
  \argmin_x \;\lVert b-Ax\rVert^2
  \label{qrls}
\end{align}
Next, suppose we have the $QR$ decomposition of $A$:
\begin{align*}
  \underset{(m\times n)}{A}
  =
  \underset{(m\times m)}{Q}
  \underset{(m\times n)}{R\vphantom{Q}}
  =
  \underbrace{%
    \begin{pmatrix}
      \underset{(m\times n)}{Q_1}
      & \underset{(m\times m-n)}{Q_2}
    \end{pmatrix}
  }_{(m\times m)}
  \underbrace{%
    \begin{pmatrix}
      \underset{(n\times n)}{R_1}
      \\
      \underset{(m-n\times n)}{0}
    \end{pmatrix}
  }_{(m\times n)}
  =
  \underset{(m\times n)}{Q_1}
  \underset{(n\times n)}{R_1\vphantom{Q_1}}
\end{align*}
Then the solution to Problem~\ref{qrls} can be written as an exactly
identified square problem
\begin{align}
  R_1 x = Q_1^T b
  \label{qrlssoln}
\end{align}
\end{prop}
\begin{rmk}
First, notice we don't need $Q_2$ to solve the least-squares problem via
Problem~\ref{qrlssoln}. We just need $Q_1$ and $R_1$; therefore, in
practice, we don't need the full $QR$ factorization.

Second, note that Problem~\ref{qrls} is just one approach to solving
overdetermined system problem $Ax\approx b$. That overdetermined system
is our original problem, while minimization Problem~\ref{qrls} is simply
a particularly attractive approach to tackling it. We might have chosen
other approaches to choosing an $x$, it's just that this approach has
nice features. For example, since the norm $\lVert\;\cdot\;\rVert$ is a
continuous function, $\lVert b-Ax\lVert$ is a continuous function of
$x$. Moreover, the resulting solution that comes out of solving
Problem~\ref{qrlssoln} has nice geometric properties, as we will discuss
below.
\end{rmk}
\begin{proof}
Since $Q$ and $Q^T=Q^{-1}$ are orthogonal matrices that preserve
length, $\lVert r\rVert = \lVert Q^Tr\rVert$, so we can write our
minimization problem as
\begin{align*}
  \min_x \lVert r\rVert^2
  =
  \min_x \left\lVert Q^Tr\right\rVert^2
  =
  \min_x \left\lVert Q^T(b-Ax)\right\rVert^2
\end{align*}
Replace $A$ with its $QR$ decomposition and simplify $Q^TQ=Q^{-1}Q=I_m$
to get
\begin{align*}
  \min_x \lVert r\rVert^2
  =
  \min_x \left\lVert Q^T(b-QRx)\right\rVert^2
  =
  \min_x \left\lVert Q^Tb-Rx\right\rVert^2
\end{align*}
Now replace $Q$ and $R$ with their block-matrix form.
\begin{align*}
  \min_x \lVert r\rVert^2
  =
  \min_x
  \left\lVert
  \begin{pmatrix}
    Q_1^T \\ Q_2^T
  \end{pmatrix}
  b-
  \begin{pmatrix}
    R_1 \\ 0
  \end{pmatrix}
  x
  \right\rVert^2
  =
  \min_x
  \left\lVert
  \begin{pmatrix}
    Q_1^Tb - R_1x \\
    Q_2^Tb
  \end{pmatrix}
  \right\rVert^2
\end{align*}
Because of the block structure of the matrix, this simplifies nicely to
\begin{align*}
  \min_x \lVert r\rVert^2
  =
  \min_x
  \left\lVert
    Q_1^Tb - R_1x
  \right\rVert^2
  +
  \left\lVert
    Q_2^Tb
  \right\rVert^2
\end{align*}
Notice that the second term is unnaffected by choice of $x$. Therefore,
we can focus on minimizing the first term. But also notice that we can
set the first term to zero \emph{exactly} by back-solving the
square upper-triangular system given by Equation~\ref{qrlssoln}.
\end{proof}


\begin{rmk}({Stability and Conditioning})
Now a word about numerical stability and conditioning concerns when
solving the least squares problem. Recall the two steps to solving
$Ax\approx b$:
\begin{enumerate}[label=(\roman*)]
  \item Project $b$ onto the column space $\range(A)$ to get fitted
    values $Ax$
  \item Solve for the coefficient vector $x$ that gives this
    projection/fitted values
\end{enumerate}
There are possible problems with each step, and they are as follows:
\begin{enumerate}[label=(\roman*)]
  \item
    If $b$ is nearly orthogonal to $\range(A)$, then the projection of
    $b$ onto $\range(A)$ can change a lot in response to small
    perturbations of the $b$ vector. This problem is easy to spot
    though, because $b$ is nearly orthogonal to $\range(A)$ only for
    dumb, terrible regressions where the fit is garbage. So we can
    generally spot the problem.
  \item If the columns of $A$ are highly correlated so that $A$ is
    nearly less than full column rank, it's generally tough to solve for
    the $x$ such that $Ax$ equals the projection (or at least solve for
    a \emph{uniquely} identified $x$). Here, we say the problem is
    \emph{ill-conditioned}. It is precisely these types of issues where
    the QR decomposition is useful.
\end{enumerate}
\end{rmk}


\begin{prop}
\emph{(Solving System with SVD)}
Given any $A\in\Rmn$ that is possibly tall \emph{or}
square-but-rank-deficient, we can compute the SVD and rewrite the system
as
\begin{align*}
  Ax = b
  \quad\iff\quad
  U\Sigma V^Tx
  =
  U_1\Sigma_1 V_1^Tx
  = b
\end{align*}
We can generate a particular solution to this problem
using $U_1,\Sigma_1,V_1$:
\begin{align}
  x = V_1\Sigma_1^{-1}U_1^Tb
  \label{svdsoln}
\end{align}
This is a ``\emph{particular}'' solution because the system does not
have a unique solution.
Because of rank defficiency, $\nul(A)$ is nontrivial and, for any
$y\in\nul(A)$, $Ax=A(x+y)$, implying $x+y$ also a solution.
\end{prop}

\begin{rmk}
Similar to the square full-rank case, this sequence of multiplications
does three things:
(1) $U_1^Tb$ to convert $b$ to basis given by $U_1$ which spans
subspace $\range(A)\subset \Rm$, ignoring components we can't
match anyway, (2) $\Sigma_1^{-1}$ to undo scaling, (3) $V_1$ to
transform out of orthonormal basis given by $V_1$.
\end{rmk}



%\begin{prop}
%\emph{(Solving Undetermined Problems with the QR)}
%Suppose that we have $A\in\Rmn$ and $b\in\Rm$ with $m<n$. Then we have
%an undetermined system $Ax=b$ that has many solutions---we just need to
%pick one.
%\end{prop}






\clearpage
\subsection{Symmetric Matrices}

Symmetric matrices are a very special class of square matrices. They
come up a lot since covariance matrices and Hessian matrices are
symmetric.  Therefore, this section will detail many nice results that
are unique to
symmetric matrices.

%\begin{cor}\emph{(PD = PSD + Invertible)}
%Matrix $A$ is positive definite iff and only if $A$ positive
%\emph{semi}-definite and invertible.
%\end{cor}

\begin{prop}\emph{(Symmetric PD Preservation)}
Suppose $A\in\Rmm$ symmetric positive definite.
Then for any tall matrix $B\in\R^{m\times k}$ ($m\geq k$) with full
column $\rank(B)=k$, matrix $B^TAB\in\Rkk$ will be symmetric positive
definite as well.
\end{prop}
\begin{proof}
Symmetric obvious.
To show positive definiteness, suppose arbitrary $x\neq 0\in\Rk$ given.
We must show that $x^T B^TABx >0$.

To start, define $y:=Bx\in\Rm$.
Since full column $\rank(B)=k$ with $m\geq k$,
vector $x$ cannot linearly combine columns of $B$ to produce the
zero vector.\footnote{%
  With full column rank and $m\geq k$, only $x=0$ can produce $y=Bx=0$,
  but we assumed $x\neq 0$.
}
Hence, $y=Bx\neq 0$ and
\begin{align*}
  x^TB^TABx &= y^TAy > 0
\end{align*}
because $A$ positive definite.
\end{proof}


\begin{thm}\emph{(Eigenvalue Results)}
\label{thm:symmetric-eig}
Given symmetric matrix $A\in\Rmm$,
\begin{enumerate}[label=\emph{(\roman*)}]
  \item
    \emph{(Eigenvalue Decomposition):}
    $A$ is diagonalizable by orthogonal matrix $Q$ such that
    \begin{align*}
      A = Q\Lambda Q^T
    \end{align*}
    The columns $(q_1,\ldots,q_m)$ of $Q$ are eigenvectors of $A$,
    normalized to unit length, while
    $\Lambda=\diag\{\lambda_1,\ldots,\lambda_m\}$ is a diagonal matrix
    of (not necessarily distinct) corresponding eigenvalues.
  \item All eigenvalues of $A$ are \emph{real}
  \item $A$ positive semi-definite iff all eigenvalues nonnegative (and
    real).
  \item $A$ positive definite iff all eigenvalues strictly positive (and
    real).
\end{enumerate}

\end{thm}
\begin{rmk}
Regarding (i),
if $A$ diagonal, $Ax$ just stretches vector $x$ by amount $A_{ii}$ in
the direction of the $i$th Cartesian axis.
Part (i) gives a similar result, but for a different set of axes.  In
particular, if $A$ is symmetric, REesult (i) effectively says that the
columns of $Q$ (which are the unit-length orthogonal eigenvectors) form
a \emph{new special set of axes}. And the multiplication $Ax$ stretches
vector $x$ by eigenvalue $\lambda_i$ in the direction of corresponding
eigenvector (and column of $Q$) $q_i$.
\end{rmk}

\clearpage
\begin{proof}
We prove each in turn:
\begin{enumerate}[label=(\roman*)]
  \item
    By Theorem~\ref{thm:eigendecomp}, we get diagonal representation
    $A=Q\Lambda Q^{-1}$. The only new thing is showing that symmetric
    $A$ additionally gives us $Q$ orthogonal, i.e.  mutually orthogonal
    columns $\langle q_i,q_j\rangle=0$ for all $i\neq j$.
    So start by writing
    \begin{align}
      \lambda_j \langle q_i, q_j \rangle
      =
      \langle q_i, \lambda_j q_j \rangle
      = q_i^T (\lambda_j q_j)
      = q_i^T (A q_j)
      \label{proof:symmetric-eig-1}
    \end{align}
    Lastly equality from the fact that $q_j$ is the $j$th eigenvector
    with eigenvalue $\lambda_j$ so that $Aq_j=\lambda_j q_j$. By
    symmetry, $A=A^T$ so can rewrite last element in the above chain
    with
    \begin{align}
      q_i^T (A q_j)
      = q_i^T (A^T q_j)
      %= (q_i^T A^T) q_j
      = (Aq_i)^T q_j
      = (\lambda_i q_i)^T q_j
      = \langle \lambda_i q_i, q_j \rangle
      = \lambda_i \langle q_i, q_j \rangle
      \label{proof:symmetric-eig-2}
    \end{align}
    Stringing together Chains~\ref{proof:symmetric-eig-1}
    and~\ref{proof:symmetric-eig-2}, we have
    \begin{align*}
      \lambda_j \langle q_i, q_j \rangle
      = \lambda_i \langle q_i, q_j \rangle
      \quad\implies\quad
      (\lambda_j-\lambda_i) \langle q_i, q_j \rangle
      = 0
    \end{align*}
    This proves the result, hence $Q$ is an orthogonal matrix.
  \item

  \item
    We will work with and refer to the Part (i) decomposition of $A$.
    Define also the matrix
    $\sqrt{\Lambda}=\diag\{\sqrt{\lambda_1},\ldots,\sqrt{\lambda_m}\}$
    which satisfies $\sqrt{\Lambda}\sqrt{\Lambda}=\Lambda$.
    This allows us to write
    \begin{align*}
      A &= Q^T \sqrt{\Lambda} \sqrt{\Lambda} Q
      = \left(\sqrt{\Lambda} Q\right)^T \sqrt{\Lambda} Q
    \end{align*}
    Given any $x\in\Rm$, positive (semi-)definiteness of $A$ turns on
    the behavior of
    \begin{align}
      x^T A x &=
      x^T Q^T \sqrt{\Lambda} \sqrt{\Lambda} Q x
      = z^Tz
      \qquad\text{where}\quad
      z := \sqrt{\Lambda}Qx\in\Rm
      \label{proof:symmetric-eig-3}
    \end{align}
    ($\Rightarrow$)
    To draw a contradiction, suppose $A$ is positive semi-definite, but
    there exists a strictly negative eigenvalue, $\lambda_i<0$.
    Then choose $x=Q^{-1}e_i = Q^Te_i$ (which follows since $Q$
    orthogonal). Then
    \begin{align*}
      z=\sqrt{\Lambda}Qx
      =\sqrt{\Lambda}QQ^Te_i
      =\sqrt{\Lambda}e_i
      = \sqrt{\lambda_i}e_i
      \quad\implies\quad
      z^Tz = \lambda_i<0
    \end{align*}
    But since $z^Tz=x^TAx$ by Expr.~\ref{proof:symmetric-eig-3},
    contradicts $A$ positive semidefinite assumption.

    ($\Leftarrow$)
    Since all eigenvalues assumed nonnegative for this direction,
    $\sqrt{\Lambda}$ is a matrix of only real (rather than complex)
    numbers. Therefore, $z = \sqrt{\Lambda} Qx$ is a real vector in
    $\Rm$ given any $x\in\Rm$, implying that
    $z^Tz\geq 0$.
    By Expression~\ref{proof:symmetric-eig-3}, that's equivalent to
    $x^TAx\geq 0$, which is precisely the definition of positive
    semidefinite.

  \item
    We use a setup similar to the proof of Part (iii),
    and examine $z^Tz=x^TAx$, now assuming additionally $x\neq 0$.

    ($\Rightarrow$)
    To draw a contradiction, suppose $A$ positive definite but there
    exists a $\lambda_i=0$.\footnote{%
      We already know that $\lambda_i\geq 0$ by Part (iii), so this is
      the only possibility.
    }
    Take $x=Q^{-1}e_i=Q^Te_i$ (which exists because $Q$ orthogonal hence
    invertible). Then
    \begin{align*}
      z
      = \sqrt{\Lambda}Qx
      = \sqrt{\Lambda}QQ^Te_i
      = \sqrt{\Lambda}e_i
      = \sqrt{\lambda_i}e_i
      \quad\implies\quad
      z^Tz = \lambda_i = 0
    \end{align*}
    But since $z^Tz=x^TAx$ and $x=Q^Te_i\neq 0$, contradicts $A$
    positive definite assumption.

    ($\Leftarrow$)
    Assuming all eigenvalues strictly positive means that
    $\sqrt{\Lambda}$ is a matrix of strictly positive real numbers.
    Therefore, $z=\sqrt{\Lambda}Qx$ is a real nonzero vector in $\Rm$
    (provided $x\neq 0$).
    Hence $z^Tz=x^TAx>0$, which is the definition of positive definite.
\end{enumerate}
\end{proof}


\clearpage
\subsection{Quadratic Forms}

Quadratic functions map $f:\R\rightarrow\R$ and look like $f(x)=ax^2 +
bx+c$. Quadratic forms are a generalization that expand the domain from
$\R$ to $\Rn$.

\begin{defn}{(Quadratic Form)}
A \emph{quadratic form} is a function $f:\Rn\rightarrow\R$ of the form
\begin{align*}
  f(x) = (x-b)^TA(x-b)
  \qquad x,b\in\Rn \quad A\in\Rnn
\end{align*}
\end{defn}

Now this is a very short subsection, but we'll see in later sections
that quadratic forms are extremely useful for characterizing convex
functions and differentiability via function approximations.


\clearpage
\subsection{Principal Component Analysis (PCA)}

In this section, we look for factor structure in data arranged into data
matrix $X\in\R^{n\times k}$ where $n>k$ is the number of observations
and $k$ is the number of features/covariates per observation.  Assume
WLOG that the columns are mean zero (an innocuous normalization).

PCA uses information in the covariance matrix.  Given observations
$\{X_{i,\cdot}\}_{i=1}^n$, that means studying population and sample
covariance matrices, respectively\footnote{%
  Assumes unconditional covariance $\Omega=\Cov(X_{i,\cdot})$ exists.
  Cov. stationarity of underlying process sufficient.
}
\begin{align*}
  \Omega = \Cov(X_{i,\cdot})
  = \E[X_{i,\cdot}X_{i,\cdot}^T]
  \qquad
  \hat{\Omega}
  = \frac{1}{n}\sum_{i=1}^n X_{i,\cdot}X_{i,\cdot}^T
  = \frac{X^T X}{n}
\end{align*}
Both $\Omega$ and $\hat{\Omega}$ are (i) symmetric and (ii) positive
semidefinite, facts we will refer to below.

\begin{defn}(PCA from Eigenvalue Decomposition)
Start with population covariance matrix $\Omega$.  By (i), matrix
$\Omega\in\R^{k\times k}$ has eigenvalue decomposition
\begin{align*}
  \Omega = Q\Lambda Q^T
  \qquad\text{where}\quad
  Q=(q_1,\ldots,q_k),
  \quad \Lambda = \diag\{\lambda_1,\ldots,\lambda_k\}
\end{align*}
Eigenvalues $\lambda_j$ in $\Lambda$ are all nonnegative by (ii) and
assumed ordered largest to smallest.
Columns $q_j$ of $Q$ are the corresponding eigenvectors that are also
orthogonal by (i).

From the orthogonal columns $q_j$, can form matrix $Y\in\R^{n\times k}$
of \emph{principal components} (PCs) where column $Y_{\cdot,j}$ is the
$j$th PC of the data in $X$
\begin{align}
  Y := XQ
  \quad\implies\quad
  X = YQ^T
  \label{PCA}
\end{align}
If we don't know population covariance matrix $\Omega$ (and we often
don't), we instead take an eigenvalue decomposition of
$\hat{\Omega}=X^TX/n$ and construct PCs with the resulting sample
$\hat{Q}$.
\end{defn}


\begin{defn}(Sample PCA from SVD)
One problem with doing PCA on sample $\hat{\Omega}$:
If you take an eigenvalue decomposition of $\hat{\Omega}=X^TX/n$, you
need to compute $X^TX$, effectively ``squaring'' the matrix and making
it less stable numerically. Can instead take an SVD:
\begin{align*}
  X = U\Sigma V^T
\end{align*}
From there, can compute $X^TX$ and use orthogonality of $U$ so
$U^TU=I_n$ to get
\begin{align*}
  X^TX
  = \big(U\Sigma V^T\big)^TU\Sigma V^T
  = V\Sigma^T U^TU\Sigma V^T
  = V\Sigma^2 V^T
\end{align*}
Evidently, $\Sigma^2=n\Lambda$ and $V=Q$ where $\Lambda$ and $Q$ are
from the Eigenvalue Decomposition approach above. Therefore, rather than
compute the eigenvalue decomposition of $X^TX/n$ to get weights/loadings
$Q$, we could alternatively just compute the SVD of $X$ to get $V=Q$,
which is a more numerically stable procedure overall.
\end{defn}

\clearpage
What's so great about PCA anyway?
Let's discuss, assuming we have the orthogonal matrix
$Q=(q_1,\ldots,q_k)$ and the diagonal $\Lambda$ from the eigenvalue
decomposition of the \emph{population} covariance matrix $\Omega$. We
conclude with a discussion of the sample counterpart.
\begin{enumerate}[label=(\roman*)]
  \item
    \emph{PCs Uncorrelated}:
    First, $Y$ is simply a linear transformation of data $X$ onto new
    axes---those defined by the orthogonal eigenvectors of $\Omega$.
    Useful because the population covariance matrix of the
    \emph{transformed} data (transformed with fixed/nonstochastic
    population weights $Q$) is simply the \emph{diagonal} matrix
    $\Lambda$:
    \begin{align}
      \Cov(Y_{i,\cdot}) = \Cov(XQ) = Q^T\Cov(X)Q=
      Q^T \Omega Q = \Lambda
      \label{pcacov}
    \end{align}
    That's huge: PCs are \emph{uncorrelated} with variances given by the
    eigenvalues of $\Omega$.

  \item \emph{Variance Decomposition}:
    Matrix $\Lambda$ effectively decomposes total variance into the
    contributions of the $k$ orthogonal PCs. Since $\Lambda$ sorted
    eigenvalues smallest to largest,
    \begin{align}
      \text{(Variance ``Explained'' by first $p$ PCs)}
      = \frac{\sum_{i=1}^p \lambda_i}{\sum_{i=1}^k\lambda_i}
      \label{pcacovfrac}
    \end{align}

  \item
    \emph{Easy to compute}.
    Cols $q_j$ are both the \emph{weights} used to construct the $j$th
    PC from $X$
    \begin{align*}
      Y_{\cdot,j} = X\;q_j
      \qquad \text{for}\quad j=1,\ldots,k
    \end{align*}
    as well as the \emph{loadings} of observation vector $X_{i,\cdot}$
    on PCs $\{Y_{i1},\ldots,Y_{ik}\}$
    \begin{align}
      X_{i,\cdot} = Y_{i,\cdot}\;Q^T
      = q_1^T \,Y_{i1} + \cdots + q_k^T \,Y_{ik}
      \label{pcaloadings}
    \end{align}
    Column $q_j$ gives how much the observation vector $X_{i,\cdot}$
    moves in response to a change in the (scalar) $j$th PC $Y_{ij}$.
    (Note this loading is independent of $i$, i.e. assumed constant
    across observations $i$.)

  \item
    \emph{Data compression}: By Equations.~\ref{pcacovfrac}
    and~\ref{pcaloadings}, if the first $p$ PCs capture/``explain'' most
    variance, we can approximate
    \begin{align}
      X \approx
      Y_{\cdot,1}q_1^T + \cdots + Y_{\cdot,p}q_p^T
    \end{align}
\end{enumerate}
Lastly, sample PCA computed from $\hat{\Omega}$.
Everything goes through with the qualification
that we have \emph{sample} weights/loadings that give \emph{sample} PCs
that are orthogonal \emph{in sample} and explain \emph{sample} variance
in proportion to the computed \emph{in-sample} eigenvalues.



\clearpage
\section{Differential Equations}

\subsection{Solving Linear System of Differential Equations}

\begin{prop}
\emph{(Solution to Linear System)}
\label{prop:sysdiffeq}
Suppose we have the following system
\begin{align*}
  \dx(t) = Ax(t)
  \qquad x(t)\in\Rn
\end{align*}
If $A$ permits eigenvalue decomp $A=V\Lambda V^{-1}$
for $\Lambda=\diag\{\lambda_1,\ldots,\lambda_n\}$ and
$V=(v_1,\ldots,v_n)$,
then the system has solution
\begin{align*}
  x(t) = \sumin c_i e^{\lambda_i t}v_i
  \quad\text{where}\quad
  x_0=\sumin c_iv_i
\end{align*}
where $x(0)=x_0$ is an initial condition necessary to pin down unknown
constants $c_i$.

Suppose also that the first $k\leq n$ eigenvalues of $A$ have negative
real parts: $\Re(\lambda_i)<0$ for $i=1,\ldots,k$.\footnote{%
  That they are the \emph{first} $k$ is wlog.
}
Then there exists a $k$-dimensional subspace of $\Rn$ denoted $X_0$
such that
\begin{align*}
  x(0)\in X_0
  :=
  \left\{
    x_0\in\Rn \;\big|\;
    x_0 = \sum_{i=1}^k c_i v_i
  \right\}
  \quad\implies\quad
  \limt x(t) = 0\in\Rn
\end{align*}
In words, if solution $x(t)$ starts at any $x_0\in X_0$, it converges to
zero.
And $x_0\in X_0$ iff it can be written as the sum of the
first $k$ eigenvectors, which have eigenvalues satisfying
$\Re(\lambda_i)<0$ by assumption.
So we rule out initial conditions $x_0$ having any component pointing in
the direction of eigenvectors $v_j$ where $\Re(\lambda_j)\geq 0$.
Since generally $X_0\subsetneq \Rn$, this is a nontrivial
restriction on the set of initial conditions $x_0$ compatibile with
convergence of solution $x(t)\ra 0$.
\end{prop}
\begin{rmk}
Given $k\leq n$---number of eigenvalues with $\Re(\lambda_i)<0$---let
$m$ be the number of predetermined state variables in a model.
Three possibilities re: convergence to steady state
\begin{itemize}
  \item $k=m$, \emph{Saddle Path Stable}:
    Standard, with unique optimal trajectory to steady state where
    negative eigenvalues determine the speed of convergence
  \item $k< m$, \emph{Unstable}: $x(t)$ does not converge to steady state
  \item $k>m$, \emph{Indeterminate}: Multiple optimal trajectories
\end{itemize}
\end{rmk}

\begin{proof}
%(Proposition~\ref{prop:sysdiffeq})
Using eigenvalue decomposition $A=V\Lambda V^{-1}$, we can rewrite the
system
\begin{align*}
  V^{-1}\dx(t) = \Lambda V^{-1}x(t)
  \quad\iff\quad
  \dz(t) = \Lambda z(t)
  \qquad \text{where} \; z(t) := V^{-1}x(t),
\end{align*}
This system in $z(t)$ is \emph{diagonal}. Therefore, easy
equation-by-equation solution:
\begin{align*}
  z_i(t) =
  c_i e^{\lambda_i t}
  %\begin{pmatrix}
    %c_1 e^{\lambda_1 t} \\
    %\vdots \\
    %c_n e^{\lambda_n t} \\
  %\end{pmatrix}
  \quad\implies\quad
  x(t) = V z(t)
  = (v_1\;\ldots\;v_n) z(t)
  = \sumin c_i e^{\lambda_i t}v_i
\end{align*}
When does solution $z(t)\ra 0$?
Since eigenvalues could be complex, write out $i$th component
\begin{align*}
  z_i(t)
  =
  c_i e^{\lambda_it}
  =
  c_i e^{[\Re(\lambda_i) + i\Im(\lambda_i)]t}
  &=
  c_i e^{\Re(\lambda_i)t} \cdot e^{i\Im(\lambda_i)t}
  %\\
  %\implies\quad
  %z_i(t)
  %&=
  =
  c_i e^{\Re(\lambda_i)t}
  \big[
  \cos\big(\Im(\lambda_i) t\big) + i \sin\big(\Im(\lambda_i) t\big)
  \big]
\end{align*}
Note that the part in brackets is oscillatory and never dies out.
So to have $z_i(t)\ra 0$, we need either $c_i$ or $e^{\Re(\lambda_i)t}$
to go to zero and kill the oscillations. Said another way,
\begin{align}
  z(t)
  \ra 0\in\Rn
  \quad&\iff\quad
  \big(\quad
  \Re(\lambda_i) \geq 0
  \;\implies\; c_i=0
  \quad\big)
  \label{diffeqconvcond}
  %\\
  %\quad&\iff\quad
  %c_i=0
  %\quad\forall i>k
\end{align}
And clearly $z(t)=V^{-1}x(t)\ra 0$ iff $x(t)\ra 0$.
So (\ref{diffeqconvcond}) holds if we replace $z(t)$ with $x(t)$.
And if solution $x(t)$ satisfies this $\forall t$, must also satisfy for
$x(0)$.
Hence, the construction of $X_0$.
\end{proof}


\subsection{Solving Non-Linear System of Differential Equations}

Even though this subsection is about solving nonlinear systems, we solve
them by linearizing about steady state, then applying
Proposition~\ref{prop:sysdiffeq} to analyze the linearized system.

\begin{prop}\emph{(Approximate Solution to Nonlinear System)}
Suppose we have system
\begin{align*}
  \dx(t) = m(x(t))
  \qquad\text{where} \quad
  m:\Rn\ra\Rn
\end{align*}
Let $x^*$ be the steady state satisfying $0=m(x^*)$.
Then we can write a first-order linear approximation of the system in
terms of deviations from steady-state $\hat{x}(t):=x_t-x^*$ as
\begin{align*}
  \dot{\hat{x}}(t) = M\hat{x}(t)
  \qquad\text{where}\quad
  M:= \frac{\partial m(x^*)}{\partial x'}
\end{align*}
where $M$ is the Jacobian matrix of $m$, evaluated at steady state
$x^*$.
\end{prop}
\begin{proof}
Do a first-order Taylor expansion about $x^*$:
\begin{align*}
  \dx(t)\approx
  m(x^*)
  +
  \frac{\partial \dx(t)}{\partial x(t)'}
  (x(t)-x^*)
  =
  0 + M\hat{x}(t)
\end{align*}
where we used $m(x^*)=0$ (by definition of steady state)
and the definition of $M$. To conclude, simply notice that
$\dot{\hat{x}}(t)=\frac{d}{dt}[x(t)-x^*]=\dot{x}(t)$.
\end{proof}

\subsection{Cookbook Appraoch to Solving System of ODEs}

Given arbitrary nonlinear system:
\begin{enumerate}[label=(\roman*)]
  \item Gather/stack all differential equations in the system and write
    them in the form $\dx(t)=m(x(t))$.
    Write out vector-valued function $m:\Rn\ra\Rn$ as explicitly as
    possible.
  \item Set $\dx(t)=0$ and solve $m(x^*)=0$ for steady state
  \item Given $m$ and $x^*$, compute $M=\frac{\partial m(x^*)}{\partial
    x'}$, the Jacobian matrix evaluated at steady state
  \item Solve linear system $\dot{\hat{x}}(t)=M\hat{x}(t)$
    using Proposition~\ref{prop:sysdiffeq}.
    In the process, look at the eigenvalues of $M$ to analyze stability
    and determine which constants $c_i$ in the solution must be set to
    zero.
\end{enumerate}

\clearpage
\section{Metric Spaces and Sequences}

Take any arbitrary set $X$. Metric spaces are simply some pair $(X,d)$
of an arbitrary set and some distance function $d$ that tells you ``how
close'' any two points in $X$ are. By defining the concept of
``closeness'', we can then talk about convergence of sequences within
$X$.

The most common space is $\R^k$; however, the treatment below
will be sufficiently general to capture any set $X$ for which a distance
function or \emph{metric} (satisfying certain properties) can be
defined.

\subsection{Supremum, Infimum, and Completeness of $\R$}


\begin{defn}{(Supremum)}
\label{defn:supdef1}
Given a set $A\subseteq X$, the \emph{supremum} denoted $\sup(A)$ is the
smallest possible upper bound for $A$.
More precisely, the supremum satisfies the following two properties
\begin{enumerate}
  \item $a\leq \sup(A)$ for all $a\in A$.
  \item If $x<\sup(A)$, then $x\in X$ is not an upper bound for $A$.
    Otherwise, we would have just taken $x$ as the supremum.
\end{enumerate}
It might be in $A$, in which case $\sup A = \max A$, or it might be
$X\setminus A$, in which case $\max A$ is undefined.
\end{defn}

\begin{ax}{(Axiom of Completeness)}
\label{ax:completeness}
Suppose that $A\subset \R^n$ is bounded above and $A\neq
\emptyset$. Then $\sup A$ exists and is finite.
\end{ax}

\begin{defn}{(Space of Bounded Functions)}
Here's a more general space than simple $\R^n$: the set of all
bounded functions in $\R^n$,
\begin{align*}
  \mathscr{B}(A,\R^n)
  :=
  \left\{
    f:A\rightarrow \R^n \; \big| \; \sup_{x\in A} |f(x)|<\infty
  \right\}
\end{align*}
Notice that this is a lot tougher to visualize than $X=\R^n$.
Here, elements of $X$ are functions, not vectors.

Notice also that this is a vector space, as
\begin{align*}
  (f+g)(x) &= f(x) + g(x)\\
  (cf)(x) &= cf(x)
\end{align*}
\end{defn}



\clearpage
\subsection{Metrics, Norms, and Metric Spaces}

\begin{defn}{(Metric Space)}
\label{defn:metric}
A \emph{metric space} is a pair $\mathscr{M}=(X,d)$ where $X$ is some
space and $d:X\times X\rightarrow \R$ is a function
called a \emph{metric} that satisfies the following conditions for all
``points'' $x,y,z\in X$
\begin{enumerate}
  \item \emph{Non-negativity}: $d(x,y)\geq 0$
  \item \emph{Identification}: $d(x,y) = 0 \quad \Leftrightarrow \quad x=y$
  \item \emph{Symmetry}: $d(x,y)=d(y,x)$
  \item \emph{Triangle Inequality}: $d(x,y) \leq d(x,z) + d(z,y)$.
\end{enumerate}
Identification requires that distinct points have positive distance.
The triangle inequality enforces the idea that the distance function
represents the \emph{shortest path} between points, i.e.\ you can't have
an intermediate pit stop somewhere that reduces the total trip length.
\end{defn}

\begin{defn}{(Metric Subspace)}
$(Y,d_Y)$ is a metric \emph{subspace} of metric space $(X,d_x)$ if and
only if
\begin{enumerate}
  \item $Y\subseteq X$
  \item $d_Y(x,y) = d_X(x,y)$ for all $x,y$.
\end{enumerate}
\end{defn}

\begin{ex}{(Distance Functions in $\R^n$)}
There are some standard distance functions in $\R^n$ that show
up a lot and have a nice geometric representation. So for $x,y\in
\R^n$, define
\begin{enumerate}
  \item Manhattan Taxi Distance
    \begin{align*}
      d_1(x,y) = \sum^n_{i=1} |x_i - y_i|
    \end{align*}

  \item Standard Euclidean Distance
    \begin{align*}
      d_2(x,y) = \sqrt{\sum^n_{i=1} (x_i - y_i)^2}
    \end{align*}

  \item $L_p$ distance
    \begin{align*}
      d_p(x,y) = \left(\sum^n_{i=1} |x_i - y_i|^p\right)^{1/p}
    \end{align*}

  \item ``max'' distance
    \begin{align*}
      d_\infty(x,y) = \max_{i} |x_i-y_i|
    \end{align*}

\end{enumerate}
\end{ex}
\begin{rmk}
The ``max'' distance is useful when $x\in\R^n$ doesn't represent
real location. Rather, $n$-dimensional $x$ might represent $n$ different
attributes or characteristics of some object (like height, weight, hair
length, etc.) that aren't directly comparable. In that case, to compare
individuals, you might just define the ``distance'' between two people
by looking at all these attributes and seeing where the differ most
sharply.
\end{rmk}


In practice, distance functions are commonly built up from \emph{norms},
which are special functions satisfying certain properties. Those
properties ensure that norms can be interpreted as measuring the
\emph{length} of vectors in some vector space.
As a result, we can \emph{always} use a norm to \emph{construct} a
metric on a vector space by setting the distance between two
points/vectors equal to the length of the vector connecting them, as we
see below.
That's all we need for now. We reserve further discussion, intuition,
and examples of norms to later Section~\ref{sec:banachhilbert} where we
delve much deeper into vector spaces, norms, inner products, and Banach
and Hilbert spaces. In this section, however, norms are no more than a
special function offering a means to a metric, full stop.

\begin{defn}(Norm)
\label{defn:norm}
Given a space $X$, a function $\lVert\,\cdot\,\rVert:X\rightarrow \R$ is
called a \emph{norm} if it satisfies the following properties:
\begin{enumerate}
  \item $\lVert x\rVert\geq 0$
  \item $\lVert x\rVert=0 \quad \iff \quad x=0$
  \item $\lVert \alpha x\rVert= |\alpha| \cdot \lVert x\rVert$
  \item $\lVert x+y\rVert \leq \lVert x\rVert + \lVert y\rVert$
\end{enumerate}
\end{defn}

\begin{prop}
If $\lVert \,\cdot\,\rVert$ is a norm, then $d:X \times X\rightarrow \R$
defined as $d(x,y):=\lVert x-y\rVert$ is a valid metric for space $X$.
\end{prop}
\begin{proof}
Must show that this induced $d$ satisfies the four properties of
Definition~\ref{defn:metric}:
\begin{enumerate}
  \item Non-negativity: Trivial by Property 1 of
    Definition~\ref{defn:norm}.
  \item Identification: ($\Leftarrow$) $x=y\implies d(x,y)=\lVert
    x-y\rVert=\lVert 0\rVert=0$ by Prop. 2 of Defn.~\ref{defn:norm}.

    ($\Rightarrow$) Suppose $0 = d(x,y) =: \lVert x-y\rVert$. Prop. 2
    of Defn.~\ref{defn:norm} says $x-y=0$, hence $x=y$.

  \item Symmetry: Take
    \begin{align*}
      d(x,y)= \lVert x-y\rVert= \lVert -(y-x)\rVert
    \end{align*}
    By Property 3 of
    Definition~\ref{defn:norm}, we can pull out the negative sign to get
    \begin{align*}
      d(x,y)= \lVert -(y-x)\rVert =|-1| \cdot \lVert y-x\rVert = d(y,x)
    \end{align*}
  \item Triangle Inequality: Take some $z$, then by Property 4 of
    Definition~\ref{defn:norm}
    \begin{align*}
      d(x,y) = \lVert x-y\rVert
      = \lVert (x-z)+ (z-y) \rVert
      &\leq \lVert x-z\rVert +\lVert z-y\rVert
      \\
      \Leftrightarrow\qquad
      &\leq d(x,z) + d(z,y)
    \end{align*}
\end{enumerate}
\end{proof}


\clearpage
\subsection{Sequences, Subsequences, Limits, and Convergence}

Now that we have defined the notions of metrics and metric spaces, which
let us talk about ``closeness'' in sets, we present a general treatment
of sequences and convergence within a metric space.

Now for some conventions that this section will use unless othwerwise
noted.  When dealing with sequences, this section will assume that the
sequence $\{x_n\}$ lives in a metric space $(X,d)$. Any functions in
this section are assumed to be of the form $f:(X,d_X)\rightarrow
(Y,d_Y)$.

\begin{defn}{(Sequence)}
A \emph{sequence} in space $X$ is an enumeration of points in $X$,
denoted $\{x_n\}_{n=1}^\infty$ and often abbreviated $\{x_n\}$.
Equivalently, it is a function from the natural numbers to points in the
space $X$, i.e.  $x:\mathbb{N}\rightarrow X$. There can be repetitions.
\end{defn}

\begin{defn}{(Subsequence)}
Given a sequence $\{x_n\}_{n=1}^\infty \subseteq X$ and an increasing
sequence $\{n_k\}_{k=1}^\infty \subseteq \mathbb{N}$, we call the
sequence $\{x_{n_k}\}_{k=1}^\infty$ a \emph{subsequence} of
$\{x_n\}_{n=1}^\infty$.
\end{defn}

\begin{defn}{(Convergence)}
\label{defn:convergence}
A sequence $\{x_n\}\subseteq X$ \emph{coverges}  if there is a point
$x\in X$ (emphasis on ``in $X$'') such that, for any $\varepsilon>0$,
there exists an $N$ where
\begin{align*}
  n > N \quad \Rightarrow\quad
  d(x_n,x) < \varepsilon
\end{align*}
Convergence is often abbreviated $x_n\rightarrow x$ or
$\{x_n\}\rightarrow x$ and $x$ is called the \emph{limit point}.
\end{defn}

\begin{ex}
Importantly, the definition of convergence requires that the
limit point $x$ is in the space $X$. So if we consider metric space
$\mathscr{M}=((0,2),d_1)$, the sequence
\begin{align*}
  \{x_n\} = \{1/n\}
\end{align*}
does not converge since the limit $0$ is not in the space $(0,2)$.
\end{ex}

\begin{note}
Here's a general recipe for proving that a sequence $\{x_n\}$ converges
to some point $x$.

Given $\varepsilon$ and limit $x$, simply write out write out $d(x_n,x)$
for arbitrary $x_n$. Try to bound that expression for the distance as
\begin{align*}
  d(x_n,x) < G(n,x)
\end{align*}
where $G$ is some function that depends only upon $x$ and $n$. If that's
possible, then equate $G(n,x)=\varepsilon$ and solve for $n$. Set the
result as your $N$.

This recipe emphasizes that our choice of $N$ should depend only upon
$\varepsilon$ and $x$, the two ``givens'' within the problem.
\end{note}

\begin{defn}{(Pointwise and Uniform Convergence of Functions)}
Consider the space of functions $X=\{f:A\rightarrow B\}$.  Consider a
sequence of functions $\{f_n\}\subseteq X$.  We say that
$\{f_n\}\rightarrow f^* \in X$ \emph{pointwise} if
\begin{align*}
  \lim_{n\rightarrow \infty} f_n(x) = f^*(x)
  \qquad \forall x\in A
\end{align*}
We say that $\{f_n\}\rightarrow f^*$ \emph{uniformly} if
\begin{align*}
  \lim_{n\rightarrow \infty} \sup_{x\in A}
  |f_n(x)-f(x)| = 0
\end{align*}
\end{defn}
\begin{prop}
\label{prop:pwunif}
Uniform convergence of functions implies pointwise convergence of
functions (though the converse does not hold in general).
\end{prop}
\begin{rmk}
This proposition implicitly contains some practical advice.
Specifically, if a sequence of functions does converge, it converges to
the pointwise limit. So when checking uniform convergence, your
candidate function will \emph{always} be the pointwise limit. You only
have to check if a sequence converges uniformly to this single function
rather than the infinite number of functions that exist.
\end{rmk}

\begin{prop}{\emph{(Uniqueness of Limits)}}
If a sequence $\{x_n\}\rightarrow x$ and $\{x_n\}\rightarrow x'$,
then $x=x'$.
\end{prop}
\begin{proof}
Suppose that $\{x_n\}\rightarrow x$ and $\{x_n\} \rightarrow x'$ such
that $x\neq x'$. Then given $\varepsilon>0$, there exists a $N_x$ and
$N_{x'}$ such that
\begin{align*}
  n > N_x
  &\quad\Rightarrow\quad
  d(x_n,x) \leq \frac{\varepsilon}{2} \\
  n > N_{x'}
  &\quad\Rightarrow\quad
  d(x_n,x') \leq \frac{\varepsilon}{2}
\end{align*}
Now take $n>\max\{N_x, N_{x'}\}$ and use the triangle inequality to get
\begin{align*}
  d(x,x') \leq d(x,x_n) + d(x_n, x')
  < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align*}
Since this is true for all $\varepsilon$, we must have $d(x,x')=0$ which
means $x=x'$.
\end{proof}

\begin{prop}{\emph{(Close Together Sequences)}}
Consider two sequences $\{x_n\}$ and $\{y_n\}$.
Suppose that one sequence converges to some limit and that the two
sequences get arbitrarily close together, i.e.
\begin{align*}
  x_n\rightarrow x
  \quad\text{and}\quad
  d(x_n,y_n)\rightarrow 0
\end{align*}
Then $y_n\rightarrow x$ as well.
\end{prop}
\begin{proof}
Suppose that $\varepsilon>0$ is given. Since $x_n\rightarrow x$, there
exists an $N_x$ such that
\begin{align*}
  n > N_x
  \quad\Rightarrow\quad
  d(x_n,x) < \frac{\varepsilon}{2}
\end{align*}
Next, since $d(x_n,y_n)\rightarrow 0$, there exists an $N_y$ such that
\begin{align*}
  n > N_y
  \quad\Rightarrow\quad
  d(x_n,y_n) < \frac{\varepsilon}{2}
\end{align*}
By the triangle inequality
\begin{align*}
  n>\max\{N_x,N_y\}
  \quad\Rightarrow\quad
  d(y_n,x) \leq
  d(y_n,x_n) + d(x_n,x)
  < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align*}
Hence $\{y_n\}\rightarrow x$ as well.

\end{proof}

\begin{prop}{\emph{(Limits of Subsequences for Convergent Sequences)}}
\label{prop:subseq}
The sequence $\{x_n\}$ converges to $x$ if and only if every subsequence
also converges to $x$.
\end{prop}


\subsection{Sequences in $\R$: Supremum, Infimum, Limit Superior, and
Limit Inferior}

This section provides further tools for dealing with suprema and infima,
along with definition of the \emph{lim sup} and \emph{lim inf}, which
are extremely useful in the study of sequences and limits.
I restrict this section to $\R$ space since the Axiom of Completeness
(Axiom~\ref{ax:completeness}) furnishes the existence of sups and infs
for bounded sequences. Much of what follows wouldn't be true or sensible
without completeness of the space.

But first, I reproduce the definition of the supremum and provide some
more ways to use and think about it, since it will be extremely
important from here on.

\begin{defn}{(Supremum)}
\label{defn:supdef2}
Given a set $A\subseteq X$, the \emph{supremum} denoted $\sup(A)$ is the
smallest possible upper bound for $A$.
More precisely, the supremum satisfies the following two properties
\begin{enumerate}
  \item $a\leq \sup(A)$ for all $a\in A$.
  \item If $x<\sup(A)$, then $x\in X$ is not an upper bound for $A$.
    Otherwise, we would have just taken $x$ as the supremum.
\end{enumerate}
The sup might be in $A$, in which case $\sup(A) = \max(A)$, or it might
be in $X\setminus A$, in which case $\max(A)$ is undefined.
\end{defn}

In practice, we often use the supremum in proofs by writing it in
special representations that are equivalent to
Definition~\ref{defn:supdef2}. We now lay those out.
\begin{prop}
Given some $A\subseteq X$, we have the following results for the
supremum:
\begin{enumerate}
  \item Given any $a\in A$, there exists an $\varepsilon\geq 0$ such
    that
    \begin{align*}
      a = \sup(A) - \varepsilon
    \end{align*}
    Moreover, $\varepsilon=0$ if and only if $\sup(A) \in A$, which is
    not always the case.
  \item Given any $\varepsilon>0$, there exists an $a\in A$ such that
    \begin{align*}
      a > \sup(A) -\varepsilon
    \end{align*}
\end{enumerate}
\end{prop}
\begin{proof}
We just want to show that the two representations above match
Definition~\ref{defn:supdef2}.

To prove (1), suppose that no such $\varepsilon\geq 0$ existed. Then $a
> \sup(A)$, which violates the definition of $\sup(A)$ as the biggest
$x\in X$ such that $x\geq a$ for all $a\in A$.

To prove(2), suppose $\varepsilon>0$ is given and that there is no $a$
such that $a>\sup(A)-\varepsilon$. Then $\sup(A)-\varepsilon$ is an
upper bound of $A$ smaller than $\sup(A)$, which violates the definition
of $\sup(A)$ as the \emph{smallest} upper bound.
\end{proof}
\begin{rmk}
This proposition is very useful. We often use (2) by taking
$\varepsilon=1/n$ and choosing elements of $A$ to build up a sequence,
as we'll do in the next proposition.
\end{rmk}

\begin{prop}
\label{prop:supseq}
Suppose that $A\subseteq \R$. We have the following results:
\begin{enumerate}
  \item If $A$ is bounded from above, then
    $\exists \; \{\bar{a}_n\} \subseteq A$ such that
    $\{\bar{a}_n\} \rightarrow \sup(A)$
  \item If $A$ is bounded from below, then
    $\exists \; \{\munderbar{a}_n\} \subseteq A$ such that
    $\{\munderbar{a}_n\} \rightarrow \inf(A)$
\end{enumerate}
\end{prop}
\begin{proof}
Since $A$ is bounded from above, $\sup A$ exists and is well defined by
Axiom~\ref{ax:completeness}.  Now use the existence of $\sup A$ to
define a particular sequence, $\{\bar{a}_n\}$. We choose the elements by
recalling the definition of $\sup A$, which says
\begin{align*}
  \forall \varepsilon = \frac{1}{n} >0
  \quad
  \exists \; \bar{a}_n\in A \;\; \text{s.t.} \;\;
  \sup A - \frac{1}{n} \leq \bar{a}_n \leq \sup A
\end{align*}
Clearly, if we choose elements this way, all the $\bar{a}_n$ terms are
in $A$ and
\begin{align*}
  \lim_{n\rightarrow \infty} \bar{a}_n &= \sup A
\end{align*}
The proof for $\inf A$ is similar.
\end{proof}

\begin{prop}
Suppose that $\{x_n\}$ is a nondecreasing sequence in $\R$, $x_n \leq
x_{n+1}$. If $\{x_n\}$ is bounded then
\begin{align*}
  \lim_{n\rightarrow \infty} x_n = \sup A
\end{align*}
where $A=\{x_1,x_2,\ldots\}$ is the image of the sequence.
\end{prop}
\begin{proof}
By Axiom~\ref{ax:completeness}, $\sup A$ exists and is well defined
since $A$ is bounded.

Now, given $\varepsilon>0$, we want an $N$ such that
\begin{align*}
  n>N\quad\Rightarrow\quad
  \sup A - x_n < \varepsilon
\end{align*}
Now by the definition of $\sup A$, given $\varepsilon>0$, there exists a
\begin{align*}
  x_m \in A
  \quad \text{s.t.} \quad
  \sup A - \varepsilon &< x_m\\
  \Leftrightarrow\quad
  \sup A - x_m&< \varepsilon
\end{align*}
Next, note that
\begin{align*}
  x_{m+i} &\geq x_m
  \qquad \forall i \geq 0 \\
  \Leftrightarrow\quad
  -x_{m+i} &\leq -x_m \\
  \sup A -x_{m+i} &\leq \sup A-x_m \\
  \Rightarrow \qquad
  \sup A -x_{m+i} &\leq \sup A-x_m < \varepsilon
  \qquad \forall i \geq 0
\end{align*}
So if we take $N=m$, we ensure that $\sup A - x_n< \varepsilon$ for all
$n>N$.
\end{proof}

\begin{cor}
\label{cor:nonincreasing}
Suppose that $\{x_n\}$ is a nonincreasing sequence in $\R$. If
$\{x_n\}$ is bounded then
\begin{align*}
  \lim_{n\rightarrow \infty} x_n = \inf A
\end{align*}
where $A=\{x_1,x_2,\ldots\}$ is the image of the sequence.
\end{cor}
\begin{proof}
Consider $\{y_n\}$ where $y_n = -x_n$ for all $n$. Then $\{y_n\}$ is a
nondecreasing sequence, so
\begin{align*}
  \lim_{n\rightarrow \infty} y_n = \sup -A
\end{align*}
But $\sup -A = \inf A$, so $\lim_{n\rightarrow \infty} x_n = \inf A$
\end{proof}

\begin{ex}
Next, we'll see a definition that can help make sense of sequences that
look like the following
\begin{align*}
  x_n = (-1)^n + \frac{1}{n}
\end{align*}
By taking $n\rightarrow\infty$, you can kind of see that the series has
two ``limits'', -1 and 1. Of course, the series doesn't converge since
there is no single number that $x_n$ converges to, but it does seem to
accumulate around the points 1 and -1 in the tail. The next definitions
will help us talk about and make sense of that idea.
\end{ex}

\begin{defn}
Given a bounded sequence $\{x_n\}$, define the limit superior and limit
inferior as
\begin{align*}
  \bar{x}=
  \limsup_{n\rightarrow \infty} x_n
  &= \lim_{n\rightarrow \infty} \bar{x}_n
  \qquad \text{where} \; \bar{x}_n = \sup_{k\geq n} x_k
  = \sup \{x_n, x_{n+1}, \ldots \}\\
  \munderbar{x}=
  \liminf_{n\rightarrow \infty} x_n
  &= \lim_{n\rightarrow \infty} \munderbar{x}_n
  \qquad \text{where} \; \munderbar{x}_n = \inf_{k\geq n} x_k
  = \inf \{x_n, x_{n+1}, \ldots \}
\end{align*}
\end{defn}
\begin{rmk}
It's important to note that elements of the sequences $\{\bar{x}_n\}$
and $\{\munderbar{x}_n\}$ are not necessarily elements of the original
sequence $\{x_n\}$. Since we're taking sup's and inf's of sets, it's
always possible that the sup's and inf's are outside the set.

For example, consider
\begin{align*}
  \{x_n\} =
  \left\{
    \frac{1}{n}
  \right\}=
  \left\{
    1, \frac{1}{2}, \frac{1}{3}, \ldots
  \right\}
\end{align*}
In that case,
\begin{align*}
  \bar{x}_n &= \sup \left\{ \frac{1}{n}, \frac{1}{n+1},\ldots \right\}
  = \frac{1}{n} \qquad \forall n\\
  \munderbar{x}_n &= \inf \left\{ \frac{1}{n}, \frac{1}{n+1},\ldots \right\}
  = 0\qquad \forall n
\end{align*}
Though $\{\bar{x}_n\}$ consists of elements in the series,
$\{\munderbar{x}_n\}$ does not since the series is constant at zero,
something not in $\{x_n\}$.
\end{rmk}

\begin{prop}
Given a bounded sequence $\{x_n\}$,
both the limit superior and limit inferior always have well defined
limits (i.e.\ they always exist), and
\begin{align*}
  \limsup_{n\rightarrow \infty} x_n
  &= \inf_{n\in \mathbb{N}} \left(\sup_{k\geq n} x_k\right)\\
  \liminf_{n\rightarrow \infty} x_n
  &= \sup_{n\in \mathbb{N}} \left(\inf_{k\geq n} x_k\right)
\end{align*}
In a sense, the $\limsup$ is the ``largest limit'' of the sequence,
while the $\liminf$ is the ``smallest limit'' of the sequence.
\end{prop}
\begin{proof}
Given a sequence $\{x_n\}$, define $\bar{x}_n$ as
\begin{align*}
  \bar{x}_n = \sup_{k\geq n} x_k
\end{align*}
Since $\{x_n\}$ is bounded, the sup taken to form $\bar{x}_n$ exists for
each $n$ by Axiom~\ref{ax:completeness}. Moreover, $\{\bar{x}_n\}$ is a
nonincreasing sequence because, in moving from $\bar{x}_n$ to
$\bar{x}_{n+1}$, you lose a candidate ($x_n$ specifically) for the
supremum. So $\bar{x}_n$ either gets smaller or stays the same. You can
only do ``worse'' with the additional constraint of $k\geq n+1$ versus
$k\geq n$. Hence by Corollary~\ref{cor:nonincreasing}, the $\bar{x}_n$
converges to the following well-defined object
\begin{align*}
  \lim_{n\rightarrow \infty}\bar{x}_n
  = \inf_{n\in \mathbb{N}} \bar{x}_n
  = \inf_{n\in \mathbb{N}} \left( \sup_{k\geq n} x_k\right)
\end{align*}
\end{proof}


\subsection{Cauchy Sequences and Complete Metric Spaces}

\begin{defn}{(Cauchy Sequence)}
Suppose we have a sequence $\{x_n\}\subseteq X$. That sequence is a
\emph{Cauchy sequence} if and only if, for all $\varepsilon>0$, there
exists an $N$ such that
\begin{align*}
  n,m>N \quad\Rightarrow\quad
  d(x_n,x_m) <\varepsilon
\end{align*}
Here's a general recipe for proving that a sequence $\{x_n\}$ is
Cauchy.

Given $\varepsilon$, simply write out write out $d(x_n,x_m)$
for arbitrary $x_n$ and $x_m$. Try to bound that expression for the
distance as
\begin{align*}
  d(x_n,x_m) < G(\max\{n,m\})
\end{align*}
where $G$ is some function that depends only upon the max of $n$ and
$m$. If that's possible, then equate $G(N)=\varepsilon$ and solve for
$N$.
\end{defn}
\begin{rmk}
Notice the difference from Definition~\ref{defn:convergence} of
convergence. For Cauchy sequences, you don't have to know the limit
point, while traditional convergence a la
Defintion~\ref{defn:convergence} requires you to know limit point.
Cauchy sequences require simply that points in the tail are
``close together.''

In fact, the main use of Cauchy sequences is precisely this. We might be
able to say very easily that points are close together in the tail,
though we have no idea what the limit point is or if one exists. But if
we're in a special type of metric space (a \emph{complete} metric space,
described below), Cauchy will be ``good enough'' in some sense---we'll
be able to say that a Cauchy sequence will converge to some limit in a
complete metric space, though we might not know what that limit is.

So we care about Cauchy sequences because \emph{sometimes} Cauchy
implies convergent. The next theorem concerns the other direction:
whether a convergent series is Cauchy.
\end{rmk}

\begin{thm}{\emph{(Convergent $\Rightarrow$ Cauchy)}}
Though Cauchy does \textbf{not} imply convergent,
a convergent sequence is \textbf{always} Cauchy.
\end{thm}
\begin{proof}
Suppose that we are given $\varepsilon>0$ and convergent sequence
$\{x_n\}\rightarrow x$.  By the triangle inequality, we know that for
any $n$ and $m$,
\begin{align*}
  d(x_n,x_m) \leq d(x_n,x) + d(x,x_m)
\end{align*}
Since $\{x_n\}$ is convergent, we can choose $N$ such that
\begin{align*}
  n>N
  \quad\Rightarrow\quad
  d(x,x_n) < \varepsilon/2
  %\begin{cases}
    %d(x,x_n) < \varepsilon/2\\
    %d(x,x_m) < \varepsilon/2
  %\end{cases}
\end{align*}
If we use this as our Cauchy $N$, that implies that
\begin{align*}
  n,m>N
  \quad \Rightarrow\quad
  d(x_n,x_m) \leq d(x_n,x) + d(x,x_m) < \frac{\varepsilon}{2} +
    \frac{\varepsilon}{2}
  =\varepsilon
\end{align*}
\end{proof}

\begin{prop}{\emph{(Cauchy $\Rightarrow$ Bounded)}}
\label{prop:cauchy-bounded}
Any Cauchy sequence is bounded.
\end{prop}
\begin{proof}
Let $\{x_n\}$ be a Cauchy sequence. Set $\varepsilon=1$. Then there is
an $N$ such that
\begin{align}
  \label{cauchybddN}
  n,m>N \quad\Rightarrow\quad
  d(x_n,x_m) < 1
\end{align}
Next, taking advantage of the fact that $\{x_n\}_{n=1}^{N+1}$ is a
finite set, let
\begin{align}
  \label{cauchybddK}
  K = \max_{\hat{n},\hat{m} \in \{1,\ldots,N+1\}}
  d(x_{\hat{n}},x_{\hat{m}})
\end{align}
Just to emphasize, $K$ exists and is finite because
$\{x_n\}_{n=1}^{N+1}$ is a finite set.

Therefore, we can conclude that for any $n,m$
\begin{align*}
  d(x_n,x_m) \leq 1+K
\end{align*}
To see this, consider all of the cases that might arise for $n,m$
\begin{enumerate}
  \item If both $n,m\leq N$, then by the definition of $K$ in
    (\ref{cauchybddK}), you have $d(x_n,x_m)<K<1+K$.
  \item If both $n,m>N$, then by (\ref{cauchybddN}), you have
    $d(x_n,x_m)<1<1+K$.
  \item If $n\leq N$ and $m> N$ (or vice versa), then by triangle
    inequality,
    \begin{align*}
      d(x_n,x_m)
      &\leq d(x_n,x_{N+1}) + d(x_{N+1},x_m)\\
      &< K + d(x_N,x_m) \qquad \text{By~(\ref{cauchybddK})}\\
      &< K + 1 \quad\qquad\qquad \text{By~(\ref{cauchybddN})}\\
    \end{align*}
\end{enumerate}
Hence, under any case $d(x_n,x_m)<1+K$ for all $n,m$.
\end{proof}
\begin{cor}
Convergent sequences are bounded.
\end{cor}

\begin{prop}
\label{prop:cauchy-subseq}
Suppose that $\{x_n\}$ is a Cauchy sequence and which has a subsequence
$\{x_{n_k}\}\rightarrow x$. Then the original sequence
$\{x_n\}\rightarrow x$ as well.

In other words, if you can find a convergent subsequence for some Cauchy
sequence, then we know the limit of the original sequence. You might not
always be able to do that, but if you can, that's good news.
\end{prop}
\begin{proof}
Suppose $\varepsilon>0$ is given.  Since $\{x_n\}$ is Cauchy, there is
an $N_{n,m}$ such that
\begin{align*}
  n,m>N_{n,m}
  \quad\Rightarrow\quad
  d(x_n,x_m) <\varepsilon/2
\end{align*}
Next, since $\{x_{n_k}\}\rightarrow x$, we can choose an $N_k \geq
N_{n,m}$ such that
\begin{align*}
  n > N_k
  \quad\Rightarrow\quad
  d(x_n, x) < \varepsilon/2
\end{align*}
Therefore, by choosing $n>N_k$ and using the triangle inequality, we can
say that
\begin{align*}
  d(x_n,x) \leq d(x_n, x_{n_k}) + d(x_{n_k},x)
  d(x_n,x) \leq \varepsilon/2 + \varepsilon/2 = \varepsilon
\end{align*}
\end{proof}

\begin{defn}{(Complete Metric Space)}
Metric space $(X,d)$ is called a \emph{complete} metric space if and
only if all Cauchy sequences are convergent.

In $\R^n$, this is a statement that can be proved by starting
with Axiom~\ref{ax:completeness}.
\end{defn}

\begin{ex}
The most common example of a compelte metric space is
$(\R,d_2)$.
\end{ex}

\begin{ex}
\label{ex:Ncomplete}
Metric space $(\N,d_1)$ is a complete metric space.
\end{ex}

\begin{ex}
Given any set $X$ and the \emph{discrete} metric
\begin{align*}
  d(x,y) =
  \begin{cases}
    1 & x\neq y \\
    0 & x= y \\
  \end{cases}
\end{align*}
the metric space $(X,d)$ is complete.
Moreover, this is ``equivalent'' to Example~\ref{ex:Ncomplete} in some
since since in that example and in this one, all convergent sequences
must be constant in the tail.
\end{ex}

\begin{prop}
\label{prop:bwproperty}
In metric space, $(X,d)$,
suppose that the \emph{Bolzano-Weierstrass} property is true:
\begin{align}
  \label{bwproperty}
  \{x_n\} \;\text{bounded}
  \quad\Rightarrow\quad
  \exists \{x_{n_k}\}\rightarrow x \in X
\end{align}
where $\{x_{n_k}\}$ is a convergent subsequence of $\{x_n\}$.  In other
words, any bounded sequence has a convergent subsequence. If this is
true, then $(X,d)$ is a complete metric space.
\end{prop}
\begin{proof}
Suppose that we have a Cauchy sequence $\{x_n\}$. By
Theorem~\ref{prop:cauchy-bounded}, $\{x_n\}$ is bounded. Since we assume
Relationship~\ref{bwproperty}, there is a convergent subsequence
$\{x_{n_k}\}\rightarrow x \in X$.

Since there is a convergent subsequence,
Proposition~\ref{prop:cauchy-subseq} states that $\{x_n\}\rightarrow
x\in X$. Hence, the sequence has a limit implying $(X,d)$ is complete.
\end{proof}

\begin{note}
%(Relation to Proposition~\ref{prop:cauchy-subseq})
On the surface, this looks very similar to
Proposition~\ref{prop:cauchy-subseq}, but they are quite different.

First, Proposition~\ref{prop:cauchy-subseq} holds for all metric spaces
no matter what. It simply says ``If you can find a convergent
subsequence for some Cauchy sequence, then we know the limit of the
original sequence.'' It doesn't say you'll be able to do this for every
Cauchy sequence. It just says ``if you can\ldots''

Contrast that with Proposition~\ref{prop:bwproperty} and note the
differences
\begin{enumerate}
  %\item As shown in the proof, it basically does say ``you'll be able to
    %find a convergent subsequence for every Cauchy sequence.'' It just
    %says it indirectly by saying ``you'll be able to do that for any
    %bounded sequence.'' And since Cauchy sequences are bounded, it
    %follows directly.
  \item This proposition doesn't work in all metric spaces, and requires
    the metric space to abide by the special condition laid out in
    Property~\ref{bwproperty}. That's restrictive for the metric space,
    while Proposition~\ref{prop:cauchy-subseq} is general for all metric
    spaces.
  \item Proposition~\ref{bwproperty} assumes that boundedness
    will \emph{generate} convergent subsequences; whereas,
    Proposition~\ref{prop:cauchy-subseq} assumed you brought a
    convergent subsequence to the table straightaway.
  \item Proposition~\ref{prop:bwproperty} makes a statement about the
    metric space---namely that it's \emph{complete}---which is hugely
    important. Property~\ref{prop:bwproperty} didn't say anything about
    the metric space.
\end{enumerate}
\end{note}

\begin{rmk}
(Relation to Traditional Bolzano-Weierstrass Theorem) This is different
than Theorem~\ref{thm:bolzano-weierstrass}, the Bolzano-Weierstrass
Theorem we will see in a bit. That specifically concerns $\R^n$,
which just so happens satisfy the Bolzano-Weierstrass property. This
statement is more general and just says that \emph{if} a metric space
satisfies the property, then the space is complete.
\end{rmk}



\clearpage
\section{Metric Topology}

Topology is the study of open sets and continuous functions in arbitrary
spaces. Metric topology is the same thing, but when you also have a
\emph{metric} or \emph{distance function} on those spaces, which can be
used to aid in defining or generating open sets and continuous
functions.

Unless otherwise noted, it assumed below that any function
$f:X\rightarrow Y$ is implicitly a function between metric spaces
$(X,d_X)$ and $(Y,d_Y)$, where the metric for each space $X$ and $Y$
might differ, hence the subscripted $d_X$ and $d_Y$.
I will sometimes write the function's metric spaces explicitly as
$f:(X,d_X)\rightarrow(Y,d_Y)$.

\subsection{Open and Closed Sets}

\begin{defn}{(Open Ball)}
Given space $X$ and real number $r>0$, we define an \emph{open ball}
about point $x$ as
\begin{align*}
  B_r(x) := \{ y \in X \;|\; d(x,y)<r\}
\end{align*}
\end{defn}

\begin{defn}{(Open Set)}
Set $A\subseteq X$ is \emph{open} if, for any $x\in A$, there exists a
$r$ such that $B_r(x)\subseteq A$.
\end{defn}

\begin{prop}
Open balls are open sets.
\end{prop}
\begin{proof}
Within metric space $(X,d)$, take the open ball $B_{r_x}(x)$.  To prove
that $B_{r_x}(x)$ is open, we need to be able to do the following:
\begin{align*}
  \text{Given any $y\in B_{r_x}(x)$, choose $r_y$ such that $B_{r_y}(y)\subseteq B_{r_x}(x)$}
\end{align*}
By the definition of open balls, the statement $B_{r_y}(y)\subseteq
B_{r_x}(x)$ is equivalent to
\begin{align*}
  d(z,y) < r_y \quad\Rightarrow\quad
  d(z,x) < r_x
  \quad\text{for any $z\in X$}
\end{align*}
So how can we choose $r_y$ such that the above statement holds?
Well, let's expand our target for bounding---i.e.\ $d(z,x)$--- via the
triangle inequality so that we get something that is a function of
$r_y$:
\begin{align*}
  d(z,x) &\leq d(z,y) + d(y,x)\\
  \Rightarrow \quad
  d(z,x) &< r_y + d(y,x) \qquad
  \text{since $z\in B_{r_y}(y) \; \Rightarrow\; d(z,y)<r_y$}
\end{align*}
Therefore, since we want our choice of $r_y$ to have the side effect of
bounding $d(z,x)$ by $r_x$ it's enough to set $r_y = r_x - d(y,x)$ and
that will do the trick.
\end{proof}

\begin{defn}{(Interior)}
Given a set $A\subseteq X$, we define the \emph{interior} of $A$ as
\begin{align*}
  \mathring{A}
  := \{x \in A \; | \; \exists\; r>0 \;\; \text{s.t.} \;\; B_r(x) \subseteq A\}
\end{align*}
Note that $r$ can be different for each $x\in A$. It need not be a fixed
number for all $x\in A$.
\end{defn}

\begin{prop}
A set $A\subseteq X$ is open if and only if $\mathring{A} = A$.
\end{prop}

\begin{defn}{(Closure)}
Given a set $A\subseteq X$, we define the \emph{closure} of $A$ as
\begin{align*}
  \bar{A}:= \{
    x\in X \;|\;
    \forall r>0 \quad B_r(x) \cap A \neq \emptyset
  \}
\end{align*}
\end{defn}

\begin{defn}{(Closed Sets, Topology)}
\label{defn:closed}
A set $A\subseteq X$ is \emph{closed} if and only if $\bar{A} = A$.
\end{defn}

\begin{prop}{\emph{(Closed Sets, Limit Points)}}
\label{prop:closed}
Equivalently, $C\subseteq X$ is \emph{closed} if and only if $C$
contains its limit points. More explicitly, for any sequence
$\{x_n\}\subseteq C$ where $x_n\rightarrow x$, the set $C$ is closed if
and only if $x\in C$.
\end{prop}
\begin{rmk}
In Proposition~\ref{prop:closed}, We see the origin of the term
``closed'', a word that is typically shorthand in math for ``closed
under some operation.'' In vector spaces, elements of the space are
``closed'' under the operations of addition and scalar
multiplication---if you arbitrarily add or multiply elements in the
space, you still get back an element in that space.

Here, the operation is \emph{taking limits}. If you have a sequence in
some set and take its limit, the limit point must also be in the set (if
the limit exists, of course).
\end{rmk}
\begin{proof}{(Proposition~\ref{prop:closed})}
($\Rightarrow$) Suppose that $\{x_n\}\subseteq C$ converges to $x$ and
$C$ is closed. We want to show that $x\in C$.

So start by supposing that $x\not\in C$. Then $x\in C^C$, which is an
open set. Since $C^C$ is open, there exists an $\varepsilon>0$ such that
the open ball $B_\varepsilon(x) \subseteq C^C$ and does not intersect
$C$. Now since $x_n\in C$ for all $n$, it is the case that
$d(x_n,x)\geq\varepsilon$ for all $n$. But this violates the assumption
that $x_n\rightarrow x$.

($\Leftarrow$) Now assume that for any sequence $\{x_n\}\subseteq C$
converging to some $x$, it is the case that $x\in C$. We want to show
that $C$ is closed according to Definition~\ref{defn:closed}.  To do so,
we basically need to show that \emph{any} $p\in X$ satsifying the
criterion for being in the closure $\bar{C}$, i.e.\
\begin{align}
  \label{pcond}
  p \quad \text{satisfying}\quad
  \forall r>0 \quad B_r(p) \cap C \neq \emptyset
\end{align}
can be written as the limit of some sequence in $C$. Then, by assumption
of this direction of the proof, that limit is in $C$, implying
$\bar{C}=C$.

So let $p$ satisfy Condition~\ref{pcond}. Then form the sequence
$\{p_n\}$ where element $p_n$ satisfies
\begin{align*}
  p_n \in B_{1/n}(p)\cap C
\end{align*}
That is, form a sequence of shrinking open balls, each with radius $1/n$
for $n=1,2,\ldots$. From the $n$th ball, pick some element in
$B_{1/n}(p)\cap C$ to serve as $p_n$. Assuming Condition~\ref{pcond}
guarantees that there is at least one such element.

Now that's a sequence converging to $p$, since for any $\varepsilon>0$
it is the case that
\begin{align*}
  n>\frac{1}{\varepsilon}
  \quad\Rightarrow\quad
  d(p_n,p) < \varepsilon
\end{align*}
Since we assumed that $p\in C$ when it is the limit of some sequence, we
can conclude $\bar{C}=C$.
\end{proof}

\begin{ex}{(Set of Bounded Nondecreasing Functions)}
The Proposition~\ref{prop:closed} characterization of closed sets is
very useful for function spaces in particular. In fact, this proof will
use it twice: once for a sequence of functions and another time for a
numerical sequence of points. Here it goes.

Consider the set of bounded nondecreasing functions:
\begin{align*}
  C = \{
    f:A\rightarrow \R \; | \;
    x\geq y
    \;\Rightarrow\;
    f(x) \geq f(y)
  \}\subset \mathscr{B}(A,\R)
\end{align*}
Suppose that we take some sequence $\{f_n\}\subseteq C$ such that
$f_n\rightarrow f$ uniformly.  By Proposition~\ref{prop:closed}, we want
to show that $f\in C$ to show that $C$ is closed.

So to draw out a contradiction, suppose instead that $f\not \in C$. Then
there exists a pair $\hat{x},\hat{y}\in A$ such that $\hat{x}\leq
\hat{y}$ and $f(\hat{y})<f(\hat{x})$ (or equivalently,
$f(\hat{y})-f(\hat{x})=a<0$).

For that pair $\hat{x},\hat{y}$, define the numerical sequence
\begin{align*}
  a_n = f_n(\hat{y})-f_n(\hat{x})
  %\qquad
  %\text{for some pair}
  %\;
  %\hat{x} \leq \hat{y}
\end{align*}
By assumption, $f_n$ is increasing for all $n$, so $a_n\geq 0$ for all $n$.
In other words $\{a_n\} \subseteq[0,\infty)$.
Since $[0,\infty)$ is a closed set, Proposition~\ref{prop:closed} again
tells us that the limit $a = \lim_{n\rightarrow \infty} a_n$ must be in
$[0,\infty)$ if the sequence converges.

Now first, it does converge. Recall Proposition~\ref{prop:pwunif}, which
says that uniform $f_n\rightarrow f$ implies pointwise
$f_n(x)\rightarrow f(x)$ for all $x\in A$. So
\begin{align*}
  \lim_{n\rightarrow \infty} a_n
  = \lim_{n\rightarrow \infty} [f_n(\hat{y}) - f_n(\hat{x})]
  = f(\hat{y}) - f(\hat{x})
  =a
\end{align*}
Hence $\{a_n\}$ converges to $a$.

However, we assumed $f\not\in C$ with $f(\hat{y})-f(\hat{x})=a<0$.
But this can't be. $a\not\in[0,\infty)$ and
Proposition~\ref{prop:closed} guaranteed it would be. A contradiction,
implying $f\in C$, implying $C$ is closed.

Again, we used Proposition~\ref{prop:closed} twice, and in both
directions. In particular, we used the fact that the inclusion
$\lim_{n\rightarrow \infty} f_n=f\in C$ \emph{implied} $C$ was closed.
Along the way, we also used the fact that $\{a_n\}\subseteq [0,\infty)$
(a set we \emph{knew} to be closed) implied the \emph{inclusion}
$\lim_{n\rightarrow \infty} a_n =a\in [0,\infty)$.
Proposition~\ref{prop:closed} did crucial double duty in this proof.
\end{ex}

\begin{prop}{\emph{(Closed and Open Relationship\footnote{Not what you think})}}
$A\subseteq X$ is open if and only if $A^C\subseteq X$ is closed.
\end{prop}
\begin{proof}
($\Rightarrow$) Suppose that the set $A\subseteq X$ is open.  Suppose
also that the set $A^C$ is not closed. THat means $A^C$ is missing a
limit point, i.e.\ there is a point $x\notin A^C$ such that
\begin{align}
\forall r>0 \qquad B_r(x)\cap A^C\neq \emptyset
\label{contra.open}
\end{align}
But $x\not\in A^c$ implies that $x\in A$. And
Statement~\ref{contra.open} implies that any ball you put around $x\in
A$ will intersect $A^C$. This is a violation of $A$ being open, since
$A$ open means that you can put some ball around $x$ that lies entirely
within $A$. Hence Satement~\ref{contra.open} implies a contradiction, so
$A^C$ is closed.

($\Leftarrow$) Now suppose that $A^C$ is closed. By a similar argument
to the other direction if $x\in A$, then it is the case that it is not a
limit point of $A^C$, i.e.\ there's some ball $B_r(x)$ that does not
interset $A^C$ (otherwise $x$ would be in $A^C$ by definition). Thus
that ball lies entirely in $A$. Since this is true for all $x\in A$, the
set $A$ is open.
\end{proof}

\begin{lem}
Given any set $A\subseteq X$, we have $\mathring{A} \subseteq A
\subseteq \bar{A}$.
\end{lem}

\begin{proof}
Given $A\subseteq X$, the set $A$ is open if and only if $A^c$ is
closed.
\end{proof}



\begin{defn}{(Metric Topology)}
A \emph{metric topology} starts with a metric space $\mathscr{M}=(X,d)$
and uses the metric to define the family of open sets $\mathscr{T} = \{A
\subseteq X\;|\; A \text{ open}\}$. The metric topology has the
following characteristics:
\begin{enumerate}
  \item $X,\emptyset$ are open sets open
  \item $A^* = \bigcup_{A\in \mathscr{T}} A$ is open, for any arbitrary
    finite or infinite union of sets in $\mathscr{T}$.
  \item $A_* = \bigcap_{i=1}^n A_i$ is open, for any arbitrary but
    finite intersection of sets in $\mathscr{T}$.
\end{enumerate}
\end{defn}

\begin{cor}
Finite unions of closed sets are closed.
Infinite intersections of closed sets are closed.
\end{cor}
\begin{proof}
De Morgan's law. To do.
\end{proof}

\subsection{Topologically Equivalent Metric Spaces}

This section can be skipped entirely, and all other sections will still
make sense. It's included simply for reference.

\begin{defn}
Two metric spaces $(X,d_1)$ and $(X,d_2)$ using different metrics for
the same underlying space $X$, we say that $d_1$ is \emph{finer} than
$d_2$ if $(X,d_1)$ has all of the open sets of $(X,d_2)$ and possibly
more. Mathematically, we write
\begin{align*}
  d_1 \succeq d_2
  \quad \Leftrightarrow \quad
  \left(
  \text{$A$ open in $(X,d_2)$}
  ; \Rightarrow\;
  \text{$A$ open in $(X,d_1)$}
  \right)
\end{align*}
\end{defn}

\begin{defn}
Metric space $(X,d_1)$ is \emph{topologically equivalent} to $(X,d_2)$
if and only if $d_1 \succeq d_2$ and $d_1 \preceq d_2$. This can be
written more compactly as $d_1\sim d_2$.
\end{defn}

\begin{prop}
If $d_1 \sim d_2$, $C$ is closed in $(X,d_1)$ if and only if $C$ is
closed in $(X,d_2)$.
\end{prop}


\begin{prop}{\emph{(Equivalence of Distance Functions from Norms)}}
For space $X$, suppose that $\hat{N}$ and $\tilde{N}$ are two norms and
we define corresponding distance functions
\begin{align*}
  \hat{d}(x,y) = \hat{N}(x-y)
  \qquad
  \tilde{d}(x,y) = \tilde{N}(x-y)
  \qquad \forall x,y\in X
\end{align*}
Then the open sets of $(X,\hat{d})$ are the same as the open sets of
$(X,\tilde{d})$. The distance functions are equivalent.
\end{prop}

\subsection{Compactness}

\begin{defn}{(Open Cover)}
Given a metric space $(X,d)$ and $K\subseteq X$, we say that the family
of open sets $\{C_\alpha\}_{\alpha \in J}$ is an \emph{open cover} of
$K$ if
\begin{align*}
  K \subseteq \bigcup_{\alpha \in J} C_\alpha
\end{align*}
\end{defn}

\begin{defn}{(Compactness, Topological)}
A set $K$ is said to be compact if and only if \emph{every} arbitrary
open cover $\{C_\alpha\}_{\alpha \in J}$ has a finite subcover, i.e.\
there exist a finite number of indices $\{\alpha_i\}_{i=1}^N$ such that
\begin{align*}
  K \subseteq \bigcup_{i=1}^N C_{\alpha_i}
\end{align*}
\end{defn}

\begin{thm}{\emph{(Compactness, Sequential)}}
\label{thm:compact}
A set $K\subseteq X$ is compact if and only if every sequence
$\{x_n\}\subseteq K$ has a convergent subsequence $\{x_{n_k}\}
\rightarrow x \in K$ (emphasis on $x\in K$).
\end{thm}

\begin{prop}{\emph{(Compact $\Rightarrow$ Closed)}}
\label{prop:compact-closed}
If a set $K$ is compact, then $K$ is also closed.
\end{prop}
\begin{proof}
We'll prove this Proposition~\ref{prop:closed}. So suppose that
$\{x_n\}$ is some convergent sequence in $K$, i.e.
\begin{align*}
  \{x_n\} \subseteq K
  \quad \text{and}\quad
  \lim_{n\rightarrow \infty} x_n=x.
\end{align*}
Take any subsequence $\{x_{n_k}\}$ of $\{x_n\}$.
By Theorem~\ref{prop:subseq}, since $\{x_n\}\rightarrow x$, all
subsequences, including $\{x_{n_k}\}$, must also converge to $x$. By
Theorem~\ref{thm:compact}, $K$ compact implies $x\in K$.  Finally,
Proposition~\ref{prop:closed} tells us that $x\in K$ implies $K$ is
closed.
\end{proof}

\begin{prop}
Closed subsets of compact sets are also compact. Mathematically, for
compact $K\subseteq X$ and closed $C \subseteq K$, the set $C$ is
closed.
\end{prop}

\begin{thm}{\emph{(Bolzano-Weierstrass)}}
\label{thm:bolzano-weierstrass}
Every bounded sequence in Euclidean space $(\R^n,d_2)$ has at
least one convergent subsequence.
\end{thm}

\begin{thm}{\emph{(Heine-Borel)}}
\label{thm:heine-borel}
Within metric space $(\R,d_1)$, a set $K\subseteq \R$ is
compact if and only if $K$ is closed and bounded.
\end{thm}
\begin{proof}
($\Rightarrow$)
Closed is easy. Proposition~\ref{prop:compact-closed} tells us that
compact implies closed. Now for ``compact implies bounded.''

The following is certainly an open cover for compact
$K\subseteq \R$:
\begin{align*}
  \{ A_n &= B_n(0) \; | \; n \in \mathbb{N}\}\\
  \Rightarrow\quad
   \R &= \bigcup_{n\in \mathbb{N}} A_n
\end{align*}
By compactness, there then exists a finite subcover
$\{A_{n_k}\}_{k=1}^N$ such that
\begin{align*}
  K \subseteq \bigcup_{k=1}^N A_{n_k}
  = B_r(0)
  \quad \text{where} \;r=\max_{k=1,\ldots,N} n_k
\end{align*}
Therefore, the set $B_r(0)$ bounds $K$.

($\Leftarrow$) Suppose $K\subseteq \R$ closed and bounded. Let's
show $K$ is compact.

Let $\{x_n\}$ be any sequence in $K$. Since $K$ is bounded,
Theorem~\ref{thm:bolzano-weierstrass} says that there is at least one
convergent subsequence of $\{x_n\}$; call it $\{x_{n_k}\}$.  Since
$\{x_{n_k}\}$ converges in closed $K$, we know that the limit is also in
$K$. By Theorem~\ref{thm:compact}, $K$ is closed.
\end{proof}


\subsection{Continuous Functions}

\begin{defn}{(Continuity, $\varepsilon$-$\delta$)}
\label{defn:cts}
%Suppose that we have two metric spaces $(X,d_X)$ and $(Y,d_Y)$.
A function $f:X\rightarrow Y$ is \emph{continuous} at point $x\in X$ if,
given any $\varepsilon>0$, there exists a $\delta$ such that
\begin{align*}
  d_X(x,p) < \delta \quad \Rightarrow\quad
  d_Y(f(x),f(p))<\varepsilon
\end{align*}
But that's a local definition that defines continuity only at some point
$x\in X$. However, if $f$ is continuous at each point $x\in X$, then we
say that ``the function $f$ is continuous'' (without any ``at $x$''
qualifier).
\end{defn}

\begin{rmk}
You'll notice that Definition~\ref{defn:cts} has a similar flavor to the
definition of convergence in Definition~\ref{defn:convergence}. Well,
there's also a similar recipe for showing that a function is continuous,
and it will look a lot like the general recipe for showing convergence.

Given $f:X\rightarrow Y$, fix $x$ and suppose that $\varepsilon$ is
given. For arbitrary $p$, try to bound the expresion for the distance in
$Y$ as
\begin{align*}
  d_Y(f(x), f(p)) < G\left(d_X(x,p), x\right)
\end{align*}
where $G$ is a function of $d_X(x,p)$ and $x$.

In other words, simply write out $d_Y(f(x),f(p))$ for given $x$ and
arbitrary $p$.  Then simplify the expression to get something that only
depends upon $d_X(x,p)$ and $x$. Finally, substitute $\delta$ for
$d_X(x,p)$ in $G$ and solve $G(\delta,x)=\varepsilon$ for $\delta$.

This recipe emphasizes that our choice of $\delta$ should depend only
upon $\varepsilon$ and $x$, two ``givens'' within the problem.
\end{rmk}

\begin{ex}
Consider an affine function $f:(\R,d_1) \rightarrow
(\R,d_1)$ written as follows:
\begin{align*}
  f(x) = ax + b
\end{align*}
Using the recipe above, let's write out $d_Y(f(x),f(p))$:
\begin{align*}
  d_Y(f(x),f(p))
  &= |f(x)-f(p)|\\
  &= |ax+b-ap+b|\\
  &= |a||x-p|\\
  &= |a|d_X(x,p)
\end{align*}
Now set this guy equal to $\varepsilon$, sub in $\delta$ for $d_X(x,p)$,
and solve for $\delta$:
\begin{align*}
  \varepsilon = |a| \delta
  \quad \Rightarrow\quad
  \delta = \frac{\varepsilon}{|a|}
\end{align*}
Note that in this case, the choice of $\delta$ is independent of $x$,
though this isn't always the case. We see this in the next example.
\end{ex}

\begin{ex}
Consider $f: (\R,d_1)\rightarrow(\R,d_1)$ where
$f=\sqrt{x}$. Before getting into it, we establish the following fact:
\begin{align}
  (a-b)(a+b)
  &= a^2 - b^2\notag\\
  \Leftrightarrow\quad
  (a-b)
  &= \frac{a^2 - b^2}{(a+b)}
  \qquad \text{for $a,b>0$}
  \label{eq:sqrtxhelper}
\end{align}
With that, again follow the recipe:
\begin{align*}
  d_Y(f(x),f(p))
  &= |f(x)-f(p)|\\
  &= |\sqrt{x}-\sqrt{p}|\\
  \text{By Equation~\ref{eq:sqrtxhelper}}\qquad
  &= \frac{|x-p|}{\left\lvert \sqrt{x}+\sqrt{p}\right\rvert}\\
  &\leq \frac{d_X(x,p)}{\left\lvert \sqrt{x}\right\rvert}
\end{align*}
So now, equate that to $\varepsilon$, sub in $\delta$, and solve
\begin{align*}
  \varepsilon &= \frac{\delta}{\left\lvert \sqrt{x}\right\rvert}\\
  \Leftrightarrow\quad
  \delta &= \varepsilon\sqrt{x}
\end{align*}
\end{ex}

There's another more general (but perfectly equivalent) way to define
continuity that doesn't make use of the $\varepsilon$-$\delta$ method
introduced in Definition~\ref{defn:cts}. Instead, we can work directly
with open sets.

\begin{thm}{\emph{(Continuity, Topology)}}
\label{defn:topcts}
Function $f:X\rightarrow Y$ is continuous if and only if
\begin{align*}
  A \subseteq Y \; \text{open}
  \quad \Rightarrow \quad
  f^{-1}(A) \subseteq X \; \text{open}
\end{align*}
\end{thm}
\begin{rmk}
Unlike Definition~\ref{defn:cts}, Definition~\ref{defn:topcts} is super
useless for proving a particular function that you are given is
continuous. Instead, it's useful for proving general properties
\emph{about} continuous functions.
\end{rmk}
\begin{proof}
(Theorem~\ref{defn:topcts})
First, the $\Rightarrow$ direction, assuming
Definition~\ref{defn:cts}-type continuity.

To prove openness of $f^{-1}(A)$,  we want to show that for any $x\in
f^{-1}(A)$ we can find a $\delta$ such that $B_\delta(x)\subseteq
f^{-1}(A)$.  To do so, note that $f(x) \in A$, and $A$ is open.
Therefore, there exists an $\varepsilon$ such that
$B_\varepsilon(f(x))\subseteq A$.  Since $f$ is continuous, there is a
corresponding $\delta$ such that $y \in B_\delta(x)$ implies $f(y) \in
B_\varepsilon(f(x))$. In other words
\begin{align*}
  f\left(B_\delta(y)\right) \subseteq
  B_\varepsilon(f(x))\subseteq A
\end{align*}
Taking the preimage
\begin{align*}
  B_\delta(y) \subseteq f^{-1}(A)
\end{align*}

($\Leftarrow$) Suppose that $f^{-1}(A)$ is open. We need to show the
$\varepsilon$-$\delta$ definition. So suppose that $x\in X$ and
$\varepsilon>0$ are given, and our task is finding $\delta$.

The set $B_\varepsilon(y)=B_\varepsilon(f(x))$ is open. Therefore, the
preimage $f^{-1}(B_\varepsilon(y))$ will be open by assumption. That
means we can find some $\delta$ such that $B_\delta(x)\subseteq
f^{-1}(B_\varepsilon(y))$. Mapping this subset relationship back into
$Y$, we ensure
\begin{align*}
  f(B_\delta(x)) \subseteq B_\varepsilon(y) = B_\varepsilon(f(x))
\end{align*}
In other words,
\begin{align*}
\left( p\in B_\delta(x)
\;\Leftrightarrow\;
d(p,x)<\delta\right)
\quad
\Rightarrow
\quad
\left( f(p)\in B_\varepsilon(f(x))
\;\Leftrightarrow\;
d(f(p),f(x))<\varepsilon\right)
\end{align*}
That is precisely the $\varepsilon$-$\delta$ definition of continuity.
\end{proof}

Below, we again reformulate the definition of continuity from the
$\varepsilon$-$\delta$ or topological definition into one concerning
convergence of sequences.
\begin{thm}{\emph{(Continuity, Sequential)}}
\label{thm:cts-sequential}
Suppose that we have a function $f:X\rightarrow Y$ and a sequence
$\{x_n\}\subset X$ where $x_n\rightarrow x$.  A function is continuous
if and only if
\begin{align*}
  \lim_{n\rightarrow \infty} f(x_n)
  &= f(x)\\
  \Leftrightarrow\qquad
  \lim_{n\rightarrow \infty} f(x_n)
  &= f(\; \lim_{n\rightarrow \infty} x_n \;)
\end{align*}
Note that $\{x_n\}$ is a sequence in metric space $(X,d_X)$, while
$\{f(x_n)\}$ is a sequence in metric space $(Y,d_Y)$.
\end{thm}


\begin{prop}
\label{prop:XtoR}
Given continuous function $f:(X,d_X)\rightarrow (\R,d_1)$, for
any $a\in \R$,
\begin{align*}
  A &= \{x\in X \; |\; f(x) > a\} \; \text{\emph{is open}} \\
  C &= \{x\in X \; |\; f(x) \geq a\} \; \text{\emph{is closed}}
\end{align*}
Similar for $<$ and $\leq$.
\end{prop}
\begin{proof}
Set $A$ is simply $f^{-1}((a,\infty))$. The interval $(a,\infty)$ is an
open set. Since $f$ is continuous, the preimage of $(a,\infty)$ is open.

$C$ can be written as $f^{-1}([a,\infty))$ or, equivalently,
$(f^{-1}((-\infty,a)))^C$, which is the representation we actually
care about. Now since $(-\infty,a)$ is open, its preimage
$f^{-1}((-\infty,a))$ is also open. Since the complement of an open
set is closed, $(f^{-1}((-\infty,a)))^C$ is closed.
\end{proof}
\begin{rmk}
Proposition~\ref{prop:XtoR} is great since continuous functions that
look like $f$ in the proposition come up a lot. We can then use this
proposition, our knowledge of open sets in $\R$, and
Definition~\ref{defn:topcts} to generate open sets in arbitrary space
$X$.\footnote{Copyright Elon Musk.}

For example, suppose that $X$ is the space of bounded functions and
$d_X$ is the sup norm. Tricky space, and I don't know what the hell open
sets look like in that.  But if $f$ is a continuous integral operator
which maps bounded functions in $X$ to real numbers---something covered
by Proposition~\ref{prop:XtoR})---we can recover sets of functions in
$X$ that are ``open'' by doing $f^{-1}((a,\infty))$.  I challenge you to
dream up open sets of bounded functions without this trick.
\end{rmk}

\begin{prop}
Given continuous functions $f:(X,d_X)\rightarrow (Y,d_Y)$ and
$g:(Y,d_Y)\rightarrow (Z,d_Z)$, the composition
\begin{align*}
  h(x) := (g\circ f)(x) = g(f(x))
\end{align*}
is also continuous.
\end{prop}

\begin{thm}{\emph{(Compactness and Continuous Functions)}}
\label{thm:contcompact}
If the function $f:(X,d_X)\rightarrow (Y,d_Y)$ is continuous, then
\begin{align*}
  K\subseteq X \; \text{compact}
  \quad \Rightarrow\quad
  f(K)\subseteq Y \; \text{compact as well}
\end{align*}
\end{thm}
\begin{proof}
We will use the topological definition of compactness, showing that any
open cover of $f(K)$ has a finite subcover. So start with an open cover
of $f(K)$:
\begin{align*}
  f(K) \subseteq \bigcup_{\alpha\in J} C_\alpha
\end{align*}
Since $f$ is continuous, we know that $f^{-1}(C_\alpha)$ is open for any
$\alpha \in J$. Moreover,
\begin{align*}
  K \subseteq \bigcup_{\alpha \in J} f^{-1}(C_\alpha)
\end{align*}
In other words, $\{f^{-1}(C_\alpha)\}$ is an open cover of $K$.
But $K$ is compact. So it has a finite subcover:
\begin{align*}
  \exists \{\alpha_i\}_{i=1}^N
  \quad\text{s.t.}\quad
  K \subseteq \bigcup_{i=1}^N f^{-1}(C_{\alpha_i})
\end{align*}
Now apply $f$ to both sides of the inclusion statement:
\begin{align*}
  f(K) \subseteq
  f\left(\bigcup_{i=1}^N f^{-1}(C_{\alpha_i})\right)
  =\bigcup_{i=1}^N f\left(f^{-1}(C_{\alpha_i})\right)
  =\bigcup_{i=1}^N C_{\alpha_i}
\end{align*}
\end{proof}

\begin{thm}
Given continuous function $f:(X,d_X) \rightarrow (Y,d_Y)$ and
compact $K\subseteq X$, the function $f$ is \emph{uniformly} continuous
on $K$.
\end{thm}

\begin{thm}{\emph{(Limit of Sequence of Bounded Continuous Real Functions)}}
\label{thm:ctslimit}
Consider the sequence $\{f_n\}$ of bounded continuous functions
$f_n:A\rightarrow \R$. If $f_n\rightarrow f$, then $f$ is continuous as
well (where $\R$ is equipped with one of the usual metrics).
\end{thm}
\begin{proof}
We want to show that $f$ is continuous according to the usual
$\varepsilon$-$\delta$ definition, so take $x\in A$ and $\varepsilon>0$
as given. Let $d$ denote one of the usual distance functions for $\R$.
We now use the triangle inequality to bound the desired distance
$d(f(x),f(y))$ by objects we know more about
\begin{align}
 \forall n \qquad d(f(x),f(y))
  &\leq d(f(x),f_n(x)) + d(f_n(x),f(y)) \notag \\
  &\leq d(f(x),f_n(x)) + d(f_n(x),f_n(y)) + d(f_n(y),f(y))\notag\\
  &\leq 2 \; d_\infty(f,f_n) + d(f_n(x),f_n(y))
  \label{Cclosed}
\end{align}
where the last line follows because $d_\infty$ is defined as $\sup_{z\in
A} d(f_n(z), f(z))$.

Now Inequality~\ref{Cclosed} holds for all $n$. The trick will
be to find a particular $N$ so that we can then bound $d_\infty(f,f_N)$
and $d(f_N(x),f_N(y))$. That will then also allow us to use the
continuity of $f_N$ to find a suitable $\delta$ for $f$. Let's do it.

Since $f_n\rightarrow f$ uniformly (or in the $d_\infty$ metric), there
exists an $N$ such that
\begin{align*}
  d_\infty(f_N, f) < \frac{\varepsilon}{4}
\end{align*}
Using that $N$, consider $f_N$. Since it is continuous, there is a
$\delta_N$ such that
\begin{align*}
  d(x,y) < \delta_N
  \quad\Rightarrow\quad
  d(f_N(x),f_N(y)) < \frac{\varepsilon}{2}
\end{align*}
If we use $\delta = \delta_N$ for $f$, then we can use
Inequality~\ref{Cclosed} to conclude
\begin{align*}
  d(x,y) < \delta_N
  \quad\Rightarrow\quad
  d(f(x),f(y))
  &\leq 2 \; d_\infty(f,f_N) + d(f_N(x),f_N(y))\\
  &\leq 2 \cdot \frac{\varepsilon}{4} + \frac{\varepsilon}{2}
  =\varepsilon
\end{align*}
Therefore $f$ is continuous.
\end{proof}

\begin{cor}{\emph{(The Space of Bounded Continuous Functions)}}
The set of bounded continuous functions $\mathscr{C}(A,\R)\subset
\mathscr{B}(A,\R)$ is a closed set in metric space $(\mathscr{B}(A,\R),
d_\infty)$.
\end{cor}

\begin{proof}
To prove that this set is closed, we again use
Proposition~\ref{prop:closed}.

Suppose that we have some sequence of bounded continuous functions
$\{f_n\}\subseteq \mathscr{C}(A,\R)$ converging uniformly to $f$.
Theorem~\ref{thm:ctslimit} says that $f$ is continuous, i.e.  $f\in
\mathscr{C}(A,\R)$ as well. Since $\mathscr{C}(A,\R)$ contains its limit
points, the set is closed by Proposition~\ref{prop:closed}.
\end{proof}



\begin{thm}{\emph{(Weierstrass)}}
\label{thm:weierstrass}
Given metric space $(X,d_X)$, function $f:X\rightarrow \R$, and
compact $K\subseteq X$, there exist maximum and minimum points for
$f$ on $K$
\begin{align*}
  \bar{x} &= \arg \max_{x\in K} f(x) \\
  \munderbar{x} &= \arg \min_{x\in K} f(x) \\
\end{align*}
\end{thm}
i.e.\ $\bar{x}$ and $\munderbar{x}$ are well-defined.
\begin{rmk}
Compactness is so important because it ensures that the set is ``finite
enough'' is some sense. We know that the max always exists for finite
sets. Now we know that the max (not just the sup) always exists when all
open covers of some set have finite subcovers. It's in the same spirit.
\end{rmk}
\begin{proof}
(Theorem~\ref{thm:weierstrass})
Since $K\subseteq X$ is compact, Theorem~\ref{thm:contcompact} says that
the set $f(K)$ is compact in $\R$.  By
Theorem~\ref{thm:heine-borel}, the set $f(K)$ is therefore closed and
bounded.

Since $f(K)\subseteq \R$ is bounded, we can apply
Proposition~\ref{prop:subseq}, which furnishes a sequence
$\{x_n\}\subseteq f(K)$ that converges to $\sup f(K)$.

Since $f(K)$ is closed and $\{x_n\}\subseteq f(K)$, the limit $\sup
f(K)$ must be in $f(K)$.  Therefore, the set $f(K)$ has a max, and we
can find $\bar{x}$ such that
\begin{align*}
  f(\bar{x}) = \sup f(K) = \sup_{x\in K} f(x)
\end{align*}
\end{proof}


\clearpage
\section{Banach and Hilbert Spaces}
\label{sec:banachhilbert}

Often, we first encounter \emph{norms} and \emph{inner products} as
special functions on vector space $\Rn$ that measure the \emph{length}
of a vector or the \emph{angle} between vectors, respectively.  However,
the definition permits much more general functions over much more
general spaces (as we'll see).

\subsection{Vector Space}

\begin{defn}(Field)
Set $K\subseteq \C$ or $K\subseteq\R$ is a \emph{field} if it satisfies
all of the following:
\begin{enumerate}[label=(\roman*)]
  \item \emph{Identity Elements} $0,1\in K$
  \item \emph{Closed under Addition and Multiplication}:
      $x,y\in K \implies$ $x+y\in K$ and $xy\in K$
  \item \emph{Additive and Multiplicative Inverse}:
    $x\in K \implies -x\in K$
    and (if $x\neq 0$) $x^{-1}\in K$.
\end{enumerate}
\end{defn}

\begin{defn}(Vector Space)
A vector space $V$ over field $K$ is a set of objects closed under the
following operations (which are well-defined):
\begin{itemize}
  \item \emph{Vector Addition}: $v+u\in V$ for $v,u\in V$.
  \item \emph{Scalar Multiplication}: $kv\in V$ where $k\in K$ and $v\in
    V$
\end{itemize}
satisfying the following properties for all $u,v,w\in V$ and $a,b\in K$:
\begin{enumerate}[label=(\roman*)]
  \item \emph{Associativity of Vector Addition}: $(u+v)+w = u + (v+w)$
  \item \emph{Commutatitivity of Vector Addition}: $u+v=v+u$.
  \item \emph{Additive Inverse}: For any $v$, there exists an element
    $-v$ such that $v+(-v)=0$.
  \item \emph{Distributive Property over Vector Addition}:
    $a(u+v) = au + av$
  \item \emph{Distributive Property over Scalar Addition}:
    $(a+b)v = av + bv$
  \item \emph{Multiplicative Associativity}: $(ab)v = a(bv)$
  \item \emph{Additive and Multiplicative Identity Element}:
    There exists a zero element $0\in V$ such that $v+0 = 0+v=0$ and a
    unit element $1\cdot u = u$.
\end{enumerate}
\end{defn}

\begin{defn}(Complex Vector Space)
We will refer to ``vector space $V$ over $K\subseteq\C$'' (where
$K\not\subseteq\R$) more succinctly as \emph{complex vector space} $V$.
\end{defn}

\begin{rmk}
Since $V$ over $K\subseteq\C$ must be closed under scalar
multiplication, using a complex field $K\subseteq\C$ ensures that
vectors in $V$ can take on complex values as well. So ``complex vector
space'' always refers to cases where \emph{both} elements of $K$ and $V$
can take on complex values.
\end{rmk}

\begin{defn}(Real Vector Space)
We will refer to ``vector space $V$ over $K\subseteq\R$'' (where elments
of $V$ take on strictly real values) more succinctly as
\emph{real vector space} $V$.
\end{defn}

\begin{ex}
Three classic examples: field $\C$ over $\Cn$, field $\C$ over $\Rn$, or
field $\R$ over $\Rn$.
\end{ex}

\clearpage
\subsection{Normed Vector Spaces and Banach Spaces}


\begin{defn}(Norm)
Given complex vector space $V$, a function
$\lVert\,\cdot\,\rVert:V\rightarrow \R$ is called a \emph{norm} if it
satisfies the following properties for all $v,u\in V$ and $\alpha\in
\C$:
\begin{enumerate}[label=(\roman*)]
  \item $\lVert v\rVert\geq 0$
  \item $\lVert v\rVert=0 \quad \iff \quad v=0$
  \item $\lVert \alpha v\rVert= |\alpha| \cdot \lVert v\rVert$
  \item $\lVert v+u\rVert \leq \lVert v\rVert + \lVert u\rVert$
\end{enumerate}
\end{defn}

\begin{defn}(Semi-Norm)
A \emph{semi-norm} is a norm \emph{except} for Condition (ii). That is,
there can exist nonzero vectors that the semi-norm maps to zero.
\end{defn}

\begin{prop}\emph{(Norms are Continuous)}
Let $\lVert \,\cdot\,\rVert$ be a norm on some complex vector space.
That norm is a (Lipschitz) continuous function.
\end{prop}
\begin{proof}
Notice that
\begin{align*}
    \big\lvert \lVert x \rVert - \lVert y\rVert \big\rvert
    \leq
    \big\lvert \lVert x - y + y\rVert - \lVert y\rVert \big\rvert
    \leq
    \big\lvert \lVert x - y \rVert + \lVert y\rVert - \lVert y\rVert \big\rvert
    \leq
    \lVert x - y \rVert
\end{align*}
So given $\varepsilon>0$, choosing $x$ and $y$ sufficiently close such
that $\lVert x-y\rVert<\varepsilon$ implies that the distance between
the function evaluated at $x$ and $y$ (i.e.
$\lVert x\rVert- \lVert y\rVert$) is sufficiently small.
\end{proof}

\begin{defn}(Normed Vector Space)
We call a complex (or real) vector space $V$ equipped with a norm a
\emph{normed vector space}.
\end{defn}


\begin{defn}(Banach Space)
We use the term \emph{Banach Space} for a a \emph{complete} normed
vector space, i.e. all Cauchy series are convergent using the metric
defined by the norm:
\begin{align*}
  d(x,y) = \lVert x-y\rVert
\end{align*}
\end{defn}

\begin{ex}(Norms in $\Rn$)
Here are a few classic norms for vectors $x\in \Rn$:
\begin{enumerate}
\item $\lVert x\rVert_1 = \sum^n_{i=1} |x_i|$
\item $\lVert x\rVert_2 = \left(\sum_{i=1}^n |x_i|^2\right)^{\frac{1}{2}}$
\item $\lVert x\rVert_p = \left(\sum_{i=1}^n
  |x_i|^p\right)^{\frac{1}{p}}$ for $p\in[1,\infty]$, not necessarily an
  integer.
\item $\lVert x\rVert_\infty = \max_{1\leq i\leq n} |x_i|$
\end{enumerate}
\end{ex}

\begin{ex}(Semi-Norm)
Suppose we take real vector space $V=\Rn$. Then for some $m<n$, define
\begin{align*}
  \lVert x\rVert = \sum_{i=1}^m |x_i|
\end{align*}
Thus we are adding up the absolute value of the first $m<n$ entries
only.
\end{ex}


\clearpage

\begin{ex}(Norms and Semi-Norms in Function Spaces)
We now consider vector spaces where vector elements are \emph{functions}
rather than points in $\Rn$.
\begin{enumerate}
  \item $\mathscr{B}(A,\R^n)$:
    The class of bounded functions $f:A\ra \Rn$ denoted $\sB(A,\Rn)$ is
    a vector space. We can define the \emph{sup-norm} on this vector
    space
    \begin{align*}
      \lVert f\rVert_\infty
      = \sup_{x\in A} |f(x)|
    \end{align*}
    Since Completeness Axiom~\ref{ax:completeness} states that the sup
    always exists for any function bounded in $\R^n$, this norm is
    well-defined.

    We can also use this norm to compactly define $\sB(A,\Rn)$. Rather
    than referring to $\sB(A,\Rn)$ as the ``Space of bounded functions
    $f:A\ra\Rn$'', can instead simply define
    \begin{align*}
      \sB(A,\Rn) := \{f:A\ra\Rn\;|\; \lVert f\rVert_\infty<\infty\}
    \end{align*}
    As always, can also use this norm to define a \emph{metric space}
    $(\sB(A,\Rn),d_\infty)$ where
    \begin{align*}
      d_\infty(f,g) = \lVert f-g\rVert
    \end{align*}
    Intuitively, this metric looks at how far apart $f$ and $g$ are at
    each $x\in A$, and then sets $d_\infty(f,g)$ equal to the gap
    between the functions at the point of ``maximal gap.''

  \item $C'([a,b])$:
    The set of once-differentiable continuous functions
    $f:[a,b]\ra\Rn$ is bounded because continuous on compact domain
    $[a,b]$. We can use the previous definition of the sup-norm to
    define a metric on $C'([a,b])$:
    \begin{align*}
      \lVert f\rVert := \lVert f\rVert_\infty + \lVert f'\rVert_\infty
    \end{align*}

  \item ($L_p$ Space)
    Consider the space $X$ of $L_p$ integrable functions:
    \begin{align*}
      X = \left\{
        f:A\rightarrow \R \; \big|\;
        \int_{x\in A} |f(x)|^p dx <\infty
      \right\}
    \end{align*}
    On this space, we can define the metric
    \begin{align*}
      d_p(f,g) = \left(\int_{x\in A} |f-g|^p \; dx\right)^{1/p}
    \end{align*}
\end{enumerate}
\end{ex}

\begin{ex}
The metric space $\left(\mathscr{B}(A,\R^n), d_\infty\right)$,
where $\mathscr{B}(A,\R^n)$ is the set of all bounded functions
$f:A\rightarrow \R^n$, is a Banach space.
\end{ex}



\clearpage
\subsection{Inner Product Spaces and Hilbert Spaces}

\begin{defn}(Inner Product)
Given complex vector space $V$, an \emph{inner product} is a function
$\langle \cdot, \cdot \rangle: V\times V \ra \C$ such that, for all
$u,v,z\in V$ and $a,b\in\C$, it satisfies
\begin{enumerate}[label=(\roman*)]
  \item $\langle v,v \rangle \geq 0$
  \item $\langle v,v\rangle =0 \iff v=0$.
  \item $\langle u,v\rangle = \overline{\langle v,u\rangle}$
  \item $\langle au + bv, z\rangle
    = a\langle u, z\rangle + b\langle v, z\rangle$
\end{enumerate}
In the special case that $V$ is a \emph{real} vector space, everything
is the same except that we can drop the complex conjugation on the RHS
of condition (iii).
\end{defn}

\begin{ex}
We can define the usual inner product on complex vector space $\Cn$:
\begin{align*}
  \langle x,y\rangle = \sum_{i=1}^n x_i \bar{y}_i
\end{align*}
\end{ex}

\begin{prop}\emph{(Induced Norm)}
Given inner product $\langle \cdot, \cdot \rangle: V\times V \ra \C$ on
complex vector space $V$, there is an \emph{induced norm} that can be
defined as
\begin{align*}
  \lVert x\rVert = \sqrt{\langle x, x\rangle}
\end{align*}
From now on, when you see the norm $\lVert \cdot \rVert$ in this
subsection, it will refer to the induced norm.
\end{prop}
\begin{proof}
To show $\lVert x \rVert =\sqrt{\langle x,x\rangle}$ is indeed a norm,
we must show it satisfies all of the properties.
\begin{enumerate}[label=(\roman*)]
  \item By Condition (i) of inner products, $\langle x,x \rangle\geq 0$;
    therefore $\lVert x\rVert = \sqrt{\langle x,x \rangle} \geq 0$.
  \item By Condition (ii) of inner products, $\langle x,x \rangle= 0$ if
    and only if $x=0$. Therefore, $\lVert x\rVert = \sqrt{\langle x,x
    \rangle} = 0$ if and only if $x=0$.
  \item Using Linearity Condition (iv),
      $\lVert \alpha x\rVert
      = \sqrt{\langle \alpha x, \alpha x\rangle}
      = \sqrt{\alpha^2\langle x, x \rangle}
      = |\alpha|\sqrt{\langle x, x \rangle}
      = |\alpha|\, \lVert x\rVert$
  \item We will again use Condition (iv) linearity along with Condition
    (iii) symmetry:
    \begin{align*}
      \lVert x+y\rVert^2
      =
      \langle x+y, x+y\rangle
      &=
      \langle x, x\rangle
      +
      \langle x, y\rangle
      +
      \langle y, x\rangle
      +
      \langle y, y\rangle
      \\
      &=
      \lVert x\rVert^2
      +
      \langle x, y\rangle
      +
      \overline{\langle x, y\rangle}
      +
      \lVert y\rVert^2
      \\
      &=
      \lVert x\rVert^2
      +
      2\Re\big( \langle x, y\rangle\big)
      +
      \lVert y\rVert^2
      \\
      &\leq
      \lVert x\rVert^2
      +
      2\left\lvert\Re\big( \langle x, y\rangle\big)\right\rvert
      +
      \lVert y\rVert^2
      \\
      \text{Since $|\Re(a+bi)|=|a| \leq \left\lvert\sqrt{a^2+b^2}\right\rvert$}\qquad\quad
      &\leq
      \lVert x\rVert^2
      +
      2\left\lvert\langle x, y\rangle\right\rvert
      +
      \lVert y\rVert^2
    \end{align*}
    Next, apply Cauchy-Schwartz (derived in the next subsection) to the
    middle term:
    \begin{align*}
      \lVert x+y\rVert^2
      &\leq
      \lVert x\rVert^2
      +
      2\lVert x\rVert\cdot\lVert y\rVert
      +
      \lVert y\rVert^2
      =
      \left(
      \lVert x\rVert
      +
      \lVert y\rVert
      \right)^2
      \\
      \implies\quad
      \lVert x+y\rVert
      &\leq
      \lVert x\rVert
      +
      \lVert y\rVert
    \end{align*}

\end{enumerate}
\end{proof}


\begin{defn}(Inner Product Space)
We use the term \emph{inner product space} for a complex (or real)
vector space $V$ that is equipped with an inner product
$\langle\,\cdot,\,\cdot\rangle$, denoted
$(V,\langle\,\cdot,\,\cdot\rangle)$.
\end{defn}

\begin{defn}(Hilbert Space)
We use the term \emph{Hilbert Space} for a \emph{complete} inner product
space $(V,\langle\,\cdot,\,\cdot\rangle)$,
i.e.\ where the \emph{induced norm} gives us a metric
\begin{align*}
  d(x,y) = \lVert x-y\rVert
  = \sqrt{\langle x-y,\; x-y\rangle}
\end{align*}
under which all Cauchy sequences in the vector space are convergent.
Therefore, all Hilbert are Banach spaces---complete under metric $d$
above.
\end{defn}

\begin{lem}
Given sequence $\{x_n\}$ on an inner product space with induced norm
$\lVert\,\cdot\,\rVert$,
\begin{align*}
  \lVert x_n - x \rVert \ra 0
  \quad\implies\quad
  \lVert x_n \rVert \ra
  \lVert x \rVert
\end{align*}
\end{lem}
\begin{proof}
This follows from the triangle inequality because
\begin{align*}
  \lVert x_n - x + x\rVert
  \leq
  \lVert x_n - x\rVert + \lVert x\rVert
  \quad\implies\quad
  0 \leq
  \big\lvert
  \lVert x_n\rVert - \lVert x\rVert
  \big\rvert
  \leq
  \lVert x_n - x\rVert
  \ra 0
\end{align*}
\end{proof}

\begin{prop}\emph{(Continuity of the Inner Product)}
Let $\{x_n\}$ and $\{y_n\}$ be sequence on an inner product space which
has induced norm $\lVert\,\cdot\,\rVert$. Then
\begin{align*}
  \begin{rcases}
    \lVert x_n -x \rVert \ra 0 \\
    \lVert y_n -y \rVert \ra 0 \\
  \end{rcases}
  \quad\implies\quad
  \langle x_n, y_n\rangle
  \ra
  \langle x, y\rangle
\end{align*}
\end{prop}
\begin{proof}
We want to prove the following goes to zero:
\begin{align*}
  \big\lvert
  \langle x_n, y_n\rangle
  -
  \langle x, y\rangle
  \big\rvert
  &=
  \big\lvert
  \langle x_n, y_n-y\rangle
  + \langle x_n, y\rangle
  - \langle x, y\rangle
  \big\rvert
  \\
  &=
  \big\lvert
  \langle x_n, y_n-y\rangle
  + \langle x_n-x, y\rangle
  \big\rvert
\end{align*}
Next, we apply Cauchy-Schwarz, which will be proved in the next
subsection, to get
\begin{align*}
  \big\lvert
  \langle x_n, y_n\rangle
  -
  \langle x, y\rangle
  \big\rvert
  &=
  \big\lvert
  \langle x_n, y_n-y\rangle
  + \langle x_n-x, y\rangle
  \big\rvert
  \\
  &\leq
  \lVert x_n \rVert
  \cdot
  \lVert y_n - y\rVert
  +
  \lVert x_n -x\rVert
  \cdot
  \lVert y\rVert
\end{align*}
By the last lemma, $\lVert x_n\rVert\ra \lVert x\rVert$ and, by
assumption both $\lVert y_n - y\rVert$ and $\lVert x_n - y\rVert$
converge to zero, the whole object converges to zero.
\end{proof}


\begin{prop}\emph{(Parallelogram Law and Polarization Identity)}
Let $\lVert\,\cdot\,\rVert$ be the induced norm from some inner product
space.
Then we have the \emph{parallelogram law}
\begin{align*}
  \lVert x+y\rVert^2 +
  \lVert x-y\rVert^2 =
  2\left( \lVert x \rVert^2 + \lVert y \rVert^2 \right)
\end{align*}
If the vector space is \emph{real}, then we also have the
\emph{polarization identity}
\begin{align*}
  \lVert x+y\rVert^2 +
  \lVert x-y\rVert^2 = 4\langle x,y \rangle
\end{align*}
\end{prop}
\begin{rmk}
Though this might appear to be a proposition about norms in general,
it's \emph{not}. It holds \emph{only} for norms induced by inner
products, which is why it's presented in this subsection.
\end{rmk}
\begin{proof}
First the parallelogram law:
\begin{align*}
  \lVert x+y\rVert^2 +
  \lVert x-y\rVert^2
  &=
  \langle x+y, x+y\rangle
  + \langle x-y, x-y\rangle \\
  &=
  \langle x, x\rangle
  + \langle x, y\rangle
  + \langle y, x\rangle
  + \langle y, y\rangle \\
  &\qquad
  + \langle x, x\rangle
  - \langle x, y\rangle
  - \langle y, x\rangle
  + \langle y, y\rangle \\
  &=
  2\langle x, x\rangle
  + 2\langle y, y\rangle \\
  &=
  2\lVert x\rVert
  + 2\lVert y\rVert
\end{align*}
And now for the polarization identity, recalling that this assumes we
have a \emph{real} vector space so that
$\langle x,y\rangle = \langle y,x\rangle$.
\begin{align*}
  \lVert x+y\rVert^2 -
  \lVert x-y\rVert^2
  &=
  \langle x+y, x+y\rangle
  - \langle x-y, x-y\rangle \\
  &=
  \langle x, x\rangle
  + \langle x, y\rangle
  + \langle y, x\rangle
  + \langle y, y\rangle \\
  &\qquad
  - \langle x, x\rangle
  + \langle x, y\rangle
  + \langle y, x\rangle
  - \langle y, y\rangle \\
  &=
  4\langle x, y\rangle
\end{align*}
\end{proof}

\begin{ex}($L^2(\Omega,\sF,P)$)
The space $\sL^2(\Omega,\sF,P)$ consists of all complex random variables
(measurable functions) $X:\Omega\ra \R$ on probabability space
$(\Omega,\sF,P)$ with finite second moment:
\begin{align*}
  \int_\Omega |X(\omega)|^2 \; P(d\omega) <\infty
\end{align*}
We can easily verify that $\sL^2(\Omega,\sF,P)$ is a vector space by
checking that sums of random variables $X+Y$ and
products of random variables $X\cdot Y$ for $X,Y\in L^2(\Omega,\sF,P)$
are also in $\sL^2(\Omega,\sF,P)$, as well as verifying the other that
must hold for vector spaces.

Next, define $L^2(\Omega,\sF,P)$ by identifying all equivalence clases
in $\sL^2(\Omega,\sF,P)$, i.e. by treating distinct $X$ and $Y$
in $\sL^2(\Omega,\sF,P)$ as a single object in $L^2(\Omega,\sF,P)$ if
they are equal almost everywhere, $P(X=Y)=1$. Then
$L^2(\Omega,\sF,P)$ equipped with inner product
\begin{align*}
  \langle X,Y \rangle
  = \E[X\overline{Y}]
  = \int_\Omega X(\omega)\overline{Y(\omega)} \; P(d\omega)
\end{align*}
is a complex Hilbert space, which is verified by checking all the
properties.
\end{ex}



\clearpage
\subsection{Cauchy-Schwarz}

\begin{thm}
\label{thm.cauchyscwarz}
\emph{(Cauchy-Schwartz Inequality)}
Given any inner product space, let
$\lVert x\rVert := \sqrt{\langle x,x\rangle}$ represent the induced
norm.  Then
\begin{align*}
  \lvert \langle x,y\rangle\rvert \leq \lVert x\rVert\cdot\lVert y\rVert
\end{align*}
The inequality becomes an equality if and only if
\begin{align*}
  x = \frac{\langle x,y\rangle}{\lVert y\rVert}y
\end{align*}
\end{thm}
\begin{proof}
Start by assuming that $|\alpha|=1$ and that $\alpha\in\mathbb{C}$. Then
define
\begin{align*}
  Q_\alpha(t) = \lVert x - t\alpha y\rVert^2
\end{align*}
By the definition of $\lVert \cdot\rVert$, it's clear that
$Q_\alpha(t)\geq 0$ for all $t\in\mathbb{R}$. Then let's use the
properties of the inner product to write
\begin{align}
    Q_\alpha(t) &= \langle x -t\alpha y,\; x -t\alpha y\rangle \notag \\
    &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2
    - t\alpha \overline{\langle x,y\rangle}
    - t\bar{\alpha} {\langle x,y\rangle} \label{cauchyhold}
\end{align}
Without loss of generality, assume $\langle x,y\rangle\neq 0$, otherwise
trivial. Now since $\alpha$ was only restricted to be such that
$|\alpha|=1$, we can choose it to be whatever we want, and use the fact
that the Equality \ref{cauchyhold} \emph{must} be true for all
$|\alpha|=1$ and $t\in\mathbb{R}$, given the definition fo the inner
product. So let
\begin{align*}
    \alpha = \frac{\langle x,y\rangle}{|\langle x,y\rangle|}
    \quad\implies\quad
    Q_\alpha(t) &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2
    - t\frac{\langle x,y\rangle\overline{\langle x,y\rangle}}{\langle x,y\rangle}
    - t\frac{\overline{\langle x,y\rangle}{\langle x,y\rangle}}{\langle x,y\rangle}  \\
    &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2
    -2t |\langle x,y\rangle|
\end{align*}
Next, we minimize the function over $t$. The first order conditions to
that minimization are
\begin{align*}
  0 &= \frac{d}{dt}[Q_\alpha(t)]
    = 2t \lVert y\rVert^2 -2 |\langle x,y\rangle|
  \qquad\implies\qquad
  t = \frac{|\langle x,y\rangle|}{\lVert y\rVert^2}
\end{align*}
We will use this as our $t$ in $Q_\alpha(t)$ and simplify:
\begin{align*}
  Q_\alpha\left(
  \frac{|\langle x,y\rangle|}{\lVert y\rVert^2}
  \right)
  &= \lVert x\rVert^2 +
  \left(\frac{|\langle x,y\rangle|}{\lVert y\rVert^2}\right)^2
  \lVert y\rVert^2
  -2
  \left(\frac{|\langle x,y\rangle|}{\lVert y\rVert^2}\right)
  |\langle x,y\rangle| \\
  &= \lVert x\rVert^2
  - \frac{|\langle x,y\rangle|^2}{\lVert y\rVert^2}
\end{align*}
Recalling that $Q_\alpha(t)\geq 0$ for all $t\in\R$, we can use this
inequality for the RHS of the above expression to conclude
\begin{align*}
  0 &\leq
   \lVert x\rVert^2
  - \frac{|\langle x,y\rangle|^2}{\lVert y\rVert^2} \\
  \implies\quad
  |\langle x,y\rangle| &\leq
   \lVert x\rVert \cdot \lVert y\rVert
\end{align*}
\end{proof}



\clearpage
\subsection{Orthogonality}

\begin{defn}(Angle)
Given \emph{any} real inner product space, define the \emph{angle}
between any two non-zero elements as
\begin{align*}
  \theta = \cos^{-1}\left(
  \frac{\langle x,y \rangle}{\lVert x\rVert \cdot \lVert y \rVert}
  \right)
\end{align*}
\end{defn}

\begin{defn}(Orthogonal)
Two vectors, $v,u\in V$ \emph{orthogonal} if $\langle u,v\rangle=0$ or,
equivalently (for a real inner product space), their angle is
$\theta=\pi/2$. We denote orthogonality by $u\perp v$.
\end{defn}

\begin{defn}(Orthogonal and Orthonormal Families)
We call a collection of vectors $\{v_i\}$ an \emph{orthogonal family} if
$v_i\perp v_j$ for all $i\neq j$. We call the family \emph{orthonormal}
if, in addition to being an orthogonal family, $\lVert v_i\rVert=1$ for
all $i$.
\end{defn}

\begin{thm}
\label{pythag}
\emph{(Pythagorean Relation)} Given $x\perp y$,
\begin{align*}
    \lVert x + y\rVert^2 = \lVert x\rVert^2 + \lVert y \rVert^2
\end{align*}
More generally, given an orthonormal family $\{u_1,\ldots,u_N\}$:
\begin{align*}
  \left\lVert \sum_{i=1}^N c_i u_i \right\rVert^2
  = \sum_{i=1}^N |c_i|^2
\end{align*}
\end{thm}
\begin{proof}
Simply write out
\begin{align*}
  \lVert x + y\rVert^2 =
  \langle x + y, x + y\rangle
  =
  \langle x , x \rangle
  +
  \langle x , y \rangle
  +
  \langle y , x \rangle
  +
  \langle y , y \rangle
\end{align*}
Since $x\perp y$, we know that $\langle x,y \rangle = \langle y,x
\rangle =0$, hence
\begin{align*}
  \lVert x + y\rVert^2 =
  \langle x , x \rangle
  +
  \langle y , y \rangle
  =
  \lVert x\rVert^2 + \lVert y\rVert^2
\end{align*}
\end{proof}

\begin{defn}(Orthogonal Complement)
Given Hilbert space $\calH=(V,\langle\,\cdot,\,\cdot\rangle)$, the
\emph{orthogonal complement} of $A\subseteq V$ in $\calH$, denoted
$A^\perp$, is defined
\begin{align*}
  A^\perp :=
  \{
    x \in V \;|\; \langle x,y\rangle = 0
    \quad \forall y\in A
  \}
\end{align*}
\end{defn}

%\begin{thm}
%Given an orthonormal set $\{u_1,\ldots,u_N\}$,
%\begin{align*}
  %\lVert x\rVert^2 =
  %\sum_{i=1}^N \bigl\lvert\langle x,u_n\rangle\bigr\rvert^2
  %+ \left\lVert x -  \sum_{i=1}^N\langle x,u_n\rangle u_n \right\rVert^2
%\end{align*}
%\end{thm}
%\begin{proof}
%In $\lVert x\rVert^2$,substitute in
%\begin{align*}
  %\lVert x\rVert^2 =
  %\left\lVert
  %\sum_{n=1}^N \langle x,u_n\rangle u_n
  %+ \left( x
  %-\sum_{n=1}^N \langle x,u_n\rangle u_n\right)
  %\right\rVert^2
%\end{align*}
%Then use the Pythagorean Relation in Theorem \ref{pythag}, since these
%two objects in the sum are orthogonal by Theorem \ref{projorth}.
%\end{proof}

%\begin{cor}
%\emph{(Bessel's Inequality)}
%Given an orthonormal set $\{u_1,\ldots,u_N\}$,
%\begin{align*}
    %\sum_{i=1}^N
    %\bigl\lvert\langle x,u_n\rangle\bigr\rvert^2
    %\leq \lVert x\rVert^2
%\end{align*}
%\end{cor}

%\begin{rmk}
%Cauchy Schwarz can also be proved via Bessel's Inequality if we take
%$N=1$ and $u_1=\frac{y}{\lVert y\rVert}$. Then
%\begin{align*}
    %\frac{\langle x,y\rangle^2}{\lVert y\rVert^2} \leq \lVert x\rVert^2
    %\quad\Rightarrow\quad
    %\langle x,y\rangle \leq \lVert x\rVert\cdot \lVert y\rVert
%\end{align*}
%\end{rmk}


\clearpage
\subsection{Projections and the Projection Theorem}

\begin{thm}\emph{(Projection Theorem)}
Given Hilbert space $\calH=(V,\langle\,\cdot,\,\cdot\rangle)$,
suppose that $A\subseteq V$ is a \emph{closed} subspace of $V$.
Then for any $x\in V$, there exists a \emph{unique}
$\hat{x}\in A$ s.t.
\begin{align}
  \lVert x-\hat{x} \rVert =
  \inf_{y\in A} \; \lVert x-y\rVert
  \label{projthm}
\end{align}
where $\lVert\;\cdot\;\rVert$ is the norm induced by the inner product
of $\calH$. In words, this theorem says that there is a unique element
$\hat{x}\in A$ that does, in fact, attain the infimum and minimize the
distance between $x$ and subspace $A$.
\end{thm}

\begin{defn}(Orthogonal Projection)
Element $\hat{x}\in A$ is called the \emph{orthogonal projection} of
$x$ onto $A$. To emphasize the space $x$ is projected \emph{onto}, we
sometimes denote $\hat{x}=\proj(x|A)$.
\end{defn}

\begin{defn}(Error/Residual)
Given $\hat{x}=\proj(x|A)$, we can define the \emph{error} or
\emph{residual}
\begin{align*}
  \varepsilon:=x-\hat{x}
  \quad\iff\quad
  x=\hat{x}+\varepsilon=\proj(x|A)+\varepsilon
\end{align*}
\end{defn}

\begin{cor}
\emph{(Alternative Characteriziation of $\hat{x}=\proj(x|A)$ using
$\varepsilon$)}
Alternatively, we can characterize the projection $\hat{x}=\proj(x|A)$
satisfying Expression~\ref{projthm} as the unique point in $A$ such that
the resulting error $\varepsilon=x-\hat{x}$ is orthogonal to all
elements of $A$:
\begin{align*}
  \langle \varepsilon, y\rangle =0
  \quad\forall y\in A
  \quad\iff\quad
  \varepsilon\in A^\perp
\end{align*}
\end{cor}

\begin{prop}\emph{(Properties of Projections)}
Given Hilbert space $\calH=(V,\langle\,\cdot,\,\cdot\rangle)$,
for all $x,y,w\in V$, $A\subseteq V$, and $a,b\in\C$,
\begin{enumerate}[label=\emph{(\roman*)}]
  \item \emph{Projections are linear}:
    $\proj(ax+by|A) = a\proj(x|A)+b\proj(y|A)$
  \item \emph{Projections decompose length}:
    $\lVert x\rVert^2
    = \lVert\hat{x}\rVert^2 + \lVert \varepsilon\rVert^2$
  \item $x\in A\subseteq V$ if and only if $\hat{x}=\proj(x|A)=x$.
  \item $x\in A^\perp \subseteq V$ if and only if
    $\hat{x}=\proj(x|A)=0$.
\end{enumerate}
\end{prop}
\clearpage
\begin{proof}
We prove each in turn
\begin{enumerate}[label=(\roman*)]
  \item
  \item As discussed, $\varepsilon\perp y$ for all $y\in A$.
    Since $\hat{x}\in A$, we have $\varepsilon\perp \hat{x}$.
    Then the Pythagorean relation and $x=\hat{x}+\varepsilon$ gives us
    $\lVert x\rVert^2 = \lVert \hat{x}+\varepsilon\rVert^2 =\lVert
    \hat{x}\rVert^2 + \lVert \varepsilon\rVert^2$
  \item
    ($\Rightarrow$)
    If $x\in A$, we can minimize Expression~\ref{projthm}
    all the way to zero by choosing $\hat{x}=\proj(x|A)=x\in A$.

    ($\Leftarrow$) If $\hat{x}=x$, the fact that we require
    $\hat{x}\in A$ implies that $x\in A$.
  \item Similar to (iii)
\end{enumerate}
\end{proof}

\begin{cor}
Define the following subspace of $V$:
\begin{align*}
  A_w:=\{\alpha w\;|\; \alpha\in \R, \; w\in V\}
\end{align*}
In words, $A_w\subset V$ is the subspace of $V$ formed by stretching
vector $w\in V$ by $\alpha\in\R$.
Then
\begin{align*}
  \langle x, y \rangle
  = \lVert x\rVert \times \lVert \proj(y|A_x)\rVert
  = \lVert y\rVert \times \lVert \proj(x|A_y)\rVert
\end{align*}
\end{cor}
\begin{proof}
We prove only
$\langle x, y \rangle
= \lVert x\rVert \times \lVert \proj(y|A_x)\rVert$.
The remaining equality is exactly analogous.
So to start, relate $y$ and $\proj(y|A_x)$ by
\begin{align*}
  y &= \alpha_y \, x + \varepsilon
  \qquad
  \text{where}\quad
  \alpha_y\, x= \proj(y|A_x)\in A_x,
  \quad \varepsilon\in A_x^\perp
\end{align*}
Notation $\alpha_y$ emphasizes that $\alpha_y$ (the stretch factor) is
specific to the vector $y$ that we are projecting onto $A_x$.
First, compute
\begin{align*}
  \lVert \proj(y|A_x)\rVert
  = \lVert \alpha_y \, x\rVert
  = \sqrt{\langle
    \alpha_y \, x,
    \;\alpha_y \, x\rangle}
  = \alpha_y \sqrt{\langle x, x\rangle}
  = \alpha_y \,\lVert x\rVert
\end{align*}
Next, use $y=a_y\,x + \varepsilon$ and $\varepsilon\perp x$ so that
$\langle x,\varepsilon\rangle=0$ to get
\begin{align*}
  \langle x, y \rangle
  =
  \langle x, \; \alpha_y \, x + \varepsilon \rangle
  =
  \alpha_y \langle x, x \rangle
  + \langle x, \varepsilon \rangle
  &=
  \alpha_y \lVert x\rVert^2
  + 0
  %\implies\quad
  %\langle x, y \rangle
  = \alpha_y \lVert x\rVert^2
  %=
  %\alpha_y \lVert x\rVert
  %\times
  %\lVert x\rVert
\end{align*}
But together with
$\lVert \proj(y|A_x)\rVert = \alpha_y\lVert x\rVert$, this last equality
implies the desired result.
\end{proof}

\clearpage
\section{Differentiability}

Note that we mostly use open sets because the derivative needs to be
defined from both sides of the point.

\subsection{Differentiability: $\R\rightarrow\R$}

\begin{defn}{(Derivative in $\R$)}
\label{defn:derivative}
Given a function $f:X\rightarrow\R$, we define define the
\emph{derivative} of $f$ at $x\in X$ as
\begin{align*}
  f'(x) := \lim_{\delta\rightarrow0}
  \frac{f(x+\delta)-f(x)}{\delta}
\end{align*}
provided that this limit exists. If the limit does not exist, the
derivative is not defined.
\end{defn}

\begin{defn}{(Differentiable in $\R$)}
\label{defn:diffable}
The function $f:\R\rightarrow\R$ is \emph{differentiable} at $x\in\R$ if
there exists an $m_x\in\R$, an $\varepsilon>0$, and a \emph{remainder}
function $r_x:(-\varepsilon,\varepsilon)\rightarrow\R$ such that
\begin{align}
  \label{eq:diffapprox}
  f(x+\delta)-f(x) = m_x \delta + r_x(\delta)
  \qquad \forall \delta\in(-\varepsilon,\varepsilon), \; \delta \neq 0
\end{align}
with remainder function $r_x$ such that
\begin{align}
  \label{lim:remainder}
  \lim_{\delta\rightarrow 0}
  \frac{r_x(\delta)}{\delta}=0
\end{align}
Whereas Definition~\ref{defn:derivative} provided the usual analysis
definition of a derivative, this definition provides more of a
``function approximation'' definition. Specifically, a function is
differentiable at $x$ if it is approximately linear in some neighborhood
of $x$.  Line~\ref{lim:remainder} tells us that ``approximately'' here
means that the remainder or approximation error be much smaller than
the size of the increment around $x$. In other words, the remainder is
quantitatively unimportant, and the linear approximation is (Larry David
voice) pretty, pretty good.

Of course, this definition is equivalent to
Definition~\ref{defn:derivative} in the sense that ``differentiability
at $x$'' is equivalent to ``the derivative exists $x$'', in accordance
with colloquial usage. And that equivalence is precisely the next
theorem.
\end{defn}

\paragraph{Main Idea} The whole point of differentiability is to
approximate increments of the function $f(x+\delta)-f(x)$ by functions
of the increment $(x+\delta)-x=\delta$. We see this clearly in
Equation~\ref{eq:diffapprox}.

\begin{thm}
$f$ is differentiable at $x$ if and only if the derivative $f'(x)$
exists. Moreover, $f'(x)=m_x$ where $m_x\in\R$ is the slope used in the
approximation.
\end{thm}
\begin{proof}
($\Rightarrow$) Suppose that $f$ is differentiable at $x$. Then there
exist $m_x$ and $r_x$ such that
\begin{align*}
  f(x+\delta)-f(x) = m_x\delta + r_x(\delta)
\end{align*}
Dividing by $\delta$ and taking the limit,
\begin{align*}
  f'(x)=\lim_{\delta\rightarrow 0}
  \frac{f(x+\delta)-f(x)}{\delta}
  &= \frac{m_x\delta + r_x(\delta)}{\delta}\\
  &= m_x + \lim_{\delta\rightarrow 0} \frac{r_x(\delta)}{\delta}\\
  \text{By assumption}\quad
  &= m_x + 0 = m_x
\end{align*}
Hence, the limit exists and equals $m_x$, therefore $f'(x)=m_x$.


($\Leftarrow$) Suppose that the derivative $f'(x)$ exists.  Then
Equation~\ref{eq:diffapprox} suggests how to back out and define
$r_x(\delta)$:
\begin{align*}
  r_x(\delta) := f(x+\delta)-f(x) - f'(x)\delta
\end{align*}
Now let's check that this thing goes to zero more quickly than $\delta$:
\begin{align*}
  \lim_{\delta\rightarrow 0}
  \frac{r_x(\delta)}{\delta}
  &=
  \frac{f(x+\delta)-f(x) - f'(x)\delta}{\delta}\\
  &=
  \lim_{\delta\rightarrow 0}
  \frac{f(x+\delta)-f(x)}{\delta}
  -f'(x) \\
  &= f'(x) -f'(x)=0
\end{align*}
So we just take $m_x=f'(x)$.
\end{proof}

\subsection{Differentiability: $\Rn\rightarrow\R$}


We want to define derivatives for functions from $\Rn$ to $\R$, similar
to what we did in the last subsection. But things are slightly more
complicated.  Specifically, in $\R$, it's quite clear how to define the
derivative. Pick a point like $x=1$ and see how much the graph of the
function changes for small steps to the left and right of $x$. But that
simplicity follows from the domain being $\R$ rather than $\Rn$. In
$\Rn$, we'll see that we can move in more than one direction within the
domain.

Take a simple case, and consider a function $f:\R^2\rightarrow \R$.
Notice that you can step in \emph{any} direction from $x$. Of course,
there are a few ``natural'' directions to step, like in the direction of
the $x$ or $y$ axes. These are like the north, south, east, west
directions. But really, you can step in \emph{any} direction in between.
So when talking about the derivative, we need to specify not only how
the function is changing, but the \emph{direction} that we're stepping
in when we compute that change. That leads us to the definition of the
\emph{directional} derivative.
\begin{figure}[htpb!]
\centering
\begin{tikzpicture}[xscale=2, shorten >= -3pt, shorten <= -3pt]
  \draw[d4black, <->] (0,5.5) node[left] {$f(x)$} -- (0,0) -- (3.5,0) node[below] {$x$};
  \draw[d4blue, ultra thick, domain=0.05:3] plot (\x, {-pow(\x,2)+3.5*\x+1});
  \draw[d4gray, thick, domain=0.05:2] plot (\x, {1.5*\x+2});
  \draw[d4gray, thick, dashed] (1,0.1) -- (1,3.4);
  \node[d4black] at (1,-0.3) {$x=1$};
\end{tikzpicture}
\end{figure}

\begin{defn}{(Directional Derivative in $\Rn$)}
Given function $f:\Rn\rightarrow\R$, vector $v\in\Rn$, and point $x\in
\Rn$, define a function of one variable only
\begin{align*}
  \varphi_v(t) = f(x+tv)
\end{align*}
You can think of this function as starting at point $f(x)$ and tracing
out points on the graph or surface of $f$ in the direction of vector
$v$, for \emph{any} possible direction.

From there, define the \emph{directional derivative} of $f$ at point $x$
in direction $v$ as
\begin{align*}
  T_x(v) := \varphi_v'(0)
  = \lim_{t\rightarrow 0} \frac{\varphi_v(t)-\varphi_v(0)}{t}
\end{align*}
Again, this holds for \emph{any} direction $v\in\Rn$. In the next
definition, we define the familiar concept of partial derivatives, which
are directional derivatives in the direction of the principal axes or
``canonical base.''
\end{defn}

\begin{defn}{(Partial Derivatives)}
Given $f:\Rn\rightarrow\R$, the \emph{partial derivative} at $x\in\Rn$
in canonical basis direction $i$ is defined as
\begin{align*}
  f_i(x) := T_x(e_i) = \frac{\partial f}{\partial x_i}
\end{align*}
Intuitively, move only in direction $e_i$ and see how the function
changes. Hold all other function arguments fixed. This is equivalent to
movement in the direction of the axes.
\end{defn}

\begin{defn}{(Gradient)}
For function $f:\Rn\rightarrow\R$, define the \emph{gradient} of $f$ as
\begin{align*}
  \nabla f :=
  \begin{pmatrix}
    f_1(x) & \cdots & f_n(x)
  \end{pmatrix}^T \in \Rn
\end{align*}
\end{defn}

We now want to consider the subject of ``differentiability,'' which, as
in the last subsection, refers to whether we can construct a good
approximation of a function in the neighborhood of some point---this
time using a \emph{hyperplane} in $\Rn$ around that point (rather than
just a line) or using a quadratic form.  Approximation by a hyperplane
is possible if the function is at least 1st order differentiable;
approximation by quadratic form if it is at least 2nd order
differentiable.  Again, we will ensure that the approximations are
``good'' by requiring the approximation error to vanish relative to size
of the increment $\delta$.

\begin{defn}{(1st Order Differentiable in $\Rn$)}
\label{defn:diffableRn}
The function $f:\Rn\rightarrow\R$ is \emph{1st order differentiable} at
$x\in\R$ if there exists a vector $m_x\in\Rn$, an $\varepsilon>0$, and a
\emph{remainder} function $r_x:B_0(\varepsilon)\rightarrow\R$ such that
\begin{align}
  \label{eq:diffapproxRn}
  f(x+\delta)-f(x) = m_x^T \delta + r_x(\delta)
  \qquad \forall \delta\in B_0(\varepsilon), \; \delta \neq 0
\end{align}
with remainder function $r_x$ such that
\begin{align}
  \label{lim:remainderRn}
  \lim_{\delta\rightarrow 0}
  \frac{r_x(\delta)}{N_2(\delta)}=0
\end{align}
with the key difference from the earlier definition being $N_2(\delta)$
in the denominator, rather than just $\delta$.
\end{defn}

\begin{defn}{(2nd Order Differentiable in $\Rn$)}
\label{defn:2diffableRn}
The function $f:\Rn\rightarrow\R$ is \emph{2nd order differentiable} at
$x\in\R$ if there exists a vector $m_x\in\Rn$, a matrix $H\in\Rnn$, an
$\varepsilon>0$, and a \emph{remainder} function
$r_x:B_0(\varepsilon)\rightarrow\R$ such that
\begin{align}
  \label{eq:2diffapproxRn}
  f(x+\delta)-f(x)
  = m_x^T \delta
  + \frac{1}{2}\delta^T H \;\delta
  + r_x(\delta)
  \qquad \forall \delta\in B_0(\varepsilon), \; \delta \neq 0
\end{align}
with remainder function $r_x$ such that
\begin{align}
  \label{lim:2remainderRn}
  \lim_{\delta\rightarrow 0}
  \frac{r_x(\delta)}{[N_2(\delta)]^2}=0
\end{align}
Notice that we square the denominator in Limit~\ref{lim:2remainderRn}
relative to the case of first order differentiability.

Moreover, it can be proved that $m=\nabla f$ and that $H$ has entries
$H_{ij} = f_{ij}(x) = \frac{\partial^2 f}{\partial x_i\partial x_j}$,
i.e.\ it is a symmetric matrix of second derivatives called the
\emph{Hessian Matrix}.
\end{defn}

\begin{prop}{\emph{(Homogeneity of Directional Derivatives)}}
Given function $f:\Rn\rightarrow\R$ and point $x\in\Rn$, if directional
derivative $T_x(v)$ exists, then
\begin{align*}
  T_x(\alpha v) = \alpha T_x(v)
  \qquad\forall\alpha\in\R
\end{align*}
\end{prop}

\begin{thm}
Suppose that $f:\Rn\rightarrow\R$ is 1st order differentiable at $x$.
Then $T_x(v)$ is a linear function
\begin{align*}
  T_x(\alpha v_1 + \beta v_2)
  = \alpha T_x(v_1) + \beta T_x(v_2)
  \qquad \forall v_1,v_2\in\Rn
\end{align*}
Moreover, $m=\nabla f$.
\end{thm}
\begin{proof}
First, we assume that $f$ is 1st order differentiable at $x\in\Rn$, so
there exists an $m_x\in\Rn$, an $\varepsilon>0$, and a function
$r_x(\delta)$ such that
\begin{align*}
  \delta\in B_0(\varepsilon)
  \implies
  f(x+\delta) - f(x) = m_x^T \delta + r_x(\delta)
\end{align*}
Now we want to show $T_x(v)$ is linear for $v\neq 0$. By definition,
that object equals
\begin{align*}
  T_x(v) = \lim_{t\rightarrow 0} \frac{f(x+tv) - f(x)}{t}
\end{align*}
But for $t$ small enough so that $\delta=tv\in B_0(\varepsilon)$,
assumed differentiability tells us that we can rewrite the numerator as
\begin{align}
  T_x(v)
  = \lim_{t\rightarrow 0} \frac{f(x+tv) - f(x)}{t}
  &= \lim_{t\rightarrow 0} \frac{m_x^T [tv] + r_x(tv)}{t}\notag\\
  &= m_x^T v + \lim_{t\rightarrow 0} \frac{r_x(tv)}{t}
  \label{eq:Tlinear}
\end{align}
That's true for any $v\in\Rn$, since $t\rightarrow 0$ ensures that there
will always be a $t$ small enough such that $tv\in B_0(\varepsilon)$.

Now it would be great if we can make that remainder term in the
expression above disappear. Then $T_x(v)=m_x^Tv$, which is definitely a
linear function. But does the remainder term disappear?
Well we know by assumed differentiability that
\begin{align*}
  0
  &= \lim_{t\rightarrow 0} \frac{r_x(tv)}{N_2(tv)}
  = \lim_{t\rightarrow 0} \frac{r_x(tv)}{|t| \; N_2(v)}\\
  &= \frac{1}{N_2(v)}
  \left(\lim_{t\rightarrow 0} \frac{r_x(tv)}{|t|}\right)
\end{align*}
Now since $v\neq 0$, we know by the properties of norms that $N_2(v)\neq
0$, so our manipulations were valid and $\frac{1}{N_2(v)}$ is a
constant, nonzero number. Therefore, we have
\begin{align*}
  \lim_{t\rightarrow 0} \frac{r_x(tv)}{N_2(tv)} = 0
  \quad\iff\quad
  \lim_{t\rightarrow 0} \frac{r_x(tv)}{|t|} = 0
\end{align*}
Therefore the remainder term does indeed go to zero and drops out of
Equation~\ref{eq:Tlinear}, leaving us with
\begin{align*}
  T_x(v) = m_x^T v
\end{align*}
This is definitely a linear function of $v$.

Moreover, we can calculate the elements of vector $m$ by plugging in the
basis elements:
\begin{align*}
  f_i(x) = T_x(e_i) = m^T_x e_i = m_i
\end{align*}
In words, the $i$th partial derivative of $f$ is the $i$th element of
$m$. Hence $m = \nabla f$.
\end{proof}

\begin{thm}{\emph{(Schwartz)}}
Given a function $f:\Rn\rightarrow\R$, suppose that
$f_i(x)=\frac{\partial f}{\partial x_i}$ exists and
$f_{ij}(x)=\frac{\partial^2 f}{\partial x_i\partial x_j}$ is continuous.
Then
\begin{align*}
  f_{ji} \; \text{exists and} \quad
  f_{ij}(x)=\frac{\partial^2 f}{\partial x_i\partial x_j}
  = \frac{\partial^2 f}{\partial x_j\partial x_i} = f_{ji}(x)
\end{align*}
\end{thm}

\clearpage
\subsection{Leibniz's Rule}

\begin{prop}\emph{(Leibniz's Rule)}
Suppose we have a function definited
\begin{align*}
  \varphi(t) := \int^{b(t)}_{a(t)} F(x,t) \; dx
\end{align*}
If $F$ and $F_t$ are continuous and $a,b,F$ are differentiable in $t$,
then $\varphi(t)$ is differentiable with
\begin{align*}
  \varphi'(t)
  = F(b(t),t)\cdot b'(t) - F(a(t),t)\cdot a'(t)
  + \int^{b(t)}_{a(t)} F_t(x,t) \; dx
\end{align*}
\end{prop}



\clearpage
\section{Convexity}

\subsection{Convex Sets}

\begin{defn}{(Convex Set)}
\label{defn:convexset}
A set $C\subseteq X$ is \emph{convex} if
\begin{align*}
  z_\lambda = \lambda x + (1-\lambda) y \in C
  \qquad \forall x,y\in C\quad \forall \lambda \in [0,1]
\end{align*}
In words, any point on the straight line connecting $x,y\in C$ is also
in $C$. By convention, the empty set $\emptyset$ is also a convex set.
\end{defn}

\begin{lem}
$C$ is convex if and only if
for all $\{x_i\}_{i=1}^m \subseteq C$ and $\{\lambda_i\}_{i=1}^m$ where
\begin{align*}
  \sum^{m}_{i=1} \lambda_i = 1
  \quad \text{and} \quad \lambda_i \geq 0 \quad \forall i
\end{align*}
we have
\begin{align*}
  \sum^m_{i=1} \lambda_i x_i \in C
\end{align*}
This is a generalization of the the two-point, single-lambda notion of
convexity in Definition~\ref{defn:convexset}.
\end{lem}

\begin{ex}{(Set of Solutions to Linear Inequalities and Equations)}
Given $A \in \R^{m\times n}$, $a\in \R^m$ and $B \in \R^{k \times n}$,
$b\in \R^k$, consider the set of solutions to linear inequalities and
equations:
\begin{align*}
  \mathscr{P}
  &= \{
  x\in \Rn \; | \; Ax \leq a, \; Bx = b
  \}
\end{align*}
Amazingly, this is a convex set.

To see this, we need to show that
\begin{align*}
  z_\lambda \in \mathscr{P}
  \quad \text{where} \quad
  z_\lambda = \lambda x + (1-\lambda)y
  \qquad
  \text{for}
  \begin{cases}
  x,y\in \mathscr{P}\\
  \lambda \in [0,1]
  \end{cases}
\end{align*}
So first, establish the following facts
\begin{alignat}{3}
  Ax &\leq a \quad \Rightarrow \quad
  k Ax &&\leq k a
  \qquad \forall k \in \R_+ \label{ex:lineq1}\\
  Bx &= b \quad \Rightarrow \quad
  k Bx &&= k b
  \qquad \forall k \in \R_+
  \label{ex:lineq2}
\end{alignat}
So given $\lambda \in [0,1]$ and $x,y\in \mathscr{P}$, we can easily
show that
\begin{align*}
  Az_\lambda
  = A\left[\lambda x + (1-\lambda) y\right]
  &= \lambda Ax + (1-\lambda) Ay \\
  &\leq \lambda a + (1-\lambda) a
  \quad\qquad \text{By~(\ref{ex:lineq1})} \\
  \Rightarrow \quad
  Az_\lambda
  &\leq a
\end{align*}
The proof that $Bz_\lambda = b$ is exactly analogous. Therefore
$z_\lambda\in \mathscr{P}$.
\end{ex}

\begin{defn}{(Path Connected)}
A set $C\subseteq X$ is \emph{path connected} if, for any pair $x,y\in
C$, there exists a continuous function $\phi(\lambda):[0,1]\rightarrow
C$ such that
\begin{align*}
  \varphi(\lambda) &\in C \qquad \forall \lambda \in [0,1]\\
  \varphi(0) &= x\\
  \varphi(1) &= y
\end{align*}
Note that $\varphi$ is depended upon choice of $x$ and $y$.

This simply generalizes the idea of convexity. Instead of connecting $x$
and $y$ by a straight line in $C$, you can connect them by any
continuous path in $C$.
\end{defn}

\begin{thm}{\emph{(Intersections of Convex Sets are Convex)}}
Let $\{C_\alpha\}_{\alpha \in J}$ be any arbitrary (possibly
uncountable) collection of convex sets in $\Rn$. Define the intersection
\begin{align*}
  C_* = \bigcap_{\alpha \in J} C_\alpha
\end{align*}
The set $C_*$ is convex.
\end{thm}
\begin{rmk}
In general, unions of convex sets will not be convex unless they are
nested like $C_n \subseteq C_{n+1}$.
\end{rmk}

\begin{proof}
Suppose that $C_* = \emptyset$. Since the empty set is convex by
convention, done. Similarly trivial, suppose that $C_*$ is nonempty and
consists of a single element, $C_* = \{x\}$. Then $\lambda x +
(1-\lambda) x = x$ will be in $C_*$, implying $C_*$ convex.

Finally, suppose that $C_*$ has more than one element, and take $x,y\in
C_*$. Since $C_*$ was constructed by intersection, we know that
$x,y\in C_\alpha$ for all $\alpha \in J$. Since all of the $C_\alpha$
are convex,
\begin{align*}
  z_\lambda = \lambda x + (1-\lambda)y \in C_\alpha
  \qquad \forall \alpha \in J
\end{align*}
Since $z_\lambda \in C_\alpha$ for all $\alpha$, it is clear that
$z_\lambda \in C_*$.
\end{proof}

\begin{defn}{(Convex Hull)}
Given any set $A\subseteq X$, the \emph{convex hull} is the smallest
convex set containing $A$. This is often denoted $\ch(A)$.
\end{defn}

\begin{thm}
Given $A\subseteq X$, let $\mathscr{F}_A$ denote the family of all
convex sets containing $A$:
\begin{align*}
  \mathscr{F}_A &= \{C\text{\emph{convex}} \;|\; A \subseteq C \; \}
\end{align*}
Then we can alternatively define the convex hull $\ch(A)$ as
\begin{align*}
  \ch(A) = \bigcap_{C \in \mathscr{F}_A} C
\end{align*}
\end{thm}

\begin{prop}
Given some set $A$, $A \subseteq C \subsetneq ch(A)$ implies that $C$ is
not convex, since we defined $\ch(A)$ to be the smallest convex set
containing $A$.
\end{prop}

\subsection{Separating Hyperplane Theorem}

\begin{defn}{(Hyperplane)}
We define a \emph{hyperplane} in $\Rn$, denoted $\mathcal{H}(a,b)$
for some $a\in \Rn$ and $b\in \R$, as
\begin{align*}
  \mathcal{H}(a,b) = \{x\in\Rn \; | \; a^Tx = b\}
\end{align*}
\end{defn}

\begin{defn}{(Half-Spaces)}
Given hyperplane $\mathcal{H}(a,b)$, we define the \emph{positive} and
\emph{negative half-spaces} of $\mathcal{H}(a,b)$, respectively, as
\begin{align*}
  \mathcal{S}^+(a,b)
  &:= \{ x \in \Rn \; |\; a^T x \geq b\} \\
  \mathcal{S}^-(a,b)
  &:= \{ x \in \Rn \; |\; a^T x \leq b\}
\end{align*}
The hyperplane splits $\Rn$ into two groups, the positive half-space is
``on or above above'' the hyperplane, while the negative half-space is
``on or below'' in a very, very loose sense.
\end{defn}

\begin{thm}{\emph{(Separating Hyperplane)}}
Given any two disjoint, nonenempty convex sets $C,D\subseteq\Rn$, there
exist constants $a\in\Rn$ and $b\in\R$ such that
\begin{align*}
  C &\subseteq \mathcal{S}^+(a,b) \\
  D &\subseteq \mathcal{S}^-(a,b)
\end{align*}
In words, we can draw a hyperplane between the convex sets and
``separate'' them entirely into the halfspaces defined by that
hyperplane.
\end{thm}


\subsection{Convex and Concave Functions}

\begin{defn}{(Convex and Concave Functions)}
Suppose we have a function $f:C\rightarrow \R$ defined on \emph{convex
set} $C$. We say the function is \emph{convex} if, for all $x,y\in C$
and $\lambda \in[0,1]$,
\begin{align}
  \label{ineq:convex}
  \lambda f(x) + (1-\lambda) f(y) \geq f(\lambda x + (1-\lambda)y) =
  f(z_\lambda)
\end{align}
The function is \emph{concave} if Inequality~\ref{ineq:convex} is
reversed. We say a function is \emph{strictly} concave or convex if the
corresponding inequality is strict.

Proving that functions are convex will mostly consist of just writing
out $f(z_\lambda) = f(\lambda x + (1-\lambda)y)$ and using assumed
properties of the function $f$ to break that up into $\lambda f(x) +
(1-\lambda)f(y)$.
\end{defn}
\begin{rmk}
We require that the domain $C$ be a convex set.  In order to test
convexity or concavity of the function, we need $z_\lambda$ to be in the
domain $C$, so that $f(z_\lambda)$ makes sense and we can check/test
Inequality~\ref{ineq:convex}. Otherwise, $f(z_\lambda)$ would be
nonsensical.
\end{rmk}

\begin{prop}
Function $f:C\subseteq\Rn\rightarrow \R$ defined on convex domain $C$ is
a convex function if and only if, for all $\{x_i\}_{i=1}^m\subseteq C$
and $\{\lambda_i\}_{i=1}^m$ where
\begin{align*}
  \sum^{m}_{i=1} \lambda_i = 1
  \quad \text{and} \quad \lambda_i \geq 0 \quad \forall i
\end{align*}
we have
\begin{align*}
  \sum^m_{i=1} \lambda_i f(x_i)
  \geq
  f\left(\sum^m_{i=1} \lambda_i x_i\right)
\end{align*}
\end{prop}

\begin{prop}{\emph{(Concave-Convex Relationship)}}
Suppose that $f:C\subseteq\Rn\rightarrow \R$ is a function defined on
convex domain $C$. Then
\begin{align*}
  f \; \text{concave}
  \iff
  -f \; \text{convex}
\end{align*}
\end{prop}

\begin{prop}
\label{prop:supconvex}
Given a (possibly uncountable) family of functions
$\mathscr{F}=\{f_\alpha\}_{\alpha\in J}$, define
\begin{align*}
  f^*(x) &:= \sup_{\alpha\in J} f(x)\\
  f_*(x) &:= \inf_{\alpha\in J} f(x)
\end{align*}
If $\mathscr{F}$ is a family of convex functions, then $f^*$ is also
convex. If $\mathscr{F}$ is a family of concave functions, then $f_*$ is
also concave.
\end{prop}
\begin{proof}
For the convex case, not that
\begin{align*}
  f^*(\lambda x + (1-\lambda)y)
  &= \sup_{\alpha \in J}
  f_\alpha(\lambda x + (1-\lambda)y)\\
  \text{$\forall \alpha\in J$ by convexity} \qquad
  &\leq \sup_{\alpha \in J}
  \lambda f_\alpha(x) + (1-\lambda)f_\alpha(y)
  \leq \lambda \sup_{\alpha \in J} f_\alpha(x)
  + (1-\lambda)\sup_{\alpha \in J} f_\alpha(y)\\
  \Leftrightarrow\quad
  &\leq \lambda f^*(x)
  + (1-\lambda) f^*(y)\\
\end{align*}
Hence $f^*$ is convex. The proof is analogous for the convex case.
\end{proof}

\begin{lem}
\label{lem:unique-invertible}
Suppose $A\in\Rnn$. Then
\begin{align*}
  x\neq y\;\Rightarrow\;
  Ax \neq Ay
  \quad \forall x,y\in\Rn
  \quad\iff\quad
  \text{$A$ invertible}
\end{align*}
\end{lem}
\begin{proof}
($\Rightarrow$) Suppose that $A$ is not invertible. Then there must
exist a vector $z\in\Rn$ such that
\begin{align*}
  Az = 0
  \qquad \text{where} \; z\neq 0
\end{align*}
Now pick any $x\in\Rn$ and choose $y\in\Rn$ as $y=x-z$. Then
$x-y=z$ so that
\begin{align*}
  A(x-y) = Az &= 0 \\
  \Rightarrow\quad
  Ax &= Ay
  \qquad x\neq y
\end{align*}
But this is a contradiction of what we assumed.

($\Leftarrow$) Suppose that there exists a pair $x\neq y$ such that
$Ax=Ay$. Then
\begin{align*}
  A(x-y) &= 0
  \quad\iff\quad
  (x-y) = A^{-1}0 = 0
  \quad\iff\quad
  x =y
\end{align*}
\end{proof}

\begin{prop}{\emph{(Invariance to Coordinate Changes)}}
\label{prop:coordinvariance}
Given function $f:\Rn\rightarrow\R$, matrix $A\in\Rnn$, and vector
$b\in\Rn$, define function $g:\Rn\rightarrow\R$ as follows:
\begin{align*}
  g(x) = f(Ax+b)
\end{align*}
Then
\begin{enumerate}
  \item If $f$ is convex (or concave), then so is $g$.
  \item If $f$ is strictly convex (or strictly concave) and $A$ is
    invertible, then $g$ is strictly convex.
\end{enumerate}
\end{prop}
\begin{proof}
(Part 1)
Suppose that $f$ is convex. We want to show that $g$ is convex. So take
any $x,y\in\Rn$ and $\lambda\in[0,1]$ and define $z_\lambda =
\lambda x + (1-\lambda) y$. Then
\begin{align*}
  g(z_\lambda) =
  f(Az_\lambda + b)
  &= f\left( \;
    A\left[
      \lambda x + (1-\lambda) y
    \right]
    +b
  \; \right)\\
  &= f\left( \;
    \lambda \left[Ax + b\right]
    + (1-\lambda)
    \left[Ay + b\right]
  \; \right)\\
  \text{By convexity of $f$}\Rightarrow\qquad
  g(z_\lambda)
  &\leq \lambda f\left( Ax + b \right)
    + (1-\lambda)
    f\left( Ay + b \right)
\end{align*}
By the definitions of $z_\lambda$ and $g$, this simplifies to
\begin{align*}
  g(\lambda x + (1-\lambda)y)
  &\leq \lambda g(x)
    + (1-\lambda) g(y)
\end{align*}
which is precisely the definition of convexity. So $g$ is convex as
well.

(Part 2)
Suppose $f$ strictly convex and $A$ invertible. Choose distinct
$x,y\in\Rn$ and $\lambda\in(0,1)$ (not including the boundary this
time). Then, similar to what we saw above,
\begin{align*}
  g(z_\lambda) =
  f(Az_\lambda + b)
  &= f\left( \;
    \lambda \left[Ax + b\right]
    + (1-\lambda)
    \left[Ay + b\right]
  \; \right)\\
  g(z_\lambda)
  &< \lambda f\left( Ax + b \right)
    + (1-\lambda)
    f\left( Ay + b \right)
\end{align*}
This time, a strict inequality followed for two reasons. First,
Lemma~\ref{lem:unique-invertible} ensures that $A$ invertible implies
$Ax+b\neq Ay+b$ for any $x\neq y$. Second, strict convexity of $f$
ensures that the inequality will be strict when dealing with distinct
points like $Ax+b$ and $Ay+b$ in the domain for $\lambda\in(0,1)$.

Therefore, by the definitions of $z_\lambda$ and $g$, the above
simplifies to
\begin{align*}
  g(\lambda x + (1-\lambda)y)
  &< \lambda g(x)
    + (1-\lambda) g(y)
\end{align*}
Hence $g$ is strictly convex.
\end{proof}



\begin{defn}{(Level, Upper-Contour, and Lower-Contour Sets)}
Given $t\in\R$ and \emph{any} function $f:C\rightarrow \R$ where $C$ is
a convex set, define
\begin{align*}
  \text{Upper Contour Set:}
  \qquad C_f^+(t)
  &:= \{ x\in C \; | \; f(x) \geq t\} = f^{-1}([t,\infty)) \\
  \text{Lower Contour Set:}
  \qquad C_f^-(t)
  &:= \{ x\in C \; | \; f(x) \leq t\} = f^{-1}((-\infty,t]) \\
  \text{Level Set:}
  \qquad L_f(t) \;
  &:= \{ x\in C \; | \; f(x) = t\} = f^{-1}(t) \\
  &= C^+_f(t) \cap C^-_f(t)
\end{align*}
We now relate these to statements about convex and quasi-convex
functions, along with their concave counterparts. In particular, they
provide a convenient way to relate convex sets and functions.
\end{defn}

\begin{prop}{\emph{(Relationship between Convex Functions and Sets)}}
\label{prop:convexfcnset}
Given function $f:C\rightarrow \R$ with convex domain $C$,
\begin{enumerate}
  \item Function $f$ convex $\; \Rightarrow$ $C_f^-(t)$ is a convex set
    for all $t\in\R$.
  \item Function $f$ concave $\Rightarrow$ $C_f^+(t)$ is a convex set
    for all $t\in\R$.
\end{enumerate}
\end{prop}

\begin{defn}{((Quasi-Convex and Concave)}
\label{defn:quasiconvex}
Given $f:C\rightarrow \R$ on convex domain $C$,
\begin{enumerate}
  \item Function $f$ is \emph{quasi-convex} if and only if $C^-_f(t)$ is
    a convex set for all $t\in\R$.
  \item Function $f$ is \emph{quasi-concave} if and only if $C^+_f(t)$
    is a convex set for all $t\in\R$.
\end{enumerate}
\end{defn}
\begin{rmk}
Proposition~\ref{prop:convexfcnset} and
Definition~\ref{defn:quasiconvex} give us the two directions between
convex functions and convex sets. Namely,
Proposition~\ref{prop:convexfcnset} says that concave or convex
functions automatically imply convex upper and lower contour sets,
respectively. How about the other direction?

Well, Definition~\ref{defn:quasiconvex} suggests that convex upper or
lower contour sets do not \emph{necessarily} imply the function is
concave or convex, respectively. So we just call functions satisfying
each property ``quasi-concave'' or ``quasi-convex'', knowing that the
set of quasi-concave and quasi-convex functions is strictly larger than
the set of simply concave or convex functions.  We'll see this in the
next example.
\end{rmk}

\begin{ex}{(Quasi-Concave Example)}
A classic example for quasi-concave functions is the normal pdf.

To show that it's quasi-concave, pick $t = 0.3$ and cut a line at $t$
through the graph of the function. That's the horizontal line in the
figure below. The upper contour-set $C^-_f(t)$ is the preimage of all
points \emph{above} the line at $t$ (hence ``upper''-contour). So we can
follow the vertical dashed lines to the set $C^+_f(t)$, which is clearly
convex. Moreover, the upper contour set will continue to be convex for
all values of $t$ that we might choose, i.e.\ as we move the $t$-line up
and down and look at all the resulting preimage $C^+_f(t)$ sets.
\begin{figure}[htpb!]
\centering
\begin{tikzpicture}[xscale=2,yscale=8]
  % Axes
  \draw[d4black, <->] (-2.5,0) -- (0,0) -- (0,0.5);
  \draw[d4black, ->]  (0,0) -- (2.5,0);
  % Normal PDF
  \draw[d4blue, ultra thick, domain=-2.5:2.5] plot (\x, {(1/sqrt(2*pi))*exp(-pow(\x,2)/2)});
  % Convex set
  \node at (1.3,0.33) {$t=0.3$};
  \draw[d4gray, -]  (-2.5,0.3) -- (2.5,0.3);
  \draw[d4gray, dashed, -]  (-0.76,0.3) -- (-0.76,0);
  \draw[d4gray, dashed, -]  (0.76,0.3) -- (0.76,0);
  \draw[d4black, decorate, decoration={brace, amplitude=10pt, mirror}] (-0.76,0) -- (0.76,0) %
    node[black,midway,yshift=-20pt] {$C_f^+(t) = f^{-1}([t,\infty))$};
\end{tikzpicture}
\end{figure}
\end{ex}

Next, we define the relationship between convex or concave functions and
continuity. In fact, the concepts are deeply related. As the next
theorem will state, convex or concave functions are continuous on the
interior of the domain. This can be seen intuitively enough by looking
at a picture of a convex function with a discontinuity on the domain's
interior.
\begin{figure}[htpb!]
\centering
\begin{tikzpicture}[xscale=2, shorten >= -3pt, shorten <= -3pt]
  % Axes
  \draw[d4black, <->] (0,5.5) -- (0,0) -- (3.5,0);

  \draw[d4blue, ultra thick, domain=0.05:0.051, -*] plot (\x, {pow(\x,2)-2.5*\x+2});
  \draw[d4blue, ultra thick, domain=0.05:2, o-o] plot (\x, {pow(\x,2)-2.5*\x+3});
  \draw[d4blue, ultra thick, domain=2:3, *-o] plot (\x, {pow(\x,2)-2.5*\x+2});
  \draw[d4blue, ultra thick, domain=2.99:3, *-] plot (\x, {pow(\x,2)-2.5*\x+3});

  \draw[d4gray, -]  (1,1.5) -- (2,1);

  \node at (0.15,1.60) {$A$};
  \node at (2.75,4.2) {$B$};
\end{tikzpicture}
\end{figure}

As you can see in the figure, around where you have the discontinuity,
you can draw a straight line that lies entirely below the function (in
gray), violating the convexity of the function. Moreover, you can also
see that discontinuities at the \emph{edge} of the domain in the
``appropriate'' direction (like at $B$ rather than $A$) can avoid this
problem. We now make this more explicit in a theorem.

\begin{thm}{\emph{(Convexity/Concavity and Continuity)}}
Given a function $f:C\rightarrow\R$ on convex domain $C$, the function
$f$ is continuous for all $x \in \mathring{C}$, the interior of the
domain.
\end{thm}

\begin{cor}
Given a function $f:C\rightarrow\R$ on convex domain $C$, if $C$ is
open, then $f$ is continuous.
\end{cor}

\subsection{Convexity and Quadratic Forms}

\begin{prop}
Given symmetric matrix $A\in\Rnn$, and quadratic form
$f:\Rn\rightarrow\R$,
\begin{enumerate}
  \item $A$ positive semidefinite $\Rightarrow$ $f$ convex.
  \item $A$ negative semidefinite $\Rightarrow$ $f$ concave.
\end{enumerate}
If you change ``semidefinite'' to ``definite'' in the statements above,
the functions become \emph{strictly} convex or concave.
\end{prop}
\begin{proof}
Suppose that symmetric matrix $A$ is positive semidefinite. By
Theorem~\ref{thm:symmetric-eig}, we can do an eigendecomposition of
matrix $A$ as follows
\begin{align*}
  A = V^T D V = \left(\sqrt{D}V\right)^T \left(\sqrt{D} V\right)
\end{align*}
where $V$ is a matrix of orthogonal eigenvectors and $D$ is a diagonal
matrix of eigenvalues. The matrix $\sqrt{D}$ is simply a real matrix
(because positive semidefinite, see
Theorem~\ref{thm:symmetric-eig}) with the roots of the
eigenvalues of $A$ on the diagonal.

We an redefine the function as
\begin{align*}
  f(x) &= (x-b)^T A (x-b) \\
  &= (x-b)^T \left(\sqrt{D}V\right)^T \left(\sqrt{D} V\right)(x-b) \\
  \Rightarrow\quad
  f(x)
  &= \left[N_2\left( \sqrt{D} V(x-b) \right)\right]^2
  := g(N_2(Ax+b'))
\end{align*}
where $N_2$ is the usual Euclidean norm, $g(x)=x^2$, $A=\sqrt{D}V$, and
$b'=-\sqrt{D}Vb$.

Now as a norm, $N_2$ is a convex function. By
Proposition~\ref{prop:coordinvariance}, changing the coordinates
coordinates from $N_2(x)$ to $N_2(Ax+b')$ will still result in a convex
function. Lastly, by Theorem~(TK), taking $g(N_2(Ax+b'))$ for increasing
and convex $g$ will also result in a convex function. So all told, $f$
is convex.
\end{proof}


\subsection{Convexity and Differentiability}

\begin{thm}{\emph{(First Order Condition for Convexity)}}
\label{thm:diffconvex}
Given $f:C\rightarrow \R$ differentiable over its entire open and convex
domain $C\subseteq\Rn$, it follows that the function $f$ is convex if
and only if
\begin{align*}
  f(y) \geq f(x) + [\nabla f(x)]^T (y-x)
  \qquad\forall x,y\in\Rn
\end{align*}
\end{thm}
\begin{proof}
($\Rightarrow$, $n=1$) We want to show that $f$ convex implies that
\begin{align*}
  f(y) \geq f(x) + f'(x)(y-x)
\end{align*}
Since $f$ is convex, for any $\lambda\in(0,1)$ and $x,y\in C$, we have
\begin{align*}
  f(\lambda y + (1-\lambda)x)
  &\leq
  \lambda f(y) + (1-\lambda)f(x) \\
  %\Leftrightarrow \quad
  %f(x + \lambda (y -x))
  %&\leq
  %\lambda f(y) + (1-\lambda)f(x)\\
  \text{Rearranging} \Rightarrow \quad
  f(x + \lambda (y -x))
  - f(x)
  &\leq
  \lambda \left[f(y) - f(x) \right]
\end{align*}
Now divide that last line through by $\lambda(y-x)$ to make this look
more like a derivative, and take the limit as $\lambda\rightarrow 0$:
\begin{align*}
  \lim_{\lambda\rightarrow 0}
  \frac{f(x + \lambda (y -x)) - f(x)}{\lambda(y-x)}
  &\leq
  \frac{f(y) - f(x)}{y-x} \\
  f'(x)
  &\leq
  \frac{f(y) - f(x)}{y-x}\\
  \Leftrightarrow\quad
  f'(x)(y-x) + f(x)
  &\leq
  f(y)
\end{align*}

($\Rightarrow$, $n>1$)
Suppose we have $x,y\in C$. Define the function
\begin{align}
  g(\lambda) = f(\lambda y + (1-\lambda)x)
  \label{eq:gdef}
\end{align}
Then $g(\lambda)$ is a convex function for $\lambda\in [0,1]$.
To see this, you need to dive into some really messy algebra, so
probably don't read this next block of equations.
\begin{align*}
  g(\gamma \lambda_1 + (1-\gamma) \lambda_2)
  &=
  f\big(\; \big[\gamma \lambda_1 + (1-\gamma) \lambda_2\big] \; y
  + \big\{1 - [\gamma \lambda_1 + (1-\gamma) \lambda_2]\big\} x
  \;\big)\\
  \text{Rearranging} \qquad
  &=
  f\big(\;
  \gamma \big[\lambda_1 y - \lambda_1 x\big] \;
  +(1-\gamma)\big[ \lambda_2 y -\lambda_2 x \big]
  +  x
  \;\big)\\
  &=
  f\big(\;
  \gamma \big[\lambda_1 y - \lambda_1 x\big] \;
  +(1-\gamma)\big[ \lambda_2 y -\lambda_2 x \big]
  +  \gamma x + (1-\gamma) x
  \;\big)\\
  &=
  f\big(\;
  \gamma \big[\lambda_1 y + (1- \lambda_1 )x\big] \;
  +(1-\gamma)\big[ \lambda_2 y + (1-\lambda_2) x \big]
  \;\big)\\
  \text{By convexity of $f$}\qquad
  &\leq
  \gamma f(\lambda_1 y + (1- \lambda_1 )x)
  +(1-\gamma)f(\lambda_2 y + (1-\lambda_2) x)
  \\
  \text{By definition of $g$}\qquad
  &\leq
  \gamma g(\lambda_1)
  +(1-\gamma)g(\lambda_2)
\end{align*}
So $g$ is convex.

Now since $g$ is convex, it is the case that
\begin{align}
  g(\lambda_2) - g(\lambda_1)
  \geq g'(\lambda_1)(\lambda_2-\lambda_1)
  \label{ineq:glambda}
\end{align}
To compute $g'(\lambda)$, take $v=\lambda y + (1-\lambda) x$ and apply
the chain rule:
\begin{align}
  g(\lambda) &= f(v)\notag\\
  \Rightarrow\quad
  g'(\lambda)
  &= \sumin f_i(v) \frac{dv_i}{d\lambda}\notag \\
  &= \sumin f_i(\lambda y + (1-\lambda)x) \cdot (y_i-x_i) \notag\\
  \Leftrightarrow \quad
  g'(\lambda)
  &= [\nabla f(\lambda y + (1-\lambda)x)]^T\cdot (y-x)
  \label{eq:gprimelambda}
\end{align}
Okay, so we know how to compute $g'(\lambda)$.

But how do we turn all of this information into a statement about $f$?
Well, notice that Equation~\ref{eq:gdef} relates $g$ to $f$, and in such
a way that we have $g(1)=f(y)$ and $g(0)=f(x)$. So let's take
$\lambda_1=0$ and $\lambda_2=1$, plug into Inequality~\ref{ineq:glambda}
and see what we get:
\begin{align*}
  g(1) - g(0)
  &\geq g'(0)(1-0) \\
  \text{By Equation~\ref{eq:gdef}}\qquad
  f(y)-f(x)
  &\geq g'(0) \\
  \text{By Equation~\ref{eq:gprimelambda}}\qquad
  f(y)-f(x)
  &\geq [\nabla f(x)]^T (y-x) \\
\end{align*}

($\Leftarrow$)
\end{proof}

\begin{thm}{(Second Order Condition for Convexity)}
Given twice-differentiable $f:C\rightarrow\R$ on open and convex domain
$C\rightarrow\R$, we have
\begin{enumerate}
  \item $f$ convex $\iff$ $H_f(x)$ is positive semidefinite for all
    $x\in S$.
  \item $f$ concave $\iff$ $H_f(x)$ is negative semidefinite for all
    $x\in S$.
\end{enumerate}
where $H_f(x)$ is the Hessian matrix of second-order partial derivatives
at $x$. The functions are \emph{strictly} convex or concave if you
swap ``semidefinite'' for ``definite'' in the conditions above.
\end{thm}


\section{The Riemann-Stieltjes Integral}

This definition of the integral was made rigorous in the 1800s by
Riemann, Darboux, and Stieltjes.  It's an intuitive way to define the
area under a curve, and it works well with numerical integration
(approximations). \emph{However}, it is incomplete in the sense that
there are functions of interest that we cannot integrate in a Riemann
sense but can in a Lebesgue sense, which retains Riemann as a special
case.

Throughout this section, unless otherwise noted, we'll largely stick to
\emph{bounded} functions that are univariate with compact interval
domain $[a,b]$ and range $\R$
$\mathbb{R}$:
\begin{align*}
  f: [a, b] \rightarrow \mathbb{R}
\end{align*}
We begin by discussing \emph{partitions} of the interval $[a,b]$
into smaller pieces, from which we construct sums that approximate
the area under the function. This leads us to a definition of the
Riemann Integral. Then, we generalize and allow the \emph{weight} we
place on sub-intervals (when summing over the entire interval) to vary,
which gives the Riemann-Stieltjes integral. From there, we discuss the
relationships between the approximating sums and the integral.

\subsection{Partitions}

\begin{defn}(Partition)
A \emph{partition}, $P$, is an ordered tuple representing a
\emph{finite} sequence on the interval $[a,b]$,
\begin{align*}
  a = x_0 < x_1 < \cdots < x_n = b
  \qquad \text{with} \quad \Delta x_i := x_i - x_{i-1}
\end{align*}
\end{defn}

\begin{defn}(Partition Norm)
Define the \emph{norm} of a partition $P$, denoted $\lVert P\rVert$ or
mesh $P$
\begin{align*}
  \lVert P \rVert := \max_i |x_i - x_{i-1}| = \max_i |\Delta x_i|
\end{align*}
\end{defn}

\begin{defn}(Partition Refinement)
If $Q$ and $P$ are both partitions of interval $[a,b]$, $Q$ is a
\emph{refinement} of $P$ if $Q \supset P$.
In words, $Q$ divides $[a,b]$ up into \emph{finer} subintervals.
\end{defn}

\begin{defn}(Common Refinement)
The \emph{common refinement} of two partitions, $P_1$ and $P_2$, is
given by $P_1 \cup P_2$.
\end{defn}

\begin{defn}(Tagged Partition)
A \emph{tagged partition} is a couplet $(P,T)$, where $P$ is some
partition $\{x_0, \ldots, x_n\}$ and $T$ is a set of evaluation points
$\{t_1, \ldots, t_n\}$ for function $f$ such that
\begin{align*}
  x_{i-1} \leq t_i \leq x_i
  \qquad\forall i
\end{align*}
\end{defn}


\begin{defn}(Increasing Interval Weighting Function)
Now generalize to allow weighting of the sub-intervals within the
partition, defined for an \emph{increasing} function
$\alpha: [a,b] \rightarrow \mathbb{R}$
\begin{align*}
  \Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}) > 0
\end{align*}
\end{defn}

\begin{rmk}
This is the main difference between plain Riemann sums and integrals,
versus the Riemann-Stieltjes (RS) sums and integrals.
The latter subsume the former as a special case by taking
$\alpha(x) = x$, which is obviously increasing.
The RS version will weight the contribution of sub-intervals to the
total sum/integral by $\Delta\alpha_i$, \emph{not} by the length of the
sub-interval $\Delta x_i$.
\end{rmk}

\subsection{Approximating Sum Definitions and Relations}

We now define the various sums approximating the Riemann and RS
integrals.

\begin{defn}(Darboux Sums)
Define the upper and lower \emph{Darboux Sums}, respectively, as follows
\begin{align*}
    U(f,P) &:= \sum^n_{i=1} M_i(f)\Delta x_i
        \quad\text{where} \quad
        M_i(f) := \sup_{x \in [x_i, x_{i-1}]} f(x)\\
    L(f,P) &:= \sum^n_{i=1} m_i(f)\Delta x_i
        \quad\text{where} \quad
        m_i(f) := \inf_{x \in [x_i, x_{i-1}]} f(x)
\end{align*}
\end{defn}

\begin{defn}(Riemann Sum)
Given $f$ (bounded) and tagged partition $(P,T)$ define
\emph{Riemann Sum}
\begin{equation}
    S(f,P,T) := \sum^n_{i=1} f(t_i) \Delta x_i
\end{equation}
\end{defn}

\begin{defn}(RS-Darboux Sums)
\label{RSD}
We define the upper and lower \emph{RS-Darboux Sums}, respectively, as
follows
\begin{align*}
    U_\alpha(f,P) &:= \sum^n_{i=1} M_i(f) \Delta \alpha_i
        \quad\text{where} \quad
        M_i(f) := \sup_{x \in [x_i, x_{i-1}]} f(x) \\
    L_\alpha(f,P) &:= \sum^n_{i=1} m_i(f)\Delta \alpha_i
        \quad\text{where} \quad
        m_i(f) := \inf_{x \in [x_i, x_{i-1}]} f(x)
\end{align*}
\end{defn}

\begin{defn}(RS Sum)
\label{RSS}
Given $f$ (bounded) and tagged partition $(P,T)$ define
\emph{Riemann-Stieltjes Sum}
    \begin{equation}
        S_\alpha(f,P,T) := \sum^n_{i=1} f(t_i) \Delta \alpha_i
    \end{equation}
\end{defn}

\begin{rmk}
We focus on and prove theorems about
the more general case of Riemann-\emph{Stieltjes} sums, keeping in mind
that the results immediately apply to special-case Riemann sums.
\end{rmk}


\begin{prop}\emph{(Ordering Sums, Common Partition)}
\label{sumineqP}
Obviously, $\forall T$ associated with $P$
\begin{align*}
  L_\alpha(f,P) \leq S_\alpha(f,P,T) \leq U_\alpha(f,P)
\end{align*}
\end{prop}

\begin{thm}\emph{(Refinements Shrink Gap in Sums)}
\label{sumineq}
If $Q$ refines $P$ (i.e. $Q\supset P$) then
\begin{align*}
  L_\alpha(f,P) \leq L_\alpha(f,Q) \leq U_\alpha(f,Q) \leq U_\alpha(f,P)
\end{align*}
\end{thm}
\begin{proof}
The proof proceeds by induction. Assume that $Q = P \cup \{x^*\}$, a
single point. Then $x^*\in [x_{i-1}, x_i]$ for some interval, and it's
easy show the relation from there.
\end{proof}

\begin{thm}\emph{(Ordering Sums, Distinct Partitions)}
\label{pineq}
For all partitions $P_1, P_2$,
\begin{align*}
  L_\alpha(f,P_1) \leq U_\alpha(f,P_2)
\end{align*}
\end{thm}
\begin{proof}
Let $Q = P_1 \cup P_2$.  Clearly, $Q$ refines both $P_1$ and $P_2$, so
by Theorem \ref{sumineq}, both
\begin{align*}
  L_\alpha(f,P_1) \leq L_\alpha(f,Q)
  \qquad\text{and}\qquad
  U_\alpha(f,Q) \leq U_\alpha(f,P_2)
\end{align*}
Then by Prop.~\ref{sumineqP},
$L_\alpha(f,Q) \leq U_\alpha(f,Q)$, allowing us to string the
inequalities together.
\end{proof}


\subsection{Definition of $\mathscr{R}_\alpha([a,b])$}


\begin{defn}(Upper and Lower RS Integrals)
We define \emph{the upper and lower Riemann-Stieltjes integrals},
respectively, in terms of the RS-Darboux Sums
\begin{align*}
    \overline{\int^b_a} f d\alpha &:= \inf_P U_\alpha(f,P)
    \qquad\quad
    \underline{\int^b_a} f d\alpha := \sup_P L_\alpha(f,P)
\end{align*}
These objects are well-defined because we assumed $f$ \emph{bounded}.
Hence all $U_\alpha(f,P)$ and $L_\alpha(f,P)$ finite, implying finite
sup/inf over partitions, i.e. finite upper/lower RS integrals.
\end{defn}

\begin{note}
We're taking the sup because the max might not exist (it generally
doesn't). Said another way, except for very simple step functions, it is
\emph{not} generally the case that a partition $P$ exists such that
$U_\alpha(f,P)=\overline{\int^b_a} f\; d\alpha$ (or similar for
$L_\alpha$).
\end{note}

\begin{prop}\emph{(Ordering Integrals)}
\label{intineq}
We have that
$\underline{\int} f d\alpha \leq \overline{\int} f d\alpha$.
\end{prop}
\begin{proof}
Thm~\ref{pineq} says
$L_\alpha(f,P_1) \leq U_\alpha(f,P_2)$.
Sup over $P_1$ on LHS, then inf over $P_2$ RHS.
\end{proof}


\begin{defn}(RS Integrability)
$f$ is \emph{Riemann-Stieltjes integrable} on $[a,b]$---denoted
$f \in \mathscr{R}_\alpha([a,b])$---if
\begin{align*}
  \overline{\int^b_a} f \;d\alpha  =
  \underline{\int^b_a} f \;d\alpha
  := {\int^b_a} f \;d\alpha
\end{align*}
\end{defn}

\begin{ex}(Non-Integrability)
A case where $f \notin \mathscr{R}_\alpha([a,b])$ is where
\begin{align*}
  f(x) = \mathbf{1}\{x\in \mathbb{Q}\cap [a,b]\}
    =\begin{cases} 1 & x\text{ rational} \\
        0 & x\text{ irrational} \end{cases}
\end{align*}
In this case, the upper integral is always 1, while the lower integral
is always zero.
\end{ex}

\begin{thm}
\emph{(Riemann's Condition)}
\label{riemcond}
$f \in \mathscr{R}_\alpha([a,b])$ iff
$\forall\varepsilon>0$ there $\exists P$ such that
\begin{align*}
  U_\alpha(f,P) - L_\alpha(f,P) \leq \varepsilon
\end{align*}
\end{thm}
\begin{proof}
($\Leftarrow$)
Given $\varepsilon$ and
$U_\alpha(f,P) - L_\alpha(f,P) \leq \varepsilon$
(assumed for this direction),
\begin{align*}
  \text{Thm~\ref{pineq}}\;\implies\quad
  U_\alpha(f,P) - \sup_P L_\alpha(f,P) &\leq \varepsilon
  \quad \implies\quad
  \inf_P U_\alpha(f,P) - \sup_P L_\alpha(f,P) \leq \varepsilon
\end{align*}
But that last inequality says precisely that
$\overline{\int} f\;d\alpha -
\underline{\int}f\;d\alpha\leq\varepsilon$.
Since that holds for all $\varepsilon>0$, conclude that the upper and
lower integrals are equal so $f\in\sR_\alpha([a,b])$.

($\Rightarrow$)
Suppose $\varepsilon>0$ given.
By definition of the RS integral and the RS-Darboux sums as sup and
inf, there exist $P_1$ and $P_2$ such that
\begin{equation}
    \label{p1}
    U_\alpha(f,P_1) < \int^b_a f d\alpha + \varepsilon/2 \qquad
    L_\alpha(f,P_2) > \int^b_a f d\alpha - \varepsilon/2
\end{equation}
Taking the common refinement $P=P_1\cup P_2$, and using Theorem
\ref{sumineq}, we get that
\begin{align*}
    U_\alpha(f,P) - L_\alpha(f,P) &\leq
    U_\alpha(f,P_1) - L_\alpha(f,P_2)
    = \left[\int^b_a f d\alpha + \varepsilon/2\right] -
        \left[\int^b_a f d\alpha -\varepsilon/2  \right]
    \leq \varepsilon
\end{align*}
\end{proof}

\subsection{Properties of $\mathscr{R}_\alpha([a,b])$}

\begin{thm}\emph{(Continous Functions Integrable)}
\label{contthm}
The set of all continuous functions on $[a,b]$, denoted $\sC([a,b])$, is
a subset of $\mathscr{R}([a,b])$.\footnote{%
  Note that we need continuity over \emph{all} of $[a,b]$. If $f$ isn't
  continuous at the endpoints, we can't apply this proof directly. We
  instead need to split up $[a,b]$ into ``good'' and ``bad'' parts.
}
\end{thm}
\begin{proof}
Want to use Theorem \ref{riemcond} and show that for all
$\epsilon>0$, there $\exists$ partition $P$ such that
\begin{align*}
    U_\alpha(f,P) - L_\alpha(f,P) < \epsilon
    \qquad\iff\qquad
    \sum^n_{i=1} (M_i(f) - m_i(f)) \Delta\alpha_i < \epsilon
\end{align*}
Since $f$ is continuous on a compact interval, $[a,b]$, $f$ is
\emph{uniformly continuous} on $[a,b]$,
hence
\begin{align*}
  \exists \; \delta >0 \quad \text{ s.t. } \quad
      |x - y| < \delta \quad \Rightarrow \quad
      |f(x) - f(y)| < \frac{\epsilon}{\alpha(b)-\alpha(a)}
\end{align*}
So just choose $P$ such that that $||P|| < \delta$.  This means that
\begin{align*}
    \sum^n_{i=1} [M_i(f) - m_i(f)] \Delta\alpha_i &\leq
    \sum^n_{i=1} \frac{\epsilon}{\alpha(b)-\alpha(a)}
        \Delta\alpha_i
    =\frac{\epsilon}{\alpha(b)-\alpha(a)}\sum^n_{i=1}
        \Delta\alpha_i
      %\\
    %\leq\frac{\epsilon}{\alpha(b)-\alpha(a)} \cdot [\alpha(b)-\alpha(a)]
    = \epsilon
\end{align*}
\end{proof}

\begin{prop}\emph{(General Properties)}
Now for some useful properties of the set of RS integrable functions.
Consider $f,g \in \mathscr{R}_\alpha([a,b])$ and $c \in \mathbb{R}$.
\begin{enumerate}[label=\emph{(\roman*)}]
    \item \emph{Linearity}: $f+g$
      and $cf \in \mathscr{R}_\alpha([a,b])$
      \begin{align*}
        \int^b_a cf \; d\alpha = c \int^b_a f\; d\alpha
          \qquad \text{and} \qquad
          \int^b_a f+g\; d\alpha = \int^b_a f \;d\alpha +
          \int^b_a g\; d\alpha
      \end{align*}
    \item \emph{Subsets}: If $[c,d]\subset[a,b]$,
      then $f \in \mathscr{R}_\alpha([c,d])$.
    \item \emph{Splitting the Interval}: If $c \in [a,b]$, then
      $\int^b_a f d\alpha = \int^c_a f d\alpha + \int^b_c f d\alpha$
    \item \emph{Monotonicity}:
      If $f\leq g$ on $[a,b]$ then
      $\int^b_a f \; d\alpha \leq \int^b_a g \; d\alpha$.
      Can also take $f=0$ to get that $g\geq 0$ implies nonnegative
      integral.
    \item \emph{Continuous Compositions}:
      Suppose $h: \mathbb{R}\rightarrow\mathbb{R}$ continuous, hence
      $h\in \sR_\alpha([a,b])$.
      Then $h \circ f\in\mathscr{R}_\alpha([a,b])$
    \item \emph{Multiplication}:
      $fg\in\mathscr{R}_\alpha([a,b])$.
    \item \emph{Absolute Value Relations}:
      We have the often-useful results
      \begin{align*}
        |f| \in \mathscr{R}_\alpha([a,b])
        \qquad
        \left\lvert\int^b_a f d\alpha \right\rvert
        \leq \int^b_a |f| \; d\alpha
      \end{align*}
\end{enumerate}
\end{prop}

\begin{proof}
We prove a few results
\begin{enumerate}[label=(\roman*)]
  \item[(v)]
    \emph{Continuous Compositions}:
    Let $m:=\inf_{[a,b]}f$ and $M:=\sup_{[a,b]}f$, both finite since $f$
    bounded. We have two results concerning compact interval $[m,M]$,
    %which is the range of $f$ on $[a,b]$ and the relevant domain of $h$.
    \begin{itemize}
      \item As a continuous function on compact interval $[m,M]$,
        the range of $h$ is bounded on $[m,M]$. In other words, there
        exists a $K$ such that $|h(x)|\leq K$ for all $x\in[m,M]$.
      \item As a continuous function on the compact interval $[m,M]$,
        $h$ is uniformly continuous. Hence, for any $\varepsilon>0$,
        there exists a $\delta>0$ such that
        \begin{equation}
            \label{cont}
              |u- v| \leq 2K\delta \quad \Rightarrow
                \quad |h(u) - h(v)| \leq
                \frac{\varepsilon}{2\left[\alpha(b)-\alpha(a)
                \right]}
        \end{equation}
    \end{itemize}
    Next, since $f \in \mathscr{R}_\alpha([a,b])$, Riemann's Condition
    says there exists a partition $P$ s.t.
    \begin{equation}
        \label{forf}
          U_\alpha(f,P)- L_\alpha(f,P) \leq \epsilon\cdot
            \delta
    \end{equation}
    Now with all pieces in place, we want to show RS integrability for
    $h\circ f$, which requires showing the upper and lower
    RS-Darboux sums are arbitrarily close. We will do so by breaking the
    sums into two parts based on how the original function $f$ behaves
    on the subintervals of partition $P$ from (\ref{forf}):
    \begin{align*}
        A = \{ i \; | \; M_i(f) - m_i(f)\leq2K\delta\}
        \qquad
        B = \{ i \; | \; M_i(f) - m_i(f)>2K\delta\}
    \end{align*}
    Now let's work out the sums for $h\circ f$:
    \begin{align}
        U_\alpha(h\circ f,P) - L_\alpha(h\circ f,P)
            &= \sum^n_{i=1} \left[
            M_i(h\circ f) - m_i(h\circ f)\right]
            \Delta\alpha_i \notag
        = \sum_{i\in A} \; + \;
            \sum_{i\in B} \notag\\
        \qquad
        &\leq \frac{\varepsilon}{2[\alpha(b)-\alpha(a)}
            \sum_{i\in A} \Delta\alpha_i+
            K \sum_{i\in B} \Delta\alpha_i
            \label{final}
    \end{align}
    The result for the sum over $A$ comes from uniform integrability and
    Inequality~\ref{cont}. The result for the sum over $B$ comes from
    $h$ being bounded by $K$ as discussed.
    Now use the very definition of $B$ together with
    Inequality~\ref{forf} to simplify further:
    \begin{align*}
        \sum_{i\in B}
        {2K}{\delta} \Delta\alpha_i &\leq
        \sum_{i\in B} \left[M_i(f) - m_i(f) \right]
        \Delta\alpha_i \leq \varepsilon\cdot\delta
        \quad\implies\quad
        \sum_{i\in B} \Delta\alpha_i
        \leq \frac{\varepsilon}{2K}
    \end{align*}
    Substituting this back into Eq.~\ref{final}, we have the
    inequality that implies $g\circ f\in\mathscr{R}_\alpha([a,b])$:
    \begin{align*}
        U_\alpha(g\circ f,P) - L_\alpha(g\circ f,P)
        &\leq \frac{\varepsilon}{2[\alpha(b)-\alpha(a)]}
            [\alpha(b)-\alpha(a)]
            + K\frac{\varepsilon}{2K}
            = \varepsilon
    \end{align*}

  \item[(vi)]
    We know that $f - g,f+g \in \mathscr{R}_\alpha([a,b])$
    by (i), and $h(x) = x^2$ continuous, so then
    \begin{align*}
      fg = \frac{1}{4} \left[(f+g)^2 - (f-g)^2 \right]
      \in\mathscr{R}_\alpha([a,b])
    \end{align*}

  \item[(vii)]
    The first expression is clearly true because $g(x)=|x|$
    is a continuous function, so $g\circ f = |f|$ is
    RS integrable by (v).
    Next, let $c$ be a single constant (either 1 or
    $-1$) such that
        \[ c\int^b_a f\; d\alpha = \left\lvert
        \int^b_a f\;d\alpha \right\rvert \]
    Then we have that $cf \leq |f|$ and by the monotonicity
    property, we have that
    \begin{align*}
        \int^b_a |f| \; d\alpha &\geq
            \int^b_a cf \; d\alpha \\
        \implies \qquad
        \int^b_a |f| \; d\alpha &\geq
            \int^b_a cf \; d\alpha =
            c \int^b_a f \; d\alpha =
            \left\lvert
            \int^b_a f \; d\alpha \right\rvert
    \end{align*}
\end{enumerate}
\end{proof}

\newpage
\subsection{Mean Value Theorem}

We prove a generalized mean value theorem (which is a little weird
looking) but retains the classical mean-value theorem as a special case.

\begin{thm}\emph{(Generalized Mean Value Theorem)}
\label{mvt}
Suppose $f,g\in\mathscr{R}_\alpha([a,b])$ and assume $g\geq0$.
Let $m=\inf_{x\in[a,b]} f(x)$ and $M=\sup_{x\in[a,b]} f(x)$.

Then $\exists\,\mu\in[m,M]$ where i.e. in between the upper and lower
bounds of $f$, such that
\begin{align}
  \int^b_a fg\;d\alpha = \mu \int^b_a g\;d\alpha
  \label{gmvt}
\end{align}
Moreover, if $f$ is continuous, then there exists $c\in[a,b]$ such that
$\mu = f(c)$.
\end{thm}
\begin{proof}
Suppose that $g\neq 0$; otherwise, the proof is trivial and
any $\mu\in[m,M]$ works.
Then looking at Equation~\ref{gmvt}, the desired result follows
immediately if we can set
\begin{align*}
  \mu = \frac{\int^b_a fg\;d\alpha}{\int^b_a g\;d\alpha}
\end{align*}
To complete the proof, we just need to check the following:
\begin{itemize}
  \item
    \emph{$\mu$ well-defined}: We know the numerator and denominator
    exist because both $f$ and $g$ were assumed integrable, so their
    product is as well. Hence $\mu$ well-defined
  \item $\mu\in[m,M]$:
    Since $m\leq f \leq M$ and $g\geq0$, monotonicity let's us conclude
    that
    \begin{align}
      mg &\leq fg \leq Mg
      \quad\implies\quad
      m\int^b_a g\;d\alpha \leq
      \int^b_a fg\;d\alpha \leq  M \int^b_a g\;d\alpha
      \label{gmvtineq}
    \end{align}
    Dividing through by $\int^b_a g\;d\alpha$ gives precisely that
    $m\leq \mu\leq M$.
\end{itemize}
Lastly, if $f$ continuous on $[a,b]$, the intermediate value theorem
gives us a $c$ such that $\mu=f(c)$.
\end{proof}

\begin{cor}\emph{(Classical Mean Value Theorem)}
If $f$ is continuous on $[a,b]$ with $\alpha$ increasing, there's some
$c\in[a,b]$ such that
\begin{align*}
  \int^b_a f\;d\alpha = f(c)[\alpha(b)-\alpha(a)]
\end{align*}
We'll prove a more general result that retains this as a special case.
\end{cor}
\begin{proof}
Just take $g=1$ in the previous theorem and again, use the intermediate
value theorem to get $c$ such that $\mu=f(c)$ since $f$ continuous.
\end{proof}



\newpage
\subsection{Convergence of Riemann-Stieltjes Sums, $S_\alpha(f,P,T)$}

Calculus intuition suggests we should be able to approximate a RS
integral $\int^b_af\; d\alpha$ by a limit of RS Sums, i.e.
$\lim_{\lVert P\rVert\ra 0} S_\alpha(f,P,T)$,
taking finer and finer partitions (hence, an increasing number of
evaluation/tagging points). So it would be very nice and convenient if
RS integrability were somehow ``if and only if equivalent'' to RS sum
convergence under a sequence of partitions with shrinking mesh, i.e.
if always and everywhere
\begin{align*}
  f\in\sR_\alpha([a,b])
  \quad\iff\quad
  \int^b_a f\; d\alpha = \lim_{\lVert P \rVert\ra 0} S_\alpha(f,P,T)
  \quad \forall T
\end{align*}
However this is \emph{not} generally true. We will really only have
the ($\Leftarrow$) direction, as we shall see.

But first something slightly different.
Rather than relate RS Sums and integrals given $\lVert P\rVert\ra 0$,
we relate the two given partitions satisfying another property.

\begin{thm}
\emph{(RS Integrability and RS Sums with Partition Refinements)}
\label{weaker}
Suppose $f$ bounded on $[a,b]$ as usual.
There are two directions
\begin{itemize}
  \item \emph{($\Rightarrow$)}:
    Suppose $f\in\sR_\alpha([a,b])$.
    Then $\forall\varepsilon>0$, there exists a corresponding $P$ such
    that
    \begin{align}
      P^*\supset P\quad\implies\quad
      \left\lvert S_\alpha(f,P^*,T) - \int^b_af\;d\alpha \right\rvert
      \leq \varepsilon \qquad \forall T
      \label{riemcondweaker}
    \end{align}
  \item \emph{($\Leftarrow$)}:
    Suppose $\exists I\in\R$ such that for all $\varepsilon>0$, there
    exists a partition $P$ where
    \begin{align}
      P^*\supset P\quad\implies\quad
      \left\lvert S_\alpha(f,P^*,T) - I \right\rvert
      \leq \varepsilon \qquad \forall T
      \label{riemcondweaker2}
    \end{align}
    Then $f\in\mathscr{R}_\alpha([a,b])$ and
     $I = \int^b_a f\;d\alpha$.
\end{itemize}
\end{thm}
\begin{note}
We could have written a weaker theorem, leaving out the $P^*\supset P$
part and replacing $P^*$ with simply $P$ in
Conditions~\ref{riemcondweaker} and \ref{riemcondweaker2}.
That weaker version is a special case of this theorem.
\emph{However}, going forward, we consider the idea of taking
finer and finer partitions (esp. when $\alpha$ might not be increasing)
which recommends this statement of the theorem.
\end{note}
\begin{rmk}
Let's discuss some subtleties of the two directions of this theorem.
\begin{itemize}
  \item ($\Rightarrow$)
    This direction effectively says that if $f\in\sR_\alpha([a,b])$,
    we can construct a sequence of $\{P_n\}_{n=1}^\infty$ such that
    $\limn S_\alpha(f,P_n,T)=\int^b_af\;d\alpha$.
    However, it \emph{does not say} that such a limit of RS Sums
    coverges for \emph{any} partition sequence $\{P_n\}$ (even if mesh
    $P_n\ra 0$).
    You must have a \emph{particular, correct} sequence of partitions.

  \item
    ($\Leftarrow$):
    Suppose you know an $I$ that the RS-sums can get arbitrarily
    close to by careful choice of $P$.
    Then $f$ RS-integrable and $I$ is the value of the integral.
    \emph{Did not say} such an $I$ \emph{must} exist.
    Instead, we're just saying it's possible, and if $I$ \emph{does}
    indeed exist, then $f$ is RS integrable.
\end{itemize}
\end{rmk}

\begin{proof}
($\Rightarrow$)
Suppose $\varepsilon>0$ given.
We need a partition $P$ such that for all $P^*\supset P$ and $T$,
Condition~\ref{riemcondweaker} holds.
Since $f\in\mathscr{R}_\alpha([a,b])$, Riemann's condition gives a $P$
such that
\begin{align}
  U_\alpha(f,P) - L_\alpha(f,P)<\varepsilon/2 \qquad
  \label{riemcondsums}
\end{align}
For any $P^*\supset P$, we have from previous thoerems and definitions
the following inequalities (where $X$ represents \emph{either}
$S_\alpha(f,P^*,T)$ or $\int^b_af\; d\alpha$):
\begin{align*}
  L_\alpha(f,P^*)\leq X \leq U_\alpha(f,P^*)
  \quad\implies\quad
  0\leq X-L_\alpha(f,P^*)
  &\leq U_\alpha(f,P^*)-L_\alpha(f,P^*) \\
  &\leq U_\alpha(f,P)-L_\alpha(f,P)
  < \varepsilon/2
\end{align*}
To get to the second line, we used the fact that any refinement
$P^*\supset P$ will only bring the upper and lower sums \emph{closer}
together, then Inequality~\ref{riemcondsums}.
Thus, we have both
\begin{align*}
  0\leq
  \left\lvert S_\alpha(f,P^*,T)-L_\alpha(f,P^*)\right\rvert
  <\varepsilon/2
  \qquad
  0\leq \int^b_a f\; d\alpha-L_\alpha(f,P^*) <\varepsilon/2
\end{align*}
%then use that
%\begin{align*}
  %\left\lvert
  %S_\alpha(f,P^*,T) - \int^b_a f\; d\alpha
  %\right\rvert
  %&=
  %\left\lvert
  %S_\alpha(f,P^*,T)
  %- L_\alpha(f,P^*)
  %+ L_\alpha(f,P^*)
  %- \int^b_a f\; d\alpha
  %\right\rvert
  %\\
  %&\leq
  %\left\lvert
  %S_\alpha(f,P^*,T)
  %- L_\alpha(f,P^*)
  %\right\rvert
  %+
  %\left\lvert
  %L_\alpha(f,P^*)
  %- \int^b_a f\; d\alpha
  %\right\rvert
  %\\
%\end{align*}
Together, these inequalities imply Condition~\ref{riemcondweaker}.

($\Leftarrow$)
Let $\varepsilon>0$ be given. We need to show there exists a partition
$Q$ such that the upper and lower Darboux sums are $\varepsilon$ close.
To get that partition $Q$, note that this direction of the proof gives
us a partition $P$ such that
\begin{align*}
  |S_\alpha(f,P^*,T)-I|\leq \varepsilon/4
  \qquad\forall T,P^*\supset P
\end{align*}
Take $Q=P^*$ for any $P^*\supset P$.
Next, note that given this $Q$, there exist $T_1$ and $T_2$ such that
\begin{align}
    S_\alpha(f,Q,T_1) &\geq U_\alpha(f,Q) - \varepsilon/4\notag\\
    S_\alpha(f,Q,T_2) &\leq L_\alpha(f,Q) + \varepsilon/4
    \label{proof1.17}
\end{align}
This follows because given any partition, we can always choose a $T_1$
to be the set of all $M_i$ and $T_2$ the set of all $m_i$.

So now, let's look at how far apart the upper and lower Darboux sums are
using this $Q$
\begin{align*}
    U_\alpha(f,Q) - L_\alpha(f,Q) &\leq
        \left[ S_\alpha(f,Q,T_1) + \varepsilon/4\right] -
        \left[ S_\alpha(f,Q,T_2) - \varepsilon/4\right] \\
    &\leq
        \left[ S_\alpha(f,Q,T_1) - I\right] -
        \left[ S_\alpha(f,Q,T_2) - I \right]
        + \varepsilon/2
        \\
    &\leq \left\lvert S_\alpha(f,Q,T_1) - I \right\rvert+
        \left\lvert S_\alpha(f,Q,T_2) - I \right\rvert
        + \varepsilon/2 \\
    \text{From Inequalities \ref{proof1.17}} \quad
        &\leq \varepsilon
\end{align*}
Since $\varepsilon$ was arbitrary, the upper and lower sums can be made
arbitrarily close. Hence $f\in\mathscr{R}_\alpha([a,b])$ and $I$ clearly
equals the value of the integral.
\end{proof}

\clearpage
Having finished this theorem relating RS Sums and Integrals with a
slightly different condition on partitions,
we can now move on to consider the relationship between RS Sums and
Integrals with ever-shrinking partitions $\lVert P\rVert\ra0$.

\begin{defn}(Limit of RS Sums)
\label{strongest}
For a bounded function $f$ on $[a,b]$, we say
\begin{align*}
  \lim_{||P||\rightarrow 0} S_\alpha(f,P,T) = I
\end{align*}
if for every $\varepsilon>0$, there exists a $\delta>0$ such that
$||P||\leq\delta$ implies that
\begin{align*}
  |S_\alpha(f,P,T) - I | \leq \varepsilon \qquad \forall T
\end{align*}
\end{defn}

\begin{thm}\emph{(Convergence of RS Sums implies Integrability)}
\label{oneway}
If $f$ bounded on $[a,b]$ and
$\lim_{||P||\rightarrow 0}S_\alpha(f,P,T)$ exists, then
$f\in\mathscr{R}_\alpha([a,b])$ and
\begin{align}
  \int^b_a f\;d\alpha = \lim_{||P||\rightarrow 0}
    S_\alpha(f,P,T)
  \label{rssumequiv}
\end{align}
\end{thm}
\begin{proof}
Apply the ($\Leftarrow$) direction of Theorem~\ref{weaker}.
\end{proof}

\begin{rmk}
So if we can show the limit exists as $||P||\rightarrow 0$, great---this
theorem tells us $f$ is integrable.
\emph{But} if the limit does not exist, we are \emph{not} justified in
concluding that $f\not\in\sR_\alpha([a,b])$ because we don't have the
converse in the ($\Rightarrow$) direction as in Theorem~\ref{weaker}.
In other words, there exist RS integrable functions where
Condition~\ref{rssumequiv} \emph{fails}. See the next example.

However, we can get some partial results in the ($\Rightarrow$)
direction if we must put more assumptions on $f$, as in the next
theorem.
\end{rmk}

\begin{ex}
Example of why the other direction in Theorem \ref{oneway} wouldn't
work. Consider
\begin{align*}
  f(x) = \begin{cases} 0 & x \in [0, 1/2) \\ 1 & x \in [1/2, 1]
        \end{cases}  \qquad
    \alpha(x) = \begin{cases} 0 & x \in [0,1/2] \\ 1& x\in(1/2, 1]
        \end{cases}
\end{align*}
Depending on whether the point $1/2$ is included in the partition, we
can construct ever-finer partitions that make the RS-Sums converge to 0
sometimes, and to 1 at other times.
\end{ex}

\begin{thm}
\emph{(Cont. $f$ or $\alpha$ $\implies$ Equivalence of RS Integrability,
RS Sum Convergence)}
Let $f\in\mathscr{R}_\alpha([a,b])$. If either $f$ or $\alpha$ is
continuous on $[a,b]$, then
\begin{equation}
    \label{limsum.toprove}
    \int^b_a f\;d\alpha = \lim_{||P||\rightarrow 0}
    S_\alpha(f,P,T)
\end{equation}
\end{thm}
\begin{rmk}
This previous theorem is important, because if we just consider \emph{Riemann} integrability, then $\alpha(x) = x$, which is continuous. Thus, Riemann-integrability and the existence of the limit of Riemann sums (as the partition intervals get smaller) are equivalent notions.
\end{rmk}

\begin{proof}
Our goal will be to show that, for all $\epsilon>0$,
\begin{equation}
    \label{contequiv.toshow}
    \lim_{||P||\rightarrow 0}
    \left\lvert \int^b_a f \; d\alpha - S_\alpha(f,P,T) \right\rvert
        \leq \varepsilon
\end{equation}
when $f$ or $\alpha$ is continuous.

($f$ Continuous)
Suppose $\varepsilon>0$ given.
First, because $f$ is continuous on compact interval, $f$ is uniformly
continuous on $[a,b]$. Thus for any $\varepsilon>0$, there exists a
$\delta$ such
\begin{align*}
  \lVert P\rVert <\delta
  \quad\implies\quad
  |f(x)-f(y)|<\frac{\varepsilon}{\alpha(b)-\alpha(a)}
\end{align*}
$\forall x,y$ in the same subinterval of $P$.
So let $P$ denote any arbitrary partition where $\lVert P\rVert<\delta$.

We now find a set of points $C$ tagging arbitrary partition $P$
such that $\int^b_a f\;d\alpha = S_\alpha(f,P,C)$. Then we can compare
sums $S_\alpha(f,P,C)$ and $S_\alpha(f,P,T)$, rather
than sum $S_\alpha(f,P,T)$ and integral $\int^b_af\;d\alpha$.  So to
start, break up the integral based on subintervals of $P$
\begin{align*}
  \int^b_a f \; d\alpha
  =
  \sum_{i=1}^n \int^{x_i}_{x_{i-1}} f\;d\alpha
\end{align*}
Next, use the Mean Value Theorem on the integrals of the RHS, taking
$g=1$
\begin{align}
  \int^b_a f \; d\alpha
  =
  \sum_{i=1}^n \int^{x_i}_{x_{i-1}} f\;d\alpha
  =
  \sum_{i=1}^n \mu_i\int^{x_i}_{x_{i-1}} 1\;d\alpha
  \qquad \mu_i \in [m_i,M_i]
  \label{contequiv}
\end{align}
Because $f$ is continuous (and this is precisely where the strong
requirement of continuity comes in!),
we can use the Intermediate Value Theorem to note that, for all $i$,
there must be some $c_i \in [x_{i-1}, x_i]$ where $f(c_i) = \mu_i$.
Substituting into Equation \ref{contequiv}, we get
\begin{align*}
  \int^b_a f \; d\alpha
    &= \sum^n_{i=1} f(c_i) \int^{x_i}_{x_{i-1}} d\alpha
    = \sum^n_{i=1} f(c_i) \Delta\alpha_i
    = S_\alpha(f,P,C)
    \qquad
    c_i \in [x_{i-1}, x_{i}]
\end{align*}
where $C=\{c_1,\ldots,c_n\}$.
So now, we can replace the integral with this sum in
Inequality~\ref{contequiv.toshow} that we want to prove:
\begin{equation}
    \label{contequiv.toshow2}
    \lim_{||P||\rightarrow 0}
    \left\lvert  S_\alpha(f,P,C)- S_\alpha(f,P,T) \right\rvert
        \leq \varepsilon
\end{equation}
But by our choice of $P$ such that $\lVert P\rVert<\delta$, we have
precisely that
\begin{align*}
    \left\lvert  S_\alpha(f,P,C)- S_\alpha(f,P,T) \right\rvert
    = \left\lvert  \sum^n_{i=1} \left[f(c_i) - f(t_i) \right]
    \Delta\alpha_i\right\rvert
    &\leq \sum^n_{i=1} \left\lvert  f(c_i) - f(t_i) \right\rvert
    \Delta\alpha_i  \\
    &\leq \frac{\varepsilon}{\alpha(b)-\alpha(a)}
        \sum^n_{i=1} \Delta\alpha_i =  \varepsilon
\end{align*}
\clearpage

($\alpha$ Continuous)
Suppose $\varepsilon>0$ given.  Since $f\in\mathscr{R}_\alpha([a,b])$,
we know that there exists a $P^*$ such that
\begin{equation}
    \label{bases}
    U_\alpha(f,P^*) < \int^b_a f\;d\alpha + \varepsilon
        \qquad L_\alpha(f,P^*) > \int^b_a f\;d\alpha - \varepsilon
\end{equation}
Let $\#P^*$ denote the number of subintervals of this $P^*$.

To construct the proof from here, we'll want eventually to compare any
arbitrary RS-Sum $S_\alpha(f,P,T)$ to these specific upper and lower
sums (using $P^*$) that we know something about---i.e. how close they
are to the target integral. Then we'll take the limit for $P$ once we've
established that relationship.

Also, assume that $f(x)>0$ over $[a,b]$. We'll resolve this simplifying
assumption later on, and it will help in the meantime.

Since interval $[a,b]$ compact and $\alpha$ continuous, $\alpha$ is
\emph{uniformly} continuous, implying that
\begin{align*}
  \exists \delta > 0 \quad \text{s.t.} \qquad
  w_\alpha(\delta) < \frac{\varepsilon}{\#P^*}
\end{align*}
And so
\begin{align*}
  \forall P \qquad ||P||<\delta \quad \Rightarrow
      \quad \alpha(x_i) - \alpha(x_{i-1}) < w_\alpha(\delta)
      <  \frac{\varepsilon}{\#P^*}
\end{align*}
Now, considering any such set $P$, let's also define the following sets,
which group the indices for the subintervals of any $P$ into two
categories, based on how they interact with $P^*$:
\begin{align*}
    I_1 &:= \{i \; | \; [x_{i-1},x_i) \cap P^* = \emptyset \}\\
    I_2 &:= \{i \; | \; [x_{i-1},x_i) \cap P^* \neq \emptyset \}
\end{align*}
And so $I_2$ contains all the indices where an subinterval of $P$
contain a point of $P^*$. $I_1$ has all the indices where an subinterval
of $P$ is contained entirely within a subinterval of $P^*$.
\\
\\
Having done all of groundwork from above, we can now write an expression
for the upper sum relative to $P$:
\begin{align}
    U_\alpha(f,P) &= \sum^n_{i=1} M_i(f) \Delta\alpha_i \notag \\
        &= \sum_{I\in I_1} M_i(f) \Delta\alpha_i
         + \sum_{I\in I_2} M_i(f) \Delta\alpha_i
         \label{twosums}
\end{align}
But clearly, $I_2$ can only contain, at most, $\#P$ points since there
are only $\#P$ intervals of $P$, so we must have that $\#I_2 \leq \#P$.
Thus, the sum over $I_2$ simplifies:
\begin{align}
        \sum_{I\in I_2} M_i(f) \Delta\alpha_i
        &\leq \sum_{I\in I_2} M \frac{\varepsilon}{\#P} =
            M\varepsilon
        \label{tocombine1}
\end{align}
where $M = \sup f(x)$ over the interval $[a,b]$.
\\
\\
To deal with the sum over $I_1$, we get the result that
\begin{equation}
    \label{tocombine2}
    \sum_{I\in I_1} M_i(f) \Delta\alpha_i  \leq U_\alpha(f,P^*)
\end{equation}
This is true because in assuming $f(x)>0$ and restricting our sum to
$I_1$, we throw out whole portions of the interval $[a,b]$ from the sum
entirely. And on top of that, we threw out the very large intervals that
span one or more of the points in the partition $P^*$.  So all that
remains in our sum over $I_1$ are those tiny subintervals of $P$ that
would fit entirely within a subinterval of $P^*$. And so we won't be
able to make up for the parts we throw out by weighting parts of the
interval $[a,b]$ with $M_i(f)$ from $P$ \emph{more than} we would in the
case of $M_i(f)$ from $P^*$.
\\
\\
Combining the representation for $U_\alpha(f,P)$ with the two sum
representation of Equation \ref{twosums} and the results in Inequalities
\ref{tocombine1} and \ref{tocombine2}, we get that
\begin{equation}
    U_\alpha(f,P) \leq U_\alpha(f,P^*) + M\varepsilon
\end{equation}
And because we had an expression for $U_\alpha(f,P^*)$ in Equation
\ref{bases}, we can substitute in and get that
\begin{equation}
    \label{ass1}
    U_\alpha(f,P) \leq \int^b_a f\;d\alpha  + (M+1)\varepsilon
\end{equation}
And by a similar argument, we also have that
\begin{equation}
    \label{ass2}
    L_\alpha(f,P) \geq \int^b_a f\;d\alpha  - (M+1)\varepsilon
\end{equation}
Now, at this point we have to dispense with the assumption that
$f(x)>0$.  We do that by choosing a $k$ such that $f(x) + k >0$, where
now $f$ can be any arbitrary function such that $\int^b_a f\;d\alpha$ as
we assume in the theorem.  Then we're in the world we described above in
choose our $P$.
\\
\\
And, since we're in that world, we get the inequality in the next line,
which allows us to conclude a very useful result
\begin{align*}
    U(f,P) + k(\alpha(b)-\alpha(a))
        = U(f+k, P) &\leq \int^b_a (f+k)\; d\alpha
        + (M+1)\varepsilon \\
    &\leq \int^b_a f\; d\alpha  + k(\alpha(b)-\alpha(a))
        + (M+1)\varepsilon \\
    \Rightarrow U(f,P) &\leq \int^b_a f\; d\alpha
        + (M+1)\varepsilon
\end{align*}
So we have our asses covered if $f(x)$ isn't always positive, and we can
stick with the results in Inequalities \ref{ass1} and \ref{ass2}.
\\
\\
And now we're almost done.  All that's left is to recognize that we can
sandwich, for all $P$ such that $||P||\leq\delta$
\begin{align*}
    \int^b_a f\;d\alpha - (M+1)\varepsilon \leq
    L_\alpha(f,P) \leq S_\alpha(f,P,T) &\leq U_\alpha(f,P) \leq
   \int^b_a f\;d\alpha + (M+1)\varepsilon  \\
        \Rightarrow \qquad \left\lvert S_\alpha(f,P,T)
        - \int^b_a f\;d\alpha\right\rvert &\leq 2(M+1)\varepsilon \end{align*}
\end{proof}


\newpage
\subsection{Integration by Parts, with Possibly Non-Increasing $\alpha$}

In contrast to our operating assumption up to this point, we
allow $\alpha$ to be possibly nonincreasing in this section.
In order to make such a notion operational, we will need to dispense
with our ``sandwiching'' approach, which uses $L_\alpha(f,P)\leq
S_\alpha(f,P,T)\leq U_\alpha(f,P)$, as it no longer holds. In fact, we
don't even get that if $P^*\supset P$, then
$U_\alpha(f,P^*) \leq U_\alpha(f,P)$ (and the analogous case for the
lower sum). With that, we give a new definition of integrability framed
entirely in terms of RS-Sums.

\begin{defn}(Interability Definition for Non-Increasing $\alpha$)
Given any $f$ and $\alpha$ (not necessarily increasing) on $[a,b]$, we
say that $f\in\mathscr{R}_\alpha([a,b])$ if and only if there exists an
$I\in\mathbb{R}$ such that for all $\varepsilon>0$, there exists a $P$
such that
\begin{align}
  P^*\supset P
  \quad\implies\quad
  \left\lvert S_\alpha(f,P^*,T) - I \right\rvert \leq
  \varepsilon \qquad \forall \; T
  \label{intdefn}
\end{align}
This $I$ is denoted by $\int^b_a f\;d\alpha$. And despite this new
definition, we still retain of the usual nice linearity properties
discussed above in the increasing $\alpha$ case.
\end{defn}

\begin{thm}
\label{thm.intbyparts}
\emph{(Integration by Parts)}
Let $f$ and $\alpha$ (not necessarily increasing) be bounded on $[a,b]$.
If $f\in\mathscr{R}_\alpha([a,b])$, then both of the follwing are true:
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $\alpha\in\mathscr{R}_f([a,b])$
  \item
    $\int^b_a f\;d\alpha = f(b) \alpha(b) - f(a)\alpha(a) - \int^b_a
    \alpha \; df$
\end{enumerate}
\end{thm}
\begin{cor}
The big result from integration by parts is that
$f\in\mathscr{R}_\alpha([a,b])$ if and only if $\alpha\in\mathscr{R}_f([a,b])$
\end{cor}

\begin{cor}
If $f\in\mathscr{R}_{\alpha_1}([a,b]) \cap \mathscr{R}_{\alpha_2}([a,b])$, then $f\in\mathscr{R}_{\alpha_1 + \alpha_2}([a,b])$ and
    \[ \int^b_a f\;d(\alpha_1 + \alpha_2) =
    \int^b_a f\;d\alpha_1 + \int^b_a f\;d\alpha_2 \]
\end{cor}


\begin{proof}(Integration by Parts)
Fix $\varepsilon>0$. To show (i), we must show there exists an
$I_\alpha\in\R$ (independent of $\varepsilon$) and a partition $Q$ (can
depend n $\varepsilon$) such that
\begin{align}
  Q^*\supset Q\quad\implies\quad
  |S_f(\alpha,Q^*,T)-I_\alpha|\leq\varepsilon
  \quad \forall T
  \label{intbypartscond}
\end{align}
The value $I_\alpha$ we need for (i) is already suggested to us by (ii).
Specifically, we need to show
\begin{align*}
  I_\alpha
  = f(b)\alpha(b)-f(a)\alpha(a)-I_f
  = f(b)\alpha(b)-f(a)\alpha(a)-\int^b_af\;d\alpha
\end{align*}
Hence we can show both (i) and (ii) at the same if we show that
there exists a $Q$ such that
\begin{align}
  Q^*\supset Q\quad\implies\quad
  \left\lvert
    S_f(\alpha,Q^*,T)-
    \left(
    f(b)\alpha(b)-f(a)\alpha(a)
    - \int^b_a f\; d\alpha
  \right)
  \right\rvert
  \leq\varepsilon
  \quad \forall T
  \label{intbypartswts}
\end{align}
That's the goal. Okay, to start, by assumption $f\in\sR_\alpha([a,b])$,
so there $\exists$ a partition $P$ s.t.
\begin{align}
  P^*\supset P\quad\implies\quad
  \lvert S_\alpha(f,P^*,T) - I_f \rvert \leq \varepsilon
  \label{intbypartscond2}
\end{align}
Now take $Q=P^*$ (any $P^*\supset P$ works) as the partition $Q$ to be
used in satisfying Condition~\ref{intbypartswts}.

Next take any $Q^*\supset Q$ and---inspired by the RHS of (ii)---rewrite
something like it, but with $S_f(\alpha,Q,T)$ instead of the integral
$\int^b_a \alpha \; df$ as follows\footnote{%
  Note, I'm \emph{not} at all saying that
  $S_f(\alpha,Q,T)=\int^b_a\alpha\;df$.
  Instead, I'm just writing down something \emph{like} the RHS of
  Equation~ with $S_f(\alpha,Q,T)$ in place of the
  integral just to see where it takes us.
}
\begin{align*}
  (*) :=
  f(b) \alpha(b) - f(a)\alpha(a) - S_f(\alpha,Q^*,T)
\end{align*}
Rewriting $f(b)\alpha(b)-f(a)\alpha(a)$ as a telescoping sum, and
writing out $S_f(\alpha,Q^*,T)$
\begin{align}
  (*)
    &= \sum^n_{i=1}
    \big[
    f(x_i) \alpha(x_i) - f(x_{i-1}) \alpha(x_{i-1})
    \big]
    - \sum^n_{i=1} \alpha(t_i) \left[f(x_i) - f(x_{i-1})
    \right]
  \notag\\
  &= \sum^n_{i=1} f(x_i) \left[\alpha(x_i) - \alpha(t_i) \right]
      +\sum^n_{i=1} f(x_{i-1}) \left[\alpha(t_i)
    - \alpha(x_{i-1})\right]\label{righthandtwosums}
\end{align}
Now $(*)$ kind of almost looks like another RS-Sum, so let's define a
partition and set of tag points so it actually is one. So define
$R=Q^*\cup T$ and $T_R$ as follows:
\begin{align*}
  R &= \{x_0,t_1,x_1,t_2,x_2,\ldots,t_{n-1},x_{n-1},t_n,x_n\}
  \qquad \text{$2n+1$ Elements}
  \\
  T_R &= \{ x_0, x_1, x_1, x_2, x_2,\ldots, x_{n-1},x_{n-1},x_n \}
  \qquad\;\; \text{$2n$ Elements}
\end{align*}
With this partition $R$ and set of tagging points $T_R$, the RHS of
Equation~\ref{righthandtwosums} represents exactly the RS-sum
$ S_\alpha(f, R, T_R)$, hence $(*)=S_\alpha(f,R,T_R)$.
Writing out $(*)$, that equality is
\begin{align}
    S_\alpha(f,R,T_R) &=
    f(b) \alpha(b) - f(a)\alpha(a) - S_f(\alpha,Q^*,T)
    \label{intbypartsSalpha}
\end{align}
Next, since $R\supset Q^* \supset P$, it's clear that by
Expression~\ref{intbypartscond2} that
\begin{align*}
  \left\lvert S_\alpha(f,R,T_R) - I_f \right\rvert \leq &\varepsilon
\end{align*}
And if we substitute in the RHS of Equation~\ref{intbypartsSalpha} for
$S_\alpha$, then we have precisely that
\begin{align*}
    \left\lvert f(b) \alpha(b) - f(a)\alpha(a)
    - S_f(\alpha,Q^*,T) -
    I_f
    \right\rvert
    &\leq
    \varepsilon \qquad \forall \; T
    \\
    \iff\qquad
    \left\lvert
    S_f(\alpha,Q^*,T)
    -
    \left(
    f(b) \alpha(b) - f(a)\alpha(a) - \int^b_a f\; d\alpha
    \right)
    \right\rvert
    &\leq
    \varepsilon
    \qquad\forall T
\end{align*}
This is exactly Condition~\ref{intbypartswts} that we we wanted to show.
Since $Q^*\supset Q$ was arbitrary and this holds for any
$Q^*\supset Q$, the desired result holds.
\end{proof}


\newpage
\subsection{Functions of Bounded Variation}

In the previous subsection on Integration by Parts, we assumed $f$ was
integrable with respect to an arbitrary integrator $\alpha$.  This
relaxed our earlier assumptions in the sense that $\alpha$ didn't have
to be increasing anymore.  \emph{However}, we just went ahead and
assumed $f\in\mathscr{R}_\alpha([a,b])$, which doesn't seem very
general.

So now, we'll relax our assumptions even further and consider what
characteristics and conditions for the functions $f$ and $\alpha$
\emph{permit} integrability---we won't just assume it.
This returns us to earlier subsections where we were concerned about
\emph{whether} $f$ would be integrable given $\alpha$. To answer that
question, we'll have to define the concept of \emph{bounded variation}.

\begin{defn}(Total Variation)
Define \emph{total variation} of function $f$ over interval $[a,b]$ as
\begin{align*}
    V_a^b(f) := \sup_P \sum^n_{i=1} \left\lvert f(x_i) - f(x_{i-1})
    \right\rvert
\end{align*}
\end{defn}


\begin{defn}(Bounded Variation)
We say that function $f$ is of \emph{bounded variation}, denoted
$f \in BV([a,b])$, if $V^b_a(f)<\infty$, i.e.\ if total variation is
finite.
\end{defn}


\begin{thm}\emph{(Properties Involving BV Functions)}
We now have a slew of results
\begin{enumerate}[label=\emph{(\roman*)}]
  \item \emph{(Monotonic Functions are BV):}
    If $f$ monotonically increasing or decreasing on $[a,b]$, then $f\in
    BV([a,b])$.
  \item
    \emph{(Continuous and Differentiable with Bounded Derivs $\implies$ BV):}
    If $f$ continuous and differentiable on $[a,b]$ and $f'$ is bounded,
    then $f\in BV([a,b])$.
  \item \emph{(BV implies Bounded):}
    For any function $f$, we have that
    \begin{align}
      \sup_{x\in[a,b]} |f(x) | \leq |f(a)| + V_a^b(f)
      \label{bvbounded}
    \end{align}
    Hence $f\in BV([a,b])$ implies $f(a),V^b_a(f)<\infty$ and $f$
    bounded on $[a,b]$.

  \item
    \emph{(Triangle Inequality \& Sum and Scalar Product of BV is BV):}
    For $f, g\in BV([a,b])$,
    \begin{align*}
      V_a^b(f+g) \leq V_a^b(f) + V_a^b(g)
      \qquad V_a^b(cf) = |c|V_a^b(f)
    \end{align*}
    This implies that $f+g$ and $cf$ are both of bounded variation as
    well.

  \item \emph{(Product of BV is BV):}
    If $f,g\in BV([a,b])$, then $f\cdot g \in BV([a,b])$.

  \item \emph{(Breaking up Total Variation):}
    If $f\in BV([a,b])$ and $c \in (a,b)$, then
    \begin{align*}
      V_a^b(f) = V_a^c(f) + V_c^b(f)
    \end{align*}

  \item \emph{(Variation is Monotonic Function):}
    $V_a^x(f)$ is a monotonically increasing function of $x$.
\end{enumerate}
\end{thm}

\begin{proof}
We prove a few results
\begin{enumerate}[label=(\roman*)]
  \item[(i)]
    Suppose $f$ is monotonically increasing.
    Then for any partition $P$
    \begin{align*}
      \sum_{i=1}^n
      \big| f(x_i) -f(x_{i-1})\big|
      =
      \sum_{i=1}^n
      f(x_i) -f(x_{i-1})
      = f(b) - f(a)
    \end{align*}
    This is independent of $P$. So taking the sup over partitions, we
    just end up with $V^b_a(f)=f(b)-f(a)$. Analogous for monotonically
    decreasing functions.

  \item[(ii)]
    Given any partition $P$, from the Mean Value Theorem, we have for
    any subintervals that
    \begin{align*}
        \exists\; c_i \in [x_{i-1}, x_i] \quad \text{s.t.}
        \quad f(x_i) - f(x_{i-1}) = f'(c_i) \left[
        x_i - x_{i-1}\right]
    \end{align*}
    Since $f'$ bounded, $M=\sup_{x\in[a,b]} |f'(x)|$ is finite. Thus for
    any partition $P$,
    \begin{align*}
      \sum^n_{i=1}
      \big\lvert
      f(x_i) - f(x_{i-1})
      \big\rvert
      &=
      \sum^n_{i=1}
      \big\lvert
          f'(c_i)
          \left[x_i - x_{i-1}\right]
      \big\rvert
      \leq \sum^n_{i=1} M \left\lvert \Delta x_i
          \right\rvert = M(b-a) < \infty
    \end{align*}
    Since the bound $M(b-a)$ is independent of the partition $P$, when
    we take the sup over $P$'s, we will still have $V_a^b(f)$ finite.

  \item[(iii)]
    Suppose that Inequality~\ref{bvbounded} were not true---i.e.\ that
    for some $x^* \in [a,b]$, we have
    \begin{align*}
      |f(x^*)| &> |f(a)| + V_a^b(f)
      \quad\implies\quad
      |f(x^*)| - |f(a)|  > V_a^b(f)
    \end{align*}
    But using identities about the absolute value, we see that the above
    statement implies
    \begin{align*}
        V_a^b(f) < |f(x^*)| - |f(a)| \leq \big\lvert \;
            |f(x^*)| - |f(a)|\;\big\rvert
            \leq |f(x^*) - f(a)|
    \end{align*}
    And if this were the case, we clearly chose $V_a^b(f)$ incorrectly
    because we could define a partition by $\{a, x^*, b\}$ to get a
    bigger value for the total variation than $V_a^b(f)$.

  \item[(vii)] By (vi), If $y>x$, then $V_a^y(f) = V_a^x(f) + V_x^y(f)$
    Clearly, $V_a^y(f) >V_a^x(f)$.
\end{enumerate}
\end{proof}

\begin{rmk}(Variation as Norm)
Given Property (iv), the space of BV functions is very nearly a linear
space with $V_a^b(f)$ serving as the norm.
We just need to make one modification because $V^a_b(f)$ is only a
\emph{semi-norm} on the space of BV functions. In other words,
$V^b_a(f)$ satisfies  all but the 0 condition for norms, since a
non-zero (constant) function still has $V^a_b(f)=0$.

But we can fix that, getting a full norm by adding the sup-norm
($||f||_\infty = \sup_{[a,b]} f$) to total variation. Thus if we define
our norm as
\begin{align*}
  \lVert f \rVert := \lVert f\rVert_\infty + V_a^b(f)
\end{align*}
we see that we've satisfied all of the conditions for a norm on the
function space.
%\begin{enumerate}
    %\item $\lVert f\rVert_\infty + V_a^b(f) = 0$ if and only if $f=0$.
    %\item $\lVert cf\rVert_\infty + V_a^b(cf) = c\lVert f\rVert_\infty + |c|V_a^b(f)
        %= c\left[\lVert f\rVert_\infty + V_a^b(f)\right]$.
    %\item $\lVert f+g\rVert_\infty + V_a^b(f+g) \leq
        %\left[\lVert f\rVert_\infty + V_a^b(f)\right] +
        %\left[\lVert g\rVert_\infty + V_a^b(g)\right]$.
%\end{enumerate}
\end{rmk}

\begin{ex}(Counterexamples for Parts of Previous Theorem)
\begin{enumerate}[label=(\roman*)]
  \item[(ii)]
    You might ask why we threw in the condition that $f'$ is bounded in
    the previous theorem for (ii).  It turns out continuous and
    differentiable doesn't necessarily mean that the derivative is
    bounded, so that was a necessarily addition.  Here's an example:
    \begin{align*}
        f(x) =
        \begin{cases}
            x^2 \sin(1/x^2) & x \neq 0\\
            0               & x = 0
        \end{cases}
    \end{align*}
    It's continuous (take the limit of the top while $x\rightarrow 0$ to
    see this), and it's differentiable:
    \begin{align*}
      f'(x) = 2x \sin(1/x^2) - \frac{2}{x} \cos(1/x^2)
    \end{align*}
    Clearly, that derivative isn't bounded as $x\rightarrow 0$.

  \item[(ii)]
    (\emph{Neither Continuity nor BV Implies the Other}):
    For continuity to imply BV, we really do need that
    \emph{differentiable with bounded derivatives} as stated in (ii).
    More generally, $f$ continuous does not imply $f\in BV$, and $f \in
    BV$ does not imply $f$ continuous.

    (Continuous but Not BV)
    Consider the followng two functions on $[0,1]$:
    \begin{align*}
        f(x) =
            \begin{cases}
                x \sin(1/x) & x\neq 0 \\
                0 & x=0
            \end{cases}
        \qquad\quad
        g(x) =
        \begin{cases}
        \frac{1}{n} & \text{$x = \frac{1}{n}$, where $n$ is an even integer}\\
        0 & \text{$x = \frac{1}{n}$, where $n$ is an odd integer} \\
        0 & \text{$x = 0$ or $x = 1$}
        \end{cases}
    \end{align*}
    with $g$ interpolated linearly in between $0$ and $1/n$ points.
    Both look something like the harmonic series that sums $1/n$, which we
    know diverges.

    (BV but Not Continous)
    Also, just to briefly show the other direction.
    Consider the function $f(x)=\mathbf{1}\{x\in[1/2,1]\}$ on $[0,1]$.
    Clearly there's a jump, but $V_a^b(f) = 1 < \infty$.
\end{enumerate}
\end{ex}


\clearpage

This next theorem is quite important as it establishes the link between
functions of bounded variation and monotonically increasing functions,
allowing us to apply some old results where we had to deal with strictly
increasing integrators.

\begin{thm}
\label{monot}
$f\in BV([a,b])$ if and only if $f = u-v$, where $u$ and $v$ are
monotonically increasing functions on $[a,b]$.
\end{thm}
\begin{proof}
($\Leftarrow$)
This is pretty trivial by (iv) of the previous theorem, which gives
\begin{align*}
  V_a^b(f) = V_a^b(u-v) \leq V_a^b(u) + V_a^b(v)
\end{align*}
And since $u$ and $v$ are monotonically increasing, the terms $V_a^b(u)$
and $V_a^b(v)$ are finite.

($\Rightarrow$)
Define functions $u$ and $v$ by
\begin{align*}
  u(x) := V_a^x(f)
  \qquad
  v(x):=V_a^x(f) - f(x)
\end{align*}
Clearly $f = u-v$.
By (vii) of the previous theorem,
we have that $u$ case is monotonically increasing.
Thus all that's left is to prove that $v$ is monotonically
increasing.

Suppose $y>x$. We want to show that $v(y) > v(x)$.
To do so, start with
\begin{align*}
    f(y) - f(x) \leq V_x^y(f)
    = V_a^y(f) - V_a^x(f)
\end{align*}
Now rearrange terms
\begin{align*}
    V_a^x(f) - f(x) &\leq V_a^y(f) - f(y)
\end{align*}
But by the definition of $v(x)$, that says precisely that $v(x)\leq
v(y)$.
\end{proof}

\clearpage
\subsection{Integration with Respect to Functions of Bounded Variation}

Now that we have the whole framework for functions of bounded variation
in place, we can return to integration where either $f$ or $\alpha$ is
of bounded variation (or both).


\begin{thm}
\emph{(Continuous Integrand and BV Integrator $\implies$ Integrable)}
If either
\begin{itemize}
  \item $f$ continuous on $[a,b]$ and $\alpha\in BV([a,b])$
  \item $f\in BV([a,b])$ and $\alpha$ continuous on $[a,b]$
\end{itemize}
then $f\in\mathscr{R}_\alpha([a,b])$.
\end{thm}
\begin{proof}
Suppose $f$ continuous and $\alpha\in BV([a,b])$. Then by Theorem
\ref{monot}, we can express $\alpha = u -v$, both of which are
increasing.
This implies $f\in\mathscr{R}_u([a,b]) \;\cap \;\mathscr{R}_v([a,b])$,
which implies $u, v \in \mathscr{R}_f([a,b])$ by Integration by Parts
(Theorem~\ref{thm.intbyparts}). 
That implies $u-v\in\mathscr{R}_f([a,b])$, which implies
$f\in\mathscr{R}_{u-v}([a,b]) = \mathscr{R}_\alpha([a,b])$
from Integration by Parts again.
\end{proof}

\begin{thm}
\emph{(Properties of $\sR_\alpha([a,b])$ with BV Integrators)}
We get a number of familiar results that match traditional integration
when $\alpha\in BV([a,b])$.
In particular, assuming that $f,g\in\mathscr{R}_\alpha([a,b])$
and $c\in(a,b)$,
\begin{itemize}
  \item $\int^c_a f\;d\alpha + \int^b_c g\;d\alpha = \int^b_a f\;d\alpha$.
  \item $h\circ f\in\mathscr{R}_\alpha([a,b])$
    if $h$ continuous,
    hence  $|f|\in\mathscr{R}_\alpha([a,b])$
  \item $fg\in\mathscr{R}_\alpha([a,b])$.
\end{itemize}
\end{thm}

\begin{thm}\emph{(Basis for Fundamental Theorem)}
\label{ftcbasis}
Suppose $\alpha$ is continuous and differentiable on $[a,b]$ with
$\alpha', f\in\mathscr{R}([a,b])$. Then
\begin{enumerate}[label=\emph{(\roman*)}]
  \item $f\in\mathscr{R}_\alpha([a,b])$---i.e. $f$ also
    Riemann-\emph{Stieltjes} integrable (not just Riemann) w.r.t.
    $\alpha$
  \item $\int^b_a f\;d\alpha = \int^b_a f \alpha'\;dx$
\end{enumerate}
Note that since $\alpha$ continuous and $\alpha'$ bounded on $[a,b]$
(needs to be bounded to be Riemann integrable), we also have
$\alpha \in BV([a,b])$.
\end{thm}
\begin{proof}
To show (i), must show $\exists I_f\in\R$ such that for any
$\varepsilon>0$, there exists a $P$ such that
\begin{align*}
  P^*\supset P
  \quad\implies\quad
  \left\lvert  S_\alpha(f,P,T) - I_f \right\rvert &\leq \varepsilon
  \qquad \forall T
\end{align*}
Our candidate for $I_f$ comes from (ii): $I_f=\int^b_a f\alpha'\; dx$.
This integral exists hence $I_f$ well-defined because $f,\alpha'
\in\mathscr{R}([a,b])$ (by assumption), so that $f\cdot
\alpha'\in\sR([a,b])$ by the previous therem.
Therefore, we can prove both (i) and (ii) if we show that
$\forall\varepsilon>0$, there exists a $P$ such that
\begin{align}
  P^*\supset P
  \quad\implies\quad
  \left\lvert
  S_\alpha(f,P^*,T) - \int^b_a f\alpha'\;dx
  \right\rvert &\leq \varepsilon
  \qquad \forall T
  \label{toexpand}
\end{align}
In order to actually do this, we first break up the inequality in
Expression~\ref{toexpand} as follows; then we will choose $P$ to bound
the pieces:
\begin{align}
  \left\lvert
  S_\alpha(f,P^*,T) - \int^b_a f\alpha'\;dx
  \right\rvert
  %&=
  %\left\lvert
  %S_\alpha(f,P^*,T) - S(f\alpha',P^*,T)
  %+
  %S(f\alpha',P^*,T)
  %- \int^b_a f\alpha'\;dx
  %\right\rvert
  %\notag
  %\\
  &\leq
  \left\lvert
  S_\alpha(f,P^*,T) - S(f\alpha',P^*,T)
  \right\rvert
  +
  \left\lvert
  S(f\alpha',P^*,T)
  - \int^b_a f\alpha'\;dx
  \right\rvert
  \label{fundbasistobound}
\end{align}
So now fix $\varepsilon>0$.
We can get our partition $P$ by the fact that $f\alpha'\in\sR([a,b])$ as
discussed. Hence there exists a $P$ such that
\begin{align*}
  P^*\supset P
  \quad\implies\quad
  \left\lvert S(f\alpha',P^*,T) - \int^b_a f\alpha'\;dx
  \right\rvert \leq \varepsilon/2
\end{align*}
Hence, our choice of $P$ has already bounded one piece
of Expression~\ref{fundbasistobound}. To bound the other, we need to
work on $S_\alpha(f,P^*,T)$. First, by the Mean Value Theorem,
\begin{align*}
  \Delta\alpha_i = \alpha'(\xi_i) \Delta x_i,
  \quad \xi_i \in [x_{i-1}, x_i]
  \quad\implies\quad
  S_\alpha(f,P^*,T)
  &= \sum^n_{i=1} f(t_i) \Delta\alpha_i
  = \sum^n_{i=1} f(t_i)\alpha'(\xi_i) \Delta x_i
\end{align*}
Hence
\begin{align*}
  \left\lvert S_\alpha(f,P,T) - S(f\alpha',P,T)
      \right\rvert &\leq
      \left\lvert \sum^n_{i=1} f(t_i)\left[\alpha'(\xi_i) -
      \alpha'(t_i)\right] \Delta x_i \right\rvert
  \leq M \cdot
      \sum^n_{i=1} \big\lvert \alpha'(\xi_i) -
      \alpha'(t_i)\big\rvert \Delta x_i
\end{align*}
where $M =\sup_{x\in[a,b]}\left\lvert f(x)\right\rvert$. Finally, note
that for each $i$,
\begin{align*}
    |\alpha'(\xi_i) - \alpha'(t_i)| \leq
    \sup_{[x_{i-1}, x]} \alpha(x) -
    \inf_{[x_{i-1}, x]} \alpha(x)
\end{align*}
like we might have if we consider the upper and lower sums. As a result,
because $\alpha'\in\mathscr{R}_\alpha([a,b])$, we can choose a $Q$ such
that
\begin{align*}
    \sum^n_{i=1} \left\lvert \alpha'(\xi_i) -
    \alpha'(t_i)\right\rvert \Delta x_i
    \leq U_\alpha(f,Q) - L_\alpha(f,Q) \leq \frac{\varepsilon}{2M}
\end{align*}
Taking our new partition to be $P \cup Q$, which is a superset of $P$
(where $P$ is from above), we see that our desired result is achieved.
\end{proof}

\newpage
\subsection{Fundamental Theorem of Calculus and Change of Variables}

\begin{thm}\emph{(Fundamental Theorem of Calculus)}
\label{ftc}
The two parts:
\begin{enumerate}[label=\emph{\Roman*.}]
  \item
    If $f$ differentiable and $f'\in\mathscr{R}([a,b])$, then
    $\int^b_a f'(x)\;dx = f(b) - f(a)$
  \item
    Suppose $f\in\mathscr{R}([a,b])$.
    Define
    \begin{align*}
      F(x) := \int^x_a f(t)\;dt
    \end{align*}
    Then $F$ is Lipschitz continuous on $[a,b]$,
    and $f$ continuous at $x$ implies $F'(x)=f(x)$.
\end{enumerate}
\end{thm}

\begin{proof}
We prove each in turn
\begin{enumerate}[label=\Roman*.]
  \item
    Careful with the notation here, but take $f$ in Theorem
    \ref{ftcbasis} to be the constant function $f(x) =1$. Then, take the
    $\alpha(x)$ in Theorem \ref{ftcbasis} to be the $f$ in this
    theorem's statement:
    \begin{align*}
      \int^b_a f'(x) \;dx = \int^b_a 1 \;df = f(b) - f(a)
    \end{align*}
  \item
    First, Lipschitz continuity. Since $f$ is bounded,
    $f(x) \leq C = \sup_{x\in[a,b]} f(x)$.
    Thus
    \begin{align*}
        |F(y) - F(x)| &= \left\lvert \int^y_x f(t)\;dt \right\rvert
            \leq \int^y_x |f(t)|\;dt
        \leq \int^y_x C\;dt = C|y-x|
    \end{align*}
    This is precisely the definition of $F$ being Lipschitz continuous.

    Next, for $x\in[a,b]$ and $h>0$, the definition of $F$ implies
    \begin{align*}
      \frac{F(x+h)-F(x)}{h} = \frac{1}{h}\int^{x+h}_x f(t)\;dt
    \end{align*}
    Applying the Mean Value Theorem for to the RHS integral, we
    get some $\mu \in [m, M]$, where $m = \inf f$ and $M = \sup f$ over
    the interval $[x,x+h]$, such that
    \begin{align}
      \mu = \frac{1}{h}\int^{x+h}_x f(t)\;dt
      = \frac{F(x+h)-F(x)}{h}
      \label{ftcmu}
    \end{align}
    Next, if $f$ is continuous at $x$, then for all $\varepsilon>0$,
    there exists a $\delta>0$ such that
    \begin{align}
        h \leq \delta \quad \implies \quad |f(y) - f(x)|
        &\leq\varepsilon
        \qquad \forall y\in[x,x+h]
    \end{align}
    In other words, $h<\delta$ is small enough so that $f(y)$ is
    $\varepsilon$ close to $f(x)$ for all $y\in[x,x+h]$.
    With $h<\delta$, we can then conclude
    \begin{align*}
      \left\lvert
      \mu - f(x)
      \right\rvert
      =
      \left\lvert
      \frac{1}{h}\int^{x+h}_x f(t)\;dt
      - f(x)
      \right\rvert
      %&=
      %\left\lvert
      %\frac{1}{h}\int^{x+h}_x\big(f(t)-f(x)\big)\;dt
      %\right\rvert
      %\\
      &\leq
      \frac{1}{h}\int^{x+h}_x\big|f(t)-f(x)\big|\;dt
      \leq
      \frac{1}{h}\int^{x+h}_x\varepsilon\;dt
      %\\
      %\implies\quad
      %\left\lvert
      %\mu - f(x)
      %\right\rvert
      %&\leq
      \leq\varepsilon
    \end{align*}
    Using Equation~\ref{ftcmu} to sub in for $\mu$, this last line says
    that
    \begin{align*}
      \left\lvert \frac{F(x+h)-F(x)}{h}- f(x)
      \right\rvert
      &\leq\varepsilon
    \end{align*}
    So $F$ is differentiable at $x$ and $F'(x)=f(x)$.
\end{enumerate}
\end{proof}

\clearpage
\begin{thm}\emph{(Change of Variables)}
Suppose we have $\varphi: [a,b] \rightarrow [\varphi(a), \varphi(b)]$, a
differentiable, monotonically increasing function with
$\varphi'\in\mathscr{R}([a,b])$. Also suppose that
$f\in\mathscr{R}([\varphi(a),\varphi(b)])$. Then $(f\circ
\varphi)\cdot\varphi' \in \mathscr{R}([a,b])$ and
\begin{align*}
  \int^b_a f\left(\varphi(x)\right) \varphi'(x)\;dx
  = \int^{\varphi(b)}_{\varphi(a)} f(u)\;du
\end{align*}
\end{thm}


\newpage
\subsection{Improper Integrals}

\begin{defn}(Cauchy Principal Value)
Sometimes we want to consider integrating a function over the real line,
we take
\begin{align*}
    \lim_{b\rightarrow\infty}
    \left(\lim_{a\rightarrow-\infty} \int^b_a f\;d\alpha
    \right) \neq
    \left(\lim_{a\rightarrow\infty} \int^a_{-a} f\;d\alpha
    \right)
\end{align*}
where the second term is the \emph{Cauchy Principle Value}. It may
sometimes exist, while the integral on the left does not.
\end{defn}

\begin{ex}
For the simplest example, take $f(x)=x$. If evaluated in the usual
sense, we get $\infty-\infty$, which makes no sense. However, if we
evaluate as on the righthand side above, we get a Cauchy Principle Value
of $0$.
\end{ex}

\begin{ex}
Now we have an example of a function that is continuous, non-negative
everywhere, and $f\not\rightarrow 0$, but $f\in\mathscr{R}([a,\infty))$
nonetheless.  To construct such a function, set up
\begin{align*}
  f(x) =
  \begin{cases}
      1 & x\in\mathbb{Z} \\
      / & [x - 1/x^2, x] \\
      \text{\textbackslash} & [x, x + 1/x^2] \\
      0 & \text{otherwise}
  \end{cases}
\end{align*}
where $/$ and \textbackslash denote increasingly linear and decreasingly
linear, respectively.

Suppose that in addition to the conditions we had above, we also want
$f$ to be unbounded. To deal with that, make the triangles higher, but
shrink the base faster.
\end{ex}



\clearpage
\section{Fixed Points}

\subsection{Preliminaries: Operators}

\begin{defn}{(Operator)}
If $X$ is some function space
$X = \{f:A\rightarrow B\}$
then an \emph{operator}, denoted $T$, is a function
\begin{align*}
  T:X&\rightarrow X \\
  f&\mapsto Tf
  \qquad \forall f\in X
\end{align*}
It is a ``function of functions'', taking elements $f\in X$ and mapping
them to some other element $Tf\in X$. Though of course $T$ is a
function, the word ``operator'' is used to distinguish $T$ from the
functions it acts on.

To avoid overly cumbersome notation, I follow convention and write $Tf$
rather than $T(f)$ when applying operator $T$ to function $f$.
\end{defn}

\begin{ex}
Here's a simple example of an operator $T:X\rightarrow \R$. It acts on
the space of bounded and integrable functions
\begin{align*}
  X = \{f: A\subseteq \R \rightarrow \R\}
\end{align*}
where $A$ is bounded.
The operator is defined
\begin{align*}
  Tf = \int_A f(x) \; dx
\end{align*}
It takes a function $f$, integrates it, and returns a number.
Therefore, $T$ is an operator.
\end{ex}

\subsection{Preliminaries: Correspondences}

\begin{defn}{(Correspondence)}
A \emph{Correspondence} $\Gamma:X\rightrightarrows Y$ is a function that
maps any element of $X$ into a subset of $Y$. Moreover, if
$\Gamma(x)\subseteq Y$ is \emph{compact} for all $x\in Y$, then we say
that $\Gamma$ is a \emph{compact correspondence}.
\end{defn}

\begin{ex}
Here's an example of a correspondence $\Gamma: \R\rightrightarrows\R$.
\begin{figure}[htpb!]
\centering
\begin{tikzpicture}[xscale=1.5]
  % Upper bound of region
  \draw[d4blue, ultra thin, domain=0.1:4, fill=d4blue, opacity=0.4] %
    (0.1,0) -- plot (\x, {0.1*pow(\x,3)-0.6*pow(\x,2)+1.2*\x+3}) -- (4,0);

  % Lower bound of region
  \draw[white, ultra thick, domain=0.1:4, fill=white] %
    (0.1,0) -- plot (\x, {0.1*pow(\x,3)-0.6*pow(\x,2)+1.2*\x+1}) -- (4,0);

  % Axis
  \draw[d4black, <->] (0,5) -- (0,0) -- (4,0);

  % Dashed Line at x
  \draw[d4black, ultra thick, dashed, -] %
    (2,0) node[below, yshift=-5pt] {$x$} -- (2,1.8);

  % Vertical \Gamma(x) line
  \draw[d4black, ultra thick, -] (2,1.8) -- (2,3) -- (2,3.8);

  % Brace marking Gamma(x)
  \draw[%
      d4black, decorate, %
      decoration={brace, amplitude=10pt, mirror}, xshift=2pt
    ] %
    (2,1.8) -- (2,3.8) node[midway, right, xshift=12pt] {$\Gamma(x)$};
\end{tikzpicture}
\end{figure}

As shown, numbers in $\R$ on the $x$-axis map into intervals along the
$y$-axis (rather than just a single point as they would for a curve).
An example is shown for one particular $x$.
\end{ex}

\begin{defn}{(Lower Hemi-Continuous)}
Correspondence $\Gamma:X\rightrightarrows Y$ is \emph{lower
hemi-continuous} (LHC) if and only if points don't ``pop up out of
nowhere'' in the limit.

Formally, choose $x\in X$. Note that we require
$\Gamma(x)\neq\emptyset$. Now let $\{x_n\}\subseteq X$ be any arbitrary
sequence converging to $x\in X$. Now choose any $y\in\Gamma(x)$.  For
$\Gamma$ to be LHC at $x$, you need to be able to construct a sequence
\begin{align*}
  \{y_n\}\rightarrow y
  \quad\text{s.t.}\quad
  y_n \in \Gamma(x_n) \quad\forall n
\end{align*}
Now the informal definition makes sense. If $y$ popped up out of
nowhere in the limit, then you could not construct a sequence $\{y_n\}$
converging to it whose elements stayed inside the sets
$\{\Gamma({x_n})\}$ the whole time.
\end{defn}
%\begin{figure}[htpb!]
%\centering
%\begin{tikzpicture}[xscale=1.5]
  %% Upper bound of region
  %\draw[d4blue, ultra thin, domain=0.1:4, fill=d4blue, opacity=0.4] %
    %(0.1,0) -- plot (\x, {0.1*pow(\x,3)-0.6*pow(\x,2)+1.2*\x+3}) -- (4,0);

  %% Left segment
  %\draw[white, ultra thick, domain=0.1:1, fill=white] %
    %(0.1,0) -- plot (\x, {0.1*pow(\x,3)-0.6*pow(\x,2)+1.2*\x+1}) -- (1,0);

  %% Middle segment
  %\draw[white, ultra thick, domain=0.1:4, fill=white] %
    %(0.1,0) -- plot (\x, {0.1*pow(\x,3)-0.6*pow(\x,2)+1.2*\x}) -- (4,0);

  %%% Lower bound of region
  %%\draw[white, ultra thick, domain=0.1:4, fill=white] %
    %%(0.1,0) -- plot (\x, {0.1*pow(\x,3)-0.6*pow(\x,2)+1.2*\x+1}) -- (4,0);

  %% Axis
  %\draw[d4black, <->] (0,5) -- (0,0) -- (4,0);

  %%% Dashed Line at x
  %%\draw[d4black, ultra thick, dashed, -] %
    %%(2,0) node[below, yshift=-5pt] {$x$} -- (2,1.8);

  %%% Vertical \Gamma(x) line
  %%\draw[d4black, ultra thick, -] (2,1.8) -- (2,3) -- (2,3.8);

  %%% Brace marking Gamma(x)
  %%\draw[%
      %%d4black, decorate, %
      %%decoration={brace, amplitude=10pt, mirror}, xshift=2pt
    %%] %
    %%(2,1.8) -- (2,3.8) node[midway, right, xshift=12pt] {$\Gamma(x)$};
%\end{tikzpicture}
%\end{figure}

\begin{defn}{(Upper Hemi-Continuous)}
The \emph{compact}-valued correspondence $\Gamma:X\rightrightarrows Y$
is \emph{upper hemi-continuous} (UHC) if and only if points don't
``disappear'' in the limit. Alternatively, we might informally say that
the graph of $\Gamma$ is closed.

Formally, choose $x\in X$. Note that we require
$\Gamma(x)\neq\emptyset$. Now let $\{x_n\}\subseteq X$ be any arbitrary
sequence such that $x_n\ra x\in X$. Also let $\{y_n\}$ be any sequence
where
\begin{align*}
  y_n \in \Gamma(x_n) \quad\forall n
\end{align*}
For $\Gamma$ to be UHC at $x$, it must the case that $\{y_n\}$ has a
convergent subsequence that converges to some $y\in \Gamma(x)$.

Now the informal definition makes sense. If there were some $y'\in Y$
such that $y_n = y' \in \Gamma(x_n)$ for all $n$ (implying
$y_n\rightarrow y'$), then we also want $y'\in \Gamma(x)$. It shouldn't
disappear in the limit.
\end{defn}
\begin{rmk}
A single-valued correspondence that is upper hemicontinuous is a
continuous function.
\end{rmk}

\begin{defn}{(Continuous Correspondence)}
Compact-valued correspondence $\Gamma:X\rightrightarrows Y$ is
\emph{continuous} at $x\in X$ if it is both UHC and LHC at $x$.
\end{defn}

\clearpage
\subsection{Contraction Maps}

\begin{defn}{(Contraction Map)}
In metric space $(X,d)$, the function $T: X\rightarrow X$ is a
\emph{contraction map} if and only there exists a $\beta \in (0,1)$ such
that
\begin{align*}
  d(T(x), T(y)) \leq \beta \; d(x,y)
  \qquad x,y\in X
\end{align*}
$\beta$ is called the \emph{modulus} of $T$.
\end{defn}
\begin{rmk}
A function $T$ is a contraction map \emph{on a metric space $(X,d)$}. If
$T$ is a constraction map on $(X,d)$, it might \emph{not}
be a contraction map on $(X,d')$ where $d'\neq d$.
\end{rmk}

\begin{prop}{\emph{(Contraction Maps are Continuous)}}
If $T$ is a contraction map, then $T$ is continuous.
\end{prop}
\begin{proof}
Suppose $\varepsilon>0$ given. Choose $\delta=\varepsilon$. Then
\begin{align*}
  d(x,y) < \delta=\varepsilon
  \quad\implies\quad
  d(T(x),T(y)) \leq \beta \; d(x,y)
  < \beta \varepsilon < \varepsilon
\end{align*}
\end{proof}


\begin{thm}{\emph{(Blackwell's Sufficient Conditions for a Contraction)}}
Suppose that we consider the metric space $(\sB(X,\R),d_\infty)$
of all bounded functions $f:X\ra\R$, and we
have some operator
$T:\mathscr{B}(X,\R)\rightarrow\mathscr{B}(X,\R)$ on that space.
Suppose $T$ satisfies
\begin{enumerate}
  \item \emph{Monotonicity}: $f(x) \leq g(x)$ for all $x$
    $\implies$ $T[f](x) \leq T[g](x)$ for all $f,g\in \sB(X,\R)$.
  \item \emph{Discounting}: For all $a\in\R_+$,
    $T[f+a](x) \leq T[f](x) + \beta a$ for some $\beta\in(0,1)$.
\end{enumerate}
Then $T$ is a contraction map of modulus $\beta$.
\end{thm}
\begin{proof}
Start with
\begin{align*}
  f(x)-g(x) &\leq \sup_{y\in X} |f(y)-g(y)| = d_\infty(f,g) \\
  \iff\qquad
  f(x) &\leq g(x) + d_\infty(f,g) \\
  \text{Monotonicity} \quad
  \implies\quad
  T[f(x)] &\leq T[g(x) + d_\infty(f,g)]
  \\
  \text{Discounting} \quad
  \implies\quad
  T[f(x)] &\leq T[g(x)] + \beta d_\infty(f,g)
\end{align*}
Rearranging,
\begin{align*}
  T[f(x)] - T[g(x)] &\leq \beta d_\infty(f,g)
\end{align*}
Taking the sup over $x$ on the LHS, we get
\begin{align}
  \sup_{x\in X} \; T[f(x)] - T[g(x)] &\leq \beta d_\infty(f,g)
  \label{blackwell-fg}
\end{align}
We can repeat all of the steps above, swapping $f$ and $g$, to derive
\begin{align}
  \sup_{x\in X} \; T[g(x)] - T[f(x)] &\leq \beta d_\infty(g,f)
  \label{blackwell-gf}
\end{align}
We can take Inequalities~\ref{blackwell-fg} and \ref{blackwell-gf} and
use symmetry $d_\infty(f,g)=d_\infty(g,f)$ to conclude
\begin{alignat*}{3}
  \begin{rcases}
  \sup_{x\in X} \; T[f(x)] - T[g(x)] &\leq \beta d_\infty(f,g)
  \\
  \sup_{x\in X} \; T[g(x)] - T[f(x)] &\leq \beta d_\infty(g,f)
  \end{rcases}
  \quad\implies&&\quad
  \sup_{x\in X} \left\lvert T[f(x)] - T[g(x)]\right\rvert
    &\leq \beta d_\infty(f,g) \\
  \iff&&\qquad
  d_\infty(T[f],T[g])
  &\leq \beta d_\infty(f,g)
\end{alignat*}
Therefore, $T$ is a contraction map.
\end{proof}

\begin{defn}(Non-Expansive Mapping)
A \emph{non-expansive mapping} on metric space $(X,d)$ is a function
such that
\begin{align*}
  d(T(x),T(y)) \leq d(x,y)
  \qquad\forall x,y\in X
\end{align*}
This is a weaker condition than the condition for a contraction mapping,
as the next proposition states.
\end{defn}

\begin{prop}
Contraction maps are nonexpansive maps.
\end{prop}
\begin{proof}
Take any two $x,y\in X$. Since $T$ is a nonexpansive mapping,
\begin{align*}
  d(T(x),T(y)) \leq \beta d(x,y) \leq d(x,y)
\end{align*}
since $\beta \in (0,1)$.
\end{proof}

\begin{ex}(Non-Expansive Mapping with No Fixed Points)
Consider the following mapping on metrics space $(\R,|x-y|)$:
\begin{align*}
  T(x) = x+1
\end{align*}
Then clearly, no point is a fixed point since $T(x)=x+1\neq x$
for any $x\in X$. Moreover,
\begin{align*}
  |T(x)-T(Y)| = |(x+1)-(y+1)| = |x-y|\leq |x-y|
\end{align*}
so that $T$ satisfies the condition for a nonexpansive mapping.
\end{ex}

\begin{ex}(Non-Expansive Mapping with Many Fixed Points)
A nonexpansive mapping with more than one fixed point is the
identity map
\begin{align*}
  T(x) = x
  \qquad \forall x\in X
\end{align*}
Every point in $S$ is a fixed point, and we have
\begin{align*}
  |T(x)-T(y)| = |x-y| \leq |x-y|
  \qquad\forall x,y\in X
\end{align*}
satisfying the condition for a nonexpansive mapping.
\end{ex}

\clearpage
\subsection{Banach Fixed Point or Contraction Mapping Theorem}

\begin{thm}{\emph{(Banach Fixed Point, Contraction Mapping Theorem)}}
\label{thm:banach}
Suppose that $T:X\rightarrow X$ is a contraction map and $(X,d)$ is a
complete metric space. Then
\begin{enumerate}
  \item $\exists! \; x^*$ such that $T(x^*)=x^*$.\footnote{%
      $\exists!$ means ``there exists a unique''}
  \item For all $x_0 \in X$,
    \begin{align}
      \label{thm:fixed-bound1}
      d(T^n(x_0), x^*) \leq \beta^n d(x_0,x^*)
    \end{align}
    where $T^n$ is the contraction map applied $n$ times.

    So applying the contraction map many, many times to any starting
    point $x_0$ will get you closer and closer to the fixed point at
    each iteration.
\end{enumerate}
\end{thm}

\begin{proof}
First, given $x_0$, construct sequence $\{x_n\}$ by defining the $n$th
term to be
\begin{align}
  \label{contraction-map-seq}
  x_n = T^n(x_0)
\end{align}
Now for the proof. It's long, so we do it in parts
\begin{enumerate}
\item
\emph{Show $\{x_n\}$ Converges}:
Let's first prove that $x_n$ converges to \emph{something}. Since the
space $(X,d)$ is complete, it's enough to show that the sequence is
Cauchy.  And to show that $\{x_n\}$ is Cauchy we want to find a bound of
the form
\begin{align*}
  d(x_n,x_m) < G(\min\{m,n\})
\end{align*}
Without loss of generality, suppose that $m\geq n$, or equivalently
$m=n+p$ for some integer $p\geq 0$. Then we can use the triangle
inequality $p$ times to write
\begin{align}
  d(x_n,x_m) = d(x_n,x_{n+p})
  &\leq d(x_n,x_{n+1}) + d(x_{n+1},x_{n+p}) \notag \\
  &\leq d(x_n,x_{n+1}) + d(x_{n+1},x_{n+2}) + d(x_{n+2},x_{n+p})\notag\\
  &\;\; \vdots\notag\\
  d(x_n,x_{n+p})
  &\leq \sum^{p-1}_{i=0} d(x_{n+i},x_{n+i+1})
  \label{fixedpt-series}
\end{align}
%Now let's take a look at each of the constituent terms in the
%Series~\ref{fixedpt-series} for $i>0$, using the fact that $T$ is a
%contraction map:
%\begin{alignat*}{3}
  %d(x_{n+1},x_{n+2}) &=
  %d(T(x_{n}),T(x_{n+1}))
  %&&\leq
  %\beta d(x_{n},x_{n+1}) \\
  %d(x_{n+2},x_{n+3}) &=
  %d(T(x_{n+1}),T(x_{n+2}))
  %&&\leq
  %\beta d(x_{n+1},x_{n+2})
  %\\
  %\vdots \quad & \quad\qquad \vdots
%\end{alignat*}
Since $T$ is a contraction map, the constituent terms of the sum in
Inequality~\ref{fixedpt-series} for $i>0$ can be written as
\begin{align*}
  d(x_{n+i},x_{n+i+1}) &=
  d(T(x_{n+i-1}),T(x_{n+i}))
  \leq
  \beta d(x_{n+i-1},x_{n+i})
  \qquad i = 1,\ldots,p-1
\end{align*}
By recursive substitution for each $i$, we can even bound each term by a
function of $\beta$ and $d(x_n,x_{n+1})$, the first term in
Series~\ref{fixedpt-series}:
\begin{align*}
  d(x_{n+i},x_{n+i+1})
  &\leq
  \beta^i d(x_{n},x_{n+1})
  \qquad i = 1,\ldots,p-1
\end{align*}
This allows us to rewrite Series~\ref{fixedpt-series} and the inequality
for $d(x_n,x_{n+p})$ as
\begin{align}
  d(x_n,x_{n+p})
  \leq \sum^{p-1}_{i=0} d(x_{n+i},x_{n+i+1})
  &\leq \sum^{p-1}_{i=0} \beta^i d(x_{n},x_{n+1})\notag\\
  &\leq  d(x_{n},x_{n+1})\sum^{\infty}_{i=0} \beta^i\notag \\
  \Rightarrow\quad
  d(x_n,x_{n+p})
  &\leq  d(x_{n},x_{n+1})\times \frac{1}{1-\beta}
  \label{fixedpt-ineq}
\end{align}
Okay, that's pretty good. Let's just bound $d(x_n,x_{n+1})$, again using
recursive substitution
\begin{align*}
  d(x_n,x_{n+1})
  = d(T(x_{n-1}), T(x_n))
  &\leq \beta d(x_{n-1}, x_n)\\
  &\leq \beta \cdot \beta d(x_{n-2}, x_{n-1})\\
  &\;\; \vdots \\
  \Rightarrow\quad
  d(x_n,x_{n+1})
  &\leq \beta^n d(x_{0}, x_1)
\end{align*}
Substituting this back into Inequality~\ref{fixedpt-ineq}, we get
\begin{align*}
  d(x_n,x_{n+1})
  &\leq \frac{\beta^n}{1-\beta} d(x_0,x_1)
\end{align*}
Since we want to bound the LHS of the last inequality by $\varepsilon$,
equate and solve for $N$:
\begin{align*}
  \varepsilon &= \frac{\beta^N}{1-\beta} d(x_0,x_1) \\
  \Rightarrow\quad
  N &=
  \frac{1}{\ln \beta}
  \left[
  \ln(1-\beta)+ \ln \varepsilon - \ln(d(x_0,x_1))
  \right]
\end{align*}
This is the choice of $N$ with $\varepsilon$ given.
Therefore $\{x_n\}$ is Cauchy and, since $X$ is complete, convergent.

\item
\emph{Show $x^*$ Exists}: Call $x$ the limit of $\{x_n\}$, which we just
showed exists. In notation, that means
\begin{align*}
  x = \limn x_n = \limn T(x_{n-1})
\end{align*}
By Theorem~\ref{thm:cts-sequential}, the continuity of $T$ means we can
exchange the limit and the function:
\begin{align*}
  x = \limn T(x_{n-1})
  =
  T\left(\limn x_{n-1}\right)
  =
  T\left(x\right)
\end{align*}
Hence, the limit of the sequence $\{x_n\}$ is the fixed point, i.e.\
$x^*=x = \limn x_n$.

\item
\emph{Show Uniqueness}:
Suppose that there were two fixed points $x^*$ and $y^*$. Then
\begin{align*}
  d(x^*, y^*) &= d(T(x^*), T(y^*))
  \quad\text{Since fixed points} \\
  \Rightarrow\quad
  d(x^*, y^*) &\leq \beta d(x^*, y^*)
  \; \quad
  \qquad\text{Since $d(T(x^*), T(y^*)) \leq \beta d(x^*, y^*)$}
\end{align*}
But since $\beta\in (0,1)$, this is impossible unless $d(x^*,y^*)=0$.

\item
\emph{Bound $d(T^n(x_0),x^*)$}:
Similar to what we did above
\begin{align*}
  d(x_n, x^*) = d(T(x_{n-1},T(x^*))
  &\leq \beta d(x_{n-1},x^*)\\
  & \;\; \vdots\\
  &\leq \beta^n d(x_0,x^*)
\end{align*}
\end{enumerate}
\end{proof}

The last theorem is a very useful bit of theory, but we can do a bit
better than the bound given by Inequality~\ref{thm:fixed-bound1}.
Specifically, to be able to tell how far $T^n(x_0)$ is from the fixed
point $x^*$, Theorem~\ref{thm:banach} requires that we know
$d(x_0,x^*)$. Well, a lot of times we don't know. And a lot of times,
we use the contraction mapping theorem to motivate forming a sequence
$T^n(x_0)$ that converges to some fixed point we don't know.
So can we bound $d(T^n(x_0),x^*)$ with stuff we do know?

In fact, we can bound $d(T^n(x_0), x^*)$ by a function of $d(T^n(x_0),
T^{n+1}(x_0))$ instead, which is the change in the sequence
$\{T^n(x_0)\}$. In practical terms, that means ``Keep iterating on the
sequence $\{T^n(x_0)\}$, and if you see that the change in the sequence
from element to element is pretty small, you can be confident that
you're close to the fixed point.'' You don't have to know what the fixed
point is. You just need to compute $T^{n+1}(x_0) - T^n(x_0)$, which is
always feasible, in general. This is the next result.
\begin{cor}
Consider contraction map $T:X\rightarrow X$ with modulus $\beta$ in
complete metric space $(X,d)$. Given any starting point $x_0\in X$,
\begin{align*}
  d(T^n(x_0), x^*)
  \leq \frac{d(T^n(x_0),T^{n+1}(x_0))}{1-\beta}
\end{align*}
\end{cor}
\begin{proof}
Again, form the sequence $\{x_n\}$ as we did above in
(\ref{contraction-map-seq}).
\begin{align*}
  x_n = T^n(x_0)
\end{align*}
The goal is to bound the distance between $T^n(x_0)$ and $x^*$ using
only information about the change
\begin{align*}
  d(T^n(x_0), x^*)
  &\leq
  G\left(\;d(T^n(x_0), T^{n+1}(x_0))\;\right)\\
  \Leftrightarrow\qquad
  d(x_n, x^*)
  &\leq
  G\left(\;d(x_n, x_{n+1})\;\right)
\end{align*}
Using the triangle inequality, the fact that $T$ is a contraction map,
and the fact that $x^*$ is a fixed point,
\begin{align*}
  d(x_n, x^*)
  &\leq d(x_n,x_{n+1}) + d(x_{n+1},x^*) \\
  &\leq d(x_n,x_{n+1}) + d(T(x_n),T(x^*)) \\
  &\leq d(x_n,x_{n+1}) + \beta d(x_n,x^*) \\
  \Rightarrow\quad
  d(x_n, x^*)
  &\leq \frac{d(x_n,x_{n+1})}{1-\beta}
\end{align*}
Since $d(x_n,x^*)=d(T^n(x_0),x^*)$, we have the desired result.
\end{proof}

\begin{defn}{($T$-Invariant Sets)}
Given an operator/function $T:X\rightarrow X$ and some set $C\subseteq
X$, we say that the set $C$ is \emph{$T$-invariant} if
\begin{align*}
  T(C)\subseteq C
\end{align*}
\end{defn}

\begin{cor}
\label{cor:cmt}
Suppose that $T: X\rightarrow X$ is a contraction map in complete space
$X$. Let $x^*$ denote the unique fixed point. Then
\begin{enumerate}
  \item If $C\subseteq X$ is a closed, nonempty, $T$-invariant set, then
    $x^* \in C$.
  \item If $T(C)\subseteq C' \subseteq C \subseteq X$ where $C$ is
    closed but $C'$ arbitrary subset, then $x^* \in C'$.
\end{enumerate}
\end{cor}
\begin{proof}
(Part 1)
Suppose $x^*\not\in C$. Take any $x_0\in C$. By Contraction Mapping
Theorem~\ref{thm:banach}, $T^n(x_0)\ra x^*$. But since $C$ is $T$
invariant, $T^n(x_0)\in C$ for all $n$. So we have a convergent
sequence $\{T^n(x_0)\}\ninf$ in a closed set $C$.
Then by Proposition~\ref{prop:closed}, $x^*\in C$, which is a

(Part 2) Clearly, $C$ is $T$-invariant since
$T(C)\subseteq C' \subseteq C$. By $C$ closed and then Part 1, we can
conclude that $x^*\in C$.
From that, we conclude that $T(x^*)\in C$ since we assumed that $T(x)
\in C'$ for all $x \in C$. But since $x^*=T(x^*)$, that's equivalent to
$x^*\in C'$.
\end{proof}

\begin{ex}
Consider the metric space of all functions with the sup-norm metric.
Call $C$ the set of weakly increasing functions. We know that this set
is closed. Therefore, by Corollary~\ref{cor:cmt}, if the operator $T$
maps weakly increasing functions to weakly increasing functions,
Statement 1 says that the fixed point of the operator $T$ will be weakly
increasing.

Now consider Statement 2. Let $C'$ denote the set of all \emph{strictly}
increasing functions. This is not a closed set, but it is a subset of
$C$, the set of all weakly increasing functions. If the operator $T$
maps weakly increasing functions to \emph{strictly} increasing
functions, then the fixed point will be \emph{strictly} increasing by
Statement 2 of Corollary~\ref{cor:cmt}
\end{ex}

\clearpage
\subsection{Brouwer and Kakutani Fixed Point Theorems}

\begin{thm}\emph{(Brouwer's Fixed Point Theorem)}
Suppose that $K\subseteq \Rn$ is nonempty, compact, and convex and that
$f:K\rightarrow K$ is a continuous function. Then there exists a fixed
point such that $x^*=f(x^*)$.
\end{thm}
\begin{rmk}
There are three key differences from Theorem~\ref{thm:banach}---the
Contraction Mapping or Banach Fixed Point Theorem.
\begin{enumerate}
  \item $f$ here need not be a contraction map like $T$ was in the CMT.
    It only needs to be a continuous function (which is weaker).
  \item We provide no way to construct a sequence that converges to the
    fixed point.
  \item The fixed point need not be unique. The theorem just furnishes
    existence.
\end{enumerate}
\end{rmk}

\begin{thm}\emph{(Kakutani's Fixed Point Theorem)}
Suppose that $K\subseteq \Rn$ is nonempty, compact, and convex and that
$f:K\rightrightarrows K$ is an upper hemicontinuous correspondence
such that $f(x)\subseteq K$ is nonempty and convex $\forall x\in K$.
Then there exists a fixed point such that $x^*\in f(x^*)$.
\end{thm}
\begin{rmk}
Kakutani is a direct generalization of Brouwer, which can be viewed as a
corollary of Kakutani. Specifically, assume $f$ and $K$ satisfy all of
the Kakutani condition and, additionally, that $f$ is a
\emph{single-valued} correspondence.  Well, a single-valued uppper
hemicontinuous function is just plain continuous (like $f$ in Brouwer),
and the fixed point that exists by Kakutani satisfies $x^*\in
\{f(x^*)\}$ by the single-valuedness of $f$, i.e. we have $x^*=f(x^*)$.
\end{rmk}



\clearpage
\section{Optimization in $\Rn$}

\subsection{The Canonical Optimization Problem}

\begin{defn}(Canonical Optimization Problem in Primal Form)
Throughout this section, we consider optimization problems for
scalar-valued functions on domain $\Rn$. The problems we consider can be
written most generally as
\begin{alignat}{3}
  \text{Objective Function:} \qquad
    && \max_{x\in\mathcal{D}} \; &f_0(x) \label{prob:max}\\
  \text{Inequality Constraints:} \qquad
    && \text{s.t.} \; &f_i(x) \geq 0 \qquad i = 1,\ldots, m\notag \\
  \text{Equality Constraints:} \qquad
    && &g_i(x) = 0 \qquad i = 1,\ldots,k\notag
\end{alignat}
Domain of the problem $\mathcal{D}$ is the intersection of the domains
of all constituent functions
\begin{align*}
  \mathcal{D} := \dom(f_0)
  \cap \left( \bigcap^m_{i=1} \dom(f_i)\right)
  \cap \left( \bigcap^k_{i=1} \dom(g_i)\right)
\end{align*}
$\dom(f)$ refers to the domain of function $f$. Generally, I
assume the domains of $f_0$, $f_i$, and $g_i$ are open for all $i$,
which implies that $\mathcal{D}$---a finite intersection of open
sets---open as well.

\paragraph{Terminology and Notation}
\begin{enumerate}
  \item \emph{Feasible Point}, $x\in\mathscr{F}_p$: Any point
    $x\in\mathcal{D}$ satisfying all inequality \& equality
    constraints.
  \item \emph{Feasible Set}, $\mathscr{F}_p\subseteq\calD$:
    The set of all feasible points in the domain.
  \item \emph{Optimal Value}, $p^*=\max_{x\in \mathscr{F}_p} f_0(x)$:
    Maximal value of problem over the feasible set.
  \item \emph{Optimal Point}, $x^*$: Any feasible point
    $x^*\in\mathscr{F}_p$ such that $f_0(x^*)=p^*$.
\end{enumerate}
I will collectively refer to the entire Problem~\ref{prob:max} as the
``Canonical Optimization Problem'' in its ``Primal Form'' (in contrast
to its ``Dual Form'' which will be introduced later). Solving
Problem~\ref{prob:max} will be the goal of this entire section. I will
proceed by starting with some general results about extrema, which
correspond to solving Problem~\ref{prob:max} without any
constraints---simple unconstrained optimization over a function's entire
domain, given particular assumptions about $f$. I will then introduce
the notion of the ``Dual Form'', which simply turns
Problem~\ref{prob:max} into a different (often simpler and equivalent)
optimization problem. I will then offer some intuition, examples, and
applications under equality and inequality constraints.
\end{defn}

\begin{rmk}
It's important to note that I am currently assuming nothing about
the underlying functions that define the problem---not continuity,
differentiability, concavity, convexity, etc. Of course, these assumptions will factor in later, since we often use continuity to
ensure existence of a maximum or concavity/convexity to ensure
uniqueness. \emph{However}, we will also derive many useful results
where we impose no assumptions on the problem. So any assumptions will
generally be made very explicit, and I will present throughout this
section a mix assumption-free and assumption-heavy results.
\end{rmk}

\subsection{Canonical Concave Programming Problem}


\begin{defn}(Canonical Concave Programming Problem)
We can place additional assumptions on the objective function and
constraints to arrive at a particularly well-behaved special case of
Problem~\ref{prob:max} called the
\emph{Canonical Concave Programming Problem}:
\end{defn}
\begin{alignat}{3}
  \text{Concave Objective Function:} \qquad
    && \max_{x\in\mathcal{D}} \; &f_0(x) \label{prob:maxconcave}\\
  \text{Concave Inequality Constraints:} \qquad
    && \text{s.t.} \; &f_i(x) \geq 0 \qquad i = 1,\ldots, m\notag \\
  \text{Affine Equality Constraints:} \qquad
    && &a_i^T x = b_i \qquad i = 1,\ldots,k\notag
\end{alignat}

\subsection{Extrema: Definitions and Facts}
\label{sec:extrema}

In this section, I cover some basic results about extrema of functions.
Note that I do not necessarily assume functions are differentiable.
Unless differentiability is explicitly stated, a result holds generally.

\begin{defn}{(Local Maximum)}
For function $f:S\subseteq\Rn\rightarrow \R$, we say that $x^* \in
S$ is a \emph{local maximum} if there exists an $\varepsilon>0$ such
that
\begin{align*}
  f(x^*)\geq f(y)
  \qquad \forall y \in B_\varepsilon(x) \cap S
\end{align*}
\end{defn}

\begin{prop}
\emph{(Extremum + Differentiable $\implies$ Derivative $=0$)}
Given function $f:S\subseteq \Rn\rightarrow \R$ on open set $S$,
\begin{align*}
  \text{$x^*$ local max and }\;\;
  \frac{\partial f(x^*)}{\partial x_i}
  \;\;\text{exists}
  \quad\implies\quad
  \frac{\partial f(x^*)}{\partial x_i} =0
\end{align*}
\end{prop}

\begin{note}
First, the converse does not generally hold, i.e. a zero derivative is
not generally sufficient for a point to be local max/min. Saddle points
have partial derivatives equal to zero, but  they are not extrema.
Only if we impose concavity/convexity conditions on $f$ will this
necessary condition be sufficient.

Second, I'm not saying the partial derivative \emph{will} exist or that
the function is differentiable. I'm instead saying that
\emph{if the partial derivative happens to exist}, then it must equal
zero.
The next definition considers functions that are, in
fact, differentiable everywhere in the domain.
\end{note}

\begin{defn}{(Critical Point)}
Suppose that $f:S\rightarrow\R$ is a differentiable function on open set
$S\in\Rn$. The point $x^*\in\Rn$ is called a \emph{critical
point} of $f$ if
\begin{align*}
  \nabla f(x^*) = 0
  \quad\text{or equivalently}\quad
  \frac{\partial f(x^*)}{\partial x_i} =0
  \quad\forall i=1,\ldots,n
\end{align*}
Take note, critical points as defined above are a concept dependent upon
differentiability.
\end{defn}

\subsection{Extrema and Concave Functions}
\label{sec:extrema-concave}

\begin{prop}{\emph{(Extrema of Differentiable Concave Functions)}}
Suppose that $f:C\rightarrow\R$ is a differentiable and concave function
on convex and open domain $C\subseteq\Rn$. Then
\begin{align*}
  \text{$x^*$ critical point, $\nabla f(x^*)=0$}
  \quad\implies\quad
  x^* \in \arg\max_{x\in C} f(x)
\end{align*}
For $f$ \emph{strictly} concave, $x^*$ is \emph{unique}.
\end{prop}
\begin{proof}
By Theorem~\ref{thm:diffconvex} (but flipped since $f$ is now concave),
we know that
\begin{align*}
  f(y) \leq f(x) + [\nabla f(x)]^T (y-x)
  \qquad \forall x,y\in C
\end{align*}
Use the critical point $x^*$ (which satisfies $\nabla f(x^*)=0$) as $x$
in the inequality above to get, for all $y\in C$,
\begin{align*}
  f(y) &\leq f(x^*) + [\nabla f(x^*)]^T (y-x^*)
  = f(x^*) + 0
  = f(x^*)
  \qquad\forall y\in C
\end{align*}
Hence $x^*$ satisfies the definition of a maximum.
\end{proof}

\begin{prop}{\emph{(Extrema of Arbitrary Concave Functions)}}
Given concave $f:C\rightarrow \R$ defined on convex domain $C$, define
$X^*\subseteq C$ as the set of ``maximizing points'' in $C$.
\begin{align*}
  X^* = \arg\max_{x\in C} f(x)
  = \{ x \in C \; | \; f(y) \leq f(x) \quad \forall y \in C\}
\end{align*}
Then we have that
\begin{enumerate}
  \item $X^*$ is convex
  \item $f$ strictly concave and $X^*\neq \emptyset$ implies $X^*$
    consists of a single, \emph{unique} maximizing point
\end{enumerate}
\end{prop}
\begin{proof}
%($X^*$ Convex) Suppose $X^*=\emptyset$. The empty set is convex so done.
%Now suppose that $X^*$ is nonempty. Choose any $x,y\in X^*$.  It must be
%the case that $f(x) = f(y)$, otherwise the smaller of the two would not
%be a maximizer of the function and, therefore, not in $X^*$. So define
%$f^* = f(x) = f(y)$.

%Now to draw a contradiction, suppose that $X^*$ is not convex. That
%means there exists a $\lambda$ such that
%\begin{align*}
  %z_\lambda = \lambda x + (1-\lambda)y \not\in X^*
%\end{align*}
%Since $z_\lambda \not\in X^*$, it is not a maximizer. Therefore
%\begin{align*}
  %f(z_\lambda) &< f^* = \lambda f^* + (1-\lambda)f^* \\
  %\Rightarrow\quad
  %f(\lambda x + (1-\lambda)y) &< \lambda f(y) + (1-\lambda)f(y) \\
%\end{align*}
%But that's a contradiction since we assumed $f$ is concave.

(Part 1, $X^*$ Convex) Suppose $X^*=\emptyset$. The empty set is convex
so done.  Now suppose that $X^*$ is nonempty. Choose any $x,y\in X^*$.
It must be the case that $f(x) = f(y)$, otherwise the smaller of the two
would not be a maximizer of the function and, therefore, not in $X^*$.
So define $f^* = f(x) = f(y)$.  Then $X^* = C^+_f(f^*)$, which is a
convex set whenever $f$ is concave by
Proposition~\ref{prop:convexfcnset}.

(Part 2, Uniqueness for Strictly Concave $f$)
Suppose $X^*$ consists of more than one distinct point. Then take any
two $x,y\in X^*$. As before $f(x)=f(y)=f^*$ for some $f^*$, othwerise
the smaller of the two would not be a maximizer.

Now take any $\lambda\in(0,1)$ and construct
\begin{align*}
  z_\lambda = \lambda x + (1-\lambda)y
\end{align*}
Since $X^*$ is convex by the first part of the proof, $z_\lambda\in
X^*$. Moreover, since $f$ is strictly concave, by the definition of
strict concavity,
\begin{align*}
  f(\lambda x + (1-\lambda) y)
  &>
  \lambda f(x) + (1-\lambda) f(y) \\
  \Leftrightarrow \quad
  f(z_\lambda) &> \lambda f^* + (1-\lambda) f^* = f^* \\
  \Rightarrow\quad
  f(z_\lambda) &> f^*
\end{align*}
In summary, $z_\lambda \in X^*$ and it's bigger than both $x$ and $y$.
That contradicts our definitions of $X^*$ and $f^*$.
\end{proof}

\subsection{The Lagrangian and the Lagrange Dual Function}

\begin{defn}{(Lagrangian)}
Given Problem~\ref{prob:max}, define the \emph{Lagrangian}
\begin{align*}
    \sL(x,\lambda,\mu)
    := f_0(x)
    + \sum^m_{i=1} \lambda_i f_i(x)
    + \sum^k_{i=1} \mu_i g_i(x)
\end{align*}
where vectors $\lambda$ and $\mu$ are called the \emph{Lagrange
multipliers}. Overall, $\sL$ is a function of $n+m+k$ variables since
$x\in\Rn$, $\lambda\in\mathbb{R}^m$, and $\mu\in\mathbb{R}^k$
\end{defn}

As explained in Boyd,\footnote{This is reproduced essentially verbatim
because I liked their explanation so much. Go read their book for an
even better and more detailed explanation}.  we can interpret the
Lagrangian as a linear approximation of Problem~\ref{prob:max}.
Specifically, we want to maximize $f_0$ subject to the constraints.
Well, if we introduce functions that impose an infinite penalty to
violating the constraints, we can write the maximization problem in one
line:
\begin{align}
  \label{prob:linapprox}
  \max_{x\in\mathcal{D}} \;
  f_0(x) + \sum^m_{i=1} I_+(f_i(x))
  + \sum^k_{i=1} I_0(g_i(x))
\end{align}
where
\begin{align*}
  I_+(y) =
  \begin{cases}
    -\infty & y < 0 \\
    0 & y \geq 0
  \end{cases}
  \qquad
  I_0(y) =
  \begin{cases}
    -\infty & y \neq 0 \\
    0 & y = 0
  \end{cases}
\end{align*}
So if you violate the constraints, you get an infinite penalty.  In
trying to maximize objection function of Problem~\ref{prob:linapprox},
we see that the optimization will result in an $x\in\mathcal{D}$ that
avoids the infinite penalties, and so satisfies the constraints.

The Lagrangian is quite similar to the objective function in
Problem~\ref{prob:linapprox}. But instead of using an infinite penalty,
it introduces a \emph{linear} penalty for some vectors $\lambda$ and
$\mu$.

\begin{defn}{(Lagrange Dual Function)}
Given canonical Problem~\ref{prob:max} and corresponding Lagrangian
$\sL(x,\lambda,\mu)$, define the \emph{Lagrange dual function}, or
simply the \emph{dual}, as
\begin{align*}
  g(\lambda,\mu) = \sup_{x\in\mathcal{D}} \; \sL(x,\lambda,\mu)
  = \sup_{x\in\mathcal{D}}
    \left\{ f_0(x)
    + \sum^m_{i=1} \lambda_i f_i(x)
    + \sum^k_{i=1} \mu_i g_i(x)
    \right\}
\end{align*}
If the sup does not exist, simply define $g(\lambda,\mu)=+\infty$.
Here, $\lambda$ and $\mu$ are parameters.
\end{defn}

The Lagrange dual function is hugely important because it is
well-behaved and extrema of the dual provide a lot of information about
the problem we really want to solve, Problem~\ref{prob:max}.  First,
while we generally have no guarantees \emph{a priori} about the shape of
the objective function or the Lagrangian (unless we impose restrictive
assumptions on $f_0$, $f_i$, and $g_i$), we will see that the Lagrange
dual function is \emph{always} convex. So we can find extrema of the
dual by applying many of the nice results that we saw in
Section~\ref{sec:extrema-concave} for extrema of convex and concave
functions without assuming anything. Second, we can use the dual to
bound from above the optimal vlaue $p^*$ of Problem~\ref{prob:max}.
Now let's turn all of these wildly ludicrous and unbelievable claims
into formal statements.

\begin{thm}{\emph{(Dual as Bound)}}
\label{thm:dual-bound}
Given Problem~\ref{prob:max} with optimal value $p^*$,
assuming that $\lambda\geq 0$, we have
\begin{align}
  f_0(x) \leq g(\lambda,\mu)
  \quad\forall x\in \sF_p
  \quad\implies\quad
  p^* \leq g(\lambda,\mu)
  \label{ineq:dual-bound}
\end{align}
So even if we don't know the maximal value $p^*$, we can bound it.
\end{thm}

\begin{proof}
Take any $x\in \mathscr{F}_p$. By the definition of feasibility it
satisfies the equality constraints
\begin{align*}
  g_i(x)=0 \;\; \forall i
  \quad\implies\quad
  \sum_{i=1}^k \mu_i g_i(x) = 0
\end{align*}
Next, since $x\in\sF_p$ must also satisfy the inequality constrants and
since $\lambda\geq 0$
\begin{align*}
  f_i(x) \geq 0 \; \forall i
  \quad\implies\quad
  \sum_{i=1}^m \lambda_i f_i(x) \geq 0
\end{align*}
So in adding these two things to $f_0(x)$, we can only increase the
value of the sum:
\begin{align}
  f_0(x) \leq f_0(x)
  + \underbrace{\sum_{i=1}^m \lambda_if_i(x)}_{\geq 0}
  + \underbrace{\sum_{i=1}^k \mu_ig_i(x)}_{=0}
  \label{ineq:dual-bound-almost}
\end{align}
Finally, take the sup of the object on the righthand side of the above
inequality over all $x\in\sF_p$---which can only increase its
value---and you end up with $g(\lambda,\mu)$, proving the first
inequality in Line~\ref{ineq:dual-bound}, $f_0\leq g(\lambda,\mu)$.
Then take sup on the LHS of this inequality to get
$p^*:=\sup_{x\in\sF_p}f_0\leq g(\lambda,\mu)$, the second inequality in
Line~\ref{ineq:dual-bound},
\end{proof}

\begin{thm}{\emph{(Dual is Convex)}}
\label{thm:dual-convex}
Given Canonical Optimization Problem~\ref{prob:max}, the Lagrange dual
function $g(\lambda,\mu)$ is \emph{convex}, even without any assumtions
on functions $f_0$, $f_i$, or $g_i$.
\end{thm}
\begin{proof}
Given $x\in\mathcal{D}$, define
\begin{align*}
  \varphi_x(\lambda,\mu) = f_0(x)
    + \sum^m_{i=1} \lambda_i f_i(x)
    + \sum^k_{i=1} \mu_i g_i(x)
\end{align*}
This function is an affine function of $\lambda$ and $\mu$. Therefore,
it is both concave and convex. So we are correct in saying that
$\{\varphi_x\}_{x\in\mathcal{D}}$ is a family of concave functions, so
\begin{align*}
  g(\lambda,\mu) = \sup_{x\in\mathcal{D}} \varphi_x(\lambda,\mu)
\end{align*}
will be convex by Proposition~\ref{prop:supconvex} since
$g(\lambda,\mu)$ is the the supremum of a family of convex functions.

\end{proof}

\subsection{The Dual Form of the Canonical Optimization Problem}

Above, we stated Canonical Optimization Problem~\ref{prob:max} in
\emph{primal} form.
However, you can always write down and solve the problem in its
\emph{dual} form which, not surprisingly, involves working with the
problem's Lagrange dual function. Often, this approach is much easier to
solve, so I now lay out that problem explicitly, just like
Problem~\ref{prob:max}.

\begin{defn}(Dual Form)
Given the dual function $g(\lambda,\mu)$ of Canonical Optimization
Problem~\ref{prob:max}, the problem can be written in \emph{dual form}
as
\begin{align}
  \min_{\lambda,\mu} \; &g(\lambda,\mu)
  \qquad \lambda\in\R^m,\;\;\mu\in\R^k
  \label{prob:dual}
  \\
  \text{s.t.} \; &\lambda \geq 0\notag
\end{align}
Since $g(\lambda,\mu)$ convex by Theorem~\ref{thm:dual-convex}, this
is (conveniently) a convex optimization problem.

\paragraph{Terminology and Notation}
\begin{enumerate}
  \item \emph{Dual Feasible}, $(\lambda,\mu)\in\mathscr{F}_d$: Pair
    $(\lambda,\mu)$ is ``dual feasible'' if $\lambda\geq 0$.
  \item \emph{Dual Value}, $d^*$: The optimum of
    Problem~\ref{prob:dual}, $d^* = \min_{(\lambda,\mu)\in\mathscr{F}_d}
    g(\lambda,\mu)$
  \item \emph{Weak Duality}, $p^*\leq d^*$:
    Follows immediately from Theorem~\ref{thm:dual-bound}
  \item \emph{Strong Duality}, $p^*=d^*$: This does not hold in general,
    but we care about precisely those cases where it does because of the
    next theorem.
  \item \emph{Duality Gap}, $p^*-d^*\in\R_+$:
    The difference in primal and dual optimal values.
    No duality gap if and only if strong duality.
\end{enumerate}
\end{defn}

\begin{thm}{\emph{(Bounding Suboptimality)}}
Given \emph{any} primal and dual feasible triplet
$(x,\lambda,\mu)$---i.e. $x\in\mathscr{F}_p$ and
$(\lambda,\mu)\in\mathscr{F}_d$---we can bound the triplet's
suboptimality as follows:
\begin{align*}
  0 \leq \underbrace{p^* - f_0(x)}_{\text{Suboptimality}}
  \leq d^* - f_0(x)
  \leq g(\lambda,\mu) - f_0(x)
\end{align*}
Hence $p^*\in [f_0(x), d^*]$, and $d^*\in[f_0(x),g(\lambda,\mu)]$.
\end{thm}
\begin{rmk}
This theorem is incredible! Without knowing $p^*$---the optimal value of
the primal problem that you really want to find---you can bound how far
away you are. For any (repeat: \emph{any}) problem in Canonical
Representation~\ref{prob:max} (with zero assumptions), if I know the
dual $g(\lambda,\mu)$ and objective function $f_0(x)$, I can tell how
far away $f_0(x)$ is from the optimal value $p^*$ that I am trying to
find. And in general, we \emph{always} know $g(\lambda,\mu)$ and
$f_0(x)$. Moreover, we can establish the tightest bound by solving the
convex Dual Problem~\ref{prob:dual} for $d^*$.

Moreover, this provides a nice stopping-criterion when solving
Problem~\ref{prob:max} with a computer. Specifically, if we want to find
an $x^*$ such that $f_0(x^*)$ is
no more than $\varepsilon$ units away from  optimum $p^*$, try different
values of $x$ (hopefully in some algorithmic, intelligent, sequential
way). At each step, compute $f_0(x)$ and $d^*$ (or $g(\lambda,\mu)$) at
each iteration, check the bound $p^*-f_0(x)$, and stop if this is
guaranteed to be less than $\varepsilon$.
\end{rmk}

\begin{defn}{(Slater's Condition)}
Given Canonical Problem~\ref{prob:max}, we say that the problem
satisfies \emph{Slater's condition} if there exists at least one
$\hat{x}\in\mathscr{F}_p$ such that
\begin{align*}
  f_i(\hat{x})>0
  \quad \forall i=1,\ldots,m
\end{align*}
In words, there exists at least one point interior to the feasible set
of the problem (i.e.\ not on the boundary for any inequality
constraint).
\end{defn}

\begin{thm}{\emph{(Slater's Theorem)}}
If you have Concave Programming Problem~\ref{prob:maxconcave} and it
satisfies Slater's Condition, then you have strong duality, i.e.\
$p^*=d^*$.
\end{thm}

\begin{rmk}
In words, the existence of at least one point interior to the primal
feasible set (not on the boundary for any primal constraint) guarantees
strong duality.
\end{rmk}


\clearpage
\subsection{KKT Conditions}

Suppose you have a candidate point $x^*$ in the domain of
Problem~\ref{prob:max}.  How can you easily check if it is optimal, or
rule it out as suboptimal? In other words, what are some necessary and
and/or sufficient conditions $x^*$ must satisfy if it really is
an optimum?  In some cases, the answer is the Karush-Kuhn-Tucker (KKT)
Conditions.

The KKT conditions are a set of sometimes-necessary, sometimes-sufficent
conditions that are useful for characterizing solutions to nonlinear
optimization problems in the form of Problem~\ref{prob:max}. When
exactly they are necessary and when exactly they are sufficient is
complicated, so special care will be taken to note this explicitly.
But first, we state the KKT conditions (along with some intuition),
before deriving them explicitly.

%, but there are also certain ``Constraint Qualifications''
%(CQ) or regularity conditions that the point must meet for the KKT
%Conditions to be necessary.  More compactly
%\begin{align*}
  %\text{$x^*$ satisfies CQ}
  %\implies
  %\text{KKT conditions necessary for $x^*$ to be an optimum}
%\end{align*}
%``Constraint Qualifications'' (CQ) are restrictions that rule out weird
%irregular cases where $x^*$ is an optimal point at the boundary of the
%constraint set but $x^*$ \emph{does not} satisfy the KKT conditions.
%Again, if CQ are satisfied, then the Karush-Kuhn-Tucker (KKT) Conditions
%are a set of \emph{first order} \emph{necessary} conditions that must
%hold for $x^*$ to be an optimal point of Problem~\ref{prob:max}.

%This suggests a way to test if $x^*$ is an optimal point of
%Problem~\ref{prob:max}. Specifically, $x^*$ is an optimum if either
%\begin{enumerate}
  %\item Point $x^*$ satisfies some constraint qualifications and the
    %consequently necessary KKT conditions.
  %\item Point $x^*$ does not satisfy some constraint qualifications, so
    %$x^*$ could still be an optimum though the KKT conditions might not
    %be satisfied.
%\end{enumerate}
%Therefore, to find optima, examine points where both the KKT conditions
%\emph{hold} and points where CQ fails.

%Note, that I've said nothing about sufficiency. Constraint
%qualifications and the KKT conditions only concern necessary conditions.
%We'll have to say more to guarantee sufficiency (and I will below).

\begin{defn}{(KKT Conditions)}
Given optimal canonical optimization Problem~\ref{prob:max},
$(x^*,\lambda^*,\mu^*)$ satisfies the KKT conditions if all of the
following hold:
\begin{enumerate}
  \item \emph{$x^*$ Stationary point of Lagrangian}:
    \begin{align*}
      0 &= \nabla_x \sL(x^*,\mu^*,\lambda^*)
        = \nabla_x f_0(x^*)
        + \sum^m_{i=1} \lambda_i^* \; \nabla_x f_i(x^*)
        + \sum^k_{i=1} \mu_i^* \; \nabla_xg_i(x^*)
    \end{align*}
  \item \emph{Primal feasibility}, $x^*\in \mathscr{F}_p$:
    Point $x^*$ satisfies all constraints in the problem's primal
    Representation~\ref{prob:max}
    \begin{align*}
      x^*\in \mathscr{F}_p
      \iff
      \begin{cases}
        f_i(x^*)\geq 0 & i = 1,\ldots,m\\
        g_i(x^*)= 0 & i = 1,\ldots,k
      \end{cases}
    \end{align*}

  \item \emph{Dual Feasibility}, $(\lambda^*,\mu^*)\in\mathscr{F}_d$:
    Pair $(\lambda^*,\mu^*)$ satisfies all constraints in the problem's
    dual Representation~\ref{prob:dual}
    \begin{align*}
      \lambda^*_i \geq 0
      \qquad i=1,\ldots,m
    \end{align*}

  \item \emph{Complementary Slackness}:
    For inequality constraint $i$, either the inequality binds so
    $f_i(x^*)=0$, or does not and multiplier $\lambda^*_i=0$.
    That can be encapsulated in one line as
    \begin{align}
      \label{kkt:slackness}
      \lambda_i^* f_i(x^*) =0
      \qquad i=1,\ldots,m
    \end{align}
     %Recall the interpretation of the multipliers: $\lambda_i^*$ is how
     %much the objective function would increase if constraint $i$ were
     %relaxed.  Well, if the constraint is non-binding, $f_i(x^*)>0$ and
     %the objective function wouldn't change after relaxing that
     %constraint.  So $\lambda_i^*$ should be zero. And if inequality
     %constraint $i$ binds, then $f_i(x^*)=0$ so $\lambda^*_i$ can be any
     %finite number So Equation~\ref{kkt:slackness} provides a nice
     %one-line encapsulation of these two possibilities.
\end{enumerate}
Now we move on to some theorems and results that tell us about when the
conditions are necessary, sufficient, or both.
\end{defn}

\begin{thm}{\emph{(Primal Solution $+$ Strong Duality $\implies$ Dual
  Soln, KKT Necessary)}}
Suppose strong duality holds.  If $x^*\in\mathscr{F}_p$ solves
Problem~\ref{prob:max}, then there exist
$(\lambda^*,\mu^*)\in\mathscr{F}_d$ solving the Dual
Problem~\ref{prob:dual} such that $(x^*,\lambda^*,\mu^*)$ satisfy the
KKT conditions.
\end{thm}
\begin{proof}
Since $x^*\in\mathscr{F}_p$ solves the problem, it trivially satisfies
KKT Condition 2, Primal Feasibility.

Next, since $x^*$ solves the primal problem, $f_0(x^*)=p^*$. By strong
duality, $d^*=p^*$ exists so the dual problem has a solution. Since
$d^*$ exists and the dual problem has a solution, we can choose
\begin{align}
  \label{eq:pick-lambda-mu}
  (\lambda^*,\mu^*) \in \mathscr{F}_d
  \quad \text{such that}
  \quad g(\lambda^*,\mu^*) = d^*
\end{align}
Since $(\lambda^*,\mu^*)\in\mathscr{F}_d$, we also know $\lambda^*\geq
0$, satisfying KKT Condition 3, Dual Feasibility.

Next, we can chain together
%since $\lambda^*\geq 0$ and $x^*\in\mathscr{F}_p$ satisifies all
%of the constraints,
\begin{align*}
  \text{Since $x^*$ solves primal} \qquad p^* &= f_0(x^*) \\
  \text{Since $x^*\in\mathscr{F}_p$ and $\lambda^*\geq 0$} \qquad
  &\leq f(x^*) + \sum^m_{i=1} \lambda^* f_i(x^*)
  + \sum^k_{i=1} \mu_i^* g_i(x^*) \\
  \text{By definition of sup} \qquad
  &\leq \sup_{x\in D} f(x) + \sum^m_{i=1} \lambda^* f_i(x)
  + \sum^k_{i=1} \mu_i^* g_i(x) \\
  \text{By definition of $g(\lambda,\mu)$} \qquad
  &= g(\lambda^*,\mu^*) \\
  \text{By Equation~\ref{eq:pick-lambda-mu}'s
    choice of $(\lambda^*,\mu^*)$} \qquad &= d^* \\
  \text{By strong duality}\qquad
  &= p^*\\
  \text{Since $x^*$ solves primal}\qquad
  &= f_0(x^*)
\end{align*}
Buried in that chain is the squeezing
\begin{align*}
  p^*
  &\leq f_0(x^*) + \sum^m_{i=1} \lambda^* f_i(x^*)
  + \sum^k_{i=1} \mu_i^* g_i(x^*)
  \leq p^*
\end{align*}
Since $f(x^*)=p^*$ and the equality constraints are defined to equal
zero for feasible $x^*$ (i.e.\ $\sum^k_{i=1}\mu^*_ig_i(x^*)=0$), it must
also be the case that
\begin{align*}
  \sum^m_{i=1} \lambda^* f_i(x^*)
  =0
\end{align*}
Hence, we satisfy KKT Condition 4, Complementary Slackness.

Lastly, also buried in that chain is
\begin{align}
  f_0(x^*)
  = f_0(x^*) + \sum^m_{i=1} \lambda^* f_i(x^*)
  + \sum^k_{i=1} \mu_i^* g_i(x^*)
  = \sup_{x\in D} f_0(x) + \sum^m_{i=1} \lambda^* f_i(x)
  + \sum^k_{i=1} \mu_i^* g_i(x)
  \label{kkt:criticalproof}
\end{align}
Now think of the guy on the very right as a function of $x$ only, where
$(\lambda^*,\mu^*)$ are fixed---call it
$\sL_{\lambda^*,\mu^*}(x):=\sL(x,\lambda^*,\mu^*)$.
Equation~\ref{kkt:criticalproof} effectively says that
$x^*\in\mathcal{D}$ maximizes $\sL_{\lambda^*,\mu^*}(x)$, so the sup is
really a max. Since the domain $\mathcal{D}$ is an open set and since
the point maximizes the function, it must be the case that
\begin{align*}
  0 &= \nabla_x \sL_{\lambda^*,\mu^*}(x^*)
  =:
  \nabla_x \sL(x^*,\lambda^*,\mu^*)
\end{align*}
And that's precisely KKT Condition 1, implying $x^*$ is a stationary
point of the Lagrangian.
\end{proof}

\clearpage
\subsection{Lagrange Multiplier Intuition}

\subsection{No Local Improvement Logic and Pictures}

Consider the following simple problem (differentiable $f$, single
equality constraint):
\begin{align}
  \max_{x\in\R^k} &\;f(x)
  \quad
  \text{s.t. } g(x) = 0
  \label{prob:eqconst}
\end{align}
We now build intuition for the following necessary condition of a
constrained local extremum:
\begin{align}
  \nabla f(x^*) = \lambda \nabla g(x^*)
  \qquad \lambda \in \R
  \label{lagrange-intuition}
\end{align}
The key to understanding Expr.~\ref{lagrange-intuition} is
to think in terms of contour lines. See figure.\footnote{%
  We ignore the case where some $x^*$ satisfies $g(x^*)=0$ and
  $f'(x^*)=0$ (i.e. the usual necessary condition in
  \emph{unconstrained} maximization problems) which would implies
  $\lambda=0$.
}
%\begin{tikzpicture}
  %\begin{axis}[view={0}{90}]
    %\addplot3[contour gnuplot] {2*x};
  %\end{axis}
%\end{tikzpicture}

In searching for local constrained extrema, we walk along
contour/constraint line $g(x)=0$ that connects all feasible points.  For
feasible $x^*$ to be a constrained local extremum, we need
\begin{enumerate}[label=(\roman*)]
  \item \emph{Feasibility}: Constraint satisfied, $g(x^*)=0$
  \item \emph{No Local Improvements}:
    You must be \emph{totally unable} to improve the value of the
    objective function by moving to a nearby point that still respects
    the constraint.

    Stated more practically: $g$-contour through $g(x^*)=0$ is
    \emph{tangent} to the $f$-contour through $f(x^*)$.
    Hence a sufficiently tiny step along the tangent line at
    $g(x^*)$---small enough so that you still remain on the constraint
    contour/set---\emph{also} keeps you on the $f$-contour
    through $f(x^*)$, thus yielding \emph{no improvement} in the
    objective function. And that's precisely what you would need/expect
    for $x^*$ to be a local constrained extremum:
    You can't do better than $x^*$ by taking tiny steps along the
    constraint curve.
\end{enumerate}
Now, if the $g$-contour through $g(x^*)$ is tangent to the $f$-contour
through $f(x^*)$, then surely their gradients (which are always
perpendicular to contour lines) are parallel. Hence,
\begin{align}
  \nabla f(x^*) = -\lambda \nabla g(x^*)
  \label{eq:lag1}
\end{align}
where $\lambda$ is simply a scaling constant. We need to include that
because they generally have different \emph{magnitude}, even though we
argued that they point in the same direction.
The \emph{magnitude} of $\lambda$ answers the question
``How much would the optimal value of the objective function change if
the constraint were slackened a tiny bit?''
To see this, note that Expression~\ref{eq:lag1} implies, for all partial
derivatives, that
\begin{align}
  %\nabla f(x^*) = -\lambda \nabla g(x^*)
  %\;\implies\;
  \frac{\partial f(x^*)}{\partial x_i}
  = -\lambda \frac{\partial g(x^*)}{\partial x_i}
  \quad\implies\quad
  \lambda
  = -\frac{\partial f(x^*)/\partial x_i}{\partial g(x^*)/\partial x_i}
  = -\frac{\partial f(x^*)}{\partial g(x^*)}
\end{align}
where the last expression is a slight abuse of notation.
But to understand this general result intuitively, simply recall the
interpretation of the gradient: ``The direction of largest increase.''
So if we loosened the constraint $g(x)=0$ by a tiny amount,
we should move in the direction of $\nabla f(x^*)$ to get the largest
increase in the objective function, i.e. to go from old extremum $x^*$
to new extremum $\tilde{x}^*$.
But of course, you have to normalize your move/step in direction
$\nabla f(x^*)$ by the size of $\nabla g(x^*)$ to ensure that you land
on the new loosened constraint contour.
This means divide each $\frac{\partial f(x^*)}{\partial x_i}$ by
$\frac{\partial f(x^*)}{\partial x_i}$ to appropriately scale your step
size.


\subsection{Game Theory Intuition}

Again, we consider simple maximization problem
\begin{align}
  \max_{x\in X} f(x)
  \quad \text{s.t.}\;\; g(x) = c
  \label{gametheory}
\end{align}
with associated Lagrangian
\begin{align}
  \sL(x,\lambda)
  = f(x) - \lambda \big(g(x)-c\big)
  \label{gametheorylagrange}
\end{align}
To solve, we specify two separate-but-related zero-sum games, solve for
the optimal strategies of each, and verify that the optimal strategies
are in fact solutions to Problem~\ref{gametheory}.  Both games involve
two Players, $X$ and $\Lambda$, who sequentially choose elements of pair
$(x,\lambda)$

\begin{defn}(Payoffs for Both Games)
Given pair $(x,\lambda)$, payoffs are
\begin{align*}
  \text{(Payoff to $X$)}
  = f(x) - \lambda(g(x)-c)
  \qquad
  \text{(Payoff to $\Lambda$)}
  = -\big[f(x) - \lambda(g(x)-c)\big]
\end{align*}
This is a zero-sum game. One player gets a good payoff iff the other
gets a bad payoff. This and game play are all common knowledge.
\end{defn}

\begin{defn}(Game 1)
Pair $(x,\lambda)$ is chosen via sequential play as follows
\begin{enumerate}
  \item Player $X$ chooses $x$, not knowing $\lambda$.
  \item Player $\Lambda$ choose $\lambda$ \emph{after} observing chosen
    $x$.
\end{enumerate}
\end{defn}

\begin{defn}(Game 2)
Pair $(x,\lambda)$ is chosen via sequential play as follows
\begin{enumerate}
  \item Player $\Lambda$ chooses chooses $\lambda$, not knowing $x$
  \item Player $X$ chooses $x$ \emph{after} observing chosen $\lambda$.
\end{enumerate}
\end{defn}

\begin{prop}
\label{prop:game1}
\emph{(Game 1 Solution)}
In Game 1, we have
\begin{enumerate}[label=\emph{(\roman*)}]
  \item Player $\Lambda$'s optimal strategy is $x$-contingent policy
    \begin{align*}
      \lambda^*(x) = \argmin_\lambda f(x)-\lambda\big(g(x)-c\big)
    \end{align*}
  \item
    Player $X$'s optimal strategy $x^*$ accounts for Player $\Lambda$'s
    strategy, hence satisfies
    \begin{align}
      x^*
      = \argmax_x f(x)-\lambda^*(x)\big(g(x)-c\big)
      = \argmax_x \left(
      \min_\lambda f(x)-\lambda\big(g(x)-c\big)
      \right)
      \label{gametheoryxopt}
    \end{align}
  \item Game play results in unique outcome $(x^*,\lambda^*(x^*))$
  \item Optimal strategy $x^*$ satisfies the constraint, $g(x^*)=c$
  \item Optimal strategy $x^*$ Solves Problem~\ref{gametheory}
\end{enumerate}
\end{prop}
\clearpage
\begin{proof}
We prove each in turn:
\begin{enumerate}[label=(\roman*)]
  \item
    Player $\Lambda$, after observing $x$, simply maximizes his payoff,
    i.e.  solves
    \begin{align*}
      \max_\lambda -\big[ f(x)-\lambda\big(g(x)-c\big)\big]
      =
      \min_\lambda  f(x)-\lambda\big(g(x)-c\big)
    \end{align*}
    which implies an $x$-contingent optimal strategy $\lambda^*(x)$.
  \item
    Since game play and payoffs common knowledge, Player $X$
    incorporates Player $\Lambda$'s optimal strategy $\lambda^*(x)$ and
    hence solves Problem~\ref{gametheoryxopt} to maximize his payoff.
  \item Optimal strategies $x^*$ and $\lambda(x^*)$, this last point is
    clear.
  \item
    Clearly $x^*$ satisfies the constraint
    since any deviation from the constraint $g(x)\neq c$ by Player $X$
    means that Player $\Lambda$ can exploit the deviation to generate
    arbitrarily high payoff for himself by choosing large
    $\lambda^*(x)$.
    In other words,
    \begin{align*}
      g(x)\neq c
      \quad\implies\quad
      \lambda^*(x)=\infty
    \end{align*}
    Since this would imply a $-\infty$ payoff for Player $X$ (the game
    is zero-sum), Player $X$ will choose an $x^*$ that avoids this
    outcome and respects the constraint.
  \item
    Combining Player $X$'s payoff function and with Result (ii),
    we see that Player $X$ is choosing a point that delivers the highest
    possible $f(x)$, subject to satisfying constraint $g(x)=c$.  But
    that is \emph{precisely} original Problem~\ref{gametheory}.  Hence,
    optimal strategy $x^*$ in this game solves that constrained
    maximization problem.
\end{enumerate}
\end{proof}

\begin{prop}
\emph{(Game 2 Solution)}
\label{prop:game2}
Completely analogous to Game 1, we have for Game 2 (which simply
reverses the order of play) that
\begin{enumerate}[label=\emph{(\roman*)}]
  \item Player $X$'s optimal strategy is $\lambda$-contingent policy
    \begin{align*}
      x^*(\lambda) = \argmax_x f(x)-\lambda\big(g(x)-c\big)
    \end{align*}
  \item
    Player $\Lambda$'s optimal strategy $\lambda^*$ accounts for Player
    $X$'s strategy, hence satisfies
    \begin{align*}
      \lambda^*
      = \argmin_\lambda f(x^*(\lambda))-\lambda \big(g(x^*(\lambda))-c\big)
      =
      \argmin_\lambda \left(
      \max_x f(x)-\lambda\big(g(x)-c\big)
      \right)
      %\label{gametheorylambdaopt}
    \end{align*}
  \item Game play results in unique outcome
    $(x^*(\lambda^*),\lambda^*)$
  \item Optimal strategy $x^*(\lambda^*)$ equals the maximizer
    of Lagrangian~\ref{gametheorylagrange} where $\lambda=\lambda^*$.
\end{enumerate}
\end{prop}
\begin{proof}
(i), (ii), and (iii) exactly analogous to previous proposition.
Part (iv) is simply noticing the connection between Part (i) and
Lagrangian~\ref{gametheorylagrange}, i.e.
$x^*(\lambda^*)$ by definition satisfies
\begin{align*}
  x^*(\lambda^*)
  =
  \argmax_x f(x) - \lambda^*\big(g(x)-c\big)
\end{align*}
\end{proof}

\begin{thm}
\emph{(Equivalence of Game Outcomes and Validity of Lagrangian Approach)}
Under certain technical conditions, we can apply the minimax theorem to
assert that
\begin{align}
  \max_x
  \left(
    \min_\lambda
    f(x) - \lambda(g(x)-c)
  \right)
  &=
  \min_\lambda
  \left(
    \max_x
    f(x) - \lambda(g(x)-c)
  \right)
  \label{minimax}
\end{align}
This implies equivalent outomes
$(x^*,\lambda^*(x^*))=(x^*(\lambda^*),\lambda^*)$ for Games 1 and 2.

Hence, there exists a particular, correct choice of Lagrange multiplier
(namely $\lambda^*$) such that the maximizer $x^*(\lambda^*)$ of
Lagrangian~\ref{gametheorylagrange} (taking $\lambda=\lambda^*$)
\emph{also} equals the the solution $x^*$ to original maximization
Problem~\ref{gametheory}.
\end{thm}
\begin{proof}
Notice that the LHS of Expression~\ref{minimax} is the maximization
problem that defines solution $(x^*,\lambda^*(x^*))$ for Game 1 by Parts
(ii) and (iii) of Proposition~\ref{prop:game1}.
Similarly, the RHS is the maximization problem that defines solution
$(x^*(\lambda^*),\lambda^*)$ for Game 2 by Part (ii) and (iii) of
Proposition~\ref{prop:game2}.
So it must be that $(x^*,\lambda^*(x^*))$ implied by LHS
\emph{equals} $(x^*(\lambda^*),\lambda^*)$ implied by RHS:
\begin{align*}
  (x^*,\lambda^*(x^*))
  = (x^*(\lambda^*),\lambda^*)
\end{align*}
Hence
$x^*(\lambda^*)$ in Game 2---which is the maximizer of
Lagrangian~\ref{gametheorylagrange} taking $\lambda=\lambda^*$ by Part
(iv) of Proposition~\ref{prop:game2}---is \emph{equivalent} to
optimal strategy $x^*$ in Game 1---which solves original optimization
Problem~\ref{gametheory} by Part (v) of Proposition~\ref{prop:game1}
\end{proof}



%\subsection{Functional Garbage}

%Again, suppose we are solving simple maximization problem
%\begin{align*}
  %\max_{x\in X} f(x)
  %\quad \text{s.t.}\;\; g(x) = c
%\end{align*}
%Let function $s:C\ra X$ denote the solution operator which returns the
%maximizer of $f(x)$ that also satisfies feasibility $g(s(c))=c$ for all
%$c$.

%We want to know how sensitive our maximized objective function $f(s(c))$
%is to changes in parameter $c$. In other words, we want to compute
%\begin{align*}
  %\frac{d}{dc} f(s(c))
  %= f'(s(c))s'(c)
%\end{align*}
%To compute this, we apply the implicit function theorem to feasibility:
%\begin{align*}
  %g(s(c)) = c
  %\quad\implies\quad
  %g'(s(c))s'(c) = 0
%\end{align*}



\clearpage
\section{Dynamic Optimization}

In the last section, we saw how to solve \emph{static} optimization
problems (i.e. one-shot problems not involving time) with a finite
number of constraints.
In this section, we tackle \emph{dynamic} optimization problems that
require choosing allocations \emph{over time}---especially into the
infinite future---subject to certain constraints in each period.

Given a dynamic optimization problem, there are three steps
\begin{enumerate}[label=(\roman*)]
  \item \emph{Classify the Problem}:
    There are two important axes for classifying a given dynamic
    optimization problem: discrete- vs. continuous time and determistic
    vs. stochastic.
  \item \emph{Formulate the Problem}: In other words, write it down.
    Broadly, there are two formulations/representations: sequential or
    recursive.
    In deterministic problems, either works; \emph{however}, for
    stochastic problems, really only the recursive approach is feasible
    or useful.
  \item \emph{Propose Solution Method}:
    Different \emph{classifications} and \emph{formulations} of a
    dynamic optimization problem recommend different solution methods.
    Sequential formulations can be solved with regular calculus FOCs or
    calculus of variations. A \emph{recursive} formulation requires
    dynamic programming techniques.
\end{enumerate}
The table below sorts and summarizes particular solution methods based
on both \emph{classification} and \emph{formulation} of a given dynamic
optimization. We discuss each in this section.
\begin{table}[htbp!]
\centering
\begin{tabular}{|r|cc|cc|}
  \hline
  & \multicolumn{2}{c}{Discrete Time}
  & \multicolumn{2}{|c|}{Continuous Time} \\\cline{2-3}\cline{4-5}
  & Sequential & Recursive & Sequential & Recursive \\\hline\hline
  Deterministic & ``Classical'' & Bellman & Hamiltonian & HJB \\\hline
  Stochastic & &Bellman & &HJB \\\hline
\end{tabular}
\end{table}



\subsection{Discrete-Time Deterministic Dynamic Optimization}
\label{sec:discrete-dynamic}

\subsubsection{Preliminaries: Theorem of the Maximum}

\begin{thm}{\emph{(Theorem of the Maximum)}}
Given objective function $f:X\times Y\rightarrow\R$ and correspondence
$\Gamma:X\rightrightarrows Y$ that describes the feasible sets for some
initial condition or parameter, $x\in X$, define
\begin{align}
  y^*(x) &= \argmax_{y\in \Gamma(x)} f(x,y) \notag\\
  v^*(x) &:= \max_{y\in \Gamma(x)} f(x,y)
  \label{prob:thm-max}
\end{align}
If $f$ is continuous and $\Gamma$ is a continuous and compact-valued
correspondence, then
\begin{enumerate}
  \item $y^*$ is nonempty (i.e.\ there is a maximizer), compact-valued,
    and UHC
  \item $v^*$ is well defined (i.e.\ the maximum is attained) and
    continuous
\end{enumerate}
\end{thm}

\begin{cor}
If $y^*(x)$ is single valued for all $x$, then $y^*(x)$ is a continuous
function.
\end{cor}


\clearpage
\subsubsection{Canonical Sequential Formulation}

\begin{defn}{(Canonical Sequential Formulation: Control-State Rep)}
\label{defn:sequential-altrep}
The control-state \emph{Canonical Sequential Formulation} requires
choosing sequence $\{y_{t}\}_{t=0}^\infty$ that maximizes
\begin{align}
  \label{prob:altrep}
  \max_{\{y_t\}_{t=0}^\infty}
  \quad &\sumtinfz \beta^t \tilde{F}(x_t,y_t)
  \qquad \text{s.t.}\quad
  \begin{cases}
  y_t \in \tilde{\Gamma}(x_t) \;\; \forall t\\
  \text{$x_{t+1} = G(x_t,y_t)$ $\;\forall t$, $x_0$ given}
  \end{cases}
\end{align}
The problem involves
\begin{enumerate}
  \item \emph{State}, $x_t$: At each time $t$, variable
    $x_t$---called the state---is something like a ``sufficient
    statistic'' for the problem, encoding all relevant information about
    the state of the world and about feasible options for $y_t$
  \item \emph{Control}, $y_{t}$: At time $t$, taking the current state
    $x_t$ as given, the agent chooses $y_{t}$.
    Since this object is chosen and controlled by the agent, $y_{t}$ is
    called the \emph{control}.
    Choice of $y_t$ influences next period's state $x_{t+1}$ by the law
    of motion $G$.
  %\item \emph{Period Return Function} $\tilde{F}(x_t,y_t)$:
    %This is often a utility function describing the return the agent
    %gets in time $t$ given state and control choice $(x_t,y_t)$.
  \item \emph{Law of Motion} $G(x_t,y_t)$: Given the current state $x_t$
    and the choice of control $y_t$, the law of motion $G$
    deterministically spits out the state next period,
    $x_{t+1}=G(x_t,y_t)$.  Often, there will often be some tradeoff
    between more $y_t$ today and less $x_{t+1}$ tomorrow.

  \item \emph{Feasibility Constraints}, $\tilde{\Gamma}(x_t)$:
    Correspondence $\tilde{\Gamma}(x_t)$ is a set of feasible values
    for $y_{t}$.
\end{enumerate}
While Representation~\ref{prob:altrep} is often easiest to
\emph{write down}, the following (equivalent) canonical sequential
formulation is easiest to \emph{solve}.
Specifically, choice of $y_t$ and deterministic law of motion
$G(x_t,y_t)$ together pin down $x_{t+1}$ entirely.
Choice of $y_t$ \emph{implies} a choice of $x_{t+1}$.  So if we can
invert law of motion $G$ to get $y_t=h(x_t,x_{t+1})$, we can sub
$h(x_t,x_{t+1})$ for $y_t$ everywhere to instead frame the problem as
a \emph{direct} choice of next-period state $x_{t+1}$
\end{defn}

\begin{defn}{(Canonical Sequential Formulation: State-Only Rep)}
The state-only \emph{Canonical Sequential Formulation} requires choosing
a sequence $\{x_{t+1}\}_{t=0}^\infty$ that maximizes
\begin{align}
  \label{prob:sequential-short}
  \max_{\{x_{t+1}\}_{t=0}^\infty}
  \quad &\sumtinfz \beta^t F(x_t,x_{t+1})
  \qquad \text{s.t.}\quad
  \text{$x_{t+1} \in \Gamma(x_t)$ $\;\forall t$, $x_0$ given}
\end{align}
Note that the period return function changes to
$F(x_t,x_{t+1})=\tilde{F}(x_t,h(x_t,x_{t+1}))$ while we replace
feasibility correspondence $\tilde{\Gamma}(x_t)$ for $y_t$ with
$\Gamma(x_t)$ for $x_{t+1}$.
\end{defn}

\begin{rmk}
In this subsection, I stick mostly to
Representation~\ref{prob:sequential-short} for proofs since it's
equivalent and more compact.
But I wanted to start with more verbose Representation~\ref{prob:altrep}
since it's also standard and often easier to write down, plus it
generalizes most naturally when we go to the \emph{continuous time},
deterministic sequential formulation.
\end{rmk}


%\begin{defn}{(Set of Feasible Plans)}
%Given Problem~\ref{prob:sequential-short} and starting value $x_0$,
%define the set of all feasible sequences
%\begin{align*}
  %\Pi(x_0)
  %&:= \{ \{x_{t+1}\}^\infty_{t=0} \; | \;
  %x_{t+1}\in \Gamma(x_t) \; \forall t, \; x_0 \,\text{given}\}
%\end{align*}
%Therefore, a plan or sequence $\tilde{x}:=\{x_{t+1}\}_{t=0}^\infty$ is
%feasible for Problem~\ref{prob:sequential-short} if
%$\tilde{x}\in\Pi(x_0)$, implying that is satisfies all feasibility
%constraints.
%\end{defn}

\begin{defn}{(Optimal Value)}
\label{defn:vstar}
Given Problem~\ref{prob:sequential-short}, define the optimal value as
\begin{align}
  v^*(x_0) := \sup_{\{x_{t+1}\}_{t=0}^\infty}
  \; &\sumtinfz \beta^t F(x_t,x_{t+1})
  \qquad \text{s.t.}\quad
  \text{$x_{t+1} \in \Gamma(x_t)$ $\;\forall t$, $x_0$ given}
  \label{vstar}
\end{align}
Note that I changed the max to a sup since the latter always exists.
So $v^*$ is well-defined, while the max in
Problem~\ref{prob:sequential-short} does not automically exist.
\end{defn}

\clearpage
\subsubsection{``Classical'' Solution Method for Canonical Sequential
Formulation}

\begin{thm}
Given Problem~\ref{prob:sequential-short}, suppose additionally that $F$
continuously differentiable and concave on a compact set, while the
constraint set is convex.
Then the following conditions are necessary and sufficient for a
sequence $\{x_{t+1}\}\tinfz$ to be optimal if $x_{t+1}\in
\interior(\Gamma(x_t))$,
\begin{alignat*}{3}
  \text{\emph{(EE)}}:&&
  \qquad
  F_2(x_{t},x_{t+1}) + \beta F_1(x_{t+1},x_{t+2})
  &= 0
  \qquad\forall t
  \\
  \text{\emph{(TVC)}}:&&
  \lim_{T\ra\infty}
  \beta^T F_2(x_{T},x_{T+1})
  x_{T+1}
  &= 0
  \\
  \text{\emph{(IC)}}:&&
  x_0 &= \hat{x}_0
\end{alignat*}
where
\begin{enumerate}[label=\emph{(\roman*)}]
  \item \emph{Initial Condition (IC):} This pins down the initial value
    of the trajectory $\{x_t\}\tinfz$
  \item \emph{Euler Equation (EE):}
    A difference equation in state variable $x_{t+1}$ and local
    condition that says ``For path $\{x_t\}\tinfz$ truly to be optimal,
    there can't be any gain from a one-period deviation from and back to
    it.''
    On an optimal path, gain $F_2(x_t,x_{t+1})$ from a little more
    $x_{t+1}$ today is offset by discounted loss $-\beta
    F_1(x_{t+1},x_{t+2})$ from less $x_{t+1}$ tomorrow.
    Note: if $x_t$ is a vector, $F_1$ and $F_2$ are column vectors, and
    this is a system of equations.

  \item \emph{Transversality Condition (TVC):}
    This generalizes the \emph{terminal boundary condition} of a
    finite-horizon problem to infinite-horizons, placing a
    restriction on the tail of $\{x_t\}\tinfz$.
    While (EE) rules out gains from temporary/local deviations from an
    optimal trajectory, (TVC) rules out gains from deviating from an
    optimal trajectory and \emph{never returning}.

    Note that it has an alternate equivalent form, derived from subbing
    (EE) into (TVC):
      \begin{align*}
        \lim_{T\ra\infty}
        \beta^{T}
        F_1(x_{T},x_{T+1})
        x_{T}
        &= 0
      \end{align*}
\end{enumerate}
\end{thm}
\begin{rmk}
In econ, TVC mostly rules out \emph{over}accumulation of wealth.
It shouldn't be confused with a no-Ponzi condition, which rules out
\emph{under}accumulation of wealth (or \emph{over}accumulation of
\emph{debt}). Though they often look similar and are used for the same
purpose: \emph{ruling out} certain paths from being an optimal solution
to a dynamic optimization problem.
\end{rmk}
\begin{rmk}
Assumption $x_{t+1}\in\interior(\Gamma(x_t))$ for all $t$
reminds us that this is a \emph{state-only} representation.
General, control variables \emph{do} run up against budget or other
constraints, so you must use these relationships to remove
all endogenous control avariables from the problem.

For example, in the standard growth model, this assumption
means that you have \emph{written out} consumption $c_t$ from the
problem, using the budget constraint $c_t = f(k_t) - k_{t+1} +
(1-\delta) k_t$ to get the problem in terms of the only state variable
$x_t=k_t$---not leaving in endogenous control $c_t$.  From there, the
Inada conditions ensure $x_{t+1}\in \interior(\Gamma(x_t))$.
\end{rmk}
\begin{proof}
Existence of solution by Extreme Value Theorem (Weierstrass). Uniqueness
by concave $F$ and convex constraint set.
\end{proof}

\begin{defn}(Steady State)
Steady state $x^*$ is a point such that $x_0=x^*$ implies $x_t=x^*$ for
all $t\geq 1$.
Given the Euler Equation of the form (EE) above, steady state $x^*$
satisfies
\begin{align*}
  F_2(x^*,x^*) + \beta F_1(x^*,x^*)
  &= 0
\end{align*}
\end{defn}


\clearpage
\begin{ex}{(Necessity of TVC)}
In this example, we set up a problem whose Bellman Equation formulation
has a solution, while the sequential problem has no optimal value
function solution. Now onto the problem.

Consider a simple consumption/investment decision:
\begin{align*}
  \max_{\{c_t\}_0^\infty} \; &\sumtinfz \beta^t c_t
  \qquad
  %&
  \text{s.t}\quad
  c_t + a_{t+1} = (1+r)a_t
  %\\
  %&
  ,\quad
  c_t \geq 0
\end{align*}
where $a_0$ is given as initial endowment. We can express this problem
in state-only form
\begin{align*}
  \max_{\{a_{t+1}\}_0^\infty} \;
  &\sumtinfz \; \beta^t [(1+r)a_t - a_{t+1}] \\
  & (1+r)a_t - a_{t+1} \geq 0
\end{align*}
where $F(a_t,a_{t+1}) = (1+r)a_t - a_{t+1}$.
We place \emph{no restrictions} on each $a_t$---they can be arbitrarily
negative, and that will be important.
Under some assumptions $1+r = \frac{1}{\beta}$.
So now we can write the Bellman Equation formulation:
\begin{align*}
  v(a) = \max_{a'} \; \left(\frac{a}{\beta} - a'\right) + \beta \, v(a')
\end{align*}
Now we use the guess and verify method. The guess is $v(a) = a/\beta$.
\begin{align*}
  \text{Check} \qquad
  \frac{a}{\beta}
  &= \max_{a'} \;
  \left(\frac{a}{\beta} - a'\right)
  + \beta \left(\frac{a'}{\beta}\right)
  %\\
  %\frac{a}{\beta}
  %&=
  =
  \max_{a'} \;
  \frac{a}{\beta}
  =
  \frac{a}{\beta}
\end{align*}
So this solves the Bellman Equation.

Time to see why this won't work. Pick initial consumption $c_0$ to be
\emph{huge}. That is, borrow a lot so you consume a lot in the first
period, consuming $c_0 >> a_0$.
Thereafter, consume nothing: $c_t=0$ for $t>0$. This results in an
investment profile of
\begin{align*}
  a_{1} &= \frac{a_0}{\beta} - c_0 < 0 \\
  \Rightarrow \quad
  a_{t+1} &= \frac{a_t}{\beta}
  = \frac{a_0}{\beta^{t+1}} - \frac{c_0}{\beta^t}
\end{align*}
Since $a_1$ is negative and $\beta<1$, we see that $a_{t+1}$ will blow
up to negative infinity.
Now in each period, $a_{t+1}$ is a finite, very negative, but
still-feasible number since we didn't put any borrowing constraints on.
And that's exactly the problem. The agent could go on borrowing forever.

Let's now make the violation of the transversality condition more
%explicit, checking the limit in Assumption~\ref{assump:transversality}.
To simplify the math, let $a_0=0$. Recall that $v(a) = a/\beta$.
\begin{align*}
  \lim_{t\rightarrow\infty} \beta^t v(a_t)
  = \lim_{t\rightarrow\infty} \beta^t \left(\frac{a_t}{\beta}\right)
  = \lim_{t\rightarrow\infty} \beta^t
  \left(
  \frac{-c_0/\beta^{t-1}}{\beta}
  \right)
  = \lim_{t\rightarrow\infty}
  -c_0
  = -c_0 \neq 0
\end{align*}
In other words, the value function is not ``small'' in the infinite
future.

So in summary, there is no optimal value function $v^*$, though we were
able to find a function satisfying the Bellman equation. In general,
only checking the transversality condition catches this kind of problem.
\end{ex}


\clearpage
\subsubsection{Canonical Recursive Formulation}

Looking at Expression~\ref{vstar}, we might reasonably think that we can
break up the sum into a recursive formula
\begin{align*}
  v^*(x_0)
  %&=
  %\max_{\{x_{t+1}\}^\infty_{t=1}}\;
  %F(x,x_1) +
  %\sum^\infty_{t=1} \beta^t F(x_t,x_{t+1})\\
  &=
  \sup_{\{x_{t+1}\}^\infty_{t=1}}
  F(x_0,x_1) +
  \beta \sum^\infty_{t=0} \beta^{t+1} F(x_{t+1},x_{t+2})
  %\\
  %\Leftrightarrow\qquad
  =
  \sup_{x_1\in\Gamma(x_0)}
  F(x_0,x_1) +
  \beta v^*(x_1)
\end{align*}
This leads us to the following definition.

\begin{defn}{(Canonical Recursive Formulation and Value Function)}
\label{defn:bellman}
The \emph{Canonical Recursive Formulation} is given by
\emph{value function} $v_B$ and defining \emph{Bellman Equation}
\begin{align}
  \label{eq:vB}
  v_B(x') &=
  \sup_{x'\in\Gamma(x)}\;
  F(x,x') +
  \beta v_B(x')
\end{align}
where $F$ is period return function, $x$ current state,
$x'$ next-period state, $\Gamma(x)$ constraint set.
\end{defn}

\begin{note}
Equation~\ref{eq:vB} \emph{defines} $v_B$.
That's why we make the notational distinction between $v_B$ and $v^*$,
which we originally used to motivate this definition.
Functions $v_B$ and $v^*$ are \emph{possibly distinct} because
they are defined \emph{completely differently}:
Equation~\ref{vstar} defines $v^*$, while
Equation~\ref{eq:vB} defines $v_B$.
It's not obvious that $v^*=v_B$.

Now in general, the reason we care \emph{at all} about
Representation~\ref{eq:vB} is that sometimes we actually do get
$v^*=v_B$ under certain assumptions.\footnote{%
  Though there are occasions when $v_B$ exists but $v^*$ not.
  In those cases, we don't really care about $v_B$.
}
But that's something to \emph{prove}.  We don't get that for free.
We take up this task in subsequent sections.
\end{note}

Lastly, why go this route? Why is recursive
Representation~\ref{eq:vB} useful? Well, we exploited the stationarity
of original Problem~\ref{prob:sequential-short} to produce a
somewhat more tractable problem.  Given today's state $x$, rather than
choose infinite sequence $\{x_{t+1}\}_{t=0}^\infty$,
I need only choose tomorrow's state $x'$. Since value function $v_B$
tells me how much starting value $x'$ is worth, I can frame the
problem as a tradeoff between $F(x,x')$ today and the discounted value
of tomorrow's $v_B(x')$.  So we've turned an infinite-dimensional
sequential problem of finding $\{x_{t+1}\}^\infty_{t=0}$ into something
much simpler, provided we know $v_B$.

Of course, we generally don't know $v_B$ or that $v_B=v^*$.
Therefore, Representation~\ref{eq:vB} is only useful if we can carry
out the following three steps:
\begin{enumerate}[label=(\roman*)]
  \item
    \emph{Prove $v_B=v^*$}:
    We need a theorem telling us when $v_B$ satisfying (\ref{eq:vB})
    equals $v^*$ and thus is a solution to original
    Problem~\ref{prob:sequential-short}

  \item \emph{Find $v_B$}:
    Solve functional Equation~\ref{eq:vB} for fixed point $v_B$, an
    unknown real-valued function. Step (i) guarantees $v_B=v^*$ so that
    we've indeed solved the problem.

  \item \emph{Compute Optimal Sequence}:
    Given $v_B$, we can determine the \emph{policy function} that
    spits out an optimal $x'$ (given $x$) that appropriately trades off
    $F(x,x')$ and $v_B(x')$. That policy function can then be used to
    generate an optimal sequence $\{x_{t+1}\}\tinfz$.
\end{enumerate}
These steps will guide our subsequent discussion below for solving
discrete-time dynamic optimization problems via the recursive Bellman
Equation approach.



\clearpage
\subsubsection{Principal of Optimality: Equivalence of Sequential \&
Recursive Reps}

This subsection considers a dynamic optimization problem written in
sequential (\ref{prob:sequential-short}) and recursive (\ref{eq:vB}) form
and seeks to clarify the relationship between $v^*$ and $v_B$.
Nothing more. We postpone actually \emph{solving} for $v^*$ and $v_B$
for the moment.

Recall $v^*$, the optimal value of the objective function in the
sequential formulation
\begin{align}
  \label{eq:vstar}
  v^*(x_0) := \sup_{\{x_{t+1}\}_{t=0}^\infty}
  \; &\sumtinfz \beta^t F(x_t,x_{t+1})
  \\
  \qquad \text{s.t.}\quad
  &
  \text{$x_{t+1} \in \Gamma(x_t)$ $\;\forall t$, $x_0$ given}
  \notag
\end{align}
along with $v_B$, the solution to the functional Bellman Equation
in the recursive formulation,\footnote{%
  We replaced max with sup so that we can refer to $v_B$ without
  worrying about existence. But, in general, we work with problems where
  the max is obtained.
}
\begin{align}
  \label{eq:vB2}
  v_B(x) &=
  \sup_{x'\in\Gamma(x)}\;
  F(x,x') +
  \beta v_B(x')
\end{align}
These problems are characterized by the problem primitives
$(\beta,X,\Gamma,F)$ where
\begin{itemize}
  \item $\beta$: Discount factor. Can also usefully think of this as
    the \emph{probabability} that the agent survives to tomorrow to
    yield future returns, rather than a $(1-\beta)$ probability of
    expiring and earning zero returns henceforth.
  \item $X$: set of possible values the state today $x$ and the
    state tomorrow $x'$ can take on
  \item $\Gamma:X \rightrightarrows X$: constraint set, which encodes
    information about the feasible sets and plans
  \item $F:A\ra \R$: the period return function where $A$ is the
    graph of $\Gamma$, defined
    \begin{align*}
      A:=\{(x,y) \in X\times X \;|\; y \in\Gamma(x)\}
    \end{align*}
\end{itemize}
From these primitives, it will also be convenient to define
\begin{itemize}

  \item Feasible Plans $\Pi(x_0)$:
    Given $x_0$,
    we define
    $\Pi(x_0) :=
    \big\{ \{x_t\}\tinfz \;|\; x_{t+1} \in \Gamma(x_t) \; \forall t\big\}$
    %\begin{align*}
      %\Pi(x_0) :=
      %\big\{ \{x_t\}\tinfz \;|\; x_{t+1} \in \Gamma(x_t) \big\}
    %\end{align*}

  \item Policy Function $g(x)$: this function maps today's state $x$
    into an optimal choice of tomorrow's state $x'$. This is a byproduct
    of solving the functional Bellman Equation~\ref{eq:vB2}.
\end{itemize}

\clearpage
\begin{assump}(Assumptions for Principal of Optimality)
We lay out and label some assumptions that we will use for proving the
Principal of Optimality.
\begin{enumerate}
  \item[A1.] \emph{Nonempty Feasible Sets}:
    $\Gamma(x)\neq\emptyset$ for all $x\in X$
    %so that we can
    %always construct a feasible sequence given any starting point.

  \item[A2.] \emph{Well-Defined Limit}:
    For all feasible $\{x_{t}\}_{t=0}^\infty \in \Pi(x_0)$,
    the following limit exists\footnote{%
      Don't need a \emph{finite} limit here. Could be $\pm \infty$. Just
      need it to exist and not bounce around in the tail.
    }
    \begin{align*}
      \lim_{T\rightarrow\infty} \sum_{t=0}^T \beta^t F(x_t,x_{t+1})
      %\quad\text{exists}
    \end{align*}
    Can ensure this limit exists if $F(x,x')$ is a bounded function and
    $\beta \in (0,1)$.
    % See Esteban week 1 slides, slide 54

  \item[A3.] \emph{Value Function Limit}:
    For all $x_0\in X$ and for all
    $\{x_{t}\}_{t=0}^\infty \in \Pi(x_0)$,\footnote{%
      Emphasis on double ``for all'' qualifiers.
    }
    solution $v_B$ to the Bellman Equation satisfies
    \begin{align*}
      \lim_{t\rightarrow\infty} \beta^t v_B(x_t) = 0
    \end{align*}
    Loosely says ``the value of stuff in the infinite future doesn't
    matter'' no matter starting $x_0$ or subsequent path
    $\{x_{t+1}\}_{t=0}^\infty$.
\end{enumerate}
\end{assump}

\begin{thm}\emph{(Principal of Optimality)}
\label{thm:vstar-solves-bellman}
Relationship between $v^*$ and $v_B$:
\begin{enumerate}[label=\emph{(\roman*)}]
  \item
    \emph{($v^*$ solves Bellman Equation)}:
    If A1-2 hold, then solution $v^*$ to the sequential problem in
    Equation~\ref{eq:vstar} satisfies the Bellman Equation~\ref{eq:vB2}.
    i.e.\
    \begin{align*}
      v^*(x) =
      \sup_{x' \in \Gamma(x)}
      F(x,x') + \beta v^*(x')
    \end{align*}

  \item
    \emph{(Characterization of Optimal Policies)}:
    If A1-2 hold and we have sequence
    $\{x^*_t\}\tinfz\in\Pi(x_0)$ that \emph{attains the supremum} in
    sequential problem Equation~\ref{eq:vstar} (not saying such a plan
    will exist, but if it does), then
    \begin{align*}
      v^*(x^*_t) = F(x^*_t,x^*_{t+1}) + \beta v^*(x^*_{t+1})
      \qquad \forall t
    \end{align*}
    Conversely, given a feasible sequence
    $\{\hat{x}_t\}\tinfz \in\Pi(x_0)$ satisfying
    \begin{align}
      v^*(\hat{x}_t)
      &= F(\hat{x}_t,\hat{x}_{t+1})
        + \beta v^*(\hat{x}_{t+1}) \qquad \forall t\notag\\
      0 &\geq
      \limsup_{t\ra\infty}
      \beta^t v^*(\hat{x}_t)
      \label{weak-tvc}
    \end{align}
    then $\{\hat{x}_t\}\tinfz$ attains the sup in Sequential
    Problem Equation~\ref{eq:vstar} with $x_0$ given.\footnote{%
      Again, not saying $\{\hat{x}_t\}$ will exist, but if it does.}

  \item
    \emph{($v^*=v_B$)}:
    If Assumptions 1, 2, and 3 hold, then $v_B = v^*$---i.e.\ solution
    $v_B$ to recursive Bellman Eqn (\ref{eq:vB2}) equals
    optimal value function $v^*$ for sequential formulation
    (\ref{eq:vstar}).

    Moreover, since $v_B=v^*$ and since Assumption 3 implies
    Condition~\ref{weak-tvc}, Result 2 above tells us that the feasible
    plan generated by policy function $g(x)$ as $\hat{x}_{t+1} =
    g(\hat{x}_t)$ starting from $\hat{x}_0=x_0$ is an optimal,
    supremum-attaining plan.
\end{enumerate}
\end{thm}

%\begin{proof}
%\end{proof}


%\begin{proof}
%For arbitrary $x\in X$, we want to show that the value $v^*(x)$ equals
%%the RHS of Equation~\ref{vstar-solves-bellman}---i.e.\ that it is indeed
%the RHS sup. By the definition of sup, that's true if and only if
%\begin{enumerate}
  %\item[A1.] $v^*(x) \geq F(x,y) + \beta v^*(y)$ for all $y\in\Gamma(x)$.
  %\item[A2.] For any $\varepsilon>0$, we can find a
    %$y_\varepsilon\in\Gamma(x)$ such that
    %$
      %v^*(x) < F(x,y_\varepsilon) + \beta v^*(y_\varepsilon) + \varepsilon
    %$
%\end{enumerate}
%That's what we need to show. Now onto what we know.
%Specifically, we know that $v^*$ is the sup over the sequential problem
%as defined in Definition~\ref{defn:vstar}. So again, by the definition
%of the sup, we know that
%\begin{enumerate}
  %\item[B1.] $v^*(x) \geq \sumtinfz \beta^t F(x_t,x_{t+1})$ for all
    %feasible sequences satisfying $x_{t+1}\in\Gamma(x_t)$ with starting
    %value $x_0=x$.
  %\item[B2.] For any $\varepsilon>0$, there exists feasible
    %$\{\tilde{x}_{t+1}\}_{t=0}^\infty$ such that
      %$
      %v^*(x) < \sumtinfz \beta^t
      %F(\tilde{x}_{t}, \tilde{x}_{t+1}) + \varepsilon
      %$
      %with starting value $x_0=x$.
%\end{enumerate}
%Our task is to show that (B1) and (B2) together imply (A1) and (A2).

%(Show A1) Let $x_0$ and $\varepsilon>0$ be given.
%Let $x_1$ be any point in $\Gamma(x_0)$. Then by (B2)
%and some index renumbering, there exists a feasible sequence
%$\{x_{t+2}\}_{t=0}^\infty$ such that
%\begin{align}
  %v^*(x_1) &< \sumtinfz \beta^t
  %F(x_{t+1}, x_{t+2}) + \varepsilon \notag \\
  %\Leftrightarrow\quad
  %v^*(x_1) - \varepsilon &< \sumtinfz \beta^t
  %F(x_{t+1}, x_{t+2})
  %\label{seqchoice}
%\end{align}
%Onto this just-chosen sequence, prepend
%$x_1\in\Gamma(x_0)$ so we now have sequence
%$\{x_{t+1}\}_{t=0}^\infty$. All elements were chosen to be
%feasible.  So by (B1), we know that for feasible sequence
%$\{x_{t+1}\}_{t=0}^\infty$
%\begin{align*}
  %v^*(x_0)
  %&\geq \lim_{T\rightarrow\infty} \sumtTz \beta^t F(x_t,x_{t+1}) \\
  %\text{Pop off first element} \qquad
  %&= F(x_0, x_1) + \lim_{T\rightarrow\infty} \sumtT \beta^t F(x_t,x_{t+1}) \\
  %\text{Renumbering indices} \qquad
  %&= F(x_0, x_1) + \beta \lim_{T\rightarrow\infty} \sumtTz \beta^{t} F(x_{t+1},x_{t+2}) \\
  %\text{By Inequality~\ref{seqchoice}} \qquad
  %&= F(x_0, x_1)
  %+ \beta \left[ v^*(x_1) - \varepsilon\right]
%\end{align*}
%Since $\varepsilon>0$, starting point $x_0$, and
%$x_1\in\Gamma(x_0)$ were arbitrary
%\begin{align*}
  %v^*(x_0)
  %&\geq F(x_0, x_1)
  %+ \beta  v^*(x_1)
%\end{align*}
%which is the same as (A1), provided you replace $x_0$ and $x_1$ with $x$
%and $y$.

%(Show A2)
%Suppose that $x$ and $\varepsilon>0$ are given.
%By (B2), there exists a feasible sequence with $x_0=x$ such that
%\begin{align*}
  %v^*(x) &< \lim_{T\rightarrow\infty} \sumtTz \beta^t
  %F(x_{t}, x_{t+1}) + \varepsilon \\
  %\Leftrightarrow\quad
  %v^*(x) &< F(x, x_1)
  %+ \lim_{T\rightarrow\infty}
  %\sumtT \beta^t
  %F(x_{t}, x_{t+1}) + \varepsilon \\
  %\Rightarrow\quad
  %v^*(x) &< F(x, x_1)
  %+ \beta v^*(x_1)
  %+ \varepsilon \\
%\end{align*}
%Note, we were able to replace the series $\sumtT \beta^t F(x_{t},
%x_{t+1})$ with $\beta v^*(x_1)$ by using B1.
%So now, just choose $y=x_1$. We know $x_1\in\Gamma(x_0)$ is feasible
%and $x_0=x$. This means A2 holds.
%\end{proof}

%\begin{proof}
%In this proof, I'll be using statements similar to A1, A2, B1, and B2
%from the proof of Theorem~\ref{thm:vstar-solves-bellman}. So let's
%restate them appropriately for this problem.

%For arbitrary $x\in X$, we want to show that $v_B(x)=v^*(x)$, with the
%latter representing the optimal value---the supremum over all sequences.
%That only occurs if
%\begin{enumerate}
  %\item[A1.] $v_B(x) \geq \sumtinfz \beta^t F(x_t,x_{t+1})$ for all
    %feasible sequences satisfying $x_{t+1}\in\Gamma(x_t)$ with starting
    %value $x_0=x$.
  %\item[A2.] For any $\varepsilon>0$, there exists feasible
    %$\{\tilde{x}_{t+1}\}_{t=0}^\infty$ such that
      %$
      %v_B(x) < \sumtinfz \beta^t
      %F(\tilde{x}_{t}, \tilde{x}_{t+1}) + \varepsilon
      %$
      %with starting value $x_0=x$.
%\end{enumerate}
%That's what we need to show. Now onto what we know.  Specifically, we
%know that $v_B$ solves the Bellman Equation, so it equals the RHS of
%%Equation~\ref{eq:bellmansup}. Since it's a supremum, by definition
%\begin{enumerate}
  %\item[B1.] $v_B(x) \geq F(x,y) + \beta v_B(y)$ for all $y\in\Gamma(x)$.
  %\item[B2.] For any $\varepsilon>0$, we can find a
    %$y_\varepsilon\in\Gamma(x)$ such that
    %$
      %v_B(x) < F(x,y_\varepsilon) + \beta v_B(y_\varepsilon) + \varepsilon
    %$
%\end{enumerate}
%Our task is to show that (B1) and (B2) together imply (A1) and (A2).

%(Show A1) Let $\{x_{t+1}\}_{t=0}^\infty$ be a feasible sequence. We can
%use B1 to recursively substitute:
%\begin{align*}
  %v_B(x) &\geq F(x,x_1) + \beta v_B(x_1) \\
  %v_B(x) &\geq F(x,x_1) + \beta F(x_1, x_2) + \beta^2 v_B(x_2) \\
  %\vdots \quad & \quad \qquad \vdots \\
  %v_B(x) &\geq \sumtTz \beta^t F(x_t,x_{t+1}) + \beta^{T+1} v_B(x_{T+1}) \\
  %\lim_{T\rightarrow\infty}
  %v_B(x) &\geq
    %\lim_{T\rightarrow\infty}
    %\left\{
    %\sumtTz \beta^t F(x_t,x_{t+1}) + \beta^{T+1} v_B(x_{T+1})
    %\right\}\\
  %v_B(x) &\geq
    %\sumtinfz \beta^t F(x_t,x_{t+1})
    %+ 0
%\end{align*}
%Where we needed the Transversality assumption
%to ensure that the term $\beta^{T+1}v(x_{T+1})$ goes to zero.

%(Show A2) Next,
%\end{proof}

\clearpage
\subsubsection{%
Bellman Operator \& Existence, Uniqueness, Properties of $v_B$
}

We now define a useful object called the \emph{Bellman Operator}, which
(together with the Contraction Mapping Theorem and some additional
assumptions) lets us assert existence, uniqueness, and properties
(increasing, concave, etc.) of solution $v_B$ to Bellman
Equation~\ref{eq:vB}.

Throughout this section, we will also assume that Assumptions A1-3 from
the previous section are satisfied, so the Principal of Optimality
applies, $v_B=v^*$. Otherwise, we don't really care about the properties
of $v_B$ if it's not related to the solution $v^*$ of the original
dynamic optimization problem.

\begin{defn}(Bellman Operator)
Given Bellman Equation~\ref{eq:vB}, define the associated
\emph{Bellman operator} $T$ acting on the set of real-valued functions
as
%$T:\mathscr{B}(X,\R)\rightarrow\mathscr{B}(X,\R)$
\begin{align}
  T[w(x)]
  &= \sup_{x'\in\Gamma(x)}
  F(x,x') + \beta w(x')
  \label{defn:bellman-operator}
\end{align}
This takes in a function $w$ and spits out another function.
If a solution $v_B$ to Bellman Equation~\ref{eq:vB} exists, it is
necessarily a fixed point of this operator, i.e. $T[v_B(x)]=v_B(x)$.
\end{defn}

\begin{rmk}
We sometimes restrict $T$ to act only on the set of real \emph{bounded}
functions. They could be bounded either because we explicitly bound the
range of the functions, \emph{or} we consider only continuous functions
on a compact domain (so the range is consequently bounded).
\end{rmk}

%\begin{prop}
%Suppose that $v$ is a continuous function in $\sB(X,\R)$, the set of
%bounded real-valued functions. Then $T[v]$ is a continuous function
%of $x$ and the optimal policy $x'_*(x)$ is a UHC correspondence. If
%$x'_*(x)$ is a function, then it is continuous.
%\end{prop}

%\begin{prop}{\emph{(Bellman Operator is a Contraction Map)}}
%\end{prop}

\begin{assump}
Assumptions we will reference in the next theorem, on top of A1-A3:
\begin{enumerate}
  \item[A4.]
    \emph{Continuous $\Gamma$ on Convex $X$}:
    $X$ is a convex subset of $\R^L$ and the correspondence
    $\Gamma:X\rightrightarrows X$ is compact-valued and continuous.

  \item[A5.]
    \emph{Bounded, Continuous $F$ plus Discounting}:
    Period return function $F$ is bounded \& continuous,
    $\beta\in (0,1)$.

  \item[A6.]
    \emph{Monotone $\Gamma$}:
    If $x \leq y$, then $\Gamma(x) \subseteq \Gamma(y)$.

  \item[A7.]
    \emph{Strictly Increasing $F$}:
    For fixed $x'$, function $F(\cdot,x')$ is strictly increasing in
    each of the $L$ components of $x\in X\subseteq \R^L$.

  \item[A8.]
    \emph{Strictly Concave $F$}:
    If $F$ is strictly concave in $(x,x')\in A$, i.e.\ for all distinct
    pairs $(x,x'), (y,y') \in A$ and $\alpha \in (0,1)$
    \begin{align*}
      F\left(\alpha x + (1-\alpha)y, \; \alpha x' + (1-\alpha)y'\right)
      >
      \alpha F(x,x') + (1-\alpha)F(y,y')
    \end{align*}

  \item[A9.]
    \emph{Convex $\Gamma$}:
    For all $\alpha\in[0,1]$ and $x,y\in X$, we have
    \begin{align*}
      \begin{rcases}
        x'&\in \Gamma(x) \\
        y'&\in \Gamma(y)
      \end{rcases}
      \quad\implies\quad
      \alpha x' + (1-\alpha) y'
      \in \Gamma(\alpha x + (1-\alpha) y)
      %\qquad\forall \alpha\in[0,1]
    \end{align*}

  \item[A10.]
    \emph{Continuously Differentiable $F$}:
    Period return function $F$ is continuously differentiable on
    $\interior(A)$.
\end{enumerate}
\end{assump}

\begin{thm}(Existence, Uniqueness, Properties of $v_B$)
Given solution $v_B$ to the recursive Bellman Equation,
we have the following results:
\begin{enumerate}
  \item
    \emph{(Uniqueness of Solution $v_B$)}:
    If Assumptions 1-5 hold, then for Bellman Operator $T$
    \begin{enumerate}[label=\emph{(\alph*)}]
      \item $T$ obviously maps the space of bounded continuous functions
        $\sC(X,\R)$ into itself, i.e.\ we have
        $Tv \in \sC(X,\R)$ for all $v\in\sC(X,\R)$.
      \item
        $T$ is a contraction map on $(\sC(X,\R),d_\infty)$ with a unique
        fixed point $v \in\sC(X,\R)$, and
        $\forall v_0 \in \sC(X,\R)$, we have
        $d(T^n[v_0],v) \leq \beta^n d(v_0,v)$
      \item
        The policy correspondence $g$ is compact-valued and upper
        hemicontinuous.
    \end{enumerate}

  \item
    \emph{(Monotonicity of $v$)}:
    Under A1-7, unique fixed point $v$ of $T$ is strictly increasing.

  \item
    \emph{(Strict Concavity of $v$ and Uniqueness of $g$)}:
    Under A1-5 and 8-9, the unique fixed point $v$ of $T$ is
    strictly concave and optimal policy $g$ is a single-valued
    continuous function.

  \item
    \emph{(Benveniste-Scheinkman)}:
    Under Assumptions 1-5 and 8-10, if $x_0\in \interior(X)$ and
    $g(x_0)\in\interior(\Gamma(x_0))$, the unique fixed point $v$
    of $T$ is continuously differentable at $x_0$ with
    \begin{align*}
      \frac{\partial v(x_0)}{\partial x^i}
      =
      \frac{\partial F(x_0, g(x_0))}{\partial x^i}
    \end{align*}
\end{enumerate}
\end{thm}



\clearpage
\subsubsection{Solving the Recursive Bellman Equation for $v_B$}

\begin{defn}(Value Function Iteration)
By the theorem of the previous subsection, if the assumptions are
satisfied so that $T$ is a contraction map, we can start with an initial
guess $v_0$ and iterate on the value function with the Bellman Operator
 $v_{n+1}(x)=T[v_n(x)$ until convergence.
\end{defn}

\begin{defn}(Guess and Verify)
Suppose that we have Bellman Equation
\begin{align*}
  v(x) = \max_{y\in\Gamma(x)} F(x,y) + \beta v(y)
\end{align*}
The method of ``guess and verify'' is a three-step procedure
\begin{enumerate}
  \item Come up with a guess for $v$.

    Note that often, we merely guess and check a functional form like
    $v(x)=a + b\log(x)$ (without specifying constant coefficients). And
    so during the ``guess and check'' procedure, we need to solve for
    the coefficient values as well. We see this in Step 3 and in the
    next example.

  \item Solve the RHS for the policy function $y^*(x)$: Substitute your
    guess for $v$ into the RHS and find the max over all feasible
    $y\in\Gamma(x)$.  Often, everything is differentiable, so this means
    differentiate the RHS, set equal to zero, solve for $y^*(x)$:
    \begin{align*}
      \text{$y^*(x)$ solves} \qquad
      0 &= \frac{\partial}{\partial y}
      \left[
        F(x,y) + \beta\, v(y)
      \right]
    \end{align*}
    I use $y^*(x)$ to highlight the dependence of the optimal value on
    the state $x$.

  \item Evaluate the RHS at the optimum: Plug the optimum $y^*(x)$ into
    the RHS. Check that the resulting function is identical to the
    original $v$ on the LHS, i.e.\ check
    \begin{align*}
      v(x) &= F(x,y^*(x)) + \beta\, v(y^*(x))
    \end{align*}
    It at is at this stage that you collect terms and identify any
    undetermined coefficients.
\end{enumerate}
\end{defn}



\begin{ex}
Suppose that we face problem
\begin{align*}
  v(k) = \max_{k'\in[0,f(k)]} u(f(k)-k') + \beta\, v(k')
\end{align*}
where $u=\log(c)$ and $f(k)=k^\alpha$.

First step: divine inspiration says guess $v(k) = a + b \; \log(k)$ for
some constants $a$ and $b$ that we'll pin down later.
Now, differentiate the RHS:
\begin{align*}
  \frac{\partial}{\partial k'}
  \left[
  u(f(k)-k') + \beta\, v(k')
  \right]
  &=
  \frac{\partial}{\partial k'}
  \left[
    \log\left(
    k^\alpha - k'
    \right)
    + \beta (a + b\, \log(k'))
  \right] \\
  &= -\frac{1}{k^\alpha-k'}
  + \frac{b\beta}{k'}
\end{align*}
Set that equal to zero and solve for $k'$
\begin{align*}
  0
  &= -\frac{1}{k^\alpha-k'}
  + \frac{b\beta}{k'}\\
  \Rightarrow\quad
  k' &= \frac{b\beta}{1+b\beta} k^\alpha
\end{align*}
Now substitute this into the RHS, and try to work it into something that
looks like $v(k) = a + b\,\log(k)$:
\begin{align*}
  RHS = u(f(k)-k') + \beta \, v(k')
  &=
  \log\left(
    k^\alpha - \frac{b\beta}{1+b\beta} k^\alpha
  \right)
  + \beta
    \left[
      a + b\,\log\left(\frac{b\beta}{1+b\beta} k^\alpha\right)
    \right] \\
  &=
  \log\left(
    \frac{k^\alpha}{1+b\beta}
  \right)
  + a\beta + b\beta\,\log\left(\frac{b\beta}{1+b\beta} k^\alpha\right) \\
  &=
  [a\beta
  + b\beta\,\log\left(b\beta\right)
  - (1+b\beta)\log\left( 1+b\beta \right)]
  +\alpha(1+b\beta)\log\left( k \right)
\end{align*}
Bit of a monster, but we have isolated a bunch of constants and a
constant times $\log k$. We just need to match up constants, so start
with the coefficient on $\log k$:
\begin{align*}
  b\log(k)
  &= \alpha(1+b\beta) \log(k) \\
  \Rightarrow\quad
  b &= \frac{\alpha}{1-\alpha \beta}
\end{align*}
Next, match up the constants to $a$ and solve for $a$ (leaving $b$ as
is):
\begin{align*}
  a
  &=
  a\beta + b\beta\,\log\left(b\beta\right)
  - (1+b\beta)\log\left( 1+b\beta \right) \\
  \Rightarrow\qquad
  a
  &=
  \frac{1}{1-\beta}
  \left[
    b\beta\,\log\left(b\beta\right)
  - (1+b\beta)\log\left( 1+b\beta \right) \right]
\end{align*}
For this choice of constants, $v(k) = a + b\,\log(k)$ solves the Bellman
Equation.
\end{ex}


\clearpage
\subsection{Continuous-Time Deterministic Dynamic Optimization}
\label{sec:continuous-dynamic}

In this section, we extend the work of
Section~\ref{sec:discrete-dynamic} on discrete-time dynamic programming
into continuous time. In practice, that means sums become integrals and
difference equations become \emph{differential} equations.  The
formulation we generalize is Problem~\ref{prob:altrep} rather than the
equivalent and more compact Problem~\ref{prob:sequential-short}.

In continuous time, we use the more natural state-control representation
to set up the sequential problem, so that our task involves choosing an
optimal \emph{function} (over time) $y(t)$ that then influences the
evolution of the state $x(t)$ through some law of motion.
To find the optimal path for the control, we use the
\emph{variational principle} of the calculus of variations.
Specifically, we derive necessary conditions of an optimal solution
$y(t)^*$ by perturbing a supposedly-optimal solution by small amounts
$y^*(t)+\varepsilon$ to generate new candidate solutions.
We then see what necessary conditions we need to place on the original
supposedly-optimal solution $y^*(t)$ so that none of the new candidate
solutions $y^*(t)+\varepsilon$ are any better.

Given that description, it's maybe not surprising that the necessary
conditions are for a \emph{local} optimum rather than a global optimum.
We're simply specifying necessary conditions on some solution $y(t)^*$
that will rule out better solutions in the neighborhood of $y(t)^*$. We
saw a similar result in the KKT section. But also as in the KKT
conditions, a few extra conditions having to do with convexity can
guarantee the solution is globablly optimal.

\subsubsection{Canonical Sequential Formulation}

\begin{defn}{(Canonical Sequential Formulation: Control-State Rep)}
\label{defn:sequential-altrep-cts}
The \emph{Canonical Sequential Formulation} requires choosing
\emph{function of time} $y(t):\R_+\ra Y$ that maximizes
\begin{align}
  \label{prob:cts-dynamic}
  %\max_{y(t)}\;
  %W(y(t)):=
  v(x_0)
  =
  \max_{y(t)}\;
  &\int_0^\infty e^{-\rho t} f(x(t),y(t)) \; dt
  \notag\\
  %\qquad
  %\begin{cases}
  \text{s.t.}\quad
  &\text{$\dot{x}_{t} = g(x(t),y(t))$ $\;\forall t$, $x_0$ given}
  \notag\\
  &
  y(t) \in Y \;\; \forall t\in[0,\infty)
  %\end{cases}
\end{align}
This is an obvious generalization of the control-state formulation in
discrete time, where
\begin{enumerate}
  \item \emph{State}, $x(t)$:
    like a ``sufficient statistic'' for the problem, encoding all
    relevant information about the state of the world and about feasible
    options for $y(t)$

  \item \emph{Control}, $y_{t}$: Chosen by the agent given state $x(t)$.
    Affects instantaneous return and influences the evolution of the
    state.

  \item \emph{Instantaneous Return}, $f(x(t),y(t))$: Given state $x(t)$ and
    choice of control $y(t)$, agent gets instantaneous return
    $f(x(t),y(t))$ at time $t$, discounted to the present at rate
    $\rho$.

  \item \emph{Law of Motion} $g(x(t),y(t))$: Given the current state $x(t)$
    and the choice of control $y(t)$,
    $x(t)$ evolves according to differential equation
    $\dot{x}(t)=g(x(t),y(t))$.
  \item \emph{Admissable Pair}, $(x(t),y(t))$: The pair of functions
    $(x(t),y(t))$ is called an admissable pair if they satisfy both
    $\dot{x}(t) = g(x(t),y(t))$ and $y(t)\in Y$ for all $t$.
\end{enumerate}
\end{defn}



\clearpage
\subsubsection{Hamiltonian for Solving the Canonical Sequential
Formulation}

\begin{defn}{(Current-Value Hamiltonian)}
The \emph{Current-Value Hamiltonian} is a real-valued functional
(function of functions) associated with Problem~\ref{prob:cts-dynamic}
is
\begin{align*}
  H(x,y,\lambda) := f(x(t),y(t)) + \lambda(t) g(x(t),y(t))
\end{align*}
where $\lambda(t)$ is a function of time called the \emph{co-state}.
The Hamiltonian is much like the Lagrangian of static optimization
problems, only for dynamic optimization.
\end{defn}

\begin{prop}\emph{(Necessary Conditions for Optimum)}
Given Problem~\ref{prob:cts-dynamic} with $f$ and $g$ continuously
differentiable, an interior continuous solution $y(t)$ with corresponding
state variable function $x(t)$ ($x_0$ given) satisfies the following
necessary conditions for all $t\geq 0$:
\begin{enumerate}
  \item $H_y (x(t),y(t),\lambda(t)) = 0$
  \item $\dot{\lambda}(t) = \rho\lambda(t)- H_x(x(t), y(t), \lambda(t))$
  \item $\dot{x}(t) = H_\lambda(x(t),y(t),\lambda(t)) = g(x(t), y(t))$
\end{enumerate}
Also have an initial boundary condition $(x_0$ given) and terminal
boundary condition by TVC
\begin{align*}
  \limT e^{-\rho T} \lambda(T)x(T)=0
\end{align*}
Note that the initial value of the costate $\lambda_0$ is \emph{not}
pinned down.
\end{prop}
\begin{rmk}
Here's the general procedure for solving a problem of this form
\begin{enumerate}[label=(\roman*)]
  \item Construct the Hamilton and compute the above derivatives
  \item Set $\dot{x}(t)=\dot{\lambda}(t)=0$ in those expressions and solve
    for steady state $x^*$ and $y^*$
  \item
    If $\lambda(t)$ and $x(t)$ scalar, draw phase diagram in
    $(x,\lambda)$-space or substitute out for $\lambda$ and draw phase
    diagram in $(x,y)$ space
\end{enumerate}
\end{rmk}

%\begin{proof}{(Kinda for Finite Horizon)}
%Suppose we only go out to horizon $T$ (rather than indefinitely), and we
%have interior continuous solution $y^*(t)$ with associated path for the
%state variable $x(t)^*$. We want to characterize the necessary conditions
%that $y(t)^*$ must satisfy to be a genuine (local) optimum.

%Start by looking at ``variations'' on $y(t)^*$. Specifically,
%fix arbitrary continuous function $\eta(t)$ and add a
%scaled-by-$\varepsilon$ version of this function to our solution $y(t)^*$
%to generate variation $y(t)(\varepsilon)$:
%\begin{align*}
  %y(t)(\varepsilon) := y(t)^* + \varepsilon\eta(t)
%\end{align*}
%Since $\eta(t)$ is continuous, $y(t)(\varepsilon)$ is continuous for any
%$\varepsilon\in\R$.
%Moreover, we think of $\varepsilon$ indexing a family variations on the
%solution $y(t)^*$ (i.e. we can sweep out the family of variations on
%$y(t)^*$ by varying $\varepsilon$). For $\varepsilon$ small, these
%variations $y(t)(\varepsilon)$ are all close to the solution
%$y(t)^*$.\footnote{%
  %To ensure feasibility of the variations, note that for fixed $\eta(t)$,
  %$\eta(t)$ is continuous on a compact interval, hence bounded.
  %Therefore, you can always choose an $\varepsilon_\eta$ such that
  %$|\varepsilon| < \varepsilon_\eta$
  %implies $y(t)(\varepsilon) \in \text{int}(Y(t))$.
  %Since we're looking for a local optimum, this is fine.
%}

%Given each variation of the control $y(t)(\varepsilon)$, define
%corresponding $x(t)(\varepsilon)$ satisfying
%\begin{align}
  %\dot{x}(t)(\varepsilon)
  %= G(x(t)(\varepsilon), y(t)(\varepsilon))
  %\qquad x_0(\varepsilon)=x_0
  %\label{xvardef}
%\end{align}
%Given admissable pair $(x(t)(\varepsilon), y(t)(\varepsilon))$, let's also
%define
%\begin{align}
  %W(\varepsilon) :=
  %\int^{T}_0 F(x(t)(\varepsilon), y(t)(\varepsilon)) \, dt
  %&\leq W(0) \label{Weps}
%\end{align}
%where the last inequality followed because $y(t)(0)=y(t)^*$, which was
%optimal.

%Now that we have this family of variations, let's derives the necessary
%conditions we need to put on $y(t)^*$ so that none of the
%$y(t)(\varepsilon)$ are better.

%(Prove the Conditions)
%By definition of Equation~\ref{xvardef}
%\begin{alignat}{3}
  %0 &= G(x(t)(\varepsilon), y(t)(\varepsilon), t) - \dot{x}(t)(\varepsilon)
  %&&\forall t\in[0,T] \notag \\
  %\Rightarrow \quad
  %0 &= \int^{T}_0
  %\lambda(t) \; [
    %G(x(t)(\varepsilon), y(t)(\varepsilon), t) -
    %\dot{x}(t)(\varepsilon)
  %] \, dt
  %\qquad&&\forall \lambda(t):[0,T]\rightarrow \R
  %\label{lambdadef}
%\end{alignat}
%where $\lambda(t)$ is any function. Assume it is continuously
%differentiable.\footnote{%
%Did I cool you down with that hand wave. I'm just going to assume that
%for now.}
%If we choose it correctly (with a few conditions on the function), it
%will be called the \emph{co-state} and will behave like a Lagrange
%multiplier.

%Now do the following addition
%\begin{align*}
  %W(\varepsilon) &= W(\varepsilon) + 0
  %= W(\varepsilon) +\text{(RHS of \ref{lambdadef})} \\
  %\Rightarrow \quad
  %W(\varepsilon) &=
  %\int^{t_1}_0
  %\left\{
  %F(x(t)(\varepsilon), y(t)(\varepsilon), t)
  %+ \lambda(t) [
    %G(x(t)(\varepsilon), y(t)(\varepsilon), t) -
    %\dot{x}(t)(\varepsilon)
  %]
  %\right\}\, dt
%\end{align*}
%Integrating the $\int \lambda(t) \dot{x}(t)(\varepsilon)$ by parts to turn
%the $\lambda$ into $\dot{\lambda}$ and $x$ into $\dot{x}$, you get
%\begin{align*}
  %\int^{t_1}_0 \lambda(t) \dot{x}(t)(\varepsilon) \, dt
%\end{align*}
%\end{proof}


\clearpage
\subsubsection{Canonical Hamilton-Jacobi-Bellman Recursive
Representation}

\begin{prop}
Given Sequential Formulation~\ref{defn:sequential-altrep-cts},
we develop a recursive rep.
Specifically, the value function satisfies the
\emph{Hamilton-Jacobi-Bellman Equation} $\forall t$:
\begin{align}
  \rho v(x)
  &= \max_{y\in Y} f(x,y) + \frac{\partial v(x)}{\partial x^T} g(x,y)
  \qquad\forall t
  \label{HJB}
\end{align}
The first order condition that the control $y$ must satisfy (for all
$t$) is:
\begin{align*}
  0 =
  \frac{\partial f(x,y)}{\partial y}
  +
  \frac{\partial g(x,y)^T}{\partial y}
  \frac{\partial v(x)}{\partial x}
\end{align*}
Given optimal $y$ satisfying this condition (i.e. an optimal policy
function), we can plug into Equation~\ref{HJB} and solve for value
function $v(x)$.
Generally (since $x$ is a vector with multiple state variables), solving
Equation~\ref{HJB} for $v(x)$ means solving a PDE.
\end{prop}
\begin{rmk}
We converted a problem in the \emph{time}-domain to one in the
\emph{state}-domain. Notice that this holds for \emph{all} $t$, and says
that the control $y$ \emph{at every period} should be chosen to
optimally trade off instantaneous flow utility today $f(x,y)$ against
the additional value $v'(x)g(x,y)$ that you can capture by influencing
the evolution of the state variable.
\end{rmk}
\begin{proof}
This proof has three steps: (1) Approximate the continuous problem with
a discrete problem, (2) linearize discrete-problem discount factor,
and (3) send the time step to zero.

First, write the continuous-time problem as a recursive discrete time
problem between periods $t$ and $t+\Delta$, for constant length $\Delta$
between discrete time periods. (We eventually send this to zero).
Note that $f(x_t,y_t)$ is a utility/return \emph{flow} per unit time
(i.e.  between $t$ and $t+1$), so we must multiply by $\Delta$
to get the utility/return flow $\Delta f(x_t,y_t)$ between $t$ and
$t+\Delta$.
Same for flow $g(x_t,y_t)$ into the next-period state, giving
\begin{align*}
  v(x_t) = \max_{y_t}\;
  &\Delta f(x_t,y_t\big)
  + \beta(\Delta)\;v(x_{t+\Delta})
  \qquad
  \text{s.t.}\quad
  x_{t+\Delta}-x_t = \Delta g(x_t,y_t)
\end{align*}
Second, $\beta(\Delta):=e^{-\rho \Delta}$ is the discount factor between
$t$ and $t+\Delta$. Since we will take limits as $\Delta\ra 0$, it will
be more useful to work with its first-order Taylor Expansion (about
$\Delta=0$)
\begin{align*}
  \beta(\Delta)
  = e^{-\rho \cdot 0}
  &=
  e^{-\rho\Delta}
  -\rho e^{-\rho \cdot 0}
  (\Delta-0)
  + o(\Delta^2)
  =
  1-\rho\Delta + o(\Delta^2)
\end{align*}
Sub this expansion for $\beta(\Delta)$ in the discrete Bellman Equation
above, then rearrange to get
%\begin{align*}
  %v(x_t) = \max_{y_t}\;
  %&\Delta f(x_t,y_t\big)
  %+ (1-\rho\Delta) \;v(x_{t+\Delta})
  %+ o(\Delta^2)
%\end{align*}
%Rearrange:
\begin{align*}
  \rho\Delta v(x_{t+\Delta})
  = \max_{y_t}\;
  &\Delta f(x_t,y_t\big)
  +
  \big[
  v(x_{t+\Delta}) - v(x_t)
  \big]
  + o(\Delta^2)
\end{align*}
Divide through this last equation and the law of motion by $\Delta$ to
end up with system
\begin{align*}
  \rho v(x_{t+\Delta})
  = \max_{y_t}\;
  & f(x_t,y_t\big)
  +
  \frac{v(x_{t+\Delta}) - v(x_t)}{\Delta}
  + \frac{o(\Delta^2)}{\Delta}
  \qquad
  \text{s.t.}\quad
   \frac{x_{t+\Delta}-x_t}{\Delta} = g(x_t,y_t)
\end{align*}
Now just take $\Delta\ra 0$.
First, the LHS of the law of motion $\ra \dot{x}_t$.
Second, $o(\Delta^2)/\Delta \ra 0$.
Lastly, (assuming scalar $x$ for simplicity, though the result is the
same for a vector $x$):
\begin{align*}
  \lim_{\Delta\ra 0}
  \frac{v(x_{t+\Delta}) - v(x_t)}{\Delta}
  =
  \lim_{\Delta\ra 0}
  \frac{v(x_{t+\Delta}) - v(x_t)}{x_{t+\Delta}-x_t}
  \frac{x_{t+\Delta}-x_t}{\Delta}
  = \frac{\partial v(x_t)}{\partial x_t^T}\dot{x}_t
\end{align*}
Together, all of these imply the Hamilton-Jacobi-Bellman equation above.
\end{proof}

\clearpage
\subsection{Continuous-Time Stochastic Dynamic Optimization}

\subsubsection{Canonical Problem}

\begin{defn}(Canonical Problem)
We want to solve
\begin{align}
  J(x,t)
  =
  \max_{y_t}
  \;&
  \E_t\left[
    \int_t^T
    \varphi(x_s,y_s,s)\;ds
    +
    \Phi(x_T,T)
  \right]
  %\label{Jdef}
  \notag
  \\
  \qquad\text{s.t.}\quad
  &
  \underset{(n\times 1)}{%
    \vphantom{\mu(x_t,y_t,t)}
    dx_t
  }
  =
  \underset{(n\times 1)}{%
    \mu(x_t,y_t,t)
  }
  \;dt
  +
  \underset{(n\times m)}{%
    \vphantom{\mu(x_t,y_t,t)}
    \sigma(x_t,y_t,t)
  }
  \underset{(m\times 1)}{%
    \vphantom{\mu(x_t,y_t,t)}
    dZ_t
  }
  \notag
  \\
  & x_t = x,\quad
  \text{Constraints on $y_t$}
  \notag
\end{align}
where $J(x,t)$ is the value function for all $t\leq T$ starting from
$x_t=x$.

Note that potentially $T=\infty$, giving an infinite horizon problem.
And as it turns out, a useful way to think about the infinite horizon
problem is simply the finite horizon problem (which can often be
often characterized more easily), and then taking the limit
$T\ra\infty$.
\end{defn}

\begin{defn}
(Canonical Stationary Infinite Horizon Problem and Value Function)
The most common special of the above canonical problem is given by
\begin{align}
  J(x,t)
  =
  \max_{y_t}
  \;&
  \E_t\left[
    \int_t^\infty
    e^{-\rho s}
    u(x_s,y_s)
    \;ds
  \right]
  %\label{Jdef}
  \notag
  \\
  \qquad\text{s.t.}\quad
  &
  dx_t
  =
  \mu(x_t,y_t)\;dt
  +
  \sigma(x_t,y_t)\;dZ_t
  \notag
  \\
  & x_t=x,
  \quad\text{Constraints on $y_t$}
  \notag
\end{align}
\end{defn}


\subsubsection{Hamiltonian Approach}

\begin{defn}(Hamiltonian)
The \emph{Hamiltonian} for the canonical problem with no constraints on
$y_t$ is given by
\begin{align*}
  \calH(x_t,y_t,\xi_t,\pi_t,t)
  =
  \varphi(x_t,y_t,t)
  +
  \xi_t'
  \mu(x_t,y_t,t)
  +
  \trace\left(
  \pi_t'
  \sigma(x_t,y_t,t_t)
  \right)
\end{align*}
for $\xi\in \R^n$ and $\pi\in\R^{n\times m}$.
Note that it's also sometimes convenient to use a trace operator
property to rewrite $\trace(\pi_t' \sigma) =\trace(\sigma\pi_t')$.
\end{defn}

\begin{prop}\emph{(Stochastic Maximum Principle)}
The solution is characterized by
\begin{itemize}
  \item
    Pair $(x_t^*,y_t^*)$ such that
    \begin{align*}
      y_t^*
      =
      \argmin_{y_t}
      \calH(x_t^*,y_t,\xi_t,\pi_t,t)
    \end{align*}
    In practice, just differentiate $\calH$ with respect to $y_t$,
    set equal to zero, and solve.

  \item Law of motion for multiplier $\xi_t$ given by
    \begin{align*}
      d\xi_t
      &=
      -\frac{\partial \calH(x_t^*,y_t^*,\xi_t,\pi_t,t)}{\partial x}
      \;dt
      +
      \pi_t
      \;dZ_t
      \qquad\text{where}\quad
      \xi_T
      =
      -\frac{\partial \Phi(x_T,T)}{\partial x}
    \end{align*}
\end{itemize}
\end{prop}
%http://www.qfl-berlin.de/sites/default/files/Yusong%20Li.pdf


\clearpage
\subsubsection{Hamilton-Jacobi-Bellman Approach}

\begin{prop}\emph{(HJB Equation for Canonical Problem)}
The \emph{Hamilton-Jacobi-Bellman Equation} $J(x,t)$ associated with the
canonical finite horizon problem satisfies
\begin{align*}
  0
  &=
  \max_{y_t}
  \;
  \varphi(x_t,y_t,t)
  +
  \frac{\partial J(x_t,t)}{\partial t}
  +
  \calL J(x_t,t)
  \\
  \qquad\text{s.t.}
  &
  \quad\qquad
  J(x_T,T)=\Phi(X_T,T)
  \\
  &
  \quad\qquad
  \text{\emph{(Any other constraints on $y_t$)}}
\end{align*}
where $\calL$ is the infinitesimal generator of the SDE for the state
$x_t$:
\begin{align*}
  \calL f(x)
  =
  \frac{\partial f(x)}{\partial x'}
  \mu(x,y,t)
  +
  \frac{1}{2}
  \trace\left(
  \Sigma(x,y,t)
  \frac{\partial^2 f(x)}{\partial x\,\partial x'}
  \right)
\end{align*}
\end{prop}
\begin{rmk}
This is then essentially a sequence of static problems.
Solve by adding a multiplier on any constraints and getting the FOCs in
$y_t$.
\end{rmk}
\begin{proof}
By the principle of optimality, can wewrite the canonical problem by
breaking up
\begin{align*}
  J(x_t,t)
  =
  \max_{y_t}
  \;&
  \E_t\left[
    \int_t^T
    \varphi(x_s,y_s,s)\;ds
    +
    \Phi(X_T,T)
  \right]
  =
  \max_{y_t}
  \;
  \E_t\left[
    \int_t^\tau
    \varphi(x_s,y_s,s)\;ds
    +
    J(x_\tau,\tau)
  \right]
\end{align*}
Second $\tau\ra t$, can rewrite the equation as
\begin{align*}
  J(x_t,t)
  =
  \max_{y_t}
  \;
  \E_t\left[
    \varphi(x_t,y_t,t)\;dt
    +
    J(x_t,t)
    +
    dJ(x_t,t)
  \right]
\end{align*}
The first two terms in the expectation are known at $t$. The final term
has expectation
$\left[\frac{\partial J(x_t,t)}{\partial t}+\calL J(x_t,t)\right]\;dt$.
Cancel $J(x_t,t)$ on both sides and divide through by $dt$.
\end{proof}

\begin{cor}
\emph{(HJB Equation for Canonical Stationary Infinite Horizon Problem)}
In the special case of the infinite horizon problem,
\begin{align*}
  \rho v(x)
  =
  \max_{y}
  \;&
  u(x,y)
  +
  \calL v(x)
  \\
  \qquad\text{s.t.}
  &
  \;
  \text{\emph{(Any constraints on $y$)}}
\end{align*}
where $v(x):=J(x,0)$ represents the time-homoegenous part.
\end{cor}
\begin{proof}
From the canonical stationary problem, it's clear that
\begin{align}
  \forall t
  \qquad
  J(x_t,t)
  &= e^{-\rho t}J(x_t,0)
  =: e^{-\rho t}v(x_t)
  \label{vdef}
\end{align}
Compute derivatives of $J(x_t,t)$ from this last definition, rewrite
in terms of $v(x_t)$, sub into the HJB, drop time
subscripts, cancel, and rearrange to get the representation above.
\end{proof}


\clearpage
\section{Numerical Optimization}

Numerical optimization is necessary when we cannot analytically solve
for the minimizing $x^*$, and still convenient when
we solving analytically is possible but very difficult.
Therefore, this section covers how to compute numerically the argmin
$x^*$ of generic optimization problem
\begin{align}
  \label{statement}
  x^* = \argmin_{x\in\Rn} f(x)
\end{align}
Note that we ignore feasibility constraints. That is, we assume the set
of feasible points for $x^*\in\Rn$ is the entire space $\Rn$.  This is
typically not a problem.  If we need to implement constraints, we just
transform the original function $f$ to $\tilde{f}$ with unbounded
support, find optimal $\tilde{x}^*$ for $\tilde{f}$, and then transform
back to optimal $x^*$ for $f$.

All methods in this section for numerically computing $x^*$ follow the
same basic approach:
\begin{enumerate}
  \item Fix an initial guess for the minimizer $x_0$
  \item Given a guess $x_n$, use a sensible update rule to get a better
    guess, $x_{n+1}$
  \item If we didn't improve much $\lVert x_{n+1}-x_n\lVert<\varepsilon$
    according to some chosen norm $\lVert\,\cdot\,\rVert$ and some
    chosen convergence threshold $\varepsilon>0$, you're done; take
    $x_{n+1}\approx x^*$.

    On the other hand, if $\lVert x_{n+1}-x_n\lVert\geq \varepsilon$,
    return to Step 2, taking $x_{n+1}$ as your guess and repeat
    until convergence.
\end{enumerate}
This section is all about Step 2: Choosing a sensible update rule
such that $f(x_{n+1}) < f(x_n)$ (i.e.\ we get some improvement) and so
that we converge \emph{quickly} to the minumum $x^*$.

All methods implicitly assume that $f$ is differentiable and convex. The
former because we construct update rules from the derivatives of $f$.
Convexity because updating with derivatives of $f$ is only guaranteed to
find \emph{local} minima of $f$ unless the function is convex.


\clearpage
\subsection{Gradient Descent}

Here's the basic logic of gradient descent: The negative of the gradient
$\nabla f(x_n)$ points in the direction of steepest descent. So to
update from $x_n$, compute the gradient at $x_n$, take a step in that
direction, and call the point where you land $x_{n+1}$.

\begin{defn}(Gradient Descent)
Suppose that $f:\Rn\ra\R$ is differentiable, and that we have some
initial guess $x_n$. Then the update step is given by
\begin{align*}
  \label{eq:multigradesc}
  x_{n+1} = x_n - \alpha [\nabla f(x_n)]
\end{align*}
where $\nabla f(x)$ denotes the gradient of $f$ at $x$, and
$\alpha\in\R$ is some scale parameter that controls how aggressively we
update.
Notice that at each iteration, we update all the components of initial
guess vector $x_n$ \emph{simultaneously}---not one-at-a-time.
\end{defn}

\begin{rmk}(Scale Parameter $\alpha$)
Because scale parameter $\alpha$ is common for \emph{all} components of
vector $x$, we should normalize to make sure the entries entries of $x$
lie on a roughly common support.
For example, if the first element of $x^*$ is likely to lie in range
$[-1,1]$ and the second in $[-1000,1000]$, we should transform the
function $f$, optimize the transformed function, then convert the result
back to the original support.  Alternatively, we can apply a weighting
vector $w$ to scale $\alpha$ differently for each element:
\begin{align}
  x_{n+1} = x_n - \alpha w^T [\nabla f(x_n)]
\end{align}
where $w_i$ small when the $i$th element of $x$ has an especially large
support of plausible values.

Lastly, we might also worry about the fact that scale parameter $\alpha$
is constant across \emph{iterations} $n$.
But this is much less of an issue.
Notice how this update procedure self-adjusts in the magnitude of the
update: as you get close to the minimum $x^*$, derivatives approach
zero, and the step size self-adjusts.  And if some coordinates of $x_n$
are closer to the minimum than others, you take smaller steps in those
coordinates to get to $x_{n+1}$.  All of this reduces the chance that
you'll blow past the minimum on an update.
\end{rmk}

We don't take account of any curvature in the objective function (though
Newton's method, detailed below, does).


\clearpage
\subsection{Newton's Method}

Here's the basic logic of Newton's method: Don't minimize the true
(generally nonlinear) objective function $f$; instead, minimize a
sequence of second-order Taylor expansions of $f$, with each
minimization getting you closer and closer to the true minimum.
Said another way, you're simply solving a sequence of \emph{easier}
optimization problems whose solutions get successively closer to the
minimum of the original/true objective function.

\begin{defn}(Newton's Method for Optimization)
\label{defn:newton}
Suppose we have differentiable objective function $f:\Rn\ra\R$ and
starting guess $x_n$.  To update, first define the step-$n$
approximation $f_n$ as a second-order Taylor expansion of true objective
function $f$ about the point $x_n$ (ignoring third and higher order
terms):
\begin{align}
  f_n(x)
  :=
  f(x_n)
  + g_n^T (x-x_n)
  + \frac{1}{2}(x-x_n)^T H_n(x-x_n)
  \label{eq:newtonapprox}
\end{align}
where $g_n:=\nabla f(x_n)$ is the gradient and $H_n:=\nabla^2 f(x_n)$
the Hessian matrix, both at point $x_n$.
We then define our next period guess $x_{n+1}$ as the minimum of the
\emph{approximation} $f_n$, i.e. as that point $x_{n+1}$ which satisfies
$\nabla f_n(x_{n+1})=0$. Based on the above expression,
\begin{align}
  0 &= \nabla f_n(x_{n+1})
  = g_n + H_n(x_{n+1}-x_n)
  \quad\implies\quad
  x_{n+1}
  =
  x_n-H_n^{-1}g_n
  \label{eq:newtonupdate1}
\end{align}
More generally, we can introduce general scale parameter $\alpha$ to get
update scheme
\begin{align}
  x_{n+1} &= x_n - \alpha H_n^{-1} g_n \label{eq:newtonupdate2}
\end{align}
Intuitively, we use the direction implied by the Hessian and
gradient, but scale that direction by $\alpha$.
And note that $\alpha$ need not be constant throughout our entire
optimization procedure.
For example, we might start at $\alpha=1$ for each update step $n$, and
then try out other values by slowly increasing $\alpha$ until we reach
an acceptable improvement.
\end{defn}

\begin{rmk}
Sometimes, rather than finding a minimum of some objective function,
Newton's method is instead introduced as a way to find zeros of some
scalar function $f$---i.e.\ to find $x$ such that $f(x)=0$.
But Newton's root-finding method is very closely related to Newton's
optimization scheme, as we see in the next definition.
\end{rmk}

\begin{defn}(Newton's Method for Finding Roots)
Suppose we want to find a root/zero of scalar function $f:\R\ra\R$ given
initial guess $x_n$.
To update, first define the step-$n$ approximation $f_n$ as a
\emph{first}-order Taylor expansion of $f$ about guess $x_n$ (ignoring
second and higher order terms):
\begin{align*}
  f_n(x) := f(x_n) + f'(x_n)(x-x_n)
\end{align*}
Define the next-period guess $x_{n+1}$ as the root of this simpler
\emph{linear approximation} $f_n$, i.e. the point satisfying
$f_n(x_{n+1})=0$:
\begin{align}
  0 &= f_n(x_{n+1})
  = f(x_n) + f'(x_n)(x_{n+1}-x_n)
  \quad\implies\quad
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\end{align}
The approach is in the same spirit as Definition~\ref{defn:newton}.
And if $f$ is a scalar function, using Definition~\ref{defn:newton} to
find the minimum of $f$ even delivers \emph{exactly} the same result as
using this procedure to find a root of $f'$.
\end{defn}


\clearpage
\subsection{Quasi-Newton Methods}

Newton's method is all well and good with a few parameters, but it's a
pain in many dimensions, which requires computing and storing a large
inverse Hessian matrix $H_n$ every time we update. What's more, we don't
even really care about the Hessian matrix \emph{itself} when we update.
Just look at Equation~\ref{eq:newtonupdate2}. We only care about the
\emph{product} of the Hessian and gradient $H_n^{-1} g_n$. That's a
sufficient statistic for the update.

This line of thinking brings us to Quasi-Newton methods, which try to
find a smart, efficient way to \emph{recursively update} the Hessian
matrix, rather than \emph{recompute} at each step.

\subsubsection{Secant Conditions}

In this section, we derive conditions useful for implementing
quasi-Newton update schemes.

\begin{defn}(Secant Conditions for an Approximate Hessian)
Suppose we want to update from $x_n$ to $x_{n+1}$.
Modify Taylor Expansion~\ref{eq:newtonapprox} (which served as our
previous starting point) in order to form the following new
\emph{quasi-Taylor Expansion} for $f$ that replaces true Hessian $H_n$
with \emph{approximate} Hessian $B_n$:
\begin{align}
  b_n(x) := f(x_n) + (x-x_n)^Tg_n + \frac{1}{2}(x-x_n)^T B_n (x-x_n)
  \label{eq:quasinewtonapprox}
\end{align}
A sensible choice of approximate Hessian $B_n$ is one that satisfies the
\emph{secant conditions}:
\begin{align}
  \underbrace{(x_{n}-x_{n-1})}_{=:s_n}
  &=
  B_n^{-1}\underbrace{(g_{n}-g_{n-1})}_{=:y_n} \label{eq:secant_compact}
\end{align}
Why is this a sensible condition for $B_n$ to satisfy? The next
proposition answers this question.
\end{defn}

\begin{prop}
Approximate Hessian $B_n$ satisfies Secant
Conditions~\ref{eq:secant_compact} if and only if the gradient of
corresponding approximation function $b_n$ (constructed with approximate
Hessian $B_n$) \emph{matches} the gradient of the \emph{true} objective
function $f$ at points $x_n$ and $x_{n-1}$, i.e.
\begin{align}
  \nabla b_n(x_n) &= \nabla f(x_n) = g_n
  \qquad\quad
  \nabla b_n(x_{n-1}) = \nabla f(x_{n-1}) = g_{n-1}
  \label{eq:secant}
\end{align}
And this seems like an uncontroversial and sensible restriction to
impose.
\end{prop}
\begin{proof}
The gradient of approximating function $b_n$ defined in
Equation~\ref{eq:quasinewtonapprox} is given by
\begin{align*}
  \nabla b_n(x) &= g_n + B_n(x-x_n)
\end{align*}
Combine this with the assumption that the secant conditions (as written
in Equations~\ref{eq:secant}) hold to write
\begin{align*}
  g_n - g_{n-1}
  &= \nabla b_n(x_n) - \nabla b_n(x_{n-1}) \\
  &= (g_n + B_n(x_n-x_n)) - (g_n + B_n(x_{n-1}-x_n)) \\
  \implies\quad
  g_n - g_{n-1}
  &= B_n(x_{n}-x_{n-1})
\end{align*}
This last line is exactly the expression for the secant conditions in
Equation~\ref{eq:secant_compact}.
\end{proof}



\subsubsection{BFGS Recursive Update}

Suppose that we have a starting, symmetric Hessian approximation $B^{-1}_n$,
that satisfies the secant conditions above in
Equation~\ref{eq:secant_compact}. How can we update to get $B^{-1}_{n+1}$
such that
\begin{enumerate}
  \item $B^{-1}_{n+1}$ also satisfies the secant conditions,
    $s_{n+1} = B^{-1}y_{n+1}$
  \item $B^{-1}_{n+1}$ is symmetric.
  \item $B^{-1}_{n+1}$ is as close to $B^{-1}_n$ as possible, where
    ``closeness'' is defined using the weighted Frobenius norm.
    \begin{align*}
      \min_{B^{-1}} &\quad \lVert B^{-1} - B^{-1}_n\rVert^2_W
    \end{align*}
\end{enumerate}
The next definition answers this question.

\begin{defn}(BFGS Recursive Update of Approximate Hessian)
Given approximate Hessin $B-n$, the Broyden, Fletcher, Goldfarb, Shanno
(BFGS) is given by
\begin{align}
  B_{n+1}^{-1} = B^{-1}_n
  + \frac{(y_n^T s_n + y_n^T B^{-1}_n y_n)(s_n s_n^T)}{(y_n^T s_n)^2}
  - \frac{s_n (B^{-1}_n y_n)^T + B^{-1}_n y_n s_n^T}{y_n^T s_n}
  %H = H0 + (1+(dg'*Hdg)/dgdx)*(dx*dx')/dgdx - (dx*Hdg'+Hdg*dx')/dgdx;
  \label{eq:bfgs}
\end{align}
Some math can show that this update scheme satisfies desired Properties
1-3 above, and that $B^{-1}_{n+1}$ is positive semi-definite whenever
$B^{-1}_n$ is. So it's not a condition we need to impose a priori, since
it follows from this updating procedure.
To choose the initial approximate Hessian $B_0$, we have a few options:
\begin{enumerate}
  \item Compute the true Hessian $H_0$, and then just apply this
    updating scheme from there.
  \item Use a diagonal approximation to the true Hessian.
  \item Use a scalar multiple of the identity matrix.
\end{enumerate}
\end{defn}

Given this, we don't have to compute the true Hessians $H_n$ every time.
Instead, we have reasonable (i.e.\ satisfying the secant conditions)
approximations $B_n$ that are updated via recurrence relation.



%\section{Linear Programming}

%\subsection{Introduction to Linear Optimization}

%\subsubsection{Definition of a Linear Program}

%Linear optimization, also known as \emph{linear programming},
%consists of a linear \emph{objective function} and \emph{linear
%inequalities}:
%\begin{align*}
   %\max \; \;& c_1 x_1 + \cdots + c_n x_n \\
   %\text{s.t.} \; \;&a_{11} x_1 + \cdots + a_{1n} x_n \leq b_n \\
   %& \qquad\qquad \vdots \\
   %&a_{m1} x_1 + \cdots + a_{mn} x_n \leq b_m
%\end{align*}
%So we want to find the values of the \emph{variables}
%$x_1, \ldots, x_n \in \mathbb{R}$ such that
%the objective function is maximized while still satisfying the
%linear inequalities. Using more convenient matrix notation,
%our linear program above can be rewritten more compactly as
%\begin{equation}
   %\label{lp}
   %\max \{ c^T x \; : \; x \in \mathbb{R}^n , \; Ax \leq b \}
%\end{equation}
%If we want to find the minimum instead of the max, we can easily
%use the fact that the minimum of some set $S$ ($\min S$) is equivalent
%to $-\max S$.

%\subsubsection{Feasability, Optimality, Boundedness}

%We call a solution $x \in \mathbb{R}^n$ \emph{feasible} if $x$
%satisfies all the linear inequalities.  We call a linear program
%feasible if it has a feasible solution.
%\\
%\\
%A feasible solution $s \in \mathbb{R}^n$ is an \emph{optimal} solution
%of a linear program if $c^T x \geq c^T y $ for all $y\in \mathbb{R}^n$.
%\\
%\\
%A linear program is \emph{bounded} if there exists a constant $M\in
%\mathbb{R}$ such that $c^T x \leq \mathbb{R}$ for all feasible
%$x \in \mathbb{R}^n$.

%\subsubsection{Linear Algebra Defintions}

%Recall that for $A \in \mathbb{R}^{m\times n}$, the \emph{kernel}
%of $A$, the \emph{image} of $A$, and the \emph{rowspace}
%of $A$ are defined
%\begin{align*}
   %\text{ker}(A) &= \{ x \in \mathbb{R}^n \; | \; Ax =0 \} \subset
      %\mathbb{R}^n\\
   %\text{im}(A) &= \{ A x \; | \; x\in \mathbb{R}^n \} \subset
      %\mathbb{R}^m\\
   %\text{rowspace}(A) &= \{ y \in \mathbb{R}^n \; | \; \exists \lambda
   %\in \mathbb{R}^m \text{ s.t. } A^T \lambda = y \}
%\end{align*}
%Note, the rowspace is just the set of all linear combinations of the
%rows of $A$.
%\\
%\\
%{\sl Result:}
%With these definitions in hand, we can show that a solution to the
%linear program in Equation \ref{lp} is feasible and unbounded if
%$b \in \text{im}(A)$ and if $c \in \text{ker} A \setminus \{ 0 \}$.
%\begin{proof}
   %First, $b \in \text{im}(A)$ proves feasibility. It also implies that
   %$\exists\; x^* \in \mathbb{R}^n$ such that $A x^* = b$. This
   %and the fact that $c \in \text{ker}(A)\setminus \{0\}$
   %allows us to write
   %\begin{align*}
      %A(x^* + \lambda c) &= Ax^* + \lambda A c \\
      %&= b + 0 = b
   %\end{align*}
   %which proves $x^* + \lambda c$ is a feasible solution, satisfying
   %all of the linear inequalities.
   %\\\\
   %So to finish up the unboundedness proof, we suppose the opposite:
   %that the linear program is bounded:
      %\[ c^T y \leq M \qquad \forall y \in \mathbb{R}^n \]
   %Since $x^* + \lambda c$ is feasible, then it should also be bounded:
   %\begin{align*}
      %\Rightarrow \quad c^T (x^* + \lambda c ) &\leq M\\
      %c^T x^* + \lambda c^T c \leq M
   %\end{align*}
   %Now since $c \neq 0$, we know that $c^T c$ will be greater than
   %0. Now let's rearrange and choose
      %\[ \lambda \geq \frac{M - c^T x^*}{c^T c} \]
   %in which case $c^T ( x^* + \lambda c) > M$.
%\end{proof}



%\newpage
%\subsection{The Geometry of Linear Programming}

%\subsubsection{General Definitions}

%{\sl Polyhedron:}
%A set $P$ of vectors in $\mathbb{R}^n$ is a polyhedron
%if $P=\{ x\in \mathbb{R}^n \; | \; Ax \leq b \}$ for some matrix $A$
%and some vector $b$.  By our definition of a linear program in
%Equation \ref{lp}, we see that the set of feasible solutions is
%a polyhedron.\footnote{Just one quick note for generality: The empty
%set, $\emptyset$, is also a polyhedron as $\emptyset = \{ x \in
%\mathbb{R}^n \; |\; \mathbf{0}^T x \leq -1 \}$,
%satisfying the definition of a polyhedron.}
%\\
%\\
%{\sl Half space:} This is defined as $\{ x \in \mathbb{R}^n
   %\; | \; a^T x \leq \beta \}$, where $a \in \mathbb{R}^n \setminus
   %\mathbf{0}$.
%\\\\
%{\sl Hyperplane:} This is defined as $\{ x \in \mathbb{R}^n
   %\; | \; a^T x = \beta \}$, where $a \in \mathbb{R}^n \setminus
   %\mathbf{0}$.
%\\\\
%{\sl Valid:} An inequality $a^T x \leq \beta$ is valid for a
   %polyhedron, $P$, if each $x^* \in P$ satisfies $a^T x^* \leq
   %\beta$.
%\\\\
%{\sl Active:} An inequality $a^T x \leq \beta$ is active
   %at $x^* \in \mathbb{R}^n$ if $a^T x^* = \beta$.


%\subsubsection{Vertices}

%We'll offer three equivalent definitions, each of which offers some
%different intuition or an additional operational advantage:
%\begin{enumerate}
   %\item A point $x^* \in P$ is a \emph{vertex} of $P$ if
      %there exists an inequality $a^T x \leq \beta$ such that
      %\begin{enumerate}
	 %\item $a^T x \leq \beta$ is valid for $P$.
	 %\item $a^T x \leq \beta$ is active at $x^*$ and not active
			%at any other point in $P$.
      %\end{enumerate}
   %\item There's also an equivalent definition which says $x^* \in P$
      %is a vertex if and only if there exists a vector $c \in
      %\mathbb{R}^n$ such that $x^*$ is the unique optimal solution of
      %the linear program $\max \{ c^T x \; | \; x \in P \}$.
   %\item Suppose we have $x^* \in P = \{ x \in \mathbb{R}^n \; | \;
      %Ax \leq b \}$ as a vertex of $P$.
      %Then this vertex satisfies a smaller number of active contraints
      %(relative to all of the constraints that define the linear
      %program and $P$). Therefore, we express those active constraints
      %that we want to enforce with $\bar{A}x \leq \bar{b}$.

      %Now since $x^*$ is a vertex satisfying those
      %certain active constraints, that means $x^*$ is the unique
      %solution of $\bar{A} x = \bar{b}$. That's equivalent to the
      %statement that $\text{rank}(\bar{A}) = n$, which is equivalent
      %to the columns of $\bar{A}$ being linearly independent.

      %So if we suspect that $x^*$ is a vertex of our linear program,
      %then we examine the subsystem, $\bar{A} x \leq \bar{b}$,
      %of our LP that $x^*$ satisfies with equality, and we look
      %at the rank of $\bar{A}$. If $x^*$ really is a vertex of $P$,
      %the the rank of $\bar{A}$ equals $n$ and the columns will
      %be linearly independent.

%\end{enumerate}











\end{document}


%Suppose that an infinitely-lived agent gets utility in period $t$ from
%consuming $y_t$. The variable $y_t$ is called the \emph{control}; its
%value is chosen by the agent at time $t$ given some pre-determined
%value, $x_t$, called the \emph{state}.  Between periods, the future
%state $x_{t+1}$ is a deterministic function of $x_t$ and $y_t$, i.e.
%\begin{align}
  %\label{xevol}
  %x_{t+1} = g(x_t, y_t)
  %\qquad
  %\text{$x_0$ given}
%\end{align}
%Typically, the function $g$ will involve some tradeoff between $y_t$ and
%$x_{t+1}$. For example, given $x_t$, the more $y_t$ (perhaps
%consumption) an agent chooses today, the less $x_{t+1}$ he gets next
%period (perhaps capital from investment).

%Next, our problem is to maximize discounted lifetime utility:
%\begin{align}
  %\label{toughproblem}
  %\sum^\infty_{t=0} \beta^t u(y_t)
  %\qquad \beta\in (0,1)
%\end{align}
%by choosing a feasible and optimal path $\{ y_t \}_0^\infty$, again
%subject to an initial condition $x_0$.  Here,
%``optimal'' means utility maximizing. Typically, feasibility
%will be determined by some sort of resource constraint
%(such as $y_t\leq x_t$ or $y_t\leq f(x_t)$) or some
%restriction to non-negativity of the sequence(s) $\{ y_t
%\}$ and/or $\{x_t\}$.

%To make things simpler, we will now define a bit of
%notation for feasibility. At time $t$, the set of all
%feasible $y_t$ is denoted $\mathcal{F}_t$. Therefore, $y_t$
%is feasible if $y_t\in \mathcal{F}_t$. More generally, a
%sequence $\{y_t\}^\infty_0$, abbreviated $\{y_t\}$, is
%feasible if $\{y_t\} \subset \mathcal{F}$.


%\subsection{Value Function and Bellman Equation}

%For reasons that we'll see soon, we'll want to define the
%Bellman Equation for our optimization problem. That first
%entails defining the value function, $v^*$:
%\begin{align*}
  %v^*(x_0) = \sup_{ \{y_t\} \subset \mathcal{F}}
  %\sum^\infty_{t=0} \beta^t u(y_t)\;
  %\qquad
  %\text{$x_0$ given}
%\end{align*}
%In words, the value function $v^*(x_0)$ describes that
%maximum discounted value that can be obtained from initial
%state $x_0$.

%With the value function defined, we can now write the
%Bellman Equation, which looks very similar to what we had
%for the Shortest Path problem described above:
%\begin{align}
  %v^*(x_t) &= \max_{y_t \in \mathcal{F}_t}
  %\left\{ u(y_t) + \beta \; v^*\big(g(x_t, y_t)\big) \right\} \\
    %\text{By (} \quad \Leftrightarrow \quad
  %&= \max_{y_t \in \mathcal{F}_t}
    %\left\{ u(y_t) + \beta v^*(x_{t+1}) \right\} \notag
%\end{align}
%Intuitively, this equation says that you maximize the value from the
%current state, $x_t$, by trading off current reward, $u(y_t)$, in
%exchange for the discounted future value of the resulting state,
%$\beta v^*(x_{t+1})$.

%Since we've formulated the problem in Bellman terms, it's
%clear that we'll eventually want to apply dynamic
%programming methods to solve for the value function $v^*$.
%That's what we did in the Shortest Path problem above; it's
%likely what we'll want to do again.


%\emph{However}, solving for the value function $v^*$ above
%doesn't \emph{quite} give us what we want. We don't just
%want to know the maximum lifetime utility of an agent; we
%want to know what path they will choose for the control.
%In other words, we want to know $\{y_t\}_0^\infty$.

%To solve for this sequence, we now discuss the policy function approach,
%which allows us to answer this question while also redefining the
%motivating problem and the Bellman Equation in simpler, more practical
%terms.


%\subsection{The Policy Function Approach}

%To start, note that even if we can solve for the value function,
%$v^*(x_0)$ (and we can't yet), our real goal of directly specifying the
%optimal path $\{y_t\}_0^\infty$ is hard. It amounts to selecting a point
%in infinite-dimensional space.  So we'll do something easier.
%Specifically, we'll seek a \emph{policy function}---some function or
%``decision rule'' $\sigma$ that depends only upon the current state such
%that
%\begin{align}
  %\label{markov}
  %y_t = \sigma(x_t) = \sigma(g(x_{t-1}, y_{t-1}))
  %\qquad \sigma \in \Sigma
%\end{align}
%where $\Sigma$ is the set of all feasible policies. So rather than
%trying to say \emph{what} an agent will choose, we'll describe
%\emph{how} that agent will choose.

%Now a well-known result from dynamic programming says that, in the types
%of problems we want to consider, an optimal consumption sequence must be
%Markov just as in Equation \ref{markov} above, so that a policy function
%$\sigma$ will, indeed, exist.  This function $\sigma$ will also be
%time-homogeneous, so it will remain the same for all $t$.  This is all
%just to say that we're doing something reasonable that will allow us to
%find the optimal path $\{y_t\}$.

%\subsubsection{Adapting the Problem and Objective}

%So first, we can rewrite our motivating problem as
%expressed in () and () in
%terms of the policy function:
%\begin{align}
  %\label{policyapproach}
  %\max_{\sigma \in \Sigma}
  %&\left\{ \sum^\infty_{t=0} \beta^t u(\sigma(x_t)) \right\}\\
  %\text{where} &\quad
  %x_{t+1} = g(x_t, \sigma(x_t))
  %\qquad \text{$x_0$ given}
  %\notag
%\end{align}
%Since $g$ is assumed deterministic, we see that the only
%unknown in the problem written above is the policy
%function, $\sigma$. We have everything else we need.

%This is the crux of the policy function approach.
%Rather than having our unknown be a utility-maximizing
%infinite sequence $\{y_t\}^\infty_0$, we find a single
%function $\sigma$ that provides a map or decision rule to
%go from the state ($x_t$) to the optimal choice for the
%control ($y_t$).  This choice then propagates to the next
%period via deterministic $g$ to define the state next
%period.

%\subsubsection{Adapting the Value Function}

%Now that we've translated the basic problem into policy
%function terms, let's see what that means for the value
%function and Bellman equation we defined above.

%So first, for each feasible policy $\sigma\in
%\Sigma$, define a \emph{policy value
%function}: \begin{align*}
  %v_\sigma(x_0) :=
  %\left\{ \sum^\infty_{t=0} \beta^t u(\sigma(x_t)) \right\}
%\end{align*}
%This gives the utility conditional on a policy and an initial
%condition.  Our goal will be to choose an \emph{optimal} policy,
%i.e. a policy function that yields the highest utility over the
%set of all $\sigma \in \Sigma$.

%Therefore, our definition of the \emph{value function} $v^*$ can
%be rewritten as
%\begin{align}
  %\label{valfn}
  %v^*(x_0) := \sup_{\sigma\in\Sigma} \; v_\sigma(x_0)
%\end{align}
%A policy is called ``optimal'' if it attains the supremum in
%(\ref{valfn}) for all $x_0$.

%\subsubsection{Policies and the Bellman Equation}

%Finally, let's finish off the last link in the chain and consider the
%Bellman Equation in terms of a policy function. As we'll see in the next
%section, we'll actually

%First, let's define a concept: Consider some continuous function $w$.
%We say that a policy $\sigma\in\Sigma$ is \emph{$w$-greedy} if
%$\sigma(x)$ solves
%\begin{align*}
  %\max_{y_t \in F_t}
  %\left\{ u(y_t) + \beta w\big(g(x_t, y_t)\big)\right\}
%\end{align*}
%There might be many policies that solve this; therefore, there might be
%a set of $w$-greedy policies given any initial state $x_t$ and functions
%$u$, $g$, and $w$.

%Now this works for \emph{any} continuous function $w$. The function $w$ could even
%be our value function $v^*$, provided it's continuous. We'll talk about
%the conditions under which

%\subsection{Dynamic Programming Solution}

%In this subsection, we now solve the problem as formulated in policy
%function terms since, for the reasons given above, that will be
%easiest.







%%%% APPPENDIX %%%%%%%%%%%

% \appendix

%\cite{LabelInSourcesFile}
%\citep{LabelInSourcesFile} Cites in parens
%\nocite{LabelInSourceFile} includes in refs w/o specific citation
%\bibliographystyle{apalike}
%\bibliography{sources.bib} where sources.bib is file






%% APPPENDIX %%
% \appendix

% Dynamic programming
% - Continuous time dynamic programming and hamiltonian
% - Bellman operator
%   - Contraction: Monotonicity, Discounting
%   - Fixed point and completeness of the space
% - Properties of the value function
%   - bounded
%   - cts
% - Euler equations
% - Finish v_B -> v^*
% - Envelope

%Basic results about Derivatives
  %- Implicit function
  %- inverse function
  %- Leibnitz
  %- Rn\rightarrow\Rm
% - Function spaces as an up front section
% - Norms in an up front section with basics

%\subsection{Shortest Path Problem}

%To get the intuition for the Bellman Equation, we will consider the shortest path problem, which has the following features:
%\begin{enumerate}

    %\item You want to get from point $1$ to $N$.

    %\item There are a number of possible nodes in between $1$ and $N$,
	%permitting a number of different possible paths you might take.

    %\item The paths from node to node have different costs.

%\end{enumerate}

%Therefore, we want a general solution to the problem that
%provides us with the \emph{least cost} path to travel from
%$1$ to $N$.


%\subsection{The Bellman Equation}

%Suppose that at every single node, $i \in \{1, \ldots, N\}$, we \emph{knew}
%the least-cost path to get from node $i$ to node $N$. Of course we don't,
%unless we're trivially already at node $N$, but just suspend disbelief for
%a second and suppose we do.

%Now since we know the least-cost paths, we know the ``best-case'' cost to
%get from node $i$ to $N$, for all $i$. Denote that ``best-case'' cost
%by $J(i)$. Now it makes sense that this function
%$J$, for each $i$, should also satisfy what's called the
%\emph{Bellman Equation}:
%\begin{equation}
    %\label{bellman}
    %J(i) = \min_{j \in F_i} \left\{ c(i, j) + J(j) \right\}
%\end{equation}
%In words, this equation says
%\begin{enumerate}
    %\item To find the best-case cost to go from $i$
	%to $N$, consider the set of nodes that you can reach from node $i$,
	%denoted $F_i$.

    %\item Next, consder the cost of jumping from node $i$ to node $j$,
	%denoted $c(i,j)$.

    %\item Finally, since you know the best-case cost,
	%$J(j)$ for $j \in F_i$, pick the minimum of $c(i,j)$ plus $J(j)$.
%\end{enumerate}

%The best solution to our problem must, therefore, satisfy Equation
%\ref{bellman}.

%\subsection{Solving for the Minimum Cost Function, $J$}

%We now detail the standard algorithm to find $J$.
%\begin{enumerate}
    %\item For some large $M$, set
	%\begin{equation}
			%J_0(i) = \begin{cases} M & i \neq N \\
						%0 & \text{otherwise}
				%\end{cases}
	%\end{equation}

    %\item Next, set
	%$J_{n+1}(i) = \min_{j \in F_i} \left\{ c(i,j) + J_n(j)\right\}$
	%for all $i$.

    %\item If $J_{n+1} \neq J_n$, increment $n$ and return to step 2.

%\end{enumerate}
%The solution thus propagates back from the destination node, $N$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% SAMPLE CODE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %% VIEW LAYOUT %%

        \layout

    %% LANDSCAPE PAGE %%

        \begin{landscape}
        \end{landscape}

    %% BIBLIOGRAPHIES %%

        \cite{LabelInSourcesFile}  %Use in text; cites
        \citep{LabelInSourcesFile} %Use in text; cites in parens

        \nocite{LabelInSourceFile} % Includes in refs w/o specific citation
        \bibliographystyle{apalike}  % Or some other style

        % To ditch the ``References'' header
        \begingroup
        \renewcommand{\section}[2]{}
        \endgroup

        \bibliography{sources} % where sources.bib has all the citation info

    %% SPACING %%

        \vspace{1in}
        \hspace{1in}

    %% URLS, EMAIL, AND LOCAL FILES %%

      \url{url}
      \href{url}{name}
      \href{mailto:mcocci@raidenlovessusie.com}{name}
      \href{run:/path/to/file.pdf}{name}


    %% INCLUDING PDF PAGE %%

        \includepdf{file.pdf}


    %% INCLUDING CODE %%

        %\verbatiminput{file.ext}
            %   Includes verbatim text from the file

        \texttt{text}
            %   Renders text in courier, or code-like, font

        \matlabcode{file.m}
            %   Includes Matlab code with colors and line numbers

        \lstset{style=bash}
        \begin{lstlisting}
        \end{lstlisting}
            % Inline code rendering


    %% INCLUDING FIGURES %%

        % Basic Figure with size scaling
            \begin{figure}[h!]
               \centering
               \includegraphics[scale=1]{file.pdf}
            \end{figure}

        % Basic Figure with specific height
            \begin{figure}[h!]
               \centering
               \includegraphics[height=5in, width=5in]{file.pdf}
            \end{figure}

        % Figure with cropping, where the order for trimming is  L, B, R, T
            \begin{figure}
               \centering
               \includegraphics[trim={1cm, 1cm, 1cm, 1cm}, clip]{file.pdf}
            \end{figure}

        % Side by Side figures: Use the tabular environment




