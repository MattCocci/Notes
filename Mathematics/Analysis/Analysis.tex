\documentclass[12pt]{article}

\author{Matthew Cocci}
\title{\textbf{Analysis}}
\date{\today}

%% Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{fullpage}
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\pagestyle{fancy} 
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt} 
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt} 
    %---Width of the line

%\setlength{\headsep}{0.2in}    
    %---Distance from line to text
            

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}

\DeclareMathOperator*{\essup}{ess\,sup}

%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure} 
    %---For plotting multiple figures at once
\graphicspath{ {Figures/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref} 
\hypersetup{	
    colorlinks,		
        %---This colors the links themselves, not boxes
    citecolor=black,	
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{verbatim} 
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in 
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},% 
            %---Size of the numbers
        numbersep=9pt,% 
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc. 


%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{enumitem} 
    %---Has to do with enumeration	
\usepackage{appendix}
%\usepackage{natbib} 
    %---For bibliographies
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%\newcommand{\nameofcmd}{Text to display}
\newcommand*{\Chi}{\mbox{\large$\chi$}} %big chi



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\begin{document}

\maketitle

\tableofcontents %adds it here

\newpage
\section{The Riemann-Stieltjes Integral}

This definition of the integral was made rigorous in the 1800s by
Riemann, Darboux, and Stieltjes.  It's an intuitive way to define the
area under a curve, and it works well with numerical integration
(approximations). \emph{However}, it is incomplete in the sense that
there are functions of interest that we cannot integrate in a Riemann
sense but can in a Lebesgue sense, which retains Riemann as a special
case.

Throughout this section, unless otherwise noted, we'll largely stick to bounded functions that are univariate from a compact interval to $\mathbb{R}$: 

    \[ f: [a, b] \rightarrow \mathbb{R} \]

    We'll begin by discussing \emph{partitions} of that interval $[a,b]$ into smaller pieces, from which we'll construct sums that approximate the area under the curve.  This will lead us to a definition of the Riemann Integral.  Then, we'll generalize and allow the \emph{weight} we place on the sub-intervals (when summing over the entire interval) to vary, which will give us the Riemann-Stieltjes integral. From there, we discuss the relationships between the approximating sums and the integral.

\subsection{Partitions}

\begin{defn} A \emph{partition}, $P$, is an ordered tuple representing a finite sequence on the interval $[a,b]$,
    \[ a = x_0 < x_1 < \cdots < x_n = b
        \qquad \text{with} \quad \Delta x_i := x_i - x_{i-1}
    \]
\end{defn}

\begin{defn} The \emph{norm} of a partition $P$, sometimes called ``mesh $P$'' represents
    \[ || P || = \text{norm}(P) := \max_i |x_i - x_{i-1}| =
        \max_i |\Delta x_i| \] 
\end{defn}

\begin{defn} $Q$ is a \emph{refinement} of $P$ if $Q \supset P$ where $Q$ and $P$ are both partitions of $[a,b]$. $Q$ the intervals \emph{finer}.
\end{defn}

\begin{defn} For two partitions, $P_1$ and $P_2$, their \emph{common refinement} is $P_1 \cup P_2$.
\end{defn}

\begin{defn} A \emph{tagged partition} is a couplet $(P,T)$, where $P$ is some partition $\{x_0, \ldots, x_n\}$ and $T$ is a set of evaluation points, $\{t_1, \ldots, t_n\}$, for the function $f$  such that
    \[ x_{i-1} \leq t_i \leq x_i \]
\end{defn}

\begin{note} We will now generalize to allow weighting of the sub-intervals within the partition, defined for an \emph{increasing} function $\alpha: [a,b] \rightarrow \mathbb{R}$, where 
    \[ \Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}) > 0 \]
This is the main difference between the plain Riemann sum and integral, versus the Riemann-Stieltjes (RS) sum and integral.  The latter retains the former as a special case by taking $\alpha(x) = x$.  Therefore, the RS version is just a generalization of Riemann, weighting the contribution of the sub-intervals to the total sum/integral by the function $\alpha$, \emph{not} by the length of the sub-interval.
\end{note}

\subsection{Sum Definitions}

We now define the various sums approximating the Riemann and RS integrals.

\begin{defn} We define the upper and lower \emph{Darboux Sums}, respectively, as follows 
    \begin{align*}
        U(f,P) &:= \sum^n_{i=1} M_i(f)(x_i - x_{i-1}) 
            \quad\text{where} \quad 
            M_i(f) := \sup_{x \in [x_i, x_{i-1}]} f(x)\\
        L(f,P) &:= \sum^n_{i=1} m_i(f)(x_i - x_{i-1})
            \quad\text{where} \quad 
            m_i(f) := \inf_{x \in [x_i, x_{i-1}]} f(x) 
    \end{align*}
\end{defn}

\begin{defn} Given $f$ (bounded) and tagged partition $(P,T)$ we define the \emph{Riemann Sum} as 
    \begin{equation}
        S(f,P,T) := \sum^n_{i=1} f(t_i) (x_i - x_{i-1})
    \end{equation}
\end{defn}

\begin{defn} 
\label{RSD}
We define the upper and lower \emph{RS-Darboux Sums}, respectively, as follows 
    \begin{align*}
        U_\alpha(f,P) &:= \sum^n_{i=1} M_i(f) \Delta \alpha_i
            \quad\text{where} \quad 
            M_i(f) := \sup_{x \in [x_i, x_{i-1}]} f(x) \\
        L_\alpha(f,P) &:= \sum^n_{i=1} m_i(f)\Delta \alpha_i
            \quad\text{where} \quad 
            m_i(f) := \inf_{x \in [x_i, x_{i-1}]} f(x) 
    \end{align*}
\end{defn}

\begin{defn} 
\label{RSS}
Given $f$ (bounded) and tagged partition $(P,T)$ we define the \emph{Riemann-Stieltjes Sum} as 
    \begin{equation}
        S_\alpha(f,P,T) := \sum^n_{i=1} f(t_i) \Delta \alpha_i
    \end{equation}
\end{defn}

\subsection{Sum Relations}

\begin{rmk} Clearly, by Definitions \ref{RSD} and \ref{RSS}, for all $T$ associated with $P$
    \[ L_\alpha(f,P) \leq S_\alpha(f,P,T) \leq U_\alpha(f,P) \]
\end{rmk}

\begin{thm} 
\label{sumineq}
    If $Q \supset P$, i.e. if $Q$ refines $P$, then
    \[ L_\alpha(f,P) \leq L_\alpha(f,Q) \leq U_\alpha(f,Q) 
        \leq U_\alpha(f,P) \]
\end{thm}
\begin{proof} The proof proceeds by induction. Assume that $Q = P \cup \{x^*\}$, a single point. Then $x^*\in [x_{i-1}, x_i]$ for some interval, and it's easy show the relation from there.
\end{proof}

\begin{thm} 
\label{pineq}
    For all partitions $P_1, P_2$, 
    \[ L_\alpha(f,P_1) \leq U_\alpha(f,P_2) \]
\end{thm}
\begin{proof} Let $Q = P_1 \cup P_2$. Then by Theorem \ref{sumineq}, 
    \[ L_\alpha(f,P_1) \leq L_\alpha(f,Q) \leq U_\alpha(f,Q) 
        \leq U_\alpha(f,P_2) \]
\end{proof}


\subsection{Definition of $\mathscr{R}_\alpha([a,b])$}


\begin{defn} We define \emph{the upper and lower Riemann-Stieltjes integrals}, respectively, in terms of the RS-Darboux Sums
    \begin{align*} 
        \overline{\int^b_a} f d\alpha &:= \inf_P U_\alpha(f,P) \\
        \underline{\int^b_a} f d\alpha &:= \sup_P L_\alpha(f,P)
    \end{align*}
We can assert that the sup and inf exist because $f$ is bounded. From Theorem \ref{pineq}, it's clear that $\underline{\int} f d\alpha \leq \overline{\int} f d\alpha$.
\\
\\
\emph{Note}: This means the value of the upper (lower) integral might not be \emph{in} the set of upper (lower) sums, just like a real number will not be in a sequence of approaching rationals. 
\end{defn}

\begin{defn} We say  $f$ is \emph{Riemann-Stieltjes integrable} on $[a,b]$---i.e. $f \in \mathscr{R}_\alpha([a,b])$---if 
    \[ \overline{\int^b_a} f d\alpha  =
        \underline{\int^b_a} f d\alpha 
        := {\int^b_a} f d\alpha
        \]
\end{defn}

\begin{ex} A case where $f \notin \mathscr{R}_\alpha([a,b])$ is where 
    \[ f(x) = \chi_{\mathbb{Q}\cap [a,b]}
        =\begin{cases} 1 & $x$\text{ rational} \\
            0 & $x$\text{ irrational} \end{cases}\]
for $x\in[0,1]$. In this case, the upper integral is always 1, while the lower integral is always zero.
\end{ex}

\begin{thm}
\emph{(Riemann's Condition)}
\label{riemcond}
$f \in \mathscr{R}_\alpha([a,b])$ if and only if there exists a partition $P$ such that the upper and lower RS-Darboux sums can be made arbitrarily close given that $P$, i.e.
    \[  U_\alpha(f,P) - L_\alpha(f,P) \leq \varepsilon \]
\end{thm}
\begin{proof} First, the $\Leftarrow$ direction. Use Theorems \ref{sumineq} and \ref{pineq}. It's obvious. Next, for the $\Rightarrow$ direction. By the definition of the RS integral and the RS-Darboux sums, 
\begin{equation}
    \label{p1}
    U_\alpha(f,P_1) < \int^b_a f d\alpha + \varepsilon/2 \qquad
    L_\alpha(f,P_2) > \int^b_a f d\alpha - \varepsilon/2
\end{equation}
Taking the common refinement, and using Theorem \ref{sumineq}, we get that 
\begin{align*}
    U_\alpha(f,P_1 \cup P_2) - L_\alpha(f,P_1 \cup P_2) &\leq 
    U_\alpha(f,P_1) - L_\alpha(f,P_2) \\
    &= \left(\int^b_a f d\alpha + \varepsilon/2\right) - 
        \left(\int^b_a f d\alpha -\varepsilon/2  \right) \\
    \text{By Expression \ref{p1}}\qquad &\leq \varepsilon/2 + \varepsilon/2
\end{align*}
\end{proof}

\subsection{Properties of $\mathscr{R}_\alpha([a,b])$}

\begin{thm} 
    \label{contthm}    
The set of all continuous functions on $[a,b]$, denoted ${C}([a,b])$, is a subset of $\mathscr{R}([a,b])$.\footnote{Note that we need continuity over \emph{all} of $[a,b]$. If $f$ isn't continuous at the endpoints, we can't apply this proof directly. We instead need to split up $[a,b]$ into ``good'' and ``bad'' parts.}
\end{thm}
\begin{proof} 
By Theorem \ref{riemcond}, we want to show that, for all $\epsilon>0$, there exists a partition $P$ such that 
\begin{align*}
    U_\alpha(f,P) - L_\alpha(f,P) < \epsilon  \\
    \Leftrightarrow
    \sum^n_{i=1} (M_i(f) - m_i(f)) \Delta\alpha_i < \epsilon 
\end{align*}
Now since $f$ is continuous on a compact interval, $[a,b]$, $f$ is \emph{uniformly continuous} on $[a,b]$. That means, given our $\epsilon$ from above, 
    \[ \exists \; \delta >0 \quad \text{ s.t. } \quad
        |x_{i} - x_{i-1}| < \delta \quad \Rightarrow \quad
        |f(x_{i}) - f(x_{i-1})| < \frac{\epsilon}{\alpha(b)-\alpha(a)} \]
So we can choose $P$ such that that $||P|| < \delta$.  This means that 
\begin{align*}
    \sum^n_{i=1} [M_i(f) - m_i(f)] \Delta\alpha_i &\leq
    \sum^n_{i=1} \frac{\epsilon}{\alpha(b)-\alpha(a)} 
        \Delta\alpha_i  
    =\frac{\epsilon}{\alpha(b)-\alpha(a)}\sum^n_{i=1}  
        \Delta\alpha_i \\
    &\leq\frac{\epsilon}{\alpha(b)-\alpha(a)} \cdot [\alpha(b)-\alpha(a)] = \epsilon
\end{align*}
\end{proof}
Now for some useful properties of the set of Riemann-Stieltjes integrable functions. Consider $f,g \in \mathscr{R}_\alpha([a,b])$
and $c \in \mathbb{R}$.
\begin{itemize}
    \item \textbf{Linearity}: $f+g \in \mathscr{R}_\alpha([a,b])$
        and $cf \in \mathscr{R}_\alpha([a,b])$, with 
        \[ \int^b_a cf \; d\alpha = c \int^b_a f\; d\alpha  
            \qquad \text{and} \qquad 
            \int^b_a f+g\; d\alpha = \int^b_a f \;d\alpha + 
            \int^b_a g\; d\alpha 
        \]
    \item \textbf{Subsets}: If $[c,d]\subset[a,b]$, then $f \in \mathscr{R}_\alpha([c,d])$.
    \item \textbf{Splitting the Interval}: If $c \in [a,b]$, then 
        \[ \int^b_a f d\alpha = \int^c_a f d\alpha 
            + \int^b_c f d\alpha \]
    \item \textbf{Monotonicity}: If $f,g,h \in 
        \mathscr{R}_\alpha([a,b])$, $f\geq0$, and
        $g \leq h$ on $[a,b]$, then
            \[ \int^b_a f d\alpha \geq 0 \qquad
                \int^b_a g \; d\alpha \leq 
                \int^b_a h \; d\alpha  \]
    \item \textbf{Compositions}: Suppose that $f \in 
        \mathscr{R}_\alpha([a,b])$ and
        $g: \mathbb{R}\rightarrow\mathbb{R}$ is 
        continuous. Then $g \circ f\in\mathscr{R}_\alpha([a,b])$
        \begin{proof}
            Since $f$ is bounded, 
            let $m:=\inf_{[a,b]}f$ and $M:=\sup_{[a,b]}f$.
            Then since $g$ is continuous on the compact 
            interval $[m,M]$, $g$ is uniformly continuous.
            As a result, for all $\varepsilon>0$
            there then exists $\delta>0$ such that 
            \begin{equation}
                \label{cont}
                 |u- v| \leq 2K\delta \quad \Rightarrow
                    \quad |g(u) - g(v)| \leq 
                    \frac{\varepsilon}{2\left[\alpha(b)-\alpha(a)
                    \right]}
            \end{equation}
            where $K$ is some positive, finite number. But what
            is $K$ exactly? Well,
            $g$ also happens to be bounded because it is
            continuous on a compact interval, thus $|g(x)| \leq K$
            for some $K$ and all $x$. So we'll go with that.
            \\
            \\
            Next, since we know that $f \in 
            \mathscr{R}_\alpha([a,b])$, 
            there exists a partition $P$ such that
            \begin{equation}
                \label{forf}
                 U_\alpha(f,P)- L_\alpha(f,P) \leq \epsilon\cdot
                    \delta
            \end{equation}
            %And, considering the building blocks $M_i(f)$ and 
            %$m_i(f)$, by the uniform continuity of $g$, we
            %also have for any $i$, for all $\varepsilon$, there must
            %be a $\delta$ such that
            %\begin{equation}
            %    \label{compcont}
            %    M_i(f) - m_i(f) < 2K{\delta}
            %        \quad \Rightarrow \quad 
            %        M_i(g\circ f) - m_i(g\circ f) \leq 
            %        \frac{\varepsilon}{2[\alpha(b)-\alpha(a)]}
            %\end{equation}
            Now that everything's spelled out, 
            we're interested in showing Riemann 
            integrability for $g\circ f$, which requires us
            to show that the upper and lower RS-Darboux sums
            are arbitrarily close. We will do so by breaking
            the sums into two parts based on how
            the original function $f$ behaves on an interval:
            \begin{align*}
                A = \{ i \; | \; M_i(f) - m_i(f)\leq2K\delta\}\\
                B = \{ i \; | \; M_i(f) - m_i(f)>2K\delta\}
            \end{align*}
            Now let's work out the sums for $g\circ f$:
            \begin{align}
                U_\alpha(g\circ f,P) - L_\alpha(g\circ f,P)
                    &= \sum^n_{i=1} \left[
                    M_i(g\circ f) - m_i(g\circ f)\right]
                    \Delta\alpha_i \notag\\
                &= \sum_{i\in A} \quad + \quad
                    \sum_{i\in B} \notag\\
                \qquad
                &\leq \frac{\varepsilon}{2[\alpha(b)-\alpha(a)} 
                    \sum_{i\in A} \Delta\alpha_i+
                    K \sum_{i\in B} \Delta\alpha_i
                    \label{final}
            \end{align}
            where the result from the sum over $A$ comes from 
            Equation \ref{cont} and the result for the sum over 
            $B$ comes from $g$ being bounded.
            \\
            \\
            Now let's consider the sum over $B$ from the last
            line, and relate it back to Expression \ref{forf}.
            We know that by our definition of $B$:
            \begin{align*}
                {2K}{\delta} \sum_{i\in B} 
                \Delta\alpha_i &\leq    
                \sum_{i\in B} \left[M_i(f) - m_i(f) \right]
                \Delta\alpha_i \leq \varepsilon\cdot\delta \\
                &\Rightarrow \qquad \sum_{i\in B} \Delta\alpha_i
                \leq \frac{\varepsilon}{2K}
            \end{align*}
            Substituting this result back into Equation    
            \ref{final}, we see that we must have
            \begin{align*}
                U_\alpha(g\circ f,P) - L_\alpha(g\circ f,P)
                &\leq \frac{\varepsilon}{2[\alpha(b)-\alpha(a)]}    
                    [\alpha(b)-\alpha(a)]
                    + K\frac{\varepsilon}{2K} \\
                    &\leq \frac{\varepsilon}{2} + 
                        \frac{\varepsilon}{2} = \varepsilon
            \end{align*}
            Thus, we have that $g\circ 
            f\in\mathscr{R}_\alpha([a,b])$.
        \end{proof}
    \item \textbf{Multiplication}: If we have 
        $f,g\in\mathscr{R}_\alpha([a,b])$, then 
        $fg\in\mathscr{R}_\alpha([a,b])$.
        \begin{proof}
            We know that $f - g$ and $f+g \in
            \mathscr{R}_\alpha([a,b])$. We also know that 
            $h(x) = x^2$ is continuous, so then we can say that
            \[ fg = \frac{1}{4} \left[(f+g)^2 - (f-g)^2 \right]
                \in\mathscr{R}_\alpha([a,b])
                \]
        \end{proof}
    \item \textbf{Absolute Value Relations}: If we have
        $f \in \mathscr{R}_\alpha([a,b])$, then both
        \[ |f| \in \mathscr{R}_\alpha([a,b]) \qquad
            \left\lvert\int^b_a f d\alpha \right\rvert
            \leq \int^b_a |f| \; d\alpha \]
        \begin{proof}
            The first expression is clearly true because $g(x)=|x|$
            is a continuous function, so $g\circ f = |f|$ is
            RS integrable.
            \\
            \\
            Next, let $c$ be a single constant (either 1 or 
            $-1$) such that 
                \[ c\int^b_a f\; d\alpha = \left\lvert 
                \int^b_a f\;d\alpha \right\rvert \]
            Then we have that $cf \leq |f|$ and by the monotonicity
            property, we have that
            \begin{align*}
                \int^b_a |f| \; d\alpha &\geq 
                    \int^b_a cf \; d\alpha \\
                \Rightarrow \qquad 
                \int^b_a |f| \; d\alpha &\geq
                    \int^b_a cf \; d\alpha =
                    c \int^b_a f \; d\alpha = 
                    \left\lvert
                    \int^b_a f \; d\alpha \right\rvert 
            \end{align*}
        \end{proof}
        ($*$) This is an often-used result.
        
\end{itemize}

\newpage
\subsection{Mean Value Theorem}

The classical Mean-Value Theorem states that if $f$ is continuous on $[a,b]$ with $\alpha$ increasing, there's some $c\in[a,b]$ such that 
    \[ \int^b_a f\;d\alpha = f(c)[\alpha(b)-\alpha(a)] \]
We'll prove a more general result that retains this as a special case.
\begin{thm}
\label{mvt}
Suppose that $f,g\in\mathscr{R}_\alpha([a,b])$ and assume $g\geq0$. Then there exists a $\mu\in[m,M]$, i.e. in between the upper and lower bounds of $f$, such that 
        \[ \int^b_a fg\;d\alpha = \mu \int^b_a g\;d\alpha \]
Moreover, if $f$ is continuous, then there exists $c\in[a,b]$ such that $\mu = f(c)$.
\end{thm}
\begin{proof}
Since $g\geq0$, we can conclude that 
\begin{align*}
    m &\leq f \leq M \\
    mg &\leq fg \leq Mg \\
    \Rightarrow \qquad m\int^b_a g\;d\alpha &\leq 
        \int^b_a fg\;d\alpha \leq  M \int^b_a g\;d\alpha
\end{align*}
If we consider the case where $\int^b_a g\;d\alpha= 0$, then any old $\mu$ will work since we'll have that $\int^b_a fg\;d\alpha=0$. And if it's not 0, then we can set 
\[ \mu = \frac{\int^b_a fg\;d\alpha}{\int^b_a g\;d\alpha} \]
We know the numerator and denominator exist because both $f$ and $g$ were assumed integrable, so their product is as well.
\end{proof}
\begin{rmk}
If we take $g(x)=1$, then we get the plain vanilla MVT we opened up the subsection with.  And in the case where $f$ is continuous, we know from the intermediate value theorem that there exists a $c$ such that $\mu=f(c)$. 
\end{rmk}


\newpage
\subsection{Convergence of Riemann-Stieltjes Sums, $S_\alpha(f,P,T)$}

Basic calculus makes us all think that we approximate a Riemann-Sieltjes integral by taking finer and finer partitions, which we use to weight the value of the function at an increasing number of evaluation points along the interval $[a,b]$.  So it would be very nice and very convenient if Riemann-Stieltjes sums converged to the value of the integral whenever we construct just that sort of limiting approximation. 

\emph{However}, this will not always be the case.  That is, there will be some functions that are RS-integrable, which won't \emph{necessarily} converge if you take finer and finer partitions with more and more evaluations points. So the theorems in this section will illustrate when that will and won't apply. In certain cases, arbitrarily fine partitions might converge to different values, none of which must equal the value of the integral (if it even exists).

\begin{thm}
\label{weaker}
Let $f$ be a bounded function on $[a,b]$. Then $f\in\mathscr{R}_\alpha([a,b])$ if and only if there exists a number $I$ such that for every $\varepsilon>0$, there exists a corresponding partition $P$ where, for all $P^* \supset P$, we have 
    \[ |S_\alpha(f,P^*,T) - I | \leq \varepsilon \qquad \forall T\]
And if $f\in\mathscr{R}_\alpha([a,b])$, then $I = \int^b_a f\;d\alpha$.
\end{thm}
\begin{rmk}
We could have written a weaker theorem, leaving out the part about $P^*\supset P$, which is a special case of the above theorem.  But going forward, we'll want to consider the idea of taking finer and finer partitions. This will be particularly true when $\alpha$ might not be increasing.
\end{rmk}
\begin{proof}
First, we prove the $\Rightarrow$ direction. Since $f\in\mathscr{R}_\alpha([a,b])$, we know that there's a partition $P$ such that
    \[ U_\alpha(f,P) - L_\alpha(f,P)<\varepsilon \qquad 
    \forall \varepsilon \]
Considering $P^*\supset P$, we also have
\begin{align*}
    L_\alpha(f,P^*)&\leq S_\alpha(f,P^*,T) \leq U_\alpha(f,P^*)\\
    L_\alpha(f,P^*)&\leq \int^b_a f\;d\alpha \leq U_\alpha(f,P^*)
\end{align*}
Thus, we can conclude that 
\begin{align*}
    \left\lvert S_\alpha(f,P^*,T) - \int^b_a f\;d\alpha 
    \right\rvert \leq \varepsilon \qquad \forall T
\end{align*}
It's clear that $I=\int^b_a f\;d\alpha$,
\\
\\
Next, we want to show the $\Leftarrow$ direction. We begin by noting that given any $P$, there exists a $T_1$ and $T_2$ such that
\begin{align}
    S_\alpha(f,P,T_1) &\geq U_\alpha(f,P) - \varepsilon\notag\\
    S_\alpha(f,P,T_2) &\leq L_\alpha(f,P) + \varepsilon
    \label{proof1.17}
\end{align}
This works because given any partition, we can always choose a $T_1$ and $T_2$ to satisfy by taking $T_1$ ($T_2$) as the set of all $M_i$ ($m_i$). 
\\
\\
So now, let's take $P^*$ to be the partition such that the RS sum is $\varepsilon$ close to $I$ for all $T$. (Note that this $P^*$ is taken to exist in this direction of the proof.) Then we have from the inequalities in \ref{proof1.17} that
\begin{align*}
    U_\alpha(f,P) - L_\alpha(f,P) &\leq 
        \left[ S_\alpha(f,P,T_1) + \varepsilon\right] -
        \left[ S_\alpha(f,P,T_2) - \varepsilon\right] \\
    &\leq \left[ S_\alpha(f,P,T_1) - I + \varepsilon\right] -
        \left[ S_\alpha(f,P,T_2) - I - \varepsilon\right] \\
    &\leq \left\lvert S_\alpha(f,P,T_1) - I \right\rvert+       
        \left\lvert S_\alpha(f,P,T_2) - I \right\rvert
        + \varepsilon + \varepsilon \\
    \text{From Inequalities \ref{proof1.17}} \quad
        &\leq 4\varepsilon
\end{align*}
Since the Upper and Lower sums can be made arbitrarily close, we then know that $f\in\mathscr{R}_\alpha([a,b])$.
\end{proof}
\begin{rmk}
Before moving on, let's recap what the last theorem said in words---or at least what it didn't say.  It didn't say that if you keep taking smaller partitions, you will \emph{always} force your RS-Sum to converge to the value of an integral.  What it did say was twofold:
\begin{enumerate}
    \item First, if your function \emph{is} Riemann-Integrable, then and you can find \emph{some} partition (though it might not be true for all of them) where the RS-Sums given that partition and \emph{all finer} partitions get arbitrarily close to this integral (which exists).
    \item Next, suppose you know an $I$ that the RS-sums get arbitrarily close to.  More specifically, that means if you're handed any $\varepsilon>0$, you can find a partition that gets the RS-Sums arbitrarily close to that $I$, no matter the $T$ tagging the partition $P$. Now we're not saying that the sums \emph{must} converge to some $I$ when you take finer partitions. Instead, we're just saying it's possible, and if $I$ \emph{does} indeed exist, then $f$ is RS integrable.
\end{enumerate}
Now we can move on to consider that thorny case of ever-shrinking partitions.
\end{rmk}
\begin{defn}
\label{strongest}
For a bounded function $f$ on $[a,b]$, we say 
    \[ \lim_{||P||\rightarrow 0} S_\alpha(f,P,T) = I \]
if for every $\varepsilon>0$, there exists a $\delta>0$ such that $||P||\leq\delta$ implies that
    \[ |S_\alpha(f,P,T) - I | \leq \varepsilon \qquad \forall T \]
    Now here's the big difference between this and what we've seen earlier: the sums might converge, but I never said anything about the equivalence of this statement to RS-integrability, like we had in Theorem \ref{weaker}.  In fact, we can't.
\\
\\
There might be functions where this condition does \emph{not} hold (i.e. the limit does not exist), but still the function is integrable. (See the next example.) But we do have at least one direction intact, as shown in the next theorem.
\end{defn}
\begin{thm}
\label{oneway}
Suppose that $f$ is bounded on $[a,b]$. If $\lim_{||P||\rightarrow 0}$ exists, then $f\in\mathscr{R}_\alpha([a,b])$ and
    \[ \int^b_a f\;d\alpha = \lim_{||P||\rightarrow 0} 
    S_\alpha(f,P,T) \]
\end{thm}

\begin{ex}
Now we give an example of why the other direction in Theorem \ref{oneway} wouldn't work.  Consider
\[ f(x) = \begin{cases} 0 & x \in [0, 1/2) \\ 1 & x \in [1/2, 1] 
        \end{cases}  \qquad
    \alpha(x) = \begin{cases} 0 & x \in [0,1/2] \\ 1& x\in(1/2, 1] 
        \end{cases} \]
Depending on whether the point $1/2$ is included in the partition, we can construct ever-finer partitions that make the RS-Sums converge to 0 sometimes, and to 1 at other times.
\end{ex}
\begin{rmk}
    Thus we have that the strongest statement is in Definition \ref{strongest} and Theorem \ref{oneway}. If we can show the limit exists as $||P||\rightarrow 0$, then we can show that $f$ is integrable. If we can't use that, we should think a little harder and try to find a $P$ as in Theorem \ref{weaker} before concluding that $f$ isn't RS-integrable. However, if we require more from our function $f$, we can can be more conclusive, as in the next theorem.
\end{rmk}

\begin{thm}
Let $f\in\mathscr{R}_\alpha([a,b])$. If either $f$ or $\alpha$ is continuous on $[a,b]$, then 
\begin{equation}
    \label{limsum.toprove}
    \int^b_a f\;d\alpha = \lim_{||P||\rightarrow 0} 
    S_\alpha(f,P,T) 
\end{equation}
\end{thm}
\begin{rmk}
This previous theorem is important, because if we just consider \emph{Riemann} integrability, then $\alpha(x) = x$, which is continuous. Thus, Riemann-integrability and the existence of the limit of Riemann sums (as the partition intervals get smaller) are equivalent notions.
\end{rmk}
So our goal will be to show that, for all $\epsilon>0$,
\begin{equation}
    \label{contequiv.toshow}
    \lim_{||P||\rightarrow 0}
    \left\lvert \int^b_a f \; d\alpha - S_\alpha(f,P,T) \right\rvert
        \leq \varepsilon
\end{equation}
when $f$ or $\alpha$ is continuous.

\begin{proof}($f$ Continuous)
First, suppose that $f$ is continuous. We'll use the Mean Value Theorem (taking $g=1$) to rewrite the integral in Equation \ref{contequiv.toshow} in terms of the partition $P$ that's used for the sum. This allows us to assert that for some $\mu \in [m,M]$, we have that
    \[ \int^b_a f \; d\alpha = \mu \int^b_a d\alpha \]
But this will also hold if we break up our interval into subintervals defined by the points in any partition, $P$:
\begin{equation}
    \label{contequiv}
    \sum^n_{i=1} \int^{x_i}_{x_{i-1}} f \; d\alpha 
        = \sum^n_{i=1} \mu_i \int^{x_i}_{x_{i-1}} d\alpha 
        \qquad\qquad \mu_i \in [m_i, M_i] 
\end{equation}
Next, because $f$ is continuous, we can use the Intermediate Value Theorem to note that, for all $i$, there must be some $c_i \in [x_{i-1}, x_i]$ where $f(c_i) = \mu_i$. Substituting into Equation \ref{contequiv}, we get
\begin{align*}
    \sum^n_{i=1} \int^{x_i}_{x_{i-1}} f \; d\alpha 
        &= \sum^n_{i=1} f(c_i) \int^{x_i}_{x_{i-1}} d\alpha 
        \qquad\qquad c_i \in [x_{i-1}, x_{i}] \\
        &= \sum^n_{i=1} f(c_i) \Delta\alpha_i
\end{align*}
Then, summing over $i$, we can define 
\begin{align*}
    \int^{b}_{a} f \; d\alpha 
        &= \sum^n_{i=1} f(c_i) \Delta\alpha_i
        \equiv S_\alpha(f,P,C)
\end{align*}
So now, we can replace the integral with this sum in the statement that we want to prove, as given in Inequality \ref{contequiv.toshow}:
\begin{equation}
    \label{contequiv.toshow2}
    \lim_{||P||\rightarrow 0}
    \left\lvert  S_\alpha(f,P,C)- S_\alpha(f,P,T) \right\rvert
        \leq \varepsilon
\end{equation}
This is now our equivalent target statement to prove.
\\
\\
Next, because $f$ is continuous on a compact interval, $f$ is uniformly continuous on $[a,b]$.  Thus for any $\varepsilon$, there exists a $\delta$ such that $||P||\leq\delta$ implies that $|f(x) - f(y)| \leq \varepsilon$, where $x$ and $y$ are both in the same interval.  
\\
\\
So given $\epsilon>0$, let $\delta>0$ be such that 
    \[ |x-y|\leq \delta \Rightarrow \quad |f(x)-f(y)| \leq 
        \frac{\varepsilon}{\alpha(b)-\alpha(a)} \]
Now if we take our partition such that $||P||\leq\delta$, we can write our sums as
\begin{align*}
    \left\lvert  S_\alpha(f,P,C)- S_\alpha(f,P,T) \right\rvert
    &= \left\lvert  \sum^n_{i=1} \left[f(c_i) - f(t_i) \right]
    \Delta\alpha_i\right\rvert  \\
    &\leq \sum^n_{i=1} \left\lvert  f(c_i) - f(t_i) \right\rvert
    \Delta\alpha_i  \\
    &\leq \frac{\varepsilon}{\alpha(b)-\alpha(a)}
        \sum^n_{i=1} \Delta\alpha_i =  \varepsilon 
\end{align*}
\end{proof}
\begin{proof}($\alpha$ Continuous)
Suppose $\varepsilon$ is given. Since $f\in\mathscr{R}_\alpha([a,b])$, we know that there exists a $P^*$ (with $\#P^*$, or $n+1$, subintervals) such that 
\begin{equation}
    \label{bases}
    U_\alpha(f,P^*) < \int^b_a f\;d\alpha + \varepsilon
        \qquad L_\alpha(f,P^*) > \int^b_a f\;d\alpha - \varepsilon
\end{equation}
To construct the proof from here, we'll want eventually to compare any arbitrary RS-Sum $S_\alpha(f,P,T)$ to these specific upper and lower sums (using $P^*$) that we know something about---i.e. how close they are to the target integral. Then we'll take the limit for $P$ once we've established that relationship.
\\
\\
Also, assume that $f(x)>0$ over $[a,b]$. We'll resolve this simplifying assumption later on, and it will help in the meantime.
\\
\\
So since that interval is compact and $\alpha$ is continuous, we know that $\alpha$ is \emph{uniformly} continuous, implying that
    \[ \exists \delta > 0 \quad \text{s.t.} \qquad 
    w_\alpha(\delta) < \frac{\varepsilon}{\#P^*} \]
where $\#P^*$ is the number of sub-intervals in $P^*$. And so 
    \[ \forall P \qquad ||P||<\delta \quad \Rightarrow 
        \quad \alpha(x_i) - \alpha(x_{i-1}) < w_\alpha(\delta)
        <  \frac{\varepsilon}{\#P^*} \]
Now, considering any such set $P$, let's also define the following sets, which group the indices for the subintervals of any $P$ into two categories, based on how they interact with $P^*$:
\begin{align*}
    I_1 &:= \{i \; | \; [x_{i-1},x_i) \cap P^* = \emptyset \}\\
    I_2 &:= \{i \; | \; [x_{i-1},x_i) \cap P^* \neq \emptyset \}
\end{align*}
And so $I_2$ contains all the indices where an subinterval of $P$ contain a point of $P^*$. $I_1$ has all the indices where an subinterval of $P$ is contained entirely within a subinterval of $P^*$.
\\
\\
Having done all of groundwork from above, we can now write an expression for the upper sum relative to $P$:
\begin{align}
    U_\alpha(f,P) &= \sum^n_{i=1} M_i(f) \Delta\alpha_i \notag \\
        &= \sum_{I\in I_1} M_i(f) \Delta\alpha_i 
         + \sum_{I\in I_2} M_i(f) \Delta\alpha_i
         \label{twosums}
\end{align}
But clearly, $I_2$ can only contain, at most, $\#P$ points since there are only $\#P$ intervals of $P$, so we must have that $\#I_2 \leq \#P$. Thus, the sum over $I_2$ simplifies:
\begin{align}
        \sum_{I\in I_2} M_i(f) \Delta\alpha_i 
        &\leq \sum_{I\in I_2} M \frac{\varepsilon}{\#P} =    
            M\varepsilon
        \label{tocombine1}
\end{align}
where $M = \sup f(x)$ over the interval $[a,b]$.
\\
\\
To deal with the sum over $I_1$, we get the result that 
\begin{equation}
    \label{tocombine2}
    \sum_{I\in I_1} M_i(f) \Delta\alpha_i  \leq U_\alpha(f,P^*)
\end{equation}
This is true because in assuming $f(x)>0$ and restricting our sum to $I_1$, we throw out whole portions of the interval $[a,b]$ from the sum entirely. And on top of that, we threw out the very large intervals that span one or more of the points in the partition $P^*$.  So all that remains in our sum over $I_1$ are those tiny subintervals of $P$ that would fit entirely within a subinterval of $P^*$. And so we won't be able to make up for the parts we throw out by weighting parts of the interval $[a,b]$ with $M_i(f)$ from $P$ \emph{more than} we would in the case of $M_i(f)$ from $P^*$.
\\
\\
Combining the representation for $U_\alpha(f,P)$ with the two sum representation of Equation \ref{twosums} and the results in Inequalities \ref{tocombine1} and \ref{tocombine2}, we get that
\begin{equation}
    U_\alpha(f,P) \leq U_\alpha(f,P^*) + M\varepsilon
\end{equation}
And because we had an expression for $U_\alpha(f,P^*)$ in Equation \ref{bases}, we can substitute in and get that
\begin{equation}
    \label{ass1}
    U_\alpha(f,P) \leq \int^b_a f\;d\alpha  + (M+1)\varepsilon
\end{equation}
And by a similar argument, we also have that 
\begin{equation}
    \label{ass2}
    L_\alpha(f,P) \geq \int^b_a f\;d\alpha  - (M+1)\varepsilon
\end{equation}
Now, at this point we have to dispense with the assumption that $f(x)>0$.  We do that by choosing a $k$ such that $f(x) + k >0$, where now $f$ can be any arbitrary function such that $\int^b_a f\;d\alpha$ as we assume in the theorem.  Then we're in the world we described above in choose our $P$. 
\\
\\
And, since we're in that world, we get the inequality in the next line, which allows us to conclude a very useful result
\begin{align*}
    U(f,P) + k(\alpha(b)-\alpha(a))  
        = U(f+k, P) &\leq \int^b_a (f+k)\; d\alpha 
        + (M+1)\varepsilon \\
    &\leq \int^b_a f\; d\alpha  + k(\alpha(b)-\alpha(a))
        + (M+1)\varepsilon \\
    \Rightarrow U(f,P) &\leq \int^b_a f\; d\alpha
        + (M+1)\varepsilon 
\end{align*}
So we have our asses covered if $f(x)$ isn't always positive, and we can stick with the results in Inequalities \ref{ass1} and \ref{ass2}.
\\
\\
And now we're almost done.  All that's left is to recognize that we can sandwich, for all $P$ such that $||P||\leq\delta$
\begin{align*}
    \int^b_a f\;d\alpha - (M+1)\varepsilon \leq
    L_\alpha(f,P) \leq S_\alpha(f,P,T) &\leq U_\alpha(f,P) \leq
   \int^b_a f\;d\alpha + (M+1)\varepsilon  \\
        \Rightarrow \qquad \left\lvert S_\alpha(f,P,T) 
        - \int^b_a f\;d\alpha\right\rvert &\leq 2(M+1)\varepsilon \end{align*}
\end{proof}

\newpage
\subsection{Integration for Arbitrary Integrators, $\alpha$}

By ``arbitrary'', we mean ``$\alpha$ not necessarily increasing'', which contrasts our operating assumption up until this point. 

In order to make such a notion operational, we will need to dispense with our ``sandwiching'' approach, which uses $L_\alpha(f,P)\leq S_\alpha(f,P,T)\leq U_\alpha(f,P)$, as it no longer holds. In fact, we don't even get that if $P^*\supset P$, then $U_\alpha(f,P^*) \leq U_\alpha(f,P)$ (and the analogous case for the lower sum). With that, we give a new definition of integrability framed entirely in terms of RS-Sums:

\begin{defn}
Given any $f$ and $\alpha$ (not necessarily increasing) on $[a,b]$, we say that $f\in\mathscr{R}_\alpha([a,b])$ if and only if there exists an $I\in\mathbb{R}$ such that for all $\varepsilon>0$, there exists a $P$ where $P^*\supset P$ implies
    \[ \left\lvert S_\alpha(f,P^*,T) - I \right\rvert \leq
        \varepsilon \qquad \forall \; T \]
This $I$ is denoted by $\int^b_a f\;d\alpha$. And despite this new definition, we still retain of the usual nice linearity properties discussed above in the increasing $\alpha$ case.
\end{defn}
The next subsection goes onto a very important topic when we consider arbitrary integrators---so important that I gave its own subsection, although it actually should fall under this current one.

\subsection{Integration by Parts}

\begin{thm}
\label{thm.intbyparts}
\emph{(Integration by Parts)} 
Let $f$ and $\alpha$ (not necessarily increasing) be bounded on $[a,b]$. If $f\in\mathscr{R}_\alpha([a,b])$, then $\alpha\in\mathscr{R}_f([a,b])$ and 
\begin{equation}
    \label{intbyparts}
    \int^b_a f\;d\alpha = f(b) \alpha(b) - f(a)\alpha(a) - 
    \int^b_a \alpha \; df 
\end{equation}
This is a slightly more general case that the classic calculus version, which is a special case where $\alpha(x)=x$.
\end{thm}
\begin{proof}
To start, we take $P^*$ as the partition such that 
\begin{equation}
    \label{ibpassump}
    \left\lvert S_\alpha(f,P^*,T) - \int^b_a f\;d\alpha 
    \right\rvert \leq \varepsilon 
\end{equation}
which we know exists, since the integrability of $f$ is assumed. 
\\
\\
Now we want to show that for all $\varepsilon>0$, there exists a $P$ such that $P^*\supset P$ implies that 
    \[ \left\lvert S_f(\alpha,P^*,T) -  f(b) \alpha(b) - 
        f(a)\alpha(a) - \int^b_a f \; d\alpha \right\rvert \leq 
        \varepsilon \qquad \forall \; T\]
        So we rewrite the righthand side of Equation \ref{intbyparts}, recasting them as telescoping sums, also substituting in $S_f(\alpha,P^*,T)$ for the integral:\footnote{You might object and say ``Hey, you can't do that.  You don't know that there's an equivalent RS-Sum for the integral $\int^b_a \alpha\;df$---particularly if you use that same $P^*$ we've been using all along.'' But I actually don't care whether this sum I'm substituting in is actually equivalent (yet). I'm just taking the righthand side of Equation \ref{intbyparts} as inspiration, and using the $P^*$ we've been using to write an RS-Sum. We'll see at the very end that it's a \emph{result} that this RS-Sum approximates the integral, not an assumption.}
\begin{align}
    f(b) \alpha(b) - f(a)\alpha(a) - S_f(\alpha,P^*,T)
        &= \sum^n_{i=1} f(x_i) \alpha(x_i) - f(x_{i-1})
        \alpha(x_{i-1}) \notag\\
    &\quad - \sum^n_{i=1} \alpha(t_i) \left[f(x_i) - f(x_{i-1})
        \right] \notag\\
    &= \sum^n_{i=1} f(x_i) \left[\alpha(x_i) - \alpha(t_i) \right]
        \notag\\
    &\qquad +\sum^n_{i=1} f(x_{i-1}) \left[\alpha(t_i) 
    - \alpha(x_{i-1})\right]\label{righthandtwosums}
\end{align}
Now the righthand side kind of looks like a RS-Sum, so let's define a partition and set of tag points so it is. Define define a new partition, call it $P^{**}$, which will be the union of the old $P^*$ and the points $t_i$. So $P^{**} = P^*\cup T$, with multiplicities. Next, define a new set of tagging points,
    \[ T^{**} = \{ x_0, x_1, x_1, x_2, x_2,\ldots, x_n \} \]
Now we can collapse the righthand side in Equation \ref{righthandtwosums} into a proper RS-Sum:
\begin{align*}
    f(b) \alpha(b) - f(a)\alpha(a) - S_f(\alpha,P,T)
    &= S_\alpha(f, P^{**}, T^{**})
\end{align*}
Since $P^{**}\supset P^* \supset P$, it's clear that 
\begin{align*}
    \left\lvert S_\alpha(f,P^{**},T^{**}) - \int^b_a f\;d\alpha
    \right\rvert \leq &\varepsilon  \\
    \Leftrightarrow \quad 
        \left\lvert f(b) \alpha(b) - f(a)\alpha(a) 
        - S_f(\alpha,P^*,T) - 
        \int^b_a f \; d\alpha \right\rvert &\leq 
        \varepsilon \qquad \forall \; T
\end{align*}
which is exactly what we wanted to prove, since we can take $P^*$ as our partition. 
\end{proof}

\begin{cor}
The big result from integration by parts is that 
$f\in\mathscr{R}_\alpha([a,b])$ if and only if $\alpha\in\mathscr{R}_f([a,b])$
\end{cor}

\begin{cor}
If $f\in\mathscr{R}_{\alpha_1}([a,b]) \cap \mathscr{R}_{\alpha_2}([a,b])$, then $f\in\mathscr{R}_{\alpha_1 + \alpha_2}([a,b])$ and 
    \[ \int^b_a f\;d(\alpha_1 + \alpha_2) = 
    \int^b_a f\;d\alpha_1 + \int^b_a f\;d\alpha_2 \]
\end{cor}

\newpage
\subsection{Functions of Bounded Variation}

In the previous subsection, ``Integration by Parts,'' we assumed that $f$ was integrable with respect to an arbitrary integrator $\alpha$. This relaxed our earlier assumptions in the sense that $\alpha$ didn't have to be increasing anymore.  \emph{However}, we just went ahead and assumed $f\in\mathscr{R}_\alpha([a,b])$, which doesn't seem very general.

So now, we'll relax our assumptions even further and consider what characteristics and conditions for the functions $f$ and $\alpha$ truly permit integrability---we won't just assume.  This returns us to earlier subsections where we were concerned about \emph{whether} $f$ would be integrable given $\alpha$.  To answer that question, we'll have to define the concept of \emph{bounded variation}.

\begin{defn}
Let $f$ be a function on $[a,b]$. Define the \emph{total variation} as
\[ V_a^b(f) := \sup_P \sum^n_{i=1} \left\lvert f(x_i) - f(x_{i-1})
    \right\rvert \]
We say that $f \in BV([a,b])$, i.e. ``$f$ is of bounded variation'', if $V^b_a(f)<\infty$, i.e. the total variation is finite.
\end{defn}

\begin{thm}
If $f$ is monotonically increasing on $[a,b]$, then $f\in BV([a,b])$.
\end{thm}

\begin{thm}
If $f$ is continuous and differentiable on $[a,b]$ and $f'$ is bounded, then $f\in BV([a,b])$.
\end{thm}
\begin{proof}
Given any partition $P$, from the Mean Value Theorem, we have that for all subintervals, 
    \[ \exists\; c_i \in [x_{i-1}, x_i] \quad \text{s.t.}
        \quad f(x_i) - f(x_{i-1}) = c_i \left[
        x_i - x_{i-1}\right]    \]
Since $f'$ is bounded, we know that $M=\sup_{x\in[a,b]} |f'(x)|$ is finite. Thus we can write, for any partition $P$,
\begin{align*}
    \sum^n_{i=1} \left\lvert f(x_i) - f(x_{i-1})
        \right\rvert &= \sum^n_{i=1} \left\lvert f(c_i) 
        \left[x_i - x_{i-1}\right] \right\rvert \\
    &\leq \sum^n_{i=1} M \left\lvert \Delta x_i 
        \right\rvert = M(b-a) < \infty 
\end{align*}
Thus, since the righthand side is independent of the partition $P$, when we take the sup over $P$'s, we still have that $V_a^b(f)$ is finite.
\end{proof}

\begin{ex}
You might ask why we threw in the condition that $f'$ is bounded in the previous theorem.  It turns out continuous and differentiable doesn't necessarily mean that the derivative is bounded, so that was a necessarily addition.  Here's an example:
    \[ f(x) =
        \begin{cases} 
            x^2 \sin(1/x^2) & x \neq 0\\
            0               & x = 0
        \end{cases}  \]
It's continuous (take the limit of the top while $x\rightarrow 0$ to see this), and it's differentiable:
    \[ f'(x) = 2x \sin(1/x^2) - \frac{2}{x} \cos(1/x^2) \]
Clearly, that derivative isn't bounded as $x\rightarrow 0$.
\end{ex}

\begin{thm}
$f$ continuous does not imply that $f\in BV$; $f \in BV$ does not imply $f$ continuous.
\end{thm}
\begin{ex}
As you might also suspect, continuous functions aren't necessarily of bounded variation. An example given in the book defines the following function on $[0,1]$:
    \[ f(x) = 
        \begin{cases}
        \frac{1}{n} & \text{$x = \frac{1}{n}$, where $n$ is an even integer}\\
        0 & \text{$x = \frac{1}{n}$, where $n$ is an odd integer} \\
        0 & \text{$x = 0$ or $x = 1$}
        \end{cases}
    \]
and linear in between.  Another similar example take
    \[ f(x) = 
        \begin{cases}
            x \sin(1/x) & x\neq 0 \\
            0 & x=0 
        \end{cases} \]
Both look something like the harmonic series that sums $1/n$, which we know diverges.
\end{ex}
\begin{ex}
As an example of a function of bounded variation that's not continuous, consider the function $f=\chi_{[1/2,1]}$ on $[0,1]$. Clearly there's a jump, but $V_a^b(f) = 1 < \infty$.
\end{ex}

\begin{thm}
$f\in BV([a,b])$ implies that $f$ is bounded on $[a,b]$, as 
    \[ \sup_{x\in[a,b]} |f(x) | \leq |f(a)| + V_a^b(f) \]
\end{thm}
\begin{proof}
Suppose that the statement above were not true---that for some $x^* \in [a,b]$, we have
\begin{align*}
    |f(x^*)| &> |f(a)| + V_a^b(f) \\
    \Rightarrow \qquad 
        |f(x^*)| - |f(a)|  &> V_a^b(f) 
\end{align*}
But we can keep using identities about the absolute value to show that the above statement implies
\begin{align*}
    V_a^b(f) < |f(x^*)| - |f(a)| \leq \left\lvert \;
        |f(x^*)| - |f(a)|\;\right\rvert
        \leq |f(x^*) - f(a)|
\end{align*}
And if this were the case, we clearly chose $V_a^b(f)$ incorrectly because we could define a partition by $\{a, x^*, b\}$ to get a bigger value for the total variation than $V_a^b(f)$.
\end{proof}

\begin{thm}
For $f, g\in BV([a,b])$, we have that
    \[ V_a^b(f+g) \leq V_a^b(f) + V_a^b(g)
    \qquad V_a^b(cf) = |c|V_a^b(f) \]
This implies that $f+g$ and $cf$ are both of bounded variation as well.
\end{thm}
\begin{rmk}
Given this definition and the triangle inequality, we see that Bounded Variation looks like a linear space. In fact, it's \emph{semi-norm}, meaning that it satisfies all but the 0 condition, since a non-zero (constant) function will have 0 total variation. 

But we can fix that, getting a full norm by adding the sup-norm ($||f||_\infty = \sup_{[a,b]} f$) to total variation. Thus if we define our norm as
    \[ \lVert f||_\infty + V_a^b(f) \]
we see that we've satisfied all of the conditions for a norm on the function space:
\begin{enumerate}
    \item $\lVert f\rVert_\infty + V_a^b(f) = 0$ if and only if $f=0$.
    \item $\lVert cf\rVert_\infty + V_a^b(cf) = c\lVert f\rVert_\infty + |c|V_a^b(f)
        = c\left[\lVert f\rVert_\infty + V_a^b(f)\right]$.
    \item $\lVert f+g\rVert_\infty + V_a^b(f+g) \leq 
        \left[\lVert f\rVert_\infty + V_a^b(f)\right] +
        \left[\lVert g\rVert_\infty + V_a^b(g)\right]$.
\end{enumerate}
\end{rmk}

\begin{thm}
If $f,g\in BV([a,b])$, then $f\cdot g \in BV([a,b])$.
\end{thm}

\begin{thm}
If $f\in BV([a,b])$ and $c \in (a,b)$, then 
    \[ V_a^b(f) = V_a^c(f) + V_c^b(f) \]
\end{thm}
\begin{cor}
As a result of the last theorem, we have that, as a function of $x$, $V_a^x(f)$ is monotonically increasing.
\end{cor}
\begin{proof}
This is because if $y>x$, then we have 
    \[ V_a^y(f) = V_a^x(f) + V_x^y(f) \]
So clearly, $V_a^y(f) >V_a^x(f)$.
\end{proof}

\begin{note}
This next result is an important theorem that establishes the link between functions of bounded variation and increasing functions, allowing us to apply some old results where we had to deal with strictly increasing integrators.
\end{note}

\begin{thm}
\label{monot}
$f\in BV([a,b])$ if and only if $f = u-v$, where $u$ and $v$ are monotonically increasing functions on $[a,b]$. 
\end{thm}
\begin{proof}
First, take the $\Leftarrow$ direction. This is pretty trivial since 
    \[ V_a^b(f) = V_a^b(u-v) \leq V_a^b(u) + V_a^b(v) \]
And since $u$ and $v$ are monotonically increasing, the terms $V_a^b(u)$ and $V_a^b(v)$ are finite.
\\
\\
Now for the $\Rightarrow$ direction. Set $u(x) = V_a^x(f)$, and let $v(x)=V_a^x(f) - f(x)$. Clearly, $f = u-v$; we just have to prove that $v$ is monotonically increasing, since the $u$ case is fairly obvious.
\\
\\
Supposing that $y>x$, we want to show that $v(y) > v(x)$. We can do so by starting with an obvious statement that 
    \[ f(y) - f(x) \leq V_x^y(f) \]
Then we rewrite and rearrange terms to show
\begin{align*}
    f(y) - f(x) &\leq V_x^y(f) = V_a^y(f) - V_a^x(f)  \\
    \Leftrightarrow\quad V_a^x(f) - f(x) &\leq V_a^y(f) - f(y)  \\
    \Leftrightarrow\quad v(x) &\leq v(y)  
\end{align*}
\end{proof}
Now that we have the whole framework for functions of bounded variation in place, we can return to integration where either $f$ or $\alpha$ is of bounded variation (or both). 

\subsection{Integration with Respect to Functions of Bounded Variation}

\begin{thm}
If $f$ is continuous on $[a,b]$ and $\alpha\in BV([a,b])$, then $f\in\mathscr{R}_\alpha([a,b])$. Also, since we can swap the integrand and integrator, if $f\in BV([a,b])$ and $\alpha$ continuous, then $f\in\mathscr{R}_\alpha([a,b])$
\end{thm}
\begin{proof}
Suppose that $f$ is continuous and $\alpha\in BV([a,b])$. Then by Theorem \ref{monot}, we have that $\alpha = u -v$, both of which are increasing. Thus $f\in\mathscr{R}_u([a,b]) \;\cap \;\mathscr{R}_v([a,b])$, which entails $u, v \in \mathscr{R}_f([a,b])$ by Theorem \ref{thm.intbyparts}.  This implies that $u-v\in\mathscr{R}_f([a,b])$, which implies $f\in\mathscr{R}_{u-v}([a,b]) = \mathscr{R}_\alpha([a,b])$.
\end{proof}

\begin{thm}
We get a number of familiar results from regular integration when $\alpha\in BV([a,b])$, $f,g\in\mathscr{R}_\alpha([a,b])$, $h$ continuous, and $c\in(a,b)$:
\begin{itemize}
    \item $\int^c_a f\;d\alpha + \int^b_c f\;d\alpha = \int^b_a f\;d\alpha$.
    \item $h\circ f\in\mathscr{R}_\alpha([a,b])$.
    \item $fg\in\mathscr{R}_\alpha([a,b])$.
    \item $|f|\in\mathscr{R}_\alpha([a,b])$
\end{itemize}
\end{thm}

\begin{thm}
\label{ftcbasis}
Suppose $\alpha$ is continuous and differentiable on $[a,b]$ with $\alpha', f\in\mathscr{R}([a,b])$. Then $f\in\mathscr{R}_\alpha([a,b])$ and
    \[ \int^b_a f\;d\alpha = \int^b_a f \alpha'\;dx\]
Since $\alpha$ is continuous and $\alpha'$ is bounded on $[a,b]$, $\alpha \in BV([a,b])$. 
\end{thm}
\begin{proof}
Now we know by the Mean Value Theorem that, for all subintervals,
    \[ 
        \exists \; \xi_i \in [x_{i-1}, x_i] \quad \text{s.t.} 
        \qquad \Delta\alpha_i = \alpha'(\xi_i) \Delta x_i 
    \]
Now consider the function $f\cdot\alpha'$. Since both $f,\alpha' \in\mathscr{R}([a,b])$ by assumption, we know that their product is Riemann integrable.  That means there's some partition $P$ such that $P^*\supset P$ implies that 
    \[ \left\lvert S(f\alpha',P,T) - \int^b_a f\alpha'\;dx 
    \right\rvert \leq \varepsilon/2 \]
Using partition $P$ and the MVT result above, we can write the sum for the other integral:
\begin{align*}
        S_\alpha(f,P,T) &= \sum^n_{i=1} f(t_i) \Delta\alpha_i 
        = \sum^n_{i=1} f(t_i)\alpha'(\xi_i) \Delta x_i
\end{align*}
Now, we want to show the following for any arbitrary $\varepsilon>0$:
\begin{align}
    \label{toexpand}
    \left\lvert  S_\alpha(f,P,T) - \int^b_a f\alpha'\;dx 
        \right\rvert &\leq \varepsilon
\end{align}
If we can prove this, then we know that the limit of the sum exists (so the corresponding integral exists) and that it is equal to the integral of $f\alpha'$. So let's expand out
But we can expand out Inequality \ref{toexpand}:
\begin{align*}
    \left\lvert S_\alpha(f,P,T) - \int^b_a f\alpha'\;dx 
        \right\rvert &\leq 
        \left\lvert S_\alpha(f,P,T) - S(f\alpha',P,T)
        \right\rvert \\
    &\qquad + \left\lvert S(f\alpha',P,T)
        - \int^b_a f\alpha'\;dx \right\rvert 
\end{align*}
Now the second term will be $\varepsilon/2$ small by our choice of $P$ above. As for the first term:
\begin{align*}
    \left\lvert S_\alpha(f,P,T) - S(f\alpha',P,T)
        \right\rvert &\leq 
        \left\lvert \sum^n_{i=1} f(t_i)\left[\alpha'(\xi_i) - 
        \alpha'(t_i)\right] \Delta x_i \right\rvert\\
    &\leq M \cdot
        \sum^n_{i=1} \left\lvert \alpha'(\xi_i) - 
        \alpha'(t_i)\right\rvert \Delta x_i 
\end{align*}
where $M =\sup_{x\in[a,b]}\left\lvert f(x)\right\rvert$. Finally, note that for each $i$,
\[ 
    |\alpha'(\xi_i) - \alpha'(t_i)| \leq 
    \sup_{[x_{i-1}, x]} \alpha(x) - 
    \inf_{[x_{i-1}, x]} \alpha(x) 
\]
like we might have if we consider the upper and lower sums. As a result, because $\alpha'\in\mathscr{R}_\alpha([a,b])$, we can choose a $Q$ such that 
\[ 
    \sum^n_{i=1} \left\lvert \alpha'(\xi_i) - 
    \alpha'(t_i)\right\rvert \Delta x_i 
    \leq U_\alpha(f,Q) - L_\alpha(f,Q) \leq \frac{\varepsilon}{2M}
\]
Taking our new partition to be $P \cup Q$, which is a superset of $P$ (where $P$ is from above), we see that our desired result is achieved.
\end{proof}

\newpage
\subsection{Fundamental Theorem of Calculus}

\begin{thm} \emph{(Part I)}
\label{ftc1}
If $f$ is differentiable and $f'\in\mathscr{R}([a,b])$, then 
    \[ \int^b_a f'(x)\;dx = f(b) - f(a) \]
\end{thm}
\begin{proof}
Careful with the notation here, but take the $f$ in Theorem \ref{ftcbasis} to be the constant function $f(x) =1$. Then, take the $\alpha(x)$ in Theorem \ref{ftcbasis} to be the $f$ in this theorem's statement. Then we get that
\[ \int^b_a f'(x) \;dx = \int^b_a 1 \;df = f(b) - f(a) \]
\end{proof}

\begin{thm} \emph{(Part II)}
\label{ftc2}
Suppose $f\in\mathscr{R}([a,b])$, and take 
    \[ F(x) = \int^x_a f(t)\;dt \]
Then $F$ is Lipschitz continuous on $[a,b]$, and $F$ is differentiable at those points where $f$ is continuous with $F'(x) = f(x)$ at those points.
\end{thm}
\begin{proof}
First, we deal with the continuity. Since $f$ is bounded, we know that 
    \[ 
        f(x) \leq C = \sup_{x\in[a,b]} f(x)
    \]
Thus, we can say that
\begin{align*}
    |F(y) - F(x)| &= \left\lvert \int^y_x f(t)\;dt \right\rvert
        \leq \int^y_x |f(t)|\;dt \\
    &\leq \int^y_x C\;dt = C|y-x| 
\end{align*}
Thus, $F$ is Lipschitz continuous.
\\
\\
Next, for $x\in[a,b]$ and $h\neq 0$:
\[ 
    \frac{F(x+h)-F(x)}{h} = \frac{1}{h}\int^{x+h}_x f(t)\;dt
\]
Applying the Mean Value Theorem for integrals, we know that there is some $\mu \in [m, M]$, where $m = \inf f$ and $M = \sup f$ over the interval $[x,x+h]$, such that 
    \[ \mu = \frac{1}{h}\int^{x+h}_x f(t)\;dt \]
Now, if $f$ is continuous at $x$, then for all $\varepsilon>0$, there exists a $\delta>0$ such that 
\begin{align*}
    |h| \leq \delta \quad \Rightarrow \quad |f(x) - f(y)|
        &\leq\varepsilon \\
    \Rightarrow \qquad |\mu - f(x)|&\leq\varepsilon \\
    \Rightarrow \quad \left\lvert \frac{F(x+h)-F(x)}{h}- f(x)
        \right\rvert
        &\leq\varepsilon 
\end{align*}
So $f$ is differentiable at $x$ and $F'(x)=f(x)$.
\end{proof}

\begin{thm}\emph{(Change of Variables)}
Suppose that we have $\varphi: [a,b] \rightarrow [\varphi(a), \varphi(b)]$, a differentiable, monotonically increasing function with $\varphi'\in\mathscr{R}([a,b])$. Also suppose that $f\in\mathscr{R}([\varphi(a),\varphi(b)])$. Then $(f\circ \varphi)\cdot\varphi' \in \mathscr{R}([a,b])$ and
\[
    \int^b_a f\left(\varphi(x)\right) \varphi'(x)\;dx
    = \int^{\varphi(b)}_{\varphi(a)} f(u)\;du
\]
\end{thm}
\begin{proof}
   to do 
\end{proof}


\newpage
\subsection{Improper Integrals}

\begin{defn}
(Cauchy Principal Value) Sometimes we want to consider integrating a function over the real line, we take 
\[
    \lim_{b\rightarrow\infty}
    \left(\lim_{a\rightarrow-\infty} \int^b_a f\;d\alpha
    \right) \neq 
    \left(\lim_{a\rightarrow\infty} \int^a_{-a} f\;d\alpha
    \right)
\]
where the second term is the \emph{Cauchy Principle Value}. It may sometimes exist, while the integral on the left does not. 
\end{defn}

\begin{ex}
For the simplest example, take $f(x)=x$. If evaluated in the usual sense, we get $\infty-\infty$, which makes no sense. However, if we evaluate as on the righthand side above, we get a Cauchy Principle Value of $0$.
\end{ex}

\begin{ex}
Now we have an example of a function that is continuous, non-negative everywhere, and $f\not\rightarrow 0$, but $f\in\mathscr{R}([a,\infty))$ nonetheless.
To construct such a function, set up
\[
    f(x) =
    \begin{cases}
        1 & x\in\mathbb{Z} \\
        / & [x - 1/x^2, x] \\
        \text{\textbackslash} & [x, x + 1/x^2] \\
        0 & \text{otherwise}
    \end{cases}
\]
where $/$ and \textbackslash denote increasingly linear and decreasingly linear, respectively. An example is below
\begin{figure}[h!]
   \centering
   %\includegraphics[scale=0.45]{TriangleFunction.pdf}
\end{figure}
\end{ex}

\begin{ex}
Suppose that in addition to the conditions we had above, we also want $f$ to be unbounded. To deal with that, make the triangles higher, but shrink the base faster.
\end{ex}


\newpage
\section{Sequences of Functions}

In this section, we will consider sequences of real-valued functions on the interval $[a,b]$, defined as $\{f_n\}_{n=1}^\infty$. This sequence might approach some function $f$, and will will consider how properties of functions in the sequence translate into properties for the function $f$.

\subsection{Types of Convergence}

\begin{defn} (Pointwise Convergence) A sequence $\{f_n\}_{n=1}^\infty$ converges \emph{pointwise} to $f$ if, for all $x\in[a,b]$,
\[ 
    \lim_{n\rightarrow\infty} f_n(x) = f(x) 
\]
in which case we write $f_n\rightarrow f$, p.w.
\end{defn}

\begin{defn} (Uniform Convergence) A sequence $\{f_n\}_{n=1}^\infty$ converges \emph{uniformly} to $f$ if .ns 
\[ 
    \sup_{x\in[a,b]} |f_n(x) - f(x)| \rightarrow0 
\]
in which case we write $f_n\rightarrow f$ uniformly. This uses the $\sup$-norm metric to define convergence, where the metric is defined 
\[
    d_\infty(f,g)=||f-g||_\infty := \sup_{x\in[a,b]} |f(x)-g(x)|
\]
for the functions $f$ and $g$.
\end{defn}

\begin{thm}
Suppose $f_n\rightarrow f$ uniformly. If each function $f_n$ is continuous on $[a,b]$, then $f$ is continuous on $[a,b]$.
\end{thm}
The last theorem requires \emph{uniform} convergence, not pointwise. Here's an example which demonstrates why.
\begin{ex}
Suppose that we have $f_n(x)=x^n$ on $[0,1]$. Then $f_n\rightarrow f$ pointwise, where
    \[ f(x) = \begin{cases} 0 & x \in [0,1) \\ 1 & x=1 
\end{cases} \]
Each $f_n$ is continuous, but $f$ clearly is not.
\end{ex}

\newpage
\subsection{Integrability}

\begin{thm}
Suppose that $\{f_n\}$ is a sequence of functions, with each $f_n\in\mathscr{R}_\alpha([a,b])$. If $f_n\rightarrow f$ uniformly,
then 
\[ 
    \lim_{n\rightarrow\infty} \int^b_a f_n\;d\alpha
    = \int^b_a \left(\lim_{n\rightarrow\infty} f_n \right)\;
    d\alpha = \int^b_a f\;d\alpha
\]
\end{thm}

Again, we need uniform convergence, and the next example shows why pointwise convergence is insufficient.

\begin{ex}
    \label{Qex}
    Enumerate the rationals on $[0,1]$: $\mathbb{Q}\cap[0,1] = \{q_1, q_2, \ldots\}$. Then set the sequence of functions equal to a sequence of characteristic functions:
\[ 
    f_n = \chi_{\{q_1, \ldots, q_n\}} \in \mathscr{R}([a,b])
    \quad \text{with} \quad
    \int^b_a f_n \; d\alpha = 0
    \qquad \forall n
\]
But we know that the sequence $\{f_n\}$ converges to $f=\chi_{\mathbb{Q}\cap [0,1]}\not\in\mathscr{R}([a,b])$, as
\[
    \overline{\int_a^b} f \; d\alpha = 1 
    \qquad 
    \underline{\int_a^b} f \; d\alpha = 0
\]
because any interval will contain both a real and a rational number since $\mathbb{Q}$ is dense in $\mathbb{R}$.
\end{ex}

\begin{thm}
Let $\alpha\in BV([a,b])$ and $\{f_n\}_{n=1}^\infty\subset \mathscr{R}_\alpha([a,b])$. Also assume that $f_n\rightarrow f$ uniformly on $[a,b]$. Then $f\in\mathscr{R}_\alpha([a,b])$ and 
\[ 
    \lim_{n\rightarrow\infty} \int^b_a f_n\;d\alpha
    = \int^b_a \left(\lim_{n\rightarrow\infty} f_n \right)\;
    d\alpha = \int^b_a f\;d\alpha
\]

\end{thm}



\newpage
\section{Measure Zero and Lebesgue's Theorem}

This section tackles Lebesgue's Theorem, which provides a necessary and sufficient condition for $f\in\mathscr{R}_\alpha([a,b])$, using the concept of measure zero, which will develop in the next subsection.

\subsection{Measure Zero}

\begin{defn}
We say that $X\subset\mathbb{R}$ is \emph{measure zero} (or \emph{negligible}) if, for all $\varepsilon>0$, there exists a set of open intervals $\{I_n\}_{n=1}^\infty$ such that 
\[ 
    \bigcup^\infty_{n=1} I_n \supset X \qquad
    \text{and} \qquad \sum^n_{n=1} |I_n|\leq \varepsilon
\]
\end{defn}

\begin{defn} 
We say that the statement, proposition, or condition $P(x)$ for $x$ on an interval $I$ holds \emph{almost everywhere} (a.e.) if 
\[
    \{ x \in I \; | \; P(x) \text{ is false or fails} \} 
    \;\; \text{is measure zero}
\]
\end{defn}

\begin{prop}
\label{countunion}
Finite sets are of measure zero, and a countable union of measure zero sets is itself measure zero.
\end{prop}
\begin{proof}
The finite case is trivial, so let's consider the second part of the proposition. Suppose that we have a sequence $\{X_n\}_{n=1}^\infty$ with $X_n$ measure zero for all $n$. Then Let $\{I_{n,k}\}_{k=1}^\infty$ be a set of open intervals such that 
    \[
        \bigcup^\infty_{k=1} I_{n,k} \supset X_n 
        \qquad \text{and} \qquad
        \sum^n_{k=1} |I_{n,k}| < \frac{\varepsilon}{2^n}  
    \]
Then we know that 
    \[
        \bigcup^\infty_{n=1}\bigcup^\infty_{k=1} 
        I_{n,k} \supset \bigcup^\infty_{n=1}  X_n 
    \]
and that the double union on the left is countable as the countable union of countable sets. In addition, we know that 
    \[
        \sum^\infty_{n=1}\sum^\infty_{k=1} 
        |I_{n,k}| < \sum^\infty_{n=1} \frac{\varepsilon}{2^n} 
        = \varepsilon
    \]
\end{proof}
\begin{cor}
Any countable set is measure zero, in which case $\mathbb{Q}$ is measure zero as well.
\end{cor}


\begin{ex}
The Cantor Set\footnote{See appendix for construction.} is an uncountable set that is measure zero. We know its uncountable from the proof in the appendix; now we want to show measure zero.
\\
\\
Recall that we constructed $C$ as 
\[
    C = \bigcap^\infty_{n=1} C_n \quad\Rightarrow\quad
    C \subset C_n \quad \forall\;n
\]
So if we can find an index $n^*$ such that we can cover $C_{n^*}$ by arbitrarily small open intervals, we can cover $C$ too.  To find that particular $n^*$, note that each $C_n$ is the union of $2^n$ subintervals, each of length $1/3^n$. 
\\
\\
So given $\varepsilon>0$, we choose $n^*$ so that $\left(\frac{2}{3}\right)^{n^*} < \varepsilon/2$. Then, we note that we can cover each subinterval of $C_{n^*}$ (which we'll index by $k$) by an open interval of length 
\[ 
    |I_k| < \frac{1}{3^{n^*}} + \frac{\varepsilon}{2^{{n^*}+1}} 
\] 
This is the length of each subinterval (which we mentioned above), plus a tiny amount.
\\
\\
Then, when it's time to add up the size of our open cover of $C_{n^*}$ over the $2^{n^*}$ component subintervals, we find
\begin{align*}
    \sum^{2^{n^*}}_{k=1} |I_k| &\leq \sum^{2^{n^*}}_{k=1}
    \left(\frac{1}{3^{n^*}} + \frac{\varepsilon}{2^{{n^*}+1}}\right)
    = 2^{n^*} \cdot 
    \left(\frac{1}{3^{n^*}} + \frac{\varepsilon}{2^{{n^*}+1}}\right)
    \\
    &\leq \left(\frac{2}{3}\right)^{n^*} + 
        \frac{2^{n^*}\varepsilon}{2^{{n^*}+1}}
    = \left(\frac{2}{3}\right)^{n^*} + 
        \frac{\varepsilon}{2}
\end{align*}
And by our choice of $n^*$, we know that $\left(\frac{2}{3}\right)^{n^*} < \varepsilon/2$, which implies
\begin{align*}
    \sum^{2^{n^*}}_{k=1} |I_k| &\leq 
    \frac{\varepsilon}{2} +
    \frac{\varepsilon}{2}
    = \varepsilon
\end{align*}
And so $C$ is measure zero.
\end{ex}

\begin{thm}
Any closed interval $[a,b]$ is not measure zero.
\end{thm}
\begin{proof}
    Suppose that we have some closed interval $I = [a,b]$ that is measure zero. Then given any $\varepsilon>0$, there must be an open cover 
\[  
    \{I_n\}_{n=1}^\infty \supset I 
    \quad \text{s.t.} \quad
    \sum^\infty_{i=1} |I_n|<\varepsilon
\]
Now since $I$ is compact, there must by a finite subcover $\{I_{n_i}\}^k_{i=1} \supset I$ with $\sum^k_{i=1} |J_{n_i}| <\varepsilon$.
\\
\\
But this leads to a contradiction because, via induction, we can show that the finite sum of the lengths of the intervals must be greater than $b-a$.
\end{proof}

\subsection{Applications and Implications of Measure Zero}

We now detail some nice results, and tie this section back to earlier concepts.

\begin{ex}
Continuous a.e. is \emph{not} the same thing as being equal almost everywhere to a continuous function. For example, the function $f=\chi_{\mathbb{Q} \cap [a,b]}$ equals the function $g(x)=0$ almost everywhere. \emph{However}, $f$ is discontinuous everywhere. 
\end{ex}

\begin{thm}
If $f$ is continuous on $[a,b]$ and $f=0$ almost everywhere on $[a,b]$, then $f=0$ on $[a,b]$.
\end{thm}
\begin{proof}
If not, then the continuity of $f$ would let you find a non-zero interval that does not equal zero. And from the theorem in the last subsection, intervals are not measure zero, giving you a contradiction.
\end{proof}

\begin{thm}
If $f$ is a monotonic, then $f$ has, at most, countably many points of discontinuity.
\end{thm}
\begin{proof}
At each point of discontinuity, there is a righthand and lefthand limit of the function that are not equal. Then you can index each discontinuity be a rational in between $f(x^-)$ and $f(x^+)$, the one-sided limits.
\end{proof}

\begin{cor}
If $\alpha\in BV([a,b])$, then $\alpha$ is continuous almost everywhere on $[a,b]$.
\end{cor}

\begin{thm}
If $g$ is monotonic, then $g$ is differentiable almost everywhere. Moreover, $g'\in\mathscr{L}$ as well, i.e. it is Lebesgue integrable.
\end{thm}

\subsection{Lebesgue's Theorem}

\begin{defn} 
For a bounded function $f:[a,b]\rightarrow\mathbb{R}$ and nonempty $I\subset[a,b]$, we define the \emph{oscillation of $f$ on $I$}, denoted $\Omega_f(I)$, as
\[
    \Omega_f(I) := \sup_I f - \inf_I f 
    = \sup_{x,y\in I} |f(x)-f(y)|
\]
We define the oscillation at a particular point $x\in I$ (for $I$ open) as 
\[
\Omega_f(x) = \inf_{I\ni \; x} \Omega_f(I)
\]
And just to be explicit, the infimum is taken over the set of \emph{open} intervals containing $x$.
\end{defn}

\begin{prop}
\label{osccts}
$f$ is continuous at $x$ if and only if $\Omega_f(x)=0$.
\end{prop}

\begin{prop}
\label{rewriteuml}
Tying the concept back to integration, we can write 
\[
    U(f,P)-L(f,P) 
    = \sum^n_{k=1} \Omega_f(I_k) \; |I_k|
\]
where $I_k = [x_{k-1}, x_k]$.
\end{prop}

\begin{lem}
    \label{leblem}
Let $f$ be a bounded function on a closed interval, $J$. If $\Omega_f(x) < \varepsilon$ for every $x$ in $J$, then there exists a partition $P$ of $J$ such that 
\[
    U(f,P)- L(f,P) < \varepsilon |J|
\]
\end{lem}

\begin{proof}
For each $x\in J$, then there's an open interval $I_x \ni x$ such that $\Omega_f(I_x)<\varepsilon$ by assumption and the definition of $\Omega_f(x)$. Thus we can say 
\[
    \bigcup_{x\in J} I_x \supset J
\]
But since $J$ is closed (and thus compact), there's a finite subcover $\{I_1, \ldots, I_n\}$. Let $P=\{x_0, x_1, \ldots, x_n\}$ be the set of endpoints of the intervals $\{I_1\cap J, \ldots, I_n\cap J\}$. Then $\Omega_f([x_{i-1},x_i])<\varepsilon$ for all $i$, and by Proposition \ref{rewriteuml}, we can write
 
\[
    U(f,P)- L(f,P) = \sum^n_{i=1} \Omega_f([x_{i-1}, x_i]) \Delta 
    x_i < \varepsilon |J|
\]
\end{proof}

\begin{lem}
\label{leblem2}
Suppose that $f$ is bounded on $[a,b]$ and that $\varepsilon>0$. Then the set $A = \{x \in [a,b] \; | \; \Omega_f(x) < \varepsilon \}$ is open in $[a,b]$.
\end{lem}

\begin{proof}
To see this, note that $x\in A$ implies there's an open set $I\ni x$ such that $\Omega_f(I)<\varepsilon$, by the definition of $\Omega_f(x)$ and $A$.  Now since $I$ is open and $x\in I$, $x$ is interior to $I$. That means there exists a $\delta>0$ such that $(x-\delta, x+\delta)\subset I$. 

Given this $\delta$, we then have that $|y-x|<\delta$ implies $\Omega_f(y) \leq \Omega_f(I) < \varepsilon$ because $y\in I$ (clearly), and $\Omega_f(y)$ is the minimum over all intervals containing $y$. 

Thus, $y \in A$, and $A$ is open because you can construct an open ball, $B_\eta(x)$ that is entirely in $A$, as we just did.

\end{proof}

\begin{thm}
\emph{\textbf{(Lebesgue's Theorem)}}
Let $f$ be bounded. Then $f\in\mathscr{R}([a,b])$ if and only if $f$ is continuous ``almost everywhere''--i.e. if the set of points at which $f$ is discontinuous is measure zero or ``negligible.''
\end{thm}

\begin{proof}
Let $X=\{x \; | \; \text{$f$ discts at $x$} \}$. Now we don't know whether $X$ is countable or uncountable, but regardless, we can write it as a union of countable sets.  Here's how.

Recall from Proposition \ref{osccts} that if $f$ is discontinuous at $x$, then the oscillation $\Omega_f(x) > 0$. So an equivalent way to write our set $X$ is 
\begin{align*}
    X &= \{x \; | \; \text{$f$ discts at $x$} \} \\
        &= \bigcup_{\delta>0} \{x \; | \; \Omega_f(x)\geq \delta \} 
\end{align*}
($*$) Now the next step is a big deal, because the union above is an \emph{uncountable} union over the uncountable set of $\delta>0$. But with the following trick, we're going to write this uncountable union as a \emph{countable} union instead: 
\begin{align*}
    X &= \bigcup_{\delta>0} \{x \; | \; \Omega_f(x)\geq \delta \} \\
    &= \bigcup^\infty_{n=1} \{x \; | \; \Omega_f(x)\geq 1/n \} =: \bigcup^\infty_{n=1}  X_n
\end{align*}
This works because $X$ is a union of points whose oscillation $\Omega_f(x)$ is non-zero. And by the Archimedian property, if we take $n\rightarrow \infty$ so that $1/n$ is sufficiently small, we'll capture \emph{all} of those points $\Omega_f(x)>0$ eventually, since we can get $0<1/n<\delta$ for all $\delta$. 

In fact, as $n\rightarrow\infty$, the size of each successive set $X_n$ grows to include more and more points. On top of that, it's also clear that $X_n\supset X_m$ for $n>m$. So given all of these facts, we then know that the union of those sets will equal $X$.

So now the task is to show that $X_n$ is measure zero for all $\delta$. If we can do this, then in which case the countable union of measure zero sets will also be measure zero by Proposition \ref{countunion}.
\\
\\
\textbf{($\Rightarrow$ Direction)} So consider $X_m$ (which we want to show is measure zero), and suppose that $\varepsilon>0$ is given. Then since $f\in\mathscr{R}([a,b])$, we know that there exists a partition $P$ such that
\begin{equation}
    \label{lebeg1}
    U(f,P) - L(f,P) = 
    \sum^n_{i=1} \Omega_f(I_i)\; |I_i| < \frac{\varepsilon}{2m} 
\end{equation}
where we rewrote the sum according to Proposition \ref{rewriteuml}, and $m$ corresponds that of $X_m$.
\\
\\
Now since $P$ is finite, it suffices to show that $X_m\setminus P$ is measure zero. And so we can say
\begin{equation}
    \label{lebeg2}
    \forall x \in X_m \setminus P \qquad
    \exists I_{j} \ni x \quad \text{s.t.} \quad
    \Omega_f(I_j) \geq \Omega_f(x) \geq \frac{1}{m}
\end{equation}
In words, this says that if $x$ is in the set $X_m\setminus P$, then it's in some open interval $I_j = (x_{j-1}, x_j)$. (Open since we removed the endpoints; these are our open sets covering $X_m\setminus P$, which we'll show are measure zero.) And that interval will have an oscillation greater than the ``minimum'' oscillation over all intervals containing $x$. And that minimum happens to be greater than or equal to $1/m$, since we're working with $X_m$, which is the set of all points $x$ where $\Omega_f(x)\geq 1/m$.
\\
\\
And so let $I_{j}$ for $j=1,\ldots,k$ denote those intervals containing some $x\in X_m\setminus P$. Then combining Inequality \ref{lebeg1} with the information we have about the oscillation from the inequalities in \ref{lebeg2}, we see that
\begin{align*}
    \frac{1}{m} \sum^k_{j=1} |I_j| \leq 
    \sum^k_{j=1} \Omega_f(x)\; |I_j| &\leq 
    \sum^n_{k=1} \Omega_f(I_i)\; |I_i| < \frac{\varepsilon}{2m} \\
    \Rightarrow \quad 
    \sum^k_{j=1} |I_j| < \frac{\varepsilon}{2}
\end{align*}
And so taking the open sets $I_j = (x_{j-1}, x_j)$, where the endpoints correspond to the partition $P$ we got from the integrability of $f$, we can say that these intervals cover $X_m\setminus P$, and we can make them as small as necessary. 

Adding in a finite number of sufficiently small open sets to account for any points we removed that were in $P$ (which we can easily do), we see that we can cover $X_m$, implying it's measure zero for all $m$.
\\
\\
\textbf{($\Leftarrow$ Direction)} Next, assume that $f$ is continuous almost everywhere. We'll use the properties of $\Omega_f$ to find a good partition. Also, we'll again let $X$ be the set of points at which $f$ is discontinuous, and the sets $X_m$ will be as above as well.

So let $\varepsilon>0$ (which dictates how close we'll have to make $U$ and $L$) be given. Without loss of generality, assume $f$ is not constant, which implies $\Omega_f([a,b]) > 0$.\footnote{Otherwise, $f$ is trivially integrable.} Also choose $m$ such that $\frac{b-a}{m}<\frac{\varepsilon}{2}$. Since $X_m$ is measure zero, we know that there exists a set of open intervals $\{I_n\}_1^\infty$ such that
\[ 
    \bigcup^\infty_{n=1} I_n \supset X_m \qquad  \text{and} \qquad
    \sum^\infty_{n=1} |I_n| < \frac{ \varepsilon}{2\Omega_f([a,b])}
\]
We know that $\Omega_f([a,b])<\infty$ because $f$ is bounded.

Next, we'll use the fact that $X_m$ is closed for all $m$ with respect to the relative topology of $[a,b]$. This is because the following set, by Lemma \ref{leblem2}, is open:
\[ 
    X_m^c = \{ x \in [a,b] \; | \; \Omega_f(x) < 1/m \}
\]
Since $X_m$ is closed and $[a,b]$ is compact, we know that $X_m$ is compact (as a closed subset of a compact space). This means there exists a finite subcover
\begin{equation}
    \label{badpartcover}
    \bigcup^k_{j=1} I_{n_j} \supset X_m
\end{equation}
Now, express the interval not covered by those sets in \ref{badpartcover} as 
\[ 
    [a,b] \setminus \left( \bigcup^k_{j=1} I_{n_j}\right)
    = \bigcup^p_{i=1} J_i 
\]
It's clear that the $J_i$ will be closed and disjoint. We just need to control the oscillation on these intervals, since the ``bad'' parts are already covered by the tiny $I_{n_j}$.

Now by our definitions of everything up to this point, $x\in J_i$ implies that $x\not\in X$, which implies $x\in X^c_m$, which implies $\Omega_f(x) < 1/m$. So we can apply Lemma \ref{leblem} to say that there exists a partition $P_i$ for each $J_i$ such that 
\[ 
    U(f,P_i) - U(f,P_i) \leq \frac{1}{m}|P_i|
\]
Taking $P = \bigcup_{i=1}^p P_i \cup \{a,b\}$, we see that $P$ is a partition of $[a,b]$. Thus
\begin{align*}
    U(f,P) - U(f,P) &= \sum^p_{i=1} \left[
        U(f,P_i) - U(f,P_i)\right] + \sum^k_{j=1}
        \Omega_f(I_{n_j}) |I_{n_j}| \\
    &\leq \frac{1}{m} \sum^p_{i=1} |J_i| 
        + \Omega_f([a,b]) \sum^k_{j=1} |I_{n_j}| \\
    \text{By choice of $m$ at beginning} \quad 
        &\leq \frac{b-a}{m} + \Omega_f([a,b]) 
        \frac{\varepsilon}{2\Omega_f([a,b])} \\
    &\leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} =
        \varepsilon
\end{align*}
\end{proof}

\begin{rmk}
The main method used in the last proof represents fairly standard approach to solving integration problems (and analysis problems, even more generally).  Namely, you separate the function into ``good'' and ``bad'' parts. (In the previous theorem, those parts were determined by the oscillations of $f$ on $[a,b]$.) Then, you make the contribution from the ``bad'' parts very small.
\end{rmk}

\subsection{Applications of Lebesgue's Theorem and Related Theorems}

\begin{cor}
If $f\in\mathscr{R}([a,b])$ and $\int^b_a |f| = 0$, then $f=0$ almost everywhere.
\end{cor}

\begin{proof}
$f$ is continuous almost everywhere (by Lebesgue's Theorem); therefore, $|f|$ is continuous almost everywhere. 

If $f$ were continuous at a point $x$ for which $f(x)\neq 0$, then there exists an interval $I\ni x$ such that $|I|>0$ and $|f|>0$, which would imply $\int^b_a |f| > 0$. Hence we see that that the set of points $\{x \; | \; f(x)\neq 0\} \subset \{x \; | \; f \text{ discont.}\}$, which is of measure zero.
\end{proof}

\begin{cor}
If $f\in\mathscr{R}([a,b])$ and $\int^x_a f = 0$ for all $x\in(a,b]$, then $f(x)=0$ almost everywhere for all $x\in[a,b]$.
\end{cor}
\begin{proof}
We know by Lebesgue that $f$ is continuous almost everywhere. We also know that when $f$ is continuous, $F(x)$ is differentiable and $F'(x)=f(x)$. And clearly, $F'(x)=0$ so that $f(x)=0$ almost everywhere (i.e. when $F$ is differentiable).
\end{proof}

\begin{cor}
If $f\in\mathscr{R}([a,b])$, then $F(x):=\int^x_0 f(t) \; dt$ implies that $F$ is differentiable almost everywhere and $F'(x)=f(x)$ almost everywhere.
\end{cor}

\newpage
\section{$\sigma$-Algebras, Measure Spaces, and Measurable and Simple Functions}

Riemann integrals are good for continuous or piecewise continuous
functions.  But for more irregular and general functions, and
particularly for sequences of functions, Riemann integration breaks
down.

This stems from a deficiency in Riemann integrals owing to the
relatively rigid structure of intervals---the sets over which we
integrate. So this chapter lays the groundwork for replacing intervals
with more general ``measurable sets'', which contain intervals as a
special case.

We'll also develop a generalized notion of ``measure'' for subsets
$A\subset X$, which we'll denote $\mu(A)$, a function for weighting
subsets of the domain. Only in later sections will we restrict $X$ to
the real line, when we consider the more specific Lebesgue Measure and
Integral.



\subsection{Extended Real Number System}

\begin{defn}
We denote the extended reals by $\bar{\mathbb{R}}$, and it is constructed as 
\[ 
    \bar{\mathbb{R}} = {\mathbb{R}} \cup \{-\infty, \infty\}
    = [-\infty,\infty]
\]
Notice, we can now write any intervals with $-\infty$ or $+\infty$ using \emph{closed} brackets now, since they are points in our set; whereas, we could only write $[a,\infty)$ before.

The order relations, addition, and multiplication are extended exactly as one would expect. Only $\infty-\infty$ and $0\cdot \infty$ are undefined.
\end{defn}

\begin{defn}
Open sets of $\bar{\mathbb{R}}$ are defined as the open sets of $\mathbb{R}$, any sets of the form $[-\infty, a)$ or $(a,\infty]$ for $a\in\mathbb{R}$, plus unions of these basic building blocks too.
\end{defn}

\begin{rmk}
For the remainder of this section, we will construct our theory for the \emph{extended} reals (unless otherwise noted) to allow for the greatest possible generality.
\end{rmk}

\subsection{$\sigma$-Algebras and Measurable Spaces}

\begin{defn}
\label{algebra}
(Algebra of Sets)
Let $X$ be any set, and let $\mathscr{A}$ be a collection of subsets of $X$ (so $\mathscr{A}\subset\mathscr{P}(X)$). We say that $\mathscr{A}$ is an \emph{algebra} if 
\begin{enumerate}
    \item $A\in\mathscr{A}\Rightarrow A^c\in\mathscr{A}$
    \item $A,B\in\mathscr{A} \Rightarrow A\cup B\in\mathscr{A}$
\end{enumerate}
Implications of our definition:
\begin{itemize}
    \item Since $A\cap B = (A^c \cup B^c)^c$, we then know that $A,B\in\mathscr{A}\Rightarrow A\cap B\in \mathscr{A}$.
    \item Since $A, A^c \in \mathscr{A}$ and so are intersections, then $\emptyset = A\cap A^c \in \mathscr{A}$.
    \item The trivial set---the simplest algebra---is $\{\emptyset, X\}$.
\end{itemize}
\end{defn}

\begin{defn} 
($\sigma$-algebra)
An algebra $\mathscr{A}$ (satisfying the conditions in Definition \ref{algebra}) is a \emph{$\sigma$-algebra} if it's closed under countable unions,  i.e. whenever $\{A_n\}^\infty_{n=1}$ is a sequence in $\mathscr{A}$, then $\bigcup^\infty_{n=1} A_n \in \mathscr{A}$. As a consequence, in a $\sigma$-algebra, $\bigcap^\infty_{n=1} A_n \in \mathscr{A}$ as well.
\end{defn}

\begin{prop}
If $\{\mathscr{M}_\alpha\}_{\alpha\in I}$ is a collection of $\sigma$-algebras, then $\bigcap_{\alpha\in I} \mathscr{M}_\alpha$ is one too.
\end{prop}

\begin{ex}
The easiest non-trivial $\sigma$-algebra is the power set $\mathscr{P}(X)$.
\end{ex}

\begin{ex}
The \emph{Borel $\sigma$-algebra} is defined as the smallest $\sigma$ algebra that contains all open sets (and hence, all closed sets too). It can be written as the intersection of all $\sigma$-algebras containing all open sets of $X$, which we know to be nonempty, since the power set $\mathscr{P}(X)$ is one such set.
\end{ex}

\begin{ex}
There exist algebras which are not $\sigma$-algebras. Take $X=\mathbb{Z}$. Then take $\mathscr{A} = \{A\subset \mathbb{Z} \; |\; \text{either $A$ or $A^c$ finite}\}$. 
\end{ex}

\begin{defn} 
Let $X$ be a set and $\mathscr{M}$ be a $\sigma$-algebra on $X$. Then the couplet $(X, \mathscr{M})$ is called a \emph{measurable space}. It is ``measur\emph{able}'' in the sense that we can assign a \emph{measure} to it, a concept we'll discuss in this next subsection.
\end{defn}

\begin{defn}
\label{relsigal}
Suppose we have a $\sigma$-algebra $\mathscr{M}$ for $X$. Then for any set $E\subset X$, we call
\[
    \mathscr{M}_E = \{ A \cap E \; | \; A \in \mathscr{M}\}
\]
a \emph{relative} $\sigma$-algebra. But before taking this for granted, we'll have to prove it which we do in the next theorem.
\end{defn}

\begin{thm}
\label{relsigal.thm}
The set $\mathscr{M}_E$ in Definition \ref{relsigal} is, indeed, a $\sigma$-algebra.
\end{thm}
\begin{proof}
To do.
\end{proof}





\subsection{Non-Negative Measures and Measure Spaces}

\begin{defn} 
A \emph{non-negative measure} on a measurable space $(X,\mathscr{M})$ is a set function $\mu: \mathscr{M} \rightarrow [0,\infty]$ that satisfies 
\begin{enumerate}
    \item $\mu(A)<\infty$ for at least one $A\in\mathscr{M}$ 
    \item \emph{Countable additivity} for all sequences $\{A_n\}_{n=1}^\infty$ in $\mathscr{M}$. That means 
        \[
            \mu\left(\bigcup^\infty_{n=1}A_n\right) 
            = \sum^n_{n=1} \mu(A_n)
        \]
        where the $A_i$ are pairwise disjoint.
\end{enumerate}
\end{defn}

\begin{defn} 
If we add a non-negative measure to a measurable space, then the triplet
$(X,\mathscr{M},\mu)$ is called a \emph{measure space}. 
\end{defn}

\begin{ex}
(Counting Measure) One of the easiest examples of a non-negative measure is the \emph{counting measure}, defined as follows. For any set $X$ and $\sigma$-algebra $\mathscr{M} =\mathscr{P}(X)$, define the following measure $\mu$ on measurable space $(X,\mathscr{P}(X))$ to be 
\[
    \mu(A) = \text{card}(A)  \qquad A\in\mathscr{M}
\]
So $\mu$ just counts the number of elements in any set.
\end{ex}

\subsection{Properties of Measure Spaces}
\label{subsec:propmeas}

Measure spaces have a number of properties that follow from the framework and conditions imposed upon them, which we saw in the definitions of $\sigma$-algebras and positive measures. Here, we enumerate those properties and provide justifications for them:
\begin{enumerate}

\item $\mu(\emptyset)=0$. To see this, write for any non-empty set $A=A\cup\emptyset\cup\emptyset\cdots$. Then use countable additivity.

\item Finite additivity, which follows from countable additivity and an argument like (1).

\item If $A,B\in \mathscr{M}$, then $A\setminus B \in \mathscr{M}$. To see this, write $A\setminus B = A \cap B^c$.

\item If $A \subseteq B$, then $\mu(A) \leq \mu(B)$. 
\begin{proof}
This follows because $B = A \cup (B\setminus A)$, which is a union of disjoint sets. Then use finite additivity to say $\mu(B) = \mu(A) + \mu(B\setminus A) \geq \mu(A)$.
\end{proof}

\item Given a nesting of measurable sets $A_1\subset A_2\subset A_3 \subset \cdots$
\[
    \mu\left(\bigcup^\infty_{n=1} A_n\right) = 
    \lim_{n\rightarrow\infty} \mu(A_n)
\]
\begin{proof}
To exchange sums and unions, we need to use countable additivity. But we can only do so if the sequences are pairwise disjoint, so construct the following pairwise disjoint sequence of sets 
\begin{align*}
    B_1 = A_1 \qquad B_2 = A_2 \setminus A_1
    \qquad B_3 = A_3 \setminus A_2 \qquad \cdots
\end{align*}
Then note that we can equivalently work with the sequence $\{B_n\}_1^\infty$ instead of $\{A_n\}_1^\infty$:
\[ 
    A_N = \bigcup^N_{n=1} A_n =
    \bigcup^N_{n=1} B_n 
    \qquad \Rightarrow \qquad
    \bigcup^\infty_{n=1} A_n =
    \bigcup^\infty_{n=1} B_n 
\]
So then we can exploit countable and finite additivity for disjoint sets:
\begin{align*}
    \mu\left(\bigcup^\infty_{n=1} A_n \right)
    = \mu\left(\bigcup^\infty_{n=1} B_n \right)
    &= \sum^\infty_{n=1} \mu(B_n)
        = \lim_{N\rightarrow\infty}\sum^N_{n=1} \mu(B_n) 
    = \lim_{N\rightarrow\infty}
        \mu\left(\bigcup^N_{n=1} B_n\right) \\
    &= \lim_{N\rightarrow\infty}
        \mu\left(\bigcup^N_{n=1} A_n\right) 
    = \lim_{N\rightarrow\infty}
        \mu\left(A_n\right)  
\end{align*}
\end{proof}


\item \emph{Countable Sub-Additivity}: For \emph{any} (non necessarily pairwise disjoint or nested) sequence of sets $\{A_n\}_1^\infty$, we have
    \[ 
        \mu\left(\bigcup^\infty_{n=1} A_n\right)
        \leq \sum^n_{i=1} \mu(A_n)
    \]
\begin{proof}
Similar to the proof of the last property, construct the sets
\[
    B_1 = A_1 \qquad B_2 = A_2 \setminus A_1 
    \qquad B_3 = A_3 \setminus (B_1 \cup B_2)
\]
Clearly, we have $B_i \cap B_j$ for all $i\neq j$. On top of that $B_n \subset A_n$ for all $n$. Thus
\[
    \mu\left(\bigcup^\infty_{n=1} A_n\right)
    = \mu\left(\bigcup^\infty_{n=1} B_n\right)
    = \sum^n_{i=1}\mu(B_n) 
    \leq \sum^n_{i=1} \mu(A_n)
\]
\end{proof}


\item Let $A_1 \supset A_2 \supset A_3 \supset \cdots$. If there exists a $k$ such that $\mu(A_k)<\infty$, then 
    \[
        \mu\left(\bigcap^\infty_{n=1} A_n\right)
        = \lim_{n\rightarrow\infty} \mu(A_n)
    \]
\begin{proof}
Without loss of generality, assume that $k=1$. Then construct the following sets
\[
    B_1 = A_1 \setminus A_2 \qquad
    B_2 = A_2 \setminus A_3 \qquad
    \cdots\qquad
    B_n = A_n \setminus A_{n+1} 
\]
Then, for all $n$, we can express $A_1$ as a disjoint union
\begin{align}
    A_1 &= B_1 \cup B_2 \cup \cdots \cup B_n
    \cup A_{n+1} \notag\\
    \Rightarrow \qquad 
    \mu(A_1) &= \sum^n_{\ell=1} \mu(B_\ell) + 
    \mu(A_{n+1}) \label{prop.6.1}
\end{align}
Next, we can also say that if $x\not\in \bigcup^\infty_{n=1} B_n$, then we must have $x\in A_n$ for all $n$. This implies that we can express $A_1$ as the following disjoint union:
\begin{align}
    A_1 &= \left(\bigcup^\infty_{n=1} B_n \right)\cup 
        \left(\bigcap^\infty_{n=1} A_n\right) \notag \\
    \Rightarrow \qquad 
        \mu(A_1) &= 
        \sum^\infty_{n=1} \mu(B_n) +
        \mu\left(\bigcap^\infty_{n=1} A_n\right) 
        \label{prop.6.2}
\end{align}
And since we know that $\lim_{n\rightarrow \infty} \sum^n_{\ell=1} \mu(B_\ell) = \sum^\infty_{\ell=1} \mu(B_\ell)$, we can deduce from Equations \ref{prop.6.1} and \ref{prop.6.2} (and from the fact that $\mu(A_{n+1}) \leq \mu(A_n) <\infty$) that 
\[ \mu(A_{n+1})\rightarrow 
    \mu\left(\bigcap^\infty_{n=1} A_n\right)
\]
\end{proof}
\end{enumerate}


\begin{ex}
Take $X=\mathbb{Z}$ and $\mathscr{M}=\mathscr{P}(\mathbb{Z})$. Then if we take a measure $\mu(A) = \text{card}\left( A \right)$, the counting measure, simple reasoning shows that $(X,\mathscr{M},\mu)$ is a measure space.
\end{ex}

\begin{ex}
We stipulated in property (7) for our measure space that there must be some $k$ such that $\mu(A_k)<\infty$. Suppose that didn't hold, and we can construct a counterexample. 
    
    Consider $X = \mathbb{N}$ and $\mathscr{M} =\mathscr{P}(\mathbb{N})$. Next let $A_n = \{n, n+1, n+2, \ldots\}$. In this case, $\mu(A_n) = \infty$ for all $n$, but
    \[ 
        \mu\left(\bigcap^\infty_{n=1} A_n\right) = 
        \mu(\emptyset) = 0
    \]
\end{ex}


\subsection{Complete and Relative Measure Spaces}

\begin{defn}
Suppose we have a measure space $(X,\mathscr{M},\mu)$ such that, when taken together,
\[
    \left.
    \begin{array}{r}
        B\in\mathscr{M} \\
        \mu(B) = 0\\
        A\subset B
    \end{array} \right\}
    \quad \Rightarrow\quad
    A\in\mathscr{M}
\]
Then we call $\mathscr{M}$ a \emph{complete measure space}. Said another
way, complete measure spaces contain \emph{all} subsets of measure zero
sets.
\end{defn}

\begin{thm}
Given an abitrary measure space $(X,\mathscr{M},\mu)$, we can extend it
to be a \emph{complete} measure space, denoted
$(X,\bar{\mathscr{M}},\bar{\mu})$ by defining
\[
    \bar{\mathscr{M}} = \{S\cup A \} 
    \quad \text{where} \quad 
    \left\{
        \begin{array}{l}
        S, B\in\mathscr{M} \\
        \mu(B) = 0 \\ 
        A \subset B 
    \end{array}
    \right.
    \qquad \text{and} \qquad
    \bar{\mu}(S\cup A) = \mu(S)
\]
In this way, we solve a logical gap that might occur with just
$(X,\mathscr{M},\mu)$. Namely, there might be subsets of measure zero
sets that aren't in $\mathscr{M}$. 
\end{thm}
\begin{proof}
(Check that $\bar{\mu}$ is Well-Defined) Let $\bar{S}\in\mathscr{M}$ and suppose we have
\[
    \bar{S} = S_1 \cup A_1 = S_2 \cup A_2 
    \quad\text{with}\quad
    A_i \subset B_i, \quad \mu(B_i) = 0, 
    \quad S_i, B_i \in \mathscr{M}
\]
We need to show that $\mu(S_1) = \mu(S_2)$ to show $\bar{\mu}(S_1\cup
A_1) = \bar{\mu}(S_2\cup A_2$. To do so, use monotonicity of $\mu$ on
$\mathscr{M}$, the equivalent representations of $\bar{S}$, and
countable additivity on $\mathscr{M}$:
\begin{align*}
    \mu(S_2) \leq \mu(S_2 \cup B_2) \leq 
        \mu(S_1 \cup B_1) \leq \mu(S_1) + \mu(B_1)
        \leq \mu(S_1) + 0 = \mu(S_1)  \\
    \mu(S_1) \leq \mu(S_1 \cup B_1) \leq 
        \mu(S_2 \cup B_2) \leq \mu(S_2) + \mu(B_2)
        \leq \mu(S_2) + 0 = \mu(S_2)  \\
\end{align*}
Therefore, $\mu(S_1) = \mu(S_2)$.
\\
\\
(Show $\bar{\mathscr{M}}$ is a $\sigma$-algebra) We need to show a few properties.
\begin{enumerate}
\item
First, we show closed under countable unions:
\begin{align*}
    \bigcup^\infty_{n=1} (S_n\cup A_n) = 
    \left(\bigcup^\infty_{n=1} S_n\right)
    \cup \left(\bigcup^\infty_{n=1}\cup A_n\right)
\end{align*}
Morever, it's clear that for the sets $B_n \supset A_n$:
\begin{align*}
    B = \bigcup^\infty_{n=1} B_n &\supset 
    \bigcup^\infty_{n=1} A_n 
    \qquad \text{with} \quad \mu(B) = 0
\end{align*}
This implies that
\[
    \left(\bigcup^\infty_{n=1} S_n\right) \cup B \in 
    {\mathscr{M}} 
\]
which allows us to conclude that $\cup A_n$, as a subset of the measure
zero set $B$, can be joined to the $S_n$ sets so that $\cup (S_n \cap
A_n)\in\bar{\mathscr{M}}$.

\item
Next, we show that complements are in $\mathscr{M}$. So take
\begin{align*}
    (S\cup A)^c = S^c \cap A^c \supset S^c \cap B^c
\end{align*}
Now it's clear that $S^c \cap B^c\in \mathscr{M}$. In addition, we can
rewrite $S^c\cap A^c$ as
\[
    S^c\cap A^c = (S^c \cap B^c) \cup (B\cap A^c)
\]
Then $B \cap A^c = B\setminus A$ will be measure zero, implying that
$S^c \cap A^c\in \bar{\mathscr{M}}$.

\item Countable additivity is straightforward, given how $\bar{\mu}$ is defined.
\end{enumerate}
\end{proof}

\begin{thm}
Given any measure space $(X,\mathscr{M},\mu)$, if $E\subset X$ and $E\in\mathscr{M}$, then the triplet $(E,\mathscr{M}_E,\mu_E)$ is called a \emph{relative measure space} with
\[
    \mathscr{M}_E = \{ A \cap E \; | \; A \in \mathscr{M}\}
    \qquad 
    \mu_E(A) = \mu(A\cap E)
\]
Moreover, $\mathscr{M}_E$ is a proper $\sigma$-algebra and $\mu_E$ will, indeed, be a non-negative measure.
\end{thm}
\begin{proof}
For the proof that $\mathscr{M}_E$ is a $\sigma$-algebra, see Theorem \ref{relsigal.thm}. To show $\mu_E$ is a non-negative measure, we must show countable additivity. To do so, use the fact that any set $B\in\mathscr{M}_E$ can be respresented as $A\cap E$ for some $A\in\mathscr{M}$:
\begin{align}
    \label{relsigal.chain}
    \mu_E\left(\bigcup^\infty_{n=1} A_n \right)
    &= \mu\left(\left(\bigcup^\infty_{n=1} A_n 
    \right) \cap E \right)
    = \mu\left(\bigcup^\infty_{n=1} (A_n 
    \cap E)\right) 
\end{align}
And since $A_n$ is in $\mathscr{M}_E$ for all $n$, we know that it can be represented as $A_n=B_n\cap E$ for some $B_n\in \mathscr{M}$ by construction. So $B_n \cap E \cap E$ will be in $\mathscr{M}$ for all $n$, allowing us to apply countable additivity of $\mu$ to the final representation in Expression \ref{relsigal.chain}. Thus, countable additivity for $\mu_E$ will follow.
\end{proof}


\subsection{Measurable Functions: Motivation and Intuition}

Given any function $f: X\rightarrow\bar{\mathbb{R}}$, the most natural way to characterize the function is to take all $x$ in $X$ and specify a rule for assigning $x$ to $f(x)\in\bar{\mathbb{R}}$. 

\emph{However}, you can flip that logic around to construct another equally valid characterization of the function $f$ via the range. Namely, step over the elements $a\in\bar{\mathbb{R}}$ and define the disjoint collection $\{E_a\}_{a\in\bar{\mathbb{R}}}$ where $E_a = f^{-1}\left(\{a\}\right)$. In this way, we characterize the function \emph{and} partition the space $X$ based on what $f$ maps different subsets of $X$ to.  Some of the sets $E_a$ will be measurable (i.e. they will be in $\mathscr{M}$) while others will not.

So keep in mind that pre-images say a lot about the function, and we'll work with this characterization to define the concept of measurable functions.

\subsection{Measurable Functions: Definition}

We now define the important notion of measurable functions, which will be the class of functions for which we define the more general integral in the next section. Note that they \emph{only} require a measurable space to be defined; we don't need a full measure space unless we want to do integration.

\begin{defn}
Let $(X,\mathscr{M})$ be a measurable space and let $f:X\rightarrow\bar{\mathbb{R}}$ be a function mapping to the extended reals. We say that $f$ is a \emph{measurable function} if $f^{-1}(U)$ is measurable (i.e.  $f^{-1}(U)\in\mathscr{M}$) whenever $U\subset\bar{\mathbb{R}}$ is open.\footnote{And, hence, when $U$ is closed as well.} 
\end{defn}

\begin{rmk}
The notion of a measurable function is \emph{heavily} dependent upon our choice of $\mathscr{M}$.  Specifically, measurable functions are always \emph{relative to a given $\sigma$-algebra}, as we'll see in the next example.
\end{rmk}

\begin{ex}
Let $X$ be any set. If we take $\mathscr{M} = \mathscr{P}(X)$, the power set of $X$, then \emph{any} function $f$ is measurable, since $f^{-1}(U)$ will be in $\mathscr{P}(X)$ for any open set $U\subset \bar{\mathbb{R}}$.
\end{ex}

\begin{thm}
\label{measequiv}
Consider the measurable space $(X,\mathscr{M})$. Take $f:X\rightarrow\bar{\mathbb{R}}$. Then the following are equivalent:
\begin{itemize}
    \item $f$ is measurable.

    \item $\{x \; | \; f(x) > a\} = f^{-1}\left((a,\infty]\right)\in\mathscr{M}$ for all $a\in\mathbb{R}$. 

    \item $\{x \; | \; f(x) \geq a\} = f^{-1}\left([a,\infty]\right)\in\mathscr{M}$ for all $a\in\mathbb{R}$. 

    \item $\{x \; | \; f(x) < b\} = f^{-1}\left([-\infty,b)\right)\in\mathscr{M}$ for all $b\in\mathbb{R}$. 

    \item $\{x \; | \; f(x) \leq b\} = f^{-1}\left([-\infty,b]\right)\in\mathscr{M}$ for all $b\in\mathbb{R}$. 
\end{itemize}
Note: \emph{This is a \emph{very} important theorem, because it's often much easier to work with these pre-image sets than with arbitrary open sets.}
\end{thm}

\begin{thm}
Suppose $f:X\rightarrow\bar{\mathbb{R}} $ is a measurable function on measure space $(X,\mathscr{M},\mu)$. Then $f|_E$ is a measurable function on the \emph{relative} measure space $(E,\mathscr{M}_E,\mu_E)$, where $E\in\mathscr{M}$.
\end{thm}
\begin{proof}
Suppose that we have an open set $B$. Then 
\[
    f^{-1}|_E(B) = f^{-1}(B)\cap E
\]
by definition. Moreover, since $f$ is measurable, $f^{-1}(B)$ is in $\mathscr{M}$ so that $f^{-1}(B) \cap E$ will be in $\mathscr{M}_E$.
\end{proof}

\begin{thm}
Suppose that for $E\subset X$, the function $f:E\rightarrow\bar{\mathbb{R}}$ is $\mathscr{M}_E$ measurable on the relative measure space $(E,\mathscr{M}_E,\mu_E)$. Then the function
\[
    g(x) = \begin{cases}
            f(x) & x \in E \\
            0 & x \in X\setminus E \\
        \end{cases}
\]
will be measurable on $(X,\mathscr{M},\mu)$.
\end{thm}
\begin{proof}
We will use Theorem \ref{measequiv}. So suppose first that $r>0$, in which case
\[
    \{x \in X \; | \; g(x) > r\} = 
    \{x \in E\; | \; f(x) > r\} 
\]
Since $f$ is $\mathscr{M}_E$ measurable, this set will be in $\mathscr{M}_E$, implying that they are in $\mathscr{M}$ as well.
\\
\\
Next, suppose that $r\leq 0$. Then
\[
    \{x \in X \; | \; g(x) > r\} = 
    \{x \in E\; | \; f(x) > r\} 
    \cup \{x\in X\setminus E \}
\]
Now the first set will be in $\mathscr{M}_E$ since $f$ is $\mathscr{M}_E$-measurable, implying it is in $\mathscr{M}$ as well. Moreover, the second set will be in $\mathscr{M}$ as well, because $E\in\mathscr{M}$, so that $X\setminus E$ is also. 


\end{proof}


\subsection{The Class of Measurable Functions}

We now wish to discuss properties of measurable functions, and show that they are closed under certain operations. 

\begin{prop}
Given a measurable space $(X,\mathscr{M})$ and a measurable function $f$, we can generate a $\sigma$-algebra for the range, $\bar{\mathbb{R}}$. Namely, define the set
\[
    S = \{B \subset \bar{\mathbb{R}} \; | \; f^{-1}(B) 
    \text{ \emph{measurable}} \}
\]
Then $S$ is a $\sigma$-algebra on $\bar{\mathbb{R}}$ that contains all open sets of $\bar{\mathbb{R}}$. So we can make a $\sigma$-algebra for the range set, $\bar{\mathbb{R}}$, if we have a $\sigma$-algebra defined on the domain.
\end{prop}

\begin{rmk}
If we consider the Borel Set for $\bar{\mathbb{R}}$, denoted $\mathscr{B}_{\bar{\mathbb{R}}}$, then $f^{-1}(B)\in \mathscr{M}$ for all open sets $B\in\mathscr{B}_{\bar{\mathbb{R}}}$, provided $f$ is a measurable function.
\end{rmk}

\begin{thm}
If $X=\mathbb{R}$ and $\mathscr{M}=\mathscr{B}_\mathbb{R}$ with $f:\mathbb{R}\rightarrow\mathbb{R}$ continuous, then $f$ is measurable.
\end{thm}
\begin{proof}
Recall the definition of continuous functions: for $U\subset\mathbb{R}$ open, then $f^{-1}(U)$ is open, in which case it will be in $\mathscr{B}_{\mathbb{R}}$.
\end{proof}

\begin{lem}
Let $f:X\rightarrow\mathbb{R}$ be measurable and $\varphi: \mathbb{R}\rightarrow\mathbb{R}$ be continuous. Then $\varphi\circ f$ is also measurable.
\end{lem}
\begin{proof}
For an open set $U\subset\mathbb{R}$, 
\begin{align*}
    (\varphi\circ f)^{-1}(U) &= 
        f^{-1}\left(\varphi^{-1}(U)\right)
    = f^{-1}(V) \qquad \text{where }V = \varphi^{-1}(U)
\end{align*}
We know that $V$ is open by the continuity of $\varphi$, and since $f$ is measurable, $f^{-1}(V)\in\mathscr{M}$.
\end{proof}

\begin{cor}
If $f$ is measurable, then so is $c f$ for $c\in\mathbb{R}$, $|f|$, and $f^2$. 
\end{cor}
\begin{proof}
This follows from the previous lemma for real-valued functions, but it also holds for extended real-valued functions. Although we can't use the previous lemma to show this, it is easily shown for the functions listed if we recall Theorem \ref{measequiv}.
\end{proof}

\begin{thm}
Let $\{f_n\}^\infty_1$ be a sequence of measurable functions with $f_n: X\rightarrow\bar{\mathbb{R}}$. Then the following functions are measurable as well:
\begin{itemize}
    \item $f(x) := \sup_n f_n(x)$
    \item $f(x) := \inf_n f_n(x)$
    \item $f(x) := \lim \sup f_n(x) = \lim_{k\rightarrow\infty} \left(\sup_{n\geq k} f_n\right)$.
    \item $f(x) := \lim \inf f_n(x) = \lim_{k\rightarrow\infty} \left(\inf_{n\geq k} f_n\right)$.
\end{itemize}
\emph{\emph{Note:} This is an important proof for when we get to integration and consider the various convergence theorems.}
\end{thm}
\begin{proof}
We prove the statement for $sup_n f_n(x)$ only, as the other cases are similar. For all $a\in\mathbb{R}$, note that 
\begin{equation}
    \label{measseq}
    \{x \; | \; f(x) > a \} = \bigcup_n \; \{x \; | \; f_n(x)>a\}
\end{equation}
To see this, note that if $f(x)>a$, then there's some $n$ for which $f_n(x)>a$, given that same $x$. So the lefthand side is a subset of the righthand side. On the other hand, if $f_m(x)>a$ for some $m$, then the sup over the $f_n$ will be at least as large as $f_m(x)$. Hence the right is a subset of the left, proving equality.

Next, note that we can rewrite Equality \ref{measseq} above as 
\[ 
    f^{-1}\left((a,\infty]\right) = 
    \{x \; | \; f(x) > a \} = \bigcup_n \; \{x \; | \; f_n(x)>a\}
    = \bigcup_n \; f_n^{-1}\left((a,\infty]\right) 
\]
Since $f_n^{-1}\left((a,\infty]\right)$ will be in $\mathscr{M}$ for all $n$ by the measurability of each $f_n$, we can say that the union of all these sets in $\mathscr{M}$ will also be in $\mathscr{M}$ by the definition of $\mathscr{M}$ being a $\sigma$-algebra. Thus, $f^{-1}\left( (a, \infty]\right) \in\mathscr{M}$, implying that $f$ is measurable.
\end{proof}

\begin{cor}
\label{convmeas}
If a sequence of measurable functions $\{f_n\}\rightarrow f$ pointwise, then $f$ is measurable. 
\end{cor}

\begin{rmk}
This shows that measurable functions are potentially more powerful, because recall that continuity isn't preserved so nicely. That requires more stringent uniform convergence. This is another theorem that we'll use when we consider integration and the pillar convergence theorems.
\end{rmk}

\begin{thm}
    \label{summeas}
If $f, g$ are measurable, then $f+g$ is also measurable.
\end{thm}
\begin{proof}
To prove, we will make use of Theorem \ref{measequiv}. That means we have to show that $(f+g)^{-1}\left( (a, \infty]\right)$ is in $\mathscr{M}$ for all $a\in\mathbb{R}$. So let's start with another way to represent that set
\[
    (f+g)^{-1}\left( (a, \infty]\right)
    =\{x \; | \; f(x) + g(x) > a\} 
\]
But we can rewrite the expression as follows, rearranging terms a bit as we go:
\begin{align*}
    \{x \; | \; f(x) + g(x) > a\} &= 
        \{x \; | \; f(x) > a - g(x) \}  \\
    &= \bigcup_{r\in \mathbb{R}} 
        \left( \{ x \; | \: f(x) > r\}
        \cap \{ x \; | \; r > a - g(x) \}
        \right) \\
    &= \bigcup_{r\in \mathbb{R}} 
        \left( \{ x \; | \: f(x) > r\}
        \cap \{ x \; | \; g(x) > a - r \}
        \right) 
\end{align*}
Now we'll rewrite the intersection above, naming the sets in the intersection. On top of that, we'll also take the union over $\mathbb{Q}$ instead of $\mathbb{R}$. See the end of the proof for why we can do that, but in the meantime.
\begin{align*}
    \{x \; | \; f(x) + g(x) > a\} 
    &= \bigcup_{r\in \mathbb{Q}} 
        A_r \cap B_r  \\\\
    \text{where } \quad 
        A_r = \{ x \; | \: f(x) > r\} &\qquad
        B_r = \{ x \; | \; g(x) > a - r \}
\end{align*}
Now since $f$ is measurable and $g$ is measurable, we know that $A_r = f^{-1}\left( (r, \infty]\right)$ and $B_r = g^{-1}\left( (a-r, \infty])\right)$ will both be in $\mathscr{M}$. And thus, their intersection will be, and a countable union will be as well, implying that $f^{-1}\left( (a,\infty]\right)$, our starting set, is in $\mathscr{M}$. And so $f+g$ is measurable.

Oh, and that part about taking the union over $\mathbb{Q}$ instead of $\mathbb{R}$? We can do that for this reason. Suppose there's some $r \in \mathbb{R} \setminus \mathbb{Q}$ such that $f(x)>r$ and $r>a-g(x)$. Also suppose, without loss of generality, that $r>0$. 

Well then, since the rationals are dense in the reals, we know that there's a rational, call it $q$ between $r$ and $a-g(x)$ for all $f\in\mathbb{R}$. That means $f(x)>r>q$. It also means that $q>a-g(x)$. Thus, $r$ and $q$ accomplish the same task: we'll have \emph{both} $f(x)>n$ \emph{and} $n>a-g(x)$ whether $n=r$ or $n=q$. So we'll hit all the points we need to if we just take the union we saw above over $\mathbb{Q}$ instead of $\mathbb{R}$.
\end{proof}

\begin{cor}
This last theorem shows that measurable functions form a \emph{vector space}, since they are closed under addition and scalar multiplication.
\end{cor}

\begin{cor}
If $f$ and $g$ are measurable functions, then so is $fg$.
\end{cor}
\begin{proof}
Recall that we can write the product
\[ 
    fg = \frac{1}{2}\left[(f+g)^2 - f^2 - g^2\right]
\]
Then apply the theorems we had about sums and continuous compositions.
\end{proof}

\begin{cor}
If $\{f_n\}$ is a sequence of measurable functions and $\sum^\infty_{n=1} f_n$ converges, then $\sum^\infty_{n=1} f_n$ is also measurable. 
\end{cor}
\begin{proof}
We know that $S_N = \sum^N_{i=1} f_n$ is also measurable by Theorem \ref{summeas}. Then, since $S_N\rightarrow F$, we know that $F$ is measurable too by Theorem \ref{convmeas}.
\end{proof}


\subsection{General Notions of Measure-Zero and Convergence}

We now develop a more general notion of concepts we previously defined only for subsets of $\mathbb{R}$,extending to arbitrary measure spaces in the process.

First among these concepts is measure-zero, which will then allow us to define more general alternatives to pointwise and uniform convergence for a sequence of functions $\{f_n\}$.  This is less restrictive because $\{f_n\}$ won't have to converge to a function $f$ \emph{everywhere}. We'll just require that the set of points were the sequence does \emph{not} converge is very small.

\begin{defn}
Suppose we have a measure space $(X,\mathscr{M},\mu)$, with $A, E\in\mathscr{M}$. Then, if a property $P$ holds for every $x\in E\setminus A$ and if $\mu(A)=0$, we say that $P$ holds $\mu$-almost everywhere (or $\mu$-a.e.) on $E$.
\end{defn}


\begin{defn}
Consider the measure space $(X,\mathscr{M},\mu)$. We say that a sequence functions $\{f_n\}$ converges $\mu$-almost everywhere to $f$ either uniformly (or pointwise) if the set of points $S$ where $f_n \not\rightarrow f$ uniformly (or pointwise) is such that $\mu(S) = 0$.
\end{defn}

\begin{thm}
Let $(X,\mathscr{M},\mu)$ be a measure space and let $f$ and $\{f_n\}$ be measurable functions on $X$ such that for every $\delta>0$, 
\begin{equation}
    \label{convinmeas}
    \lim_{n\rightarrow\infty} \mu\left(
    \{x \; : \; |f_n(x) - f(x)| > \delta\}\right)
    = 0
\end{equation}
Then we say $f_n$ converges \emph{in measure} to $f$. Moreover, there exists a subsequence $\{f_{n_k}\}$ that converges pointwise to $f$ $\mu$-almost everywhere, i.e there is an $S\in\mathscr{M}$ with $\mu(S)=0$ such that  
\[
    \lim_{k\rightarrow\infty} f_{n_k}(x) = f(x)
    \qquad \forall x \in X\setminus S
\]
Before going to the proof, we first offer an example of function of why this only holds for a \emph{subsequence}, and why Property \ref{convinmeas} does \emph{not} imply pointwise convergence $\mu$-a.e. for the original sequence $\{f_n\}$.
\end{thm}
\begin{rmk}
Key idea: ``Converge in measure'' $\Rightarrow$ ``Convergent Subsequence''
\end{rmk}
\begin{ex}
    Consider the shrinking, sliding indicator function on $[0,1]$ defined
\begin{align*}
    f_1 &= \Chi_{[0,\;1/2]} \qquad
    f_2 = \Chi_{[1/2,\;1]} \\
    f_3 &= \Chi_{[0,\;1/3]} \qquad
    f_4 = \Chi_{[1/3,\;2/3]} \qquad
    f_5 = \Chi_{[2/3,\;1]} \\
    f_6 &= \Chi_{[0,\;1/4]} \qquad
    f_7 = \Chi_{[1/4,\;2/4]} \qquad
    f_8 = \Chi_{[2/4,\;3/4]} \qquad
    f_9 = \Chi_{[3/4,\;1]} \\
    \vdots & 
    \qquad\qquad\qquad\qquad \vdots
    \qquad\qquad\qquad\qquad \vdots
    \qquad\qquad\qquad\qquad \vdots
\end{align*}
We keep shrinking the size of the subintervals, then have the function step over them.
\\
\\
Clearly, the subintervals where $f_n$ is away from 0 (equal to 1) become arbitrarily small as $n\rightarrow\infty$, so that Condition \ref{convinmeas} holds. \emph{But}, each point will be 1 \emph{infinitely many} times. So while the function $f_n\rightarrow 0$, we can't say $f_n\rightarrow 0$ \emph{pointwise}. This is the reason for our caution in the last Theorem where we only claimed pointwise convergence for a \emph{subsequence}.
\\
\\
Now let's see why the restriction to a subsequence works.
\end{ex}
\begin{proof}
Suppose Property \ref{convinmeas} holds. Now we'll complete the proof in two separate steps.
\\
\\
First, start by fixing $\delta = 1/j$, and apply Property \ref{convinmeas}:
\begin{equation}
    \lim_{n\rightarrow\infty} \mu\left(
    \{x \; : \; |f_n(x) - f(x)| > 1/j \}\right)
    = 0
\end{equation}
But now suppose we stop short of that limit. That is, suppose we want to look at $f_{n_j}$ where $n_j$ is finite, though still large enough to get us really \emph{close} to the limit above. And by close, let's say $1/2^j$ close. So now, we choose $n_j$ such that    
\[
    \mu(\{ x \; : \; |f_{n_j}(x) -f(x)|>1/j\}) < \frac{1}{2^j} 
\]
To be more compact in our notation, let's now define $E_j = \{ x \; : \; |f_{n_j}(x) -f(x)|>1/j\}$. So now, we can form the set
\[
    S = \bigcap^\infty_{k=1} \bigcup_{j=k}^\infty E_j 
    \quad \text{where}\quad
    \mu(S)=0
\]
Finally, if $x\in S$ then $x\not\in\bigcup^\infty_{j=k}$ for some $k$, so that $x\not\in E_j$ for all $j=k,k+1,\ldots$, which implies that $|f_{n_j}(x)-f(x)|\leq 1/j$.

\end{proof}

\begin{defn}
Although we will discuss $L^1$ in more detail later, it is now useful to define convergence in $L^1$ for a sequence of functions $\{f_n\}\subset\mathscr{L}(X,\mathscr{M},\mu)$. We say $f_n \rightarrow f$ in $L^1$ if 
\begin{align*}
    \lim_{n\rightarrow\infty} \int_X |f_n-f| \; d\mu
    = 0
\end{align*}
The definition of the integral above comes in the next section.
\end{defn}



\newpage
\subsection{Simple Functions}

Before we get into the specifics of simple functions, note that we approximated the Riemann integral by (effectively) a step function over the intervals of $[a,b]$. In fact, we can cast a Riemann sum in the notation we'll see for simple functions:
    \[ S(f,P,T) = \sum^N_{i=1} c_i \chi_{I_i} \]
We'll do something exactly similar in this section.

\begin{defn}
Let $(X,\mathscr{M})$ be given. Also let $A=\{A_1, \ldots, A_N\}$ be a disjoint collection in $\mathscr{M}$. Then any function that can be written 
\begin{equation}
    \label{simplefn}
    f = \sum^N_{i=1} c_i \cdot \chi_{A_i} \quad
    \text{ where } A_i = f^{-1}(c_i)
\end{equation}
is called a \emph{simple function}. Thus, $A$ is a \emph{measurable partition} of $X$. Said even more simply, simple functions have a finite range, $\{c_1, \ldots, c_N\}$, making Equation (\ref{simplefn}) a very natural, compact representation.
\end{defn}

\begin{rmk} Simple functions form a vector space.
\end{rmk}

\begin{thm}
\label{lebapprox}
Let $f: X\rightarrow[0,\infty]$ be measurable. Then there exists a sequence $\{s_n\}^\infty_1$ of simple and monotonic functions 
\[
    0 \leq s_1 \leq s_2 \leq \cdots \leq f 
    \qquad \forall x\in X
\]
such that $s_n\rightarrow f$ (pointwise, or ``everywhere'') from below.
\end{thm}
\begin{proof}
We prove by successive approximation. Let's denote how we'll partition our intervals and then construct our approximation functions:
\begin{align*}
    [0,1) \cup [1,\infty] = [0,\infty] \quad&\quad  
        s_1 := 0\cdot \chi_{f^{-1}([0,1) )}
        + 1 \cdot \chi_{f^{-1}([1,\infty] )} \\
    [0,1/2) \cup [1/2,1] \cup [3/2, 2)
        \cup [2, \infty] = [0,\infty] \quad&\quad  
        s_2 := 0\cdot \chi_{f^{-1}([0,1/2) )}
        + \frac{1}{2} \chi_{f^{-1}([1/2,1] )} \\
    &\qquad
        + \cdots
        + 2 \cdot \chi_{f^{-1}([2,\infty] )} 
\end{align*}
So what are we doing here? Well, in the case of $s_1$, for all those points $x\in X$ that $f$ maps to $[0,1)$, we weight them by $0$, the minimum possible value $f(x)$ could take for $x\in f^{-1}([0,1))$. Similarly, for all points $x\in X$ that $f$ maps to $[1,\infty]$, we weight them by $1$, the minimum possible value \emph{they} could take.  

And so we see the basic logic here. Each $s_n$ splits the range of $f$, which is $[0,\infty]$ into two parts. The first is a finely approximated part, running from $[0,n]$ where we have many subdivisions of $[0,n]$. Second, there's a badly approximated part, where we just take the whole $[n,\infty]$. We pull these subdivisions of $[0,\infty]$ back into $X$, allowing us to partition the space $X$. Then, for each point in the pre-image $f^{-1}([a,b))$, we assume that it maps to the \emph{smallest possible} value it could, $a$.

And so we work out the process we saw above, first extending the finely approximated part of $f$'s range by one integer span with each $n$, but then halving the size of those fine subdivisions. We also continue to approximate $f$ with the minimum value, giving us
\[
    s_n = \left(\sum^{n\cdot 2^{n-1}}_{k=0}\frac{k}{2^{n-1}}
    \chi_{f^{-1}\left( \left[\frac{k}{2^{n-1}}, \;
        \frac{k+1}{2^{n-1}} \right)\right)}\right)
        + n \chi_{f^{-1}\left([n,\infty]\right)}
\]
Now it's clear that $s_n$ approaches $f$ from above, since with each $s_n$, we're taking the \emph{smallest possible} approximation of $f$. So we can never have $s_n>f$ for any $f$. It's also clear that $s_n\leq s_m$ for $m>n$ for the same reasons that a finer partition can only increase the value of a Lower Darboux sum.

Finally, we just wish to show that we, in fact, do have $s_n(x)\rightarrow f(x)$ for all $x\in X$. Let's take it case by case.
\begin{enumerate}
\item Suppose that $f(x)=\infty$. Since $x\in f^{-1}([n,\infty])$ for all $n$, we know that 
\[
    \lim_{n\rightarrow\infty} s_n(x) = \lim_{n\rightarrow\infty}
    n\chi_{f^{-1}([n,\infty])} = \lim_{n\rightarrow\infty}
    n = \infty = f(x)
\]
\item Next, suppose that $f(x) = 0$. Then $x\in f^{-1}([0,1/2^{n-1}))$ for all $n$, so 
\[
    \lim_{n\rightarrow\infty} s_n(x) = \lim_{n\rightarrow\infty}
    \frac{0}{2^{n-1}}\chi_{f^{-1}([0,1/2^{n-1}])} = 
    \lim_{n\rightarrow\infty} 0 = 0 = f(x)
\]
\item Finally, suppose $f(x)\in(0,\infty)$. Then, $N>f(x)$ for some $N$. That implies 
\[ 
    f(x) \in \left[\frac{k}{2^{N-1}}, \frac{k+1}{2^{N-1}}\right)
        \qquad
    \text{for some $k\in\{0,1,\ldots,N\cdot 2^{N-1}\}$} 
\]
This, in turn, implies that
\[
    f(x) - S_N \leq \frac{1}{2^{N-1}}
    \quad \Rightarrow\quad
    \lim_{n\rightarrow\infty}  S_N  =
    f(x)
\]
\end{enumerate}
\end{proof}


\newpage
\section{General Theory of Measure Integration}

In this section, we define a general theory of integration of measurable functions, where the integral is denoted:
\[
    \int_X f \; d\mu 
\]
From this definition, we see that we need a measure space $(X,\mathscr{M},\mu)$ to make sense of this object.\footnote{The $\sigma$-algebra, $\mathscr{M}$ is necessary so we can work with simple $\mathscr{M}$-measurable functions that approximate $f$.} In the next section, we will restrict to the case where $X$ is real line and $\mu$ is the Lebesgue Measure, to obtain the Lebesgue Integral, a subset of this general case.

\subsection{Intuition for General Integration}

What we did in the proof of Theorem \ref{lebapprox} is \emph{the key} to our general theory of integration.  And it all has to do with how we partitioned $X$.

Namely, we partitioned $X$ in almost \emph{the complete opposite way} compared to what we did for Riemann Integrals. Instead of imposing that our partitions of $X$ will be intervals, we partitioned $X$ by what the points $x\in X$ \emph{get mapped to} by $f$. The sets of this partition could be crazy---things way more complicated that an interval $[x_{i-1}, x_i]$, because the $x\in f^{-1}([a,b))$ don't even have to be near each other. And that's key.

So instead of starting with a partition of $X$ into intervals and studying how a function $f$ behaves on those intervals, we look at the range of values $f$ can take, then pull it back to $X$.

\paragraph{Metaphor}
A wise man, Sam Kapon, in quoting Lebesgue himself, described the situation in the following way. Your coffee is rung up for \$1.74. Riemann starts taking successive coins out of his pocket one by one until he has enough to pay. He does so blindly, without any reference to their value (their $f(x)$) and to the fact that one coin could represent different amounts.

On the other hand, Mr. Lebesgue first takes all of the coins out of his pocket and surveys their value (their $f(x)$) before putting down a series of coins that makes sense \emph{given} their value. He pays in a way that makes a distinction between quarters, dimes, and nickels rather than lumping them all together simply as coins. 

\subsection{Notation}

In this section, many theorems will have conditions like ``suppose $s$ is a simple, nonegative, measurable function'', or some subset, combination, or permutation thereof. To help clarify, the theorems, definitions, and examples in this section will have flags after them to denote these restrictions. 

So $(s, +, m)$ measurable function will denote that the result concerns ``simple, non-negative, measurable functions'', which is the most restrictive case we'll encounter. The absence of one of those markers says that the result has sufficient generality that the restriction is not needed.

\subsection{Integration of Non-Negative Measurable Functions}


\begin{defn} 
\label{simpleint}    
(s, m)
Let $s$ be a simple measurable function on the measure space $(X,\mathscr{M},\mu)$. Then define the integral of $s$ over $X$ with respect to $\mu$ as 
\begin{equation}
    \int_X s \; d\mu = \sum^n_{i=1} c_i \; \mu(A_i)
\end{equation}
where, as in our definition of a simple function,  $A_i = s^{-1}(c_i)$.
\end{defn}

\begin{rmk}
I also want to mention that this definition of the integral does, indeed, make a lot sense. We see that this definition prescribes that we average $s$ over $X$.  It says simply to weight the values of the function by, effectively, how many points of the domain map there (i.e. ``the measure of their preimage'').This is similar to what we did for the Riemann integral, only now, our $A_i$ partitioning the domain can be more arbitrary measurable sets, rather than more restrictive intervals.
\end{rmk}

\begin{defn} (+, m)
Consider the measure space $(X,\mathscr{M},\mu)$, and let $f\geq0$ be a measurable function. Then define
\[
    \int_X f \; d\mu := \sup_{0\leq s\leq f} \int_X s \; d\mu
\]
where $s$ is any such simple function, and the rightand side integral is defined as in Definition \ref{simpleint}. Note that the integral is \emph{only} defined for measurable functions.
\end{defn}


\begin{ex}
    Next, suppose that we have the measure space $(\mathbb{N},\mathscr{P}(\mathbb{N}), \mu)$, where $\mu$ is the counting measure. Then any function $f: \mathbb{N}\rightarrow[0,\infty]$  will be measurable, and functions are simply sequences, where we use $f=\{f(n)\}_1^\infty$ to denote any arbitrary function on $[0,\infty]$.\footnote{Recall that a sequence is a function from the natural numbers to some set ($[0,\infty]\subset\bar{\mathbb{R}}$, in this case).} I claim that the value of the integral can be written 
\begin{equation}
    \int_X f \; d\mu = \sum^\infty_{n=1}f(n) 
    \label{ex4.8.3.1}
\end{equation}
\begin{proof}
To prove this, we now consider a few cases for the integral of $f$:
\begin{enumerate}
    \item {\sl $f$ simple}: Start by unpacking the definition of the integral,
    \begin{equation}
        \label{ex4.8.3.2}
        \int_X f \; d\mu := \sum^n_{i=1} c_i \; \mu(A_i)
    \end{equation}
    Now we consider whether Equation \ref{ex4.8.3.2} does indeed equal Equation \ref{ex4.8.3.1} given all the possible values \ref{ex4.8.3.2} could take on:
    \begin{enumerate}
        \item Suppose that Equation \ref{ex4.8.3.2} equals $\infty$. Then either there's a $c_i=\infty$ (in which case Equation \ref{ex4.8.3.1} would clearly be infinite as well) or there exists a $0< c_i<\infty$ such that $f = c_i$ at infinitely many points. Then $f(n)=c_i$ for infinitely many $n$, which implies that Equation \ref{ex4.8.3.1} equals $\infty$ as well.
        \item Suppose that Equation \ref{ex4.8.3.2} is less than $\infty$. Then by the same reasoning as we had before, $c_i<\infty$ for all $i$, and there must be only finitely many non-zero terms. In that case, we must also have an $N$ such that $f(n)=0$ for all $n>N$, in which case Equation \ref{ex4.8.3.1} equals Equation \ref{ex4.8.3.2}.
    \end{enumerate}

\item {\sl $f$ is any non-negative function on $\mathbb{N}$}: First, assume that $f$ is finite valued.\footnote{In the case where it is \emph{not} finite valued, the statement is trivially true.}  Then let $s_N = f\cdot\chi_{[1,N]}$. Because we're restricting the non-zero values of the function $s_N$ to a finite domain, it will have finite range too: $\{f(n)\}$ for $n=\{1,\ldots,N\}$, and 0 for $n>N$. 
    
So clearly, $s_N$ is a simple function such that $0\leq s_N \leq f$ for all $N$, since we're just considering non-negative functions. That means
\begin{equation}
    \label{4.8.3tosub}
    \int_\mathbb{N} s_N \;d\mu \leq
    \int_\mathbb{N} f \;d\mu 
\end{equation}
since the latter is the sup over all simple functions with $0\leq s\leq f$. But we can simplify the integral of $s_N$, using the fact that Equation \ref{ex4.8.3.1} is true for simple functions like $s_N$ (which we proved in Case 1 of this exercise):
    \[
        \int_\mathbb{N} s_N \;d\mu = \sum^\infty_{n=1}s_N(n) 
        = \sum^N_{n=1} f(n)
    \]
Substituting this expression into Equation \ref{4.8.3tosub}, and taking the limit, we get that 
\begin{equation}
    \label{toreverse}
    \sum^\infty_{n=1} f(n) \leq \int_\mathbb{N} f\; d\mu 
\end{equation}
Note that the righthand side is unaffected by the limit; it is independent of $N$.

Next, we need to show that we can reverse the inequality in Equation \ref{toreverse} to show equality as in Equation \ref{ex4.8.3.1}. To do so, suppose that $\sum^\infty_{n=1} f(n)<\infty$, otherwise there is nothing to prove. Then consider a simple function $s$ such that $0\leq s \leq f$. Then it's clear that 
\[
    \sum^\infty_{n=1} s(n)<\infty
\]
It's also clear that, since $s$ is simple, we can write it as
\[
    \int_\mathbb{N} s \; d\mu = \sum^\infty_{n=1} s(n)
    \leq \sum^\infty_{n=1} f(n)
\]
given the work we did above for the case where $f$ is simple. Finally, note that since $s$ was an arbitrary simple function, since the integral of $f$ involves taking the sup over all simple functions, and since $\sum^\infty_{n=1}f(n)$ does not depend upon $s$, we can assert that
\[
    \int_\mathbb{N} f \; d\mu \leq \sum^\infty_{n=1} f(n)
\]
which, together with Equation \ref{toreverse} establishes Equation \ref{ex4.8.3.1} for arbitrary non-negative functions on $(\mathbb{N}, \mathscr{P}(\mathbb{N}), \mu)$.
\end{enumerate}
\end{proof}
\end{ex}


\subsection{Integration of Arbitrary Measurable Functions}

Here, we relax the assumption that $f$ is non-negative or simple.

\begin{defn} (m)
For any measurable function $f$ in $(X,\mathscr{M},\mu)$, define
\[
    f_+(x) := \max\{f(x), 0\}
    \qquad
    f_-(x) := -\min\{f(x), 0\}
\]
Because both $f$ and $0$ are measurable, these functions that equal the pointwise max and min will \emph{also} be measurable. Thus $f_+$ and $f_-$ are measurable. Also note, by our definition, 
\begin{itemize}
    \item $f = (f_+) - (f_-)$ and $|f| = (f_+) + (f_-)$
    \item $f_+$ and $f_-$ will be non-negative \emph{everywhere}.  
\end{itemize}
These items will provide a link to the results from the previous subsection.
\end{defn}

\begin{defn} (m)
Let $f$ be any \emph{arbitrary} measurable function (i.e. non neccessarily non-negative) on the measure space $(X,\mathscr{M},\mu)$. Then define 
\[
    \int_X f\; d\mu := \int_X f_+ \; d\mu - \int_X f_- \; d\mu
\]
provided that at least one is finite---otherwise the integral is undefined.

Moreover, we say that $f$ is integrable on $X$ with respect to $\mu$ if \emph{both} $\int_X f_+ \; d\mu$ and $\int_X f_- \; d\mu$ are \textbf{finite}, denoted $f\in\mathscr{L}(X,\mathscr{M},\mu)$, the set of all integrable functions $f:X\rightarrow\mathbb{R}$.
\end{defn}

\begin{ex}
    If we take $(\mathbb{N}, \mathscr{P}(\mathbb{N}), \mu)$ where $\mu$ is the counting measure, then $f$ is integrable if and only if $\sum^\infty_{n=1}|f(n)|<\infty$, as $|f| = f_+ + f_-$.
\end{ex}

\subsection{Properties of $\mathscr{L}(X,\mathscr{M},\mu)$}

\begin{thm} 
\label{lebprops}    
\emph{Properties of $\mathscr{L}(X,\mathscr{M}, \mu)$:} Given $f,g\in\mathscr{L}(X,\mathscr{M},\mu)$,
    
    \begin{enumerate}
        \item {Monotonicity}: If $f\leq g$, we have $\int_X f \leq\int_X g$.
        \item If $f$ is measurable and bounded, and if $\mu(X)<\infty$, then $f\in\mathscr{L}(X,\mathscr{M},\mu)$.
        \item {Homogoneity}: $f\in\mathscr{L}(X,\mathscr{M},\mu)$ if and only if $cf\in\mathscr{L}(X,\mathscr{M},\mu)$ for all $c\in\mathbb{R}$, the non-extended reals.
        \item If $\mu(X)=0$, then \emph{any} measurable function $f: X\rightarrow\bar{\mathbb{R}}$ will be integrable with $\int_X f \; d\mu=0$, even if $f=\infty$ on $X$.
    \end{enumerate}
\end{thm}
\begin{proof} We consider each in turn:
\begin{enumerate} 

    \item Note that if $f$ and $g$ are both non-negative, $g\leq f$ implies
\[
    \{ s \; | \; 0\leq s\leq f \} \subset
    \{ s \; | \; 0\leq s\leq g \}  \quad\Rightarrow\quad
    \int_X f \; d\mu \leq
    \int_X g \; d\mu 
\]
For general $f\leq g$, we have $f_+ \leq g_+$ and $f_- \geq g_-$ which implies that 
\[
    \int_X f_+ \; d\mu \leq 
    \int_X g_+ \; d\mu
    \qquad\qquad 
    -\int_X f_- \; d\mu \leq 
    -\int_X g_- \; d\mu
\]
Taken together, the desired result follows.

    \item This is a corollary to the first part.  
        
    \item If $s$ is a simple function, then $cs$ is also a simple function. And so we can write that $\int_X cf \; d\mu = c\int_X f \; d\mu$.
        
    \item Note that all simple functions will integrate to zero, so that the sup over simple functions will be zero as well.
\end{enumerate}
\end{proof}

\begin{rmk}
You may have noticed that we didn't include the usual linearity property. We will eventually have this as a consequence of pillar convergence theorems, but it turns out to be tougher for the more general integral we've developed. We need to do the hard work in the next subsections to establish this property.
\end{rmk}

\begin{thm}
If $f,g\in\mathscr{L}(X,\mathscr{M},\mu)$ and $f=g$ $\mu$-a.e. on $X$, then 
\[
    \int_X f \; d\mu = \int_X g \; d\mu
\]
\end{thm}
\begin{proof}
If $f=0$ $\mu$-a.e., then it must be that $f_-$ and $f_+$ equal 0 $\mu$-a.e. In that case, by monotonicity and non-negativity of $f_-$ and $f_+$, 
\begin{align*}
    0 \leq f_- \leq 0  
    \quad \Rightarrow \quad
    0 \leq \int f_- \leq 0  \\
    0 \leq f_+ \leq 0  
    \quad \Rightarrow \quad
    0 \leq \int f_+ \leq 0  
\end{align*}
\end{proof}


\begin{thm}
Suppose $f$ is measurable and $f\geq 0$ for all $x\in X$. Then 
\[
    \int_X f \; d\mu = 0 \quad \Rightarrow\quad
    f = 0 \text{ $\mu$-a.e.}
\]
\end{thm}
\begin{proof}
Choose a sequence of simple functions $\{s_n\}$ that converge to $f$ from below such that 
\begin{align*}
    0 \leq s_n \leq f 
    \quad\Rightarrow\quad
    0 \leq \int s_n \leq \int f = 0
\end{align*}
That implies 
\[
    \mu\left(\{x \; | \; s_n > 0 \}\right) = 0
    \qquad \forall n
\]
otherwise an approximation of the integral by a simple function would be positive, implying $\int f$ is positive. So then we can write
\[
    \{x \; | \; f>0 \} =    
    \bigcup^\infty_{n=1} \{x \; | \; s_n>0 \} 
\]
Then, by countable additivity
\[
    \mu\left(\{x \; | \; f>0 \} \right)=    
    \mu\left(\bigcup^\infty_{n=1} \{x \; | \; s_n>0 \} 
    \right) = 0
\]
\end{proof}

\begin{thm}
$f\in\mathscr{L}(X,\mathscr{M},\mu)$ if and only if $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$.
\end{thm}
\begin{note}
This is an important theorem, since it means we that we can prove a function $f$ is in $\mathscr{L}(X,\mathscr{M},\mu)$ just by proving that $|f|$ is, which is often \emph{much}, much easier to work with since it is non-negative, allowing us to work with our integration primitive: simple functions. So we can approximate arbitrary integrals of $|f|$ with integrals of simple functions, then assert that integrability (or non-integrability) of $|f|$ holds for $f$ too.
\end{note}
\begin{proof}
($\Rightarrow$ Direction) Suppose that $f\in\mathscr{L}(X,\mathscr{M},\mu)$. Then $f_-, f_+\in\mathscr{L}(X,\mathscr{M},\mu)$. Since $|f|=f_+ + f_-$, we can conlcude that $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$ as well.
\\
\\
($\Leftarrow$ Direction) Suppose that $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$. Then we know from the fact $|f|=f_+ + f_-$ and from monotonicity that
\[
    f_-, f_+ \leq |f| \qquad \Rightarrow
    \qquad 
    \int_X f_- \; d\mu \leq \int_X f \; d\mu <\infty\qquad
    \int_X f_+ \; d\mu \leq \int_X f \; d\mu <\infty
\]
which implies that $f_-, f_+\in\mathscr{L}(X,\mathscr{M},\mu)$, so that $f\in\mathscr{L}(X,\mathscr{M},\mu)$.
\end{proof}

\begin{cor} If we have $f\in\mathscr{L}(X,\mathscr{M},\mu)$, then 
\[ 
    \left\lvert \int_X f \; d\mu\right\rvert \leq
    \int_X \left\lvert f \right\rvert \; d\mu
\]
\end{cor}
\begin{proof}
Since $f\in\mathscr{L}(X,\mathscr{M},\mu)$, we know from the previous theorem that $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$. Then it's clear that 
\[ 
    \left\lvert \int_X f \; d\mu\right\rvert 
    = c \int_X f \; d\mu\qquad\text{where $c=1$ or $c=-1$}
\]
Then, since $cf\leq |f|$, we can say that 
\[ 
    \left\lvert \int_X f \; d\mu\right\rvert 
    = c \int_X f \; d\mu 
    = \int_X c f \; d\mu \leq 
     \int_X |f| \; d\mu 
\]
\end{proof}

\begin{thm}
Suppose $f\in\mathscr{L}(X,\mathscr{M},\mu)$. Then for any $t>0$,
\[
    \mu\left(\{x\;:\;|f(x)|\geq t\}\right) \leq \frac{1}{t}\int_{X} |f|\; d\mu
\]

\end{thm}

\newpage
\subsection{Pillar Convergence Theorems: Preliminary Foundation}

Before we get to the main pillar theorems concerning convergence, it will help to define a few concepts and prove a few results that we can use freely in the next subsection.

\begin{defn} (m)
Consider the measure space $(X,\mathscr{M},\mu)$ and let $E\in\mathscr{M}$. Then define
\[
    \lambda_f(E) := \int_E f \; d\mu 
    := \int_X f\cdot\Chi_E \;d\mu
\]
\end{defn}

\begin{lem} \emph{(s, +, m)}
\label{measurelem}
If $s$ is a simple, measurable non-negative function, then $\lambda_s(E)$ is a non-negative measure on $(X,\mathscr{M})$.
\end{lem}
\begin{proof}
It's easy to see that $\lambda_s(\emptyset) = 0<\infty$. Moreover, since $s$ is non-negative, we know that $\lambda_s(A) \in [0,\infty]$. So all that's left to prove is countable additivity.
\\
\\
So let $E_1, E_2,\ldots$ be disjoint, and notice we can write
\[
    \lambda_s\left(\bigcup^\infty_{n=1} E_n\right)
    = \int_{\cup E_n} s \;d\mu =
    \int_{X} s \cdot \Chi_{\cup E_n}\;d\mu =
    \int_{X} \left(s \cdot  
    \sum^\infty_{n=1}\Chi_{E_n}\right)\;d\mu 
\]
Now let's rewrite the final expression, using the fact that $s$ is simple to swap the sum and the integral (which we couldn't necessarily do for arbitrary $s$):
\begin{align*}
    \int_{X} \left(s \cdot  \sum^\infty_{n=1}
    \Chi_{E_n}\right)\;d\mu 
    &= \sum^m_{i=1} \left(c_i \cdot  
    \sum^\infty_{n=1}\Chi_{E_n} 
    \right)\mu(A_i) \\
    &= \sum^m_{i=1} \sum^\infty_{n=1}c_i \cdot \Chi_{E_n} 
    \mu(A_i)
    =  \sum^\infty_{n=1}\sum^m_{i=1} c_i \cdot \Chi_{E_n} 
    \mu(A_i) \\
    &=   \sum^\infty_{n=1}\int_X s \cdot \Chi_{E_n} 
        \; d\mu =  \bigcup^\infty_{n=1}\lambda(E_n)
\end{align*}
So we see that because $s$ is simple, we can exchange the sum and the integral, which allows us to gain the desired result of countable additivity.
\end{proof}

\begin{cor}
\emph{(s, +, m)}
\label{cormtc}
Given $s\geq 0$ simple and measurable with $E_1\subset E_2\subset\cdots$, we have
\[
    \lim_{n\rightarrow\infty} \int_{E_n} s\;d\mu
    = \lim_{n\rightarrow\infty} \lambda_s(E_n) =
    \lambda_s(\cup E_n) 
    = \int_{\cup E_n} s \; d\mu
\]
\end{cor}
\begin{proof}
This uses Lemma \ref{measurelem} and Property 5 in Subsection \ref{subsec:propmeas}, ``Properties of Measure Spaces.''
\end{proof}

\newpage
\subsection{Pillar Convergence Theorems: Statement of Results}

The theorems in this subsection are equivalent, and they furnish several particularly important results. Among them, linearity falls right out of the Monotone Convergence Theorem.

\begin{thm}
\emph{(Monotone Convergence Theorem (m,+))} For the measure space $(X,\mathscr{M},\mu)$, let $0\leq f_n(x)\leq f_{n+1}(x)$ be measurable for $n=1,2,\ldots$, and suppose $\lim_{n\rightarrow\infty}f_n=f$. Then we can exchange limits with the integral:
\[
    \lim_{n\rightarrow\infty} \int_X f_n \; d\mu =
    \int_X \left(\lim_{n\rightarrow\infty} f_n\right) 
    \; d\mu 
\]
This is valid whether $f$ is infinite or not.
\end{thm}
\begin{proof} We'll prove the statement above by showing the lefthand side is less than the righthand side and vice versa.
\\
\\
($\leq$ Proof) Let $f = \lim f_n$. Since, $\{f_n\}$ converges to $f$ from below, we can use monotonicity: 
\[
    f_n \leq f \quad \Rightarrow\quad
    \int_X f_n \; d\mu \leq \int_X f\; d\mu
\]
But that's also true if take the limit of the inequality over $n$. The righthand side, which doesn't depend upon $n$, will remain the same:
\[
    \lim_{n\rightarrow\infty}
    \int_X f_n \; d\mu \leq \int_X f\; d\mu
\]
And if we substitute in for $f=\lim_{n\rightarrow\infty}$, we get the desired inequality:
\[
    \lim_{n\rightarrow\infty}
    \int_X f_n \; d\mu 
    \leq \int_X \lim_{n\rightarrow\infty}f\; d\mu
\]
($\geq$ Proof) Take any simple function $s$ with $0\leq s\leq f$, then set $t\in(0,1)$ and define
\begin{equation}
    \label{mtcset}
    E_n := \{x \; |\; f_n(x) \geq ts(x) \}
\end{equation}
Then it will be the case that $E_1\subset E_2\subset \cdots$ with $X = \bigcup^\infty_{n=1} E_n$.
\\
\\
To see this, suppose $s(x)=0$. Then $x\in E_n$ for all $n$. For all other $x$, we have that
\[
    s(x)>0\quad\Rightarrow\quad ts(x)< s(x)\leq f(x)
\]
Next, we form an inequality chain:
\begin{align}
    \label{ineqchain}
    \int_X f\;d\mu \geq \int_X f_n\;d\mu
    \geq \int_{E_n} f_n \;d\mu\geq \int_{E_n} ts \;d\mu
\end{align}
This first inequality follows from the fact that $f_n\rightarrow f$ from below and from monotonicity. The middle inequality also follows from monotonicity after applying the definition $\int_E f \; d\mu := \int_X f\Chi_E \; d\mu$, which shows that the $f\Chi_E\leq f$. Finally, the last inequality follows from the set definition in (\ref{mtcset}) and, you guessed it, monotonicity again.

Next, before we take the limit of that inequality chain, apply Corollary \ref{cormtc} to the simple function $ts$ to note that 
\[
    \lim_{n\rightarrow\infty} \int_{E_n} ts \; d\mu
    = \int_{\cup E_n} ts \;d\mu
    = \int_{X} ts \;d\mu
\]
Alright, now we're ready to take the limit of the inequalities in (\ref{ineqchain}) to get:
\begin{align*}
    \int_X f\;d\mu \geq \lim_{n\rightarrow\infty}
    \int_X f_n\;d\mu &\geq
     \lim_{n\rightarrow\infty}\int_{E_n} ts \;d\mu \\
    \Rightarrow \quad\lim_{n\rightarrow\infty}
    \int_X f_n\;d\mu &\geq
     t\int_{X} s \;d\mu 
\end{align*}
Finally, we note that $t$ was arbitrary within $(0,1)$, so we can let $t\rightarrow 1$ and drop it from in front of the integral. We can also take the sup over all simple functions $s$ in the last line to get
\begin{align*}
     \quad\lim_{n\rightarrow\infty}
    \int_X f_n\;d\mu &\geq
    \int_{X} f \;d\mu 
\end{align*}
Noting that $f$ can be replaced with $\lim_{n\rightarrow\infty}f$ and that we also had the reverse inequality above, we have established equality.
\end{proof}

\begin{cor}
For any positive measure space $(X,\mathscr{M},\mu)$, suppose we have measurable $f_n:X\rightarrow[0,\infty]$ such that $f_1\geq f_2\geq f_3\geq\cdots\geq 0$ and $f_n\rightarrow f$ pointwise on $X$. Then if $f_1\in\mathscr{L}(X,\mathscr{M},\mu)$, 
\[
   \lim_{n\rightarrow\infty} \int_{X} f\; d\mu = \int_{X} f\; d\mu
\]
\end{cor}
\begin{proof}
Set $g_n=f_1-f_n$, then apply the MTC.
\end{proof}

\begin{ex}
The assumption that $f_1\in\mathscr{L}(X,\mathscr{M},\mu)$ in the previous theorem is key. Otherwise consider $f_n=\Chi_{[n,\infty)}$ which converges pointwise to zero, but integrates to infinity for all $f_n$. 
\end{ex}

\begin{lem}
\label{fatoulem}
\emph{(Fatou's lemma, (+, m))} Let $\{f_n\}^\infty_1\geq0$ be measurable. Then 
\[
    \int_X \left(\lim \inf f_n\right) \; d\mu
    \leq \lim \inf \int_X f_n \; d\mu
\]
\end{lem}
\begin{proof}
Define $g_k := \inf_{n\geq k} f_n$. Then, since $f_n$ is measurable for all $n$ and the inf of a measurable function is measurable, we know $g_k$ is measurable. Also, by the definition of the lim inf, $\lim_{k\rightarrow\infty} g_k = \lim \inf f_n$. Now we know that $g_k\leq g_{k+1}$ since $g_k$ converges to $\lim \inf f_n$ from below. So, we can apply the MTC:
\begin{equation}
    \label{fatoustep}
    \lim_{k\rightarrow\infty} \int_X g_k \; d\mu = 
    \int_X \lim_{k\rightarrow\infty} g_k \; d\mu = 
    \int_X (\lim\inf f_k) \; d\mu 
\end{equation}
Next, we note that $f_k\leq g_k$ for all $k$, so by monotonicity of both integrals and lim infs, we get that
\begin{equation}
    \label{fatoustep2}
    \int_X f_k \; d\mu \geq \int_X g_k \; d\mu
    \quad \Rightarrow\quad
    \lim \inf \int_X f_k \; d\mu \geq 
    \lim \inf \int_X g_k \; d\mu
\end{equation}
But, we can rewrite the righthand side of the second inequality in (\ref{fatoustep2}) using Equation (\ref{fatoustep}):
\begin{align*}
    \lim \inf \int_X f_k \; d\mu \geq
    \lim \inf \int_X g_k \; d\mu  =
    \lim \int_X g_k \; d\mu =
    \int_X (\lim\inf f_k)\; d\mu 
\end{align*}
which is the desired result. And just note that the middle equality sandwiched in between the ends we actually care about (the one where we say the limit of integrals is equal to the lim inf of integrals) is a crucial step that's might not be obviously true. (It wasn't for me.) 

But it \emph{is} true, and here's why. Write out the first term and simplify:
\begin{equation}
    \label{wtfatou}
    \lim \inf \int_X g_k \; d\mu  = \lim_{m\rightarrow\infty}\inf_{k\geq m} \int_X \inf_{n\geq k} f_n \; d\mu
\end{equation}
So fixing $m$, you want to choose $k$ so that the integral attains its smallest value. But to minimize the integral, you should choose $k$ so that you minimize the value of the function. And you do that by taking the smallest value possible for $k$, so that $f_n$ can have the widest possible range of functions in the sequnce over which to minimize. Thus, you'll always set $k=m$, which means you can rewrite Equation \ref{wtfatou} as
\begin{equation}
    \lim \inf \int_X g_k \; d\mu  = 
        \lim_{m\rightarrow\infty} 
        \int_X \inf_{n\geq m} f_n \; d\mu
        = \lim_{m\rightarrow\infty} \int_X g_m \; d\mu
\end{equation}
So the lim inf and the inf of the integral of $g_k$ are indeed equal.
\end{proof}

\begin{thm} \emph{(Dominated Convergence Theorem, (m))}  
Let $\{f_n\}$ be a sequence of measurable functions such that $\lim f_n = f$ and $|f_n|\leq g$ for some $g\in\mathscr{L}(X,\mathscr{M},\mu)$. Then
\[
    \lim_{n\rightarrow\infty} \int_X f_n \; d\mu
    = \int_X \lim_{n\rightarrow\infty}f_n \; d\mu
\]
\end{thm}

\newpage
\subsection{Pillar Convergence Theorems: Consequences, Applications, and Additional Properties for $\mathscr{L}(X,\mathscr{M},\mu)$}

\begin{cor} 
\label{lebadd}    
\emph{(m)}
If measurable $f,g\in\mathscr{L}(X,\mathscr{M},\mu)$, then $f+g\in\mathscr{L}(X,\mathscr{M},\mu)$.
\end{cor} 
\begin{proof}
    Suppose that $f,g\geq 0$. Then let $s_n{\rightarrow} f$ from below and $t_n{\rightarrow} g$ from below, where $\{s_n\}$ and $\{t_n\}$ are simple and non-negative for all $n$, $\lim_{n\rightarrow\infty}\int s_n = \int f$, and $\lim_{n\rightarrow\infty}\int t_n=\int g$. Then $s_n+t_n\rightarrow f+g$ from below and, by the Monotone Convergence Theorem, we have 
\[
    \lim_{n\rightarrow\infty}   
    \int_X (s_n+t_n)\;d\mu = \int_X (f+g)\;d\mu 
    \qquad \text{where}\qquad
    \int_X (s_n + t_n)\;d\mu
    = \int_X s_n \;d\mu+ \int_X t_n\;d\mu
\]
We now generalize to all measurable $f\in\mathscr{L}(X,\mathscr{M},\mu)$. 
\\
\\
But first, we claim that if $f$ and $g$ are non-negative and integrable, then $\int (f-g) = \int f - \int g$. To prove this, write
\begin{align*}
    \int (f-g) &= \int (f-g)_+ - \int (f-g)_- \\
    &= \int (f-g)\Chi_A - \int (g-f) \Chi_{A^c}
    \qquad A = \{x \;|\; f\geq g\}
\end{align*}
Then, it's clear that both $(f-g)\Chi_A$ and $(g-f)\Chi_{A^c}$ will be non-negative, allowing us to apply the additivity we proved for non-negative functions: 
\[
    \int f\Chi_A = \int (f-g) \Chi_A + 
\]





\end{proof}


\begin{thm} \emph{(+, m)} 
Let $f$ be a non-negative measurable function on $X$. Then $\lambda_f(E)$ defines a non-negative measure on $\mathscr{M}$.
\end{thm}
\begin{proof}
Since $f$ is non-negative and since $\lambda_f(\emptyset)=0$, we just need to check countable additivity. So let $E=\bigcup^\infty_{n=1} E_n$ be a pairwise disjoint sequence in $\mathscr{M}$ and define
\begin{align*}
    f_n = \sum^n_{i=1} f \Chi_{E_i}
\end{align*}
Then $\{f_n\}$ is a sequence of non-negative measurable functions on $X$ with $f_n \leq f_{n+1}$ for all $n$. So we can write out the integral, using Corollary \ref{lebadd} to swap the sum and integrals since we have a finite sum:
\begin{align*}
    \int_X f_n \; d\mu &= 
    \int_X \left(\sum^n_{i=1} f\Chi_{E_i}\right)\; d\mu \\
    &= \sum^n_{i=1} \int_X f\Chi_{E_i}\; d\mu 
    = \sum^n_{i=1}\lambda_f(E_i) \\ 
    \Rightarrow \quad 
    \lim_{n\rightarrow\infty}\int_X f_n \; d\mu &= 
    \sum^\infty_{i=1}\lambda_f(E_i) 
\end{align*}
Now use the fact that $\lim_{n\rightarrow\infty}f_n = f\Chi_E$, then apply the MTC to get
\begin{align*}
    \lambda_f\left(\bigcup^\infty_{i=1} E_i\right) 
    =\lambda_f(E)
    &:= \int_X f\Chi_E \; d\mu
        = \int_X \lim_{n\rightarrow\infty}f_n \; d\mu \\
    &= \lim_{n\rightarrow\infty}\int_X f_n \; d\mu
        = \sum^\infty_{i=1} \lambda(E_i)
\end{align*}
which is countable additivity.
\end{proof}

\begin{cor} 
\label{lebsetadd}
If $f\in\mathscr{L}(X,\mathscr{M},\mu)$, then $\lambda_f(E)$ defines a countably additive set function on $\mathscr{M}$, i.e.
\[
    \lambda(A) + \lambda(B) =
    \int_A f \; d\mu + \int_B f \; d\mu 
    = \int_{A\cup B} f \; d\mu
\]
\end{cor}
\begin{proof}
Apply the previous theorem to arbitrary measurable $f$ by using it on $f_-$ and $f_+$.
\end{proof}
\begin{rmk}
In working with functions that are ``nice'' almost everywhere, we usually apply Corollary \ref{lebsetadd} to break up the integral over $X$ into an integral over the ``bad'' part $E\subset X$ (where some nice property does not hold) and $X\setminus E$ (where some nice property \emph{does} hold). Then, since $\mu(E)$ equals zero, the integral $\int_E f \; d\mu=0$, \emph{regardless} of what the function $f$ does or looks like on $E$. This is precisely how to prove the next theorem.
\end{rmk}


\begin{thm}
\emph{(Radon-Nikodym Theorem)} This is roughly the converse to the previous theorem. Namely, instead of assuming we have a measurable $f$ on $(X,\mathscr{M},\mu)$ then defining a new non-negative measure, $\lambda_f$, we assume that we have two non-negative measures $\mu$ and $\lambda$ on $(X,\mathscr{M})$. Then, provided $\mu(X)<\infty$ and $\lambda(X)<\infty$, there will exist a non-negative measurable function $f$ on $X$ such that
\[
    \lambda(E) = \int_E f \; d\mu
\]
\end{thm}


\begin{ex}
To Do: Double series result in book.
\end{ex}

\begin{thm}
\label{tiny.domain}
Suppose $f\in\mathscr{L}(X,\mathscr{M},\mu)$. Then for every $\varepsilon>0$, there exists a $\delta>0$ such that 
\[
    E\in\mathscr{M}\text{  and  }
    \mu(E)<\delta 
    \quad \Rightarrow\quad
    \int_{E} |f|\; d\mu < \varepsilon
\]
\end{thm}
\begin{proof}
We will proceed via proof by contradiction. So take $\varepsilon>0$ and let $\{E_n\}^\infty_1 \subset \mathscr{M}$ such that $\lim_{n\rightarrow\infty} \mu(E_n)=0$. We know such a set exists because we always have $\emptyset \in \mathscr{M}$, where $\mu(\emptyset)$ always. Also, suppose by contradiction that for all $\delta$, there exists a $\varepsilon>0$ such that 
\begin{equation}
    \label{q4}
    \varepsilon\leq \int_{E_n} |f| \; d\mu :=
    \int_X |f|\Chi_{E_n} \; d\mu
    \qquad\text{for all $n$}
\end{equation}
Next, define a sequence $\{f_n\}=\left\{|f|\chi_{E_n}\right\}$. A couple facts about our sequence and about $|f|$:
\begin{itemize}
    \item $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$. By Theorem 88.12, our assumption that $f\in\mathscr{L}(X,\mathscr{M},\mu)$ implies this result.
    \item $f_n$ is measurable, for all $n$. To see this, note that since $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$, $|f|$ is measurable. Also since $E_n\in\mathscr{M}$, $f_n = |f|\chi_{E_n}$ is also measurable, for all $n$.
    \item $f_n \leq |f|$: This is fairly obvious, after writing out the definition of $f_n$, 
\begin{equation}
    \label{dtcapply}
    f_n := |f|\Chi_{E_n}\leq |f|
\end{equation}
    \item $\lim_{n\rightarrow\infty}f_n=0$ almost everywhere. Note that since $\mu(E_n)\rightarrow 0$, it must be that $\Chi_{E_n}\rightarrow 0$ almost everywhere, which sends $f_n$ to zero almost everywhere.
\end{itemize}
So, let's use these facts to apply the Dominated Convergence Theorem. 

First, since $\{f_n\}$ is a measurable sequence of functions, bounded by $|f|\in\mathscr{L}(X,\mathscr{M},\mu)$ for all $n$, we know that we can apply the Dominated Convergence Theorem, and exchange the limit and integral:
\begin{align*}
    \lim_{n\rightarrow\infty} \int_X f_n \; d\mu&=
    \int_X \lim_{n\rightarrow\infty}f_n \; d\mu  
\end{align*}
Then, we can evaluate the limit within the integral, applying what was stated above:
\begin{align*}
    \lim_{n\rightarrow\infty} \int_X f_n \; d\mu &=
    \int_X 0 \; d\mu = 0 \\ 
    \Rightarrow\quad
    \lim_{n\rightarrow\infty} \int_X |f|\Chi_{E_n} \; 
    d\mu &= 0
\end{align*}
But this contradicts our assumption in Inequality \ref{q4}, which should hold for all $n$.
\end{proof}


\subsection{Pillar Convergence Theorems: Examples \& Counterexamples}

First, it might seem a that the limit and the integral sign are not always interchangeable. Second, it might not be clear why the Theorems are formulated as they are.  

So to drive home this point, here are a few examples and counterexamples for the convergence theorems. The examples will illustrate some particular aspect of a convergence theorems, while the counterexamples mostly illustrate functions where we can't exchange the the limit and the integral sign.

\begin{ex}
(Moving Bump Function for $\mathbb{N}$) This example demonstrates why you can't always exchange the limit with the integral when the assumption $f_n(x)\leq f_{n+1}(x)$ for all $x$ and $n$ fails, disallowing the application of the MTC.

Suppose that we have $(\mathbb{N},\mathscr{P}(\mathbb{N}),\mu)$ where $\mu$ is the counting measure.  Let $f_n = \Chi_{\{n\}}$.  Clearly, $f_n\rightarrow 0$ pointwise; \emph{however}, we also have that $\int_\mathbb{N} f_n = n$, whose limit goes to $\infty$. So we can't exchange.

Now you might naturally think the problem is that $f_n$ converges to $f$ only \emph{pointwise}, which we know to be weaker than uniform convergence. But this next example shows a uniformly convergent function where we can't exchange the limit and the integral.
\end{ex}

\begin{ex} (Diluted Bump Function for $\mathbb{N}$)
Let $g_n = \frac{1}{n}\Chi_{[n,2n)}$. If you take the limit, $g_n\rightarrow 0$. (Gradually, $g_n$ will spread to be $1/n$ across a larger and larger support.) What's more, this function $g_n$ even converges to 0 \emph{uniformly}. However, if we compute the integral of the simple functions, $\int_X g_n$, the value equals 1 for all $n$. So clearly, we can't exchange the limit. 
\end{ex}

\begin{ex}
(Moving Bump Function for $\mathbb{R}$) Suppose we have the measure space $(\mathbb{R},\mathscr{P}(\mathbb{R}),\mu)$, where $\mu$ is any non-negative measure. 

Let $f_n = n\Chi_{(1/n,\;2/n]}$. This function converges to 0 pointwise, since we have a bump that is growing in height, but moving ever closer to 0. However, if we were to compute the integral for the simple function $f_n$ (for any $n$), we find that $\int f_n = 1$, so we can't exchange.
\end{ex} 

\begin{ex} (Strict Inequality in Fatou) Here are two examples showing that we can have the inequality in Lemma \ref{fatoulem} (Fatou's Lemma) be strict.
\\
\\
First, suppose that we define the function on the interval $[0,1]$:
\[
    f_n = n \Chi_{(0,1/n)}
\]
It's clear that the function converges pointwise to $f(x)=0$, as we saw in the previous example. In this case, the integral of $f_n$ will always be 1 for all $n$, while the integral of $f$ will be 0; hence strict inequality.
\\
\\
Next, consider an oscillating function on $[1,2]$:
\[
    f_n = 
    \begin{cases}
        \Chi_{[0,1]} & \text{$n$ even} \\
        \Chi_{[1,2]} & \text{$n$ odd} \\
    \end{cases}
\]
Then
\[
    0 = \int_X \lim \inf f_n \; d\mu
    <  \lim \inf\int_X f_n \; d\mu = 1
\]
\end{ex}




\newpage
\section{The Lebesgue Measure and Integral}

We now have a sufficient foundation to build the Lebesgue Integral, which is just a special case of the more general integral we defined in the previous section. The Lebesgue Integral is obtained by choosing a very specific measure and $\sigma$-algebra.

Therefore, the goal of this section rests in defining that $\sigma$-alebra $\mathscr{M}$, consisting of \emph{Lebesgue Measurable Sets}, along with the special \emph{Lebesgue Measure}, $m$. Both of these will allow us to construct the \emph{Lebesgue Integral} such that
\[
    \int_{[a,b]} f \; dm = \int_a^b f \; dx
\]
for all Riemann Integrable functions. The added advantage will be that there will be additional functions that are Lebesgue Integrable, but \emph{not} Riemann Integrable, allowing us to extend the class of integrable functions so that $\mathscr{L}([a,b],\mathscr{M},m) \supsetneq \mathscr{R}([a,b])$. 



\subsection{Desired Structure and Limitations}

When it comes time to construct a more general notion of the integral, we'll need define a new way to express the weighting of intervals in our domain, something we'll accomplish through the Lebesgue Measure, $m$. There are a number of desirable features that we would like $m$ to have:
\begin{enumerate}
    \item If $A$ is in an interval, $m(A) = |A|$, the length of the interval.
    \item Translation invariance: $m(A+x) = m(A)$.
    \item Countable additivity: If $A_1, \ldots, A_n$ disjoint, then $m(\cup A_n) = \sum m(A_n)$. 
    \item If $A_1 \subset A_2$, $m(A_1) < m(A_2)$.
\end{enumerate}
Now this wishlist is incompatible if our number system consists exclusively of rationals. This is because $m(\{p\}) = 0$, for all single rationals $p\in\mathbb{Q}$. Then $m(\mathbb{Q})=\sum m(\{p_n\}) = 0$. So the measure of all sets is zero, giving us no useful theory. We also can't make it work if we define $m(\{p\})=c>0$, because then $m(\mathbb{Q})=\infty$. This normally wouldn't be a problem, but then $m([0,1]\cap \mathbb{Q}) = \infty \neq 1$. 

So we're going to focus on uncountable spaces, like $\mathbb{R}$. This focus will also explain why we opt for \emph{countable} additivity. Specifically, finite additivity of $m$ is too weak, since we'll often want to work with limits. Second, uncountable additivity is too strong, because then we must define the measure of $\{p\}$ for each $p\in\mathbb{R}$ to be either zero or finite, which will lead to problems similar to those discussed above.

It also turns out that you can't satisfy the wishlist if you try to define a measure for the power set,$\mathscr{P}(\mathbb{R})$, since any countably additivy measure defined on $\mathscr{P}(\mathbb{R})$ won't be translation invariant.  So we'll need to consider a proper subset of $\mathscr{P}(\mathbb{R})$ to define a true non-negative measure, as we'll see in Subsection \ref{LebOuterMeasure} below.

To cope with this limitation, we will, in the next subsection, define a so-called ``outer measure'' that is defined on $\mathscr{P}(\mathbb{R})$, but then restrict it to special subest of $\mathscr{P}(\mathbb{R})$ to get a proper non-negative measure.


\subsection{The Lebesgue Outer Measure\label{LebOuterMeasure}}

We will construct the Lebesgue Measure by first defining the \emph{Lebesgue Outer Measure}, denoted $m^*:\mathscr{P}(\mathbb{R})\rightarrow[0,\infty]$. But this won't be a proper measure, since it will fail countable additivity. So then we will construct a smaller $\sigma$-algebra $\mathscr{M}\subsetneq\mathbb{R}$ such that $m:=m^*|_\mathscr{M}$ is a proper measure.

\begin{defn}
We define the \emph{Lebesgue Outer Measure} $m^*:\mathscr{P}(\mathbb{R})\rightarrow[0,\infty]$
\[
    m^*(A) := \inf\left\{
    \sum^\infty_{n=1} \ell(I_n) \;| \;
    A \subset \bigcup^\infty_{n=1} I_n
    \right\} \qquad A \subset \mathbb{R}
\]
where $\ell(I_n)$ is the length $x-y$ of the open interval $I_n=(x,y)$, and where the infimum is taken over the set of all open-interval covers of $A$. Intuitively, $m^*$ gives the size of the ``least-cost'' or ``most efficient'' open-interval cover that is at-most-countable.
\\
\\
\textbf{Properties}: The Lebesgue outer measure has the following properties:
\begin{enumerate}

\item $m^*$ is translation invariant. This follows from $\ell(\cdot)$ being translation invariant. 

\item $m^*(\emptyset)=0$, $m^*(\{p\})=0$, and $m^*\left(\bigcup^\infty_{i=1} \{p_i\}\right)=0$

\item \emph{Monotonicity}: $A\subset B$ implies $m^*(A)\leq m^*(B)$

\item $m^*(I)=\ell(I)$ where $I$ is any interval.

\item \emph{Countable Sub-Additivity}: For all $\{A_n\}^\infty_1\subset\mathbb{R}$, we have
\[
    m^*\left(\bigcup^\infty_{n=1}A_n\right) \leq
    \sum^\infty_{n=1}m^*\left(A_n\right) 
\]
But take note: If the $A_n$ are disjoint, it does \textbf{not} imply additivity. This is why we call $m^*$ the Lebesgue \emph{outer} measure, which is distinct from the Lebesgue measure, which we will define as a restriction of $m^*$.

\end{enumerate}

\begin{proof}
We prove only two of the properties, both of which are very important:
\begin{enumerate}
\item[4.] Although this looks like a tautology, this does, in fact, require a proof. And although $I$ can be any interval $(a,b)$, $(a,b]$, $[a,b)$, $[a,b]$ with no assumption about the finiteness of the endpoints, consider first $[a,b]$ and suppose the endpoints are finite (otherwise the proof is trivial).

($\leq$ Direction) First define $I_1 = (a-\varepsilon, b+\varepsilon)\supset[a,b]$ for all $\varepsilon>0$. Then it's clear that $I_1$ satisfies as an open interval cover of $[a,b]$, and since $m^*$ is the \emph{infimum} of all open interval covers, we have
\begin{align}
    m^*(I) &\leq \ell(I_1) = b-a + 2\varepsilon
        \qquad \forall \varepsilon>0 \notag \\
    \text{Send $\varepsilon\rightarrow0$} \qquad 
    m^*(I) &\leq b-a = \ell(I)
\end{align}

($\geq$ Direction) Next, for any cover $\{I_n\}_1^\infty$ of open intervals covering $[a,b]$, we can apply Heine Borel since $[a,b]$ is compact to get a finite subcover, $\{I_1,\ldots,I_N\}$ such that 
\[
    \bigcup^N_{n=1} I_n \supset [a,b] 
    \quad \Rightarrow \quad
    \sum^N_{n=1} I_n \geq b-a
\]
This last result is easy to prove; the full proof is written in the homework. It just requires induction and being clever about how to order the open intervals. 

And we can do this for any at-most-countable open interval cover, so take the infimum over all such covers to get
\[
    m^*(I) \geq b-a = \ell(I)
\]
Next, for the open interval, we take any $\varepsilon>0$ and use monotonicity before applying the above result concerning closed intervals:
\begin{align*}
    [a+\varepsilon, b-\varepsilon] &\subset
    (a,b) \subset [a,b] \\
    \text{By monotonicity}\qquad
    m^*([a+\varepsilon, b-\varepsilon]) &\leq
    m^*((a,b)) \leq m^*([a,b]) \\
    b-a-2\varepsilon &\leq
    m^*((a,b)) \leq b-a \\
    \text{Since $\varepsilon$ arbitrary }\Rightarrow\qquad
    m^*((a,b)) &= b-a 
\end{align*}
Finally, for all other types of intervals, use monotonicity, squeezing the half-open intervals between the closed and open intervals, and we have all cases covered.

\item[5.] Let $\varepsilon>0$ be given. Then for all $A_n\subset\mathbb{R}$, there exists an open interval cover $\{I_{n,j}\}^\infty_{j=1}$ such that
\begin{equation}
    \label{singcov}
    \bigcup^\infty_{j=1} I_{n,j} \supset A_n   
    \qquad \text{and}\qquad
    m^*(A_n) \leq \sum^\infty_{j=1} \ell(I_{n,j})
    \leq m^*(A_n) + \frac{\varepsilon}{2^n}
\end{equation}
Without loss of generality, we will assume $m^*(A_n)<\infty$ for all $n$ (otherwise it's trivial). 

Now if we take the union of all these countable interval covers (over all $n$), we have 
\begin{align}
    \bigcup^\infty_{n=1}A_n &\subset 
        \bigcup^\infty_{j=1} 
        \bigcup^\infty_{n=1} I_{n,j} \notag\\
    \Rightarrow \quad
    m^*\left(\bigcup^\infty_{n=1}A_n\right) 
    &\leq \sum^\infty_{j=1} \sum^\infty_{n=1}\ell(I_{n,j})
    \label{countadd1}
\end{align}
Next, we can use the righthand side of \ref{singcov}, summing over all covers:
\begin{align}
    \sum^\infty_{j=1} \sum^\infty_{n=1}\ell(I_{n,j})
    &\leq \sum^\infty_{n=1} \left(m^*\left(A_n\right) 
    + \frac{\varepsilon}{2^n}\right) \notag \\
    &\leq \varepsilon
    +\sum^\infty_{n=1} m^*\left(A_n\right) 
    \label{countadd2}
\end{align}
Thus, combining \ref{countadd1} and \ref{countadd2}, we have
\begin{align}
    m^*\left(\bigcup^\infty_{n=1}A_n\right) 
    \leq \sum^\infty_{j=1} \sum^\infty_{n=1}\ell(I_{n,j})
    \leq \varepsilon+\sum^\infty_{n=1} m^*\left(A_n\right)
\end{align}
But since, $\varepsilon$ was arbitrary, we have the desired result of countable sub-additivity.
\end{enumerate}
\end{proof}
\end{defn}


\begin{ex}
Here's why we don't have countable addivity on $\mathscr{P}(\mathbb{R})$. We'll proceed by contradiction, so suppose that there is a translation-invariant, countably additive, non-negative measure $m$ on $\mathscr{P}(\mathbb{R})$ such that $m(I)=\ell(I)$ for any interval $I$. Then pick any irrational $\alpha\in\mathbb{R}\setminus\mathbb{Q}$, and define the equivalence relation
\[
    x\sim y 
    \quad \Rightarrow \quad
    x-y = n \alpha \qquad n \in\mathbb{Z}
\]
So we say that integer-spaced points in $\mathbb{R}$ are equivalent according to $\sim$. 
\\
\\
Next, consider the equivalence classes, mod 1---i.e. choosing the ``base'' of each equivalence class to be a point in $[0,1]$. Then, wrap these points around a circle, and call these disjoint equivalence classes $\{E_\lambda\}_{\lambda \in \Lambda}$. Then, pick an $x_\lambda$ from each distince equivalence class and form the set $A$.\footnote{We're using the axiom of choice here.} Then translate the set $A$ (turn it around the circle) and note
\[
    A \cap (A + n \alpha \text{ mod } 1 ) = \emptyset
    \qquad n \in \mathbb{Z}_{++}
\]
Then, we can partition $[0,1)$ by $\{A_n\}$ where
\[
    A_n := (A + n\alpha \text{ mod } 1)
\]
But, recalling that $m$ is trnaslation invariant, countably additive, and constructed so $m(I)=\ell(I)$, then
\[
    1 = m([0,1)) = m(A) = \sum^\infty_{n=1} m(A_n) = 
    \sum^\infty_{n=1} m(A) 
\]
But this would be a countradiction, since we can't choose an $m$ such that this equality would hold.

\end{ex}

\newpage
\subsection{General Outer Measures and Caratheodory's Criterion}

\begin{defn} 
\label{outermeas}
Consider any set $X$ and any function $\mu^*:\mathscr{P}(X)\rightarrow[0,\infty]$. We call $\mu^*$ an \emph{outer measure} if it satisfies
\begin{enumerate}
    \item $\mu^*(\emptyset)=0$
    \item \emph{Monontonicity}: $A\subseteq B$ implies $\mu^*(A) \leq \mu^*(B)$
    \item \emph{Countable sub-additivity}: $\mu^*\left(\bigcup_{n=1}^\infty A_n\right) \leq \sum^\infty_{n=1} \mu^*(A_n)$
\end{enumerate}
Clearly, this is a generalization of the Lebesgue outer measure.
\end{defn}

\begin{defn} 
\label{caracrit}    
(Caratheodory's Criterion)
We say that $S\subset X$ is \emph{$\mu^*$-measurable} if, for all ``test sets'' $T\subset X$, we have
\[
    \mu^*(T) = \mu^*(T\cap S) + \mu^*(T\cap S^c) 
\]
Note that $T$ can be written as a disjoint union, $T=(T\cap S) \cup (T\cap S^c)$. We then define a $\sigma$-algebra composed of $\mu^*$-measurable sets:
\[
    \mathscr{M} 
    := \{S\subset X \; | \; \forall T\subset X, \;\;
        \mu^*(T) = \mu^*(T\cap S) + \mu^*(T\cap S^c) 
    \}
\]
But how can we be sure $\mathscr{M}$ is a $\sigma$-algebra? See the next theorem.
\end{defn}
\begin{rmk}
Note that for any $T\subset X$, we have
\[
    \mu^*(T) = \mu^*\left((T\cap S)\cup(T\cap S^c)\right)
\]
where $T$ is written as a disjoint union on the righthand side. Then, by the sub-additivity of $\mu^*$, the last line automatically implies 
\begin{equation}
    \mu^*(T) 
    \leq \mu^*\left(T\cap S\right) + 
    \mu^*\left(T\cap S^c\right)
\end{equation}
Now again, this is automatic for all $T\subset X$. So to have $S\in\mathscr{M}$, we must also have
\begin{equation}
    \mu^*(T) 
    \geq \mu^*\left(T\cap S\right) + 
    \mu^*\left(T\cap S^c\right)
    \qquad \forall \; T\subset X
\end{equation}
because this will be the only type of inequality that allows $S$ to satisfy Caratheodory's Criterion. This implies that $S$ will \emph{not} be $\mu^*$-measurable if we can find a $T\subset X$ such that 
\begin{equation}
    \mu^*(T) 
    > \mu^*\left(T\cap S\right) + 
    \mu^*\left(T\cap S^c\right)
\end{equation}
where, just to emphasize the key point, the last inequality is strict.
\end{rmk}

\begin{lem}
\label{caralem}
This lemma will be applied in the proof of the next theorem, and it is related to the countable additivity of $\mu^*$ on $\mathscr{M}$. Specifically, for any $A\subset X$ and for any disjoint $\{S_n\}\subset\mathscr{M}$, we have
\[
    \mu^*\left(A \cap \left(
    \bigcup^\infty_{n=1}S_n\right)\right) = 
    \sum^\infty_{n=1}\mu^*(A\cap S_n)
\]
\end{lem}
\begin{proof}
We start with disjoint $S_1,S_2\in\mathscr{M}$, from which we can use induction to prove the main result. So in Caratheodory's Criterion, let $T=A \cap(S_1\cup S_2)$ and $S=S_1$, then simplify:
\begin{align*}
    \mu^*(A\cap(S_1\cup S_2)) &= 
    \mu^*\left([A\cap(S_1\cup S_2)] \cap S_1\right)
    + \mu^*\left([A\cap(S_1\cup S_2)] \cap S^c_1\right) \\
    &= \mu^*\left(A\cap[(S_1\cup S_2) \cap S_1]\right)
    + \mu^*\left(A\cap[(S_1\cup S_2) \cap S^c_1]\right) \\
    &= \mu^*\left(A\cap S_1 \right)
    + \mu^*\left(A\cap S_2\right) 
\end{align*}
Then, use induction to get to the countable union case.
\end{proof}
\begin{cor}
\label{caralem2}
For any $A\subset X$ and for any finite disjoint sequence $\{S_n\}_1^N\subset\mathscr{M}$, we have
\[
    \mu^*\left(A \cap \left(
    \bigcup^N_{n=1}S_n\right)\right) = 
    \sum^N_{n=1}\mu^*(A\cap S_n)
\]
\end{cor}
\begin{proof}
Turn the set into a countable collection where $S_n = \emptyset$ for all $n>N$. Apply the previous lemma.
\end{proof}


\begin{thm}
\emph{(Caratheodory's Theorem)} For an outer measure $\mu^*$ on $X$ and for $\mathscr{M}$, the collection of all $\mu^*$-measurable sets, $\mathscr{M}$, is a $\sigma$-algebra, and the restriction $\mu:=\mu^*|_\mathscr{M}$ is a non-negative measure on $\mathscr{M}$.
\end{thm}
\begin{proof}
There's a lot in this theorem so we'll proceed by breaking into down into the main steps.

\begin{enumerate}
\item (Show $\mathscr{M}$ is an Algebra) First, it's stupidly obvious, that if $S\in\mathscr{M}$, then $S^c\in\mathscr{M}$ too, and second, that $\emptyset, X\in\mathscr{M}$. 

Now, we just need to prove that for $S_1,S_2\in\mathscr{M}$, we have $S_1\cup S_2\in\mathscr{M}$. So given any $T\subset X$, we use a little set theory. But before we do that, let's just establish one helpful result:
\begin{align}
    \label{outmeasmod}
    T\cap (S_1\cup S_2) &=  
    T\cap (S_1\cup (S_2\cap S_1^c)) \notag \\
    &= (T\cap S_1)\cup (T\cap S_2\cap S_1^c) 
\end{align}
Now we can move onto the main portion, using the previous result and simplifying:
\begin{align*}
    \mu^*(T\cap(S_1\cup S_2)) +
    \mu^*(T\cap(S_1\cup S_2)^c) &=
    \mu^*((T\cap S_1)\cup (T_1 \cap S_2\cap S_1^c)) \\
    &\qquad+\mu^*(T\cap S_1^c\cap S_2^c) 
\end{align*}
Now, apply countable sub-additivity to the last expression:
\begin{align*}
    \mu^*(T\cap(S_1\cup S_2)) +
    \mu^*(T\cap(S_1\cup S_2)^c) 
    &\leq\mu^*(T\cap S_1) + \mu^*(T_1\cap S_1^c \cap S_2)\\
    &\qquad+\mu^*(T\cap S_1^c\cap S_2^c) 
\end{align*}
Defining $\tilde{T}=T\cap S_1^c$, we can rewrite the last expression:
\begin{align}
    \label{union.final}
    \mu^*(T\cap(S_1\cup S_2)) +
    \mu^*(T\cap(S_1\cup S_2)^c) 
    &\leq\mu^*(T\cap S_1) + \mu^*(\tilde{T} \cap S_2)
    +\mu^*(\tilde{T}\cap S_2^c) 
\end{align}
Before simplifying this last line, we use Caratheodory's criterion and the fact that $S_2\in\mathscr{M}$ to simplify 
\begin{align*}
    \mu^*(\tilde{T} \cap S_2)
    +\mu^*(\tilde{T}\cap S_2^c) =
    \mu^*(\tilde{T}) = \mu^*(T\cap S_1^c)
\end{align*}
Plugging this result into \ref{union.final} and applying Caratheodory again (coupled with $S_1\in\mathscr{M}$):
\begin{align*}
    \mu^*(T\cap(S_1\cup S_2)) +
    \mu^*(T\cap(S_1\cup S_2)^c) 
    &\leq\mu^*(T\cap S_1) 
    + \mu^*(T\cap S_1^c) 
    =\mu^*(T)
\end{align*}
Invoking the remark after Definition \ref{caracrit}, we have the desired result.

\item 
(Show $\mathscr{M}$ is a $\sigma$-algebra) To show countable additivity for any sequence of disjoint sets  $\{S_n\}\subset\mathscr{M}$, take $A=\bigcup^\infty_{n=1} S_n$ and apply Lemma \ref{caralem}.
\\
\\
Next, to show that $\mathscr{M}$ is closed under countable unions, we suppose $\{A_n\}_1^\infty \subset \mathscr{M}$.  Then, define 
\[
    S_1 = A_1 \qquad 
    S_n = A_n \setminus 
    \left(\bigcup^{n-1}_{i=1} S_{i}\right)
    \quad \text{$n>1$}
\]
It's clear that $\{S_i\}_{1}^\infty$ will be a pairwise disjoint subset of $\mathscr{M}$. So we define a new sequence of sets, $\{B_n\}$, which are clearly in $\mathscr{M}$:
\begin{align}
    \label{cara.bdef}
    B_n := \bigcup^n_{i=1} A_i = \bigcup^n_{i=1} S_i
    \qquad \forall n
\end{align}
Then, for any $T\subset X$, we want to show the following equals $\mu^*(T)$, satisfying Caratheodory's criterion and implying the union is in $\mathscr{M}$:
\begin{align}
    \mu^*\left(T \cap \left(\bigcup^\infty_{i=1} A_i
    \right)\right) + 
    \mu^*\left(T \cap \left(\bigcup^\infty_{i=1} A_i
    \right)^c\right) 
    &\leq 
    \mu^*\left(T \cap \left(\bigcup^\infty_{n=1} S_i
    \right)\right) + 
    \mu^*(T \cap B_\infty^c) \notag \\
    \text{Let $A=\cup^\infty_{i=1} S_i$, apply Lemma \ref{caralem}} \qquad 
    &\leq 
    \sum^\infty_{i=1}\mu^*(T\cap S_i) + 
    \mu^*(T \cap B_n^c) 
    \label{cara.chain}
\end{align}
Note, the choice of $n$ above for $B_n^c$ is arbitrary. It's clear from the definition in \ref{cara.bdef} that $B_n\subset B_{n+1}$, implying $B_n^c \supset B_{n+1}^c$. So the inequality we have above will indeed hold for any $n$ by the monotonicity of $\mu^*$. (Plust, we'll eventually send $n\rightarrow\infty$ anyway.)
\\
\\
Now given that $n$, let's approximate that sum above by a finite sum (with error) and then use Corollary \ref{caralem2},\footnote{We use it in the opposite direction, compared to how we just used Lemma \ref{caralem}.}
\begin{align}
    \label{cara.tosub}
    \sum^\infty_{i=1}\mu^*(T\cap S_i) &= 
    \sum^n_{i=1}\mu^*(T\cap S_i) + \delta_n \notag \\
    &= \mu^*(T\cap B_n) + \delta_n 
\end{align}
Since $\mathscr{M}$ was show to be an algebra, we know finite unions of sets in $\mathscr{M}$ are in $\mathscr{M}$ as well, so by the definition in (\ref{cara.bdef}), we can say that $B_n\in\mathscr{M}$. That means, using the same $T$ as above,
\[
    \mu^*(T\cap B_n) + \mu^*(T\cap B_n^c) = \mu^*(T) 
    \quad \Rightarrow\quad
    \mu^*(T\cap B_n^c) \leq \mu^*(T) 
\]
So we can substitute into Equality \ref{cara.tosub} to get 
\[
    \sum^\infty_{i=1}\mu^*(T\cap S_i) \leq \mu^*(T) + \delta_n
\]
Now, we consider what happens to $\delta_n$ when we send $n\rightarrow\infty$:
\begin{enumerate}
\item  Suppose the sum $\sum_{i=1}^n \mu^*(T\cap S_i)$ converges. Then $\delta_n\rightarrow0$, and, following the chain of steps we started all the back in Inequality \ref{cara.chain}, we get 
\[
    \mu^*\left(T \cap \left(\bigcup^\infty_{i=1} A_i
    \right)\right) + 
    \mu^*\left(T \cap \left(\bigcup^\infty_{i=1} A_i
    \right)^c\right) 
    \leq \mu^*(T)
\]
\item Alternatively, suppose that the sum $\sum_{i=1}^n \mu^*(T\cap S_i)$ does not converge. Then recal Equality \ref{cara.tosub}
\[
    \sum_{i=1}^n \mu^*(T\cap S_i) = \mu^*(T\cap B_n)
\]
We can substute this into the inequality that follows from $B_n$ being measurable:
\begin{align*}
    \mu^*(T\cap B_n) + \mu^*(T\cap B_n^c) &\leq \mu^*(T) \\
    \Leftrightarrow \quad
    \sum_{i=1}^n \mu^*(T\cap S_i) + \mu^*(T\cap B_n^c) &\leq \mu^*(T) \\
    \Rightarrow \quad
    \sum_{i=1}^n \mu^*(T\cap S_i) &\leq \mu^*(T) \qquad \forall n
\end{align*}
So if $\sum_{i=1}^n \mu^*(T\cap S_i)$ diverges, then no problem. $\mu^*(T)$ will be infinite too, so again following the chain of steps back to Inequality \ref{cara.chain}:
\[
    \mu^*\left(T \cap \left(\bigcup^\infty_{i=1} A_i
    \right)\right) + 
    \mu^*\left(T \cap \left(\bigcup^\infty_{i=1} A_i
    \right)^c\right) 
    \leq \mu^*(T)
\]
\end{enumerate}
\end{enumerate}
Hence $\bigcup^\infty_{i=1} A_i = \bigcup^\infty_{i=1} S_i$ is in $\mathscr{M}$, so that countable additivity is ensured.

\end{proof}


\begin{thm}
Given an outer measure $\mu^*$ and any set $S\subset X$, if $\mu^*(S)=0$, then $S\in\mathscr{M}$, where $\mathscr{M}$ is the set of all $\mu^*$-measurable sets. 
\end{thm}
\begin{rmk}
Since we know that $\mu:=\mu^*|_{\mathscr{M}}$ is a non-negative measure as shown in the previous theorem, we now know from this theorem that $\mu$ is a \emph{complete measure} within a complete measure space $(X, \mathscr{M}, \mu)$ that is very convenient to work with.
\end{rmk}
\begin{proof}
We can use the monotonicity of $\mu^*$ to say that
\begin{equation}
    \label{monty1}
    (T\cap S) \subset S 
    \quad \Rightarrow\quad
    0\leq\mu^*(T\cap S) \leq \mu^*(S) = 0
    \quad \Rightarrow\quad
    \mu^*(T\cap S) = 0 
\end{equation}
Then again, by monotonicity, we have for all $T\subset X$
\begin{equation}
    \label{monty2}
    (T\cap S^c) \subset T
    \quad \Rightarrow\quad
    \mu^*(T\cap S^c) \leq \mu^*(T)
\end{equation}
Putting \ref{monty1} and \ref{monty2} together, we have
\begin{align*}
    \mu^*(T) &\geq 
    \mu^*(T\cap S^c) =  
    \mu^*(T\cap S^c) + 0  \\
    &\geq \mu^*(T\cap S^c) + \mu^*(T\cap S)
\end{align*}
Then, we recall the remark after Caratheodory's Criterion (Definition \ref{caracrit}) to establish $S\in\mathscr{M}$.

\end{proof}


\newpage
\subsection{The Collection of Lebesgue Measurable Sets, $\mathscr{M}$, and the Borel $\sigma$-algebra, $\mathscr{B}_\mathbb{R}$}

We now return from general outer measures on any space $X$ to the Lebesgue measure (denoted $m$) and the Lebesgue Measure space $(\mathbb{R},\mathscr{M},m)$, which we obtain by restricting $\mathscr{M}$ to $m^*$-measurable sets and restricting $m^*$ to $m:=m^*|_\mathscr{M}$, precisely as we did in the previous subsection.

To ease back in, we will begin by considering how $\mathscr{M}$ relates to the Borel $\sigma$-algebra, $\mathscr{B}_\mathbb{R}$, the smallest $\sigma$-algebra containing all open sets of $\mathbb{R}$.

\begin{prop} $\mathscr{M}\supsetneq \mathscr{B}_\mathbb{R}$, the Borel $\sigma$-algebra on $\mathbb{R}$. 
\end{prop}
\begin{proof} We proceed by considering all open and closed sets that would need to be contained in $\mathscr{M}$ for it to contain $\mathscr{B}_\mathbb{R}$. Specifically, we will show the first statement in the following list. Each each element will then imply the next.
\begin{enumerate}
    \item $(a,\infty)\in\mathscr{M}$ for all $a\in\mathbb{R}$.
    \item $(-\infty,b]\in\mathscr{M}$ for all $b\in\mathbb{R}$.
    \item $(a,b]\in\mathscr{M}$ for all $a,b\in\mathbb{R}$ where $a<b$.
    \item $(a,b)\in\mathscr{M}$ for all $a,b\in\mathbb{R}$ where $a<b$.
    \item $\mathscr{B}_\mathbb{R}\in\mathscr{M}$.
\end{enumerate}
To see why (1) will lead to (5), suppose that (1) is true. Then it's compelement must be in $\mathscr{M}$ since it's a $\sigma$-algebra, ensuring (2). Next, note that finite intersections of sets in $\mathscr{M}$ must be in the collection as well, so intersect $(a,\infty), (-\infty,b]\in\mathscr{M}$ to get (3). Finally, take a countable union of sets $(a,b-1/n]$, which must be in $\mathscr{M}$ since $\sigma$-algebras are closed under countable unions to get (4). Together 1-4 imply $\mathscr{B}_\mathbb{R}$.
\\
\\
Now we just need to show $(a,\infty)\in\mathscr{M}$. So for any arbitrary $T\subset\mathbb{R}$, we want to show that $(a,\infty)$ satisfies Caratheodory's criterion (or, more simply, the non-trivial half of it):
\begin{equation}
    \label{borel.cara}
    m^*(T) \geq m^*(T \cap (a,\infty)) + 
    m^*(T \cap (-\infty,a])
\end{equation}
Suppose without loss of generality that $m^*(T)$ is finite, otherwise trivial. Then let $\varepsilon>0$ be given and let $\{I_n\}$ be an open interval covering of $T$ such that
\begin{equation}
    \label{borel.approx}
    \sum^\infty_{n=1} \ell(I_n) \leq m^*(T) + \varepsilon
\end{equation}
which we know to exist by the definition of $m^*$ as the infimum of all such coverings. Then let 
\[
    J_n := I_n \cap (a,\infty)
    \qquad
    K_n := I_n \cap (-\infty,a]
\]
Clearly
\[
    \ell(I_n) = \ell(J_n) + \ell(K_n)
\]
and, on top of that, recall 
\[
    m^*(J_n) = \ell(J_n) 
    \qquad
    m^*(K_n) = \ell(K_n) 
\]
Putting this all together, we know that we can write $T\cap (a,\infty)$ as a union of the sets $J_n$, which implies, by countable sub-additivity and the fact that $m^*(I) = \ell(I)$ for intervals, that
\begin{equation}
    \label{borel.1}
    m^*(T \cap (a,\infty)) =
    m^*\left(\bigcup^\infty_{n=1} J_n\right) 
    \leq \sum^\infty_{n=1}
    m^*(J_n) = \sum^\infty_{n=1} \ell(J_n)
\end{equation}
Similarly, we do the same thing for the $K_n$
\begin{equation}
    \label{borel.2}
    m^*(T \cap (-\infty,a]) =
    m^*\left(\bigcup^\infty_{n=1} K_n\right) 
    \leq \sum^\infty_{n=1}
    m^*(K_n) = \sum^\infty_{n=1} \ell(K_n)
\end{equation}
We now add Inequalities \ref{borel.1} and \ref{borel.2}:
\begin{align*}
    m^*(T \cap (a,\infty)) +
    m^*(T \cap (-\infty,a]) &\leq
    \sum^\infty_{n=1} \ell(J_n)
    + \sum^\infty_{n=1} \ell(K_n) 
    \leq \sum^\infty_{n=1} \ell(I_n) \\
    \text{By \ref{borel.approx}} \Rightarrow
    \qquad &\leq m^*(T) + \varepsilon
\end{align*}
But $\varepsilon>0$ was arbitrary, so we have the desired result.
\end{proof}


\subsection{Regularity Properties of the Lebesgue Measure on $\mathbb{R}$\label{sec.support1}}


Having shown that $\mathscr{M}\supset \mathscr{B}_\mathbb{R}$, we will use this fact to our advantage. Namely, we will now consider some \emph{regularity properties} of the Lebesgue measure, which tell us how well we can approximate the measure of \emph{any} arbitrary subset of $\mathbb{R}$ by open sets or other more familiar sets of $\mathscr{B}_\mathbb{R}\subset\mathscr{M}$. This will be very handy.

\begin{defn}
We say that a set $G\in G_\delta$ ($G$ is ``$G$-delta'') if $G$ can be written as a countable intersection of open sets.
\end{defn}

\begin{defn}
We say that a set $F\in F_\sigma$ ($F$ is ``$F$-sigma'') if $F$ can be written as a countable union of closed sets.
\end{defn}

\begin{thm}
\label{lebreg}
Given \emph{any} arbitrary $A\subset\mathbb{R}$, we have the following regularity results 
\begin{enumerate}

\item $m^*(A) = \inf_{U} \{m(U) \; | \; U \text{ open }, U\supset A\}$.

\item There exists a $G\in G_\delta$ such that $G\supset A$ and $m^*(A) = m(G)$.

%\item There exists a $F\in G_\delta$ such that $F\subset A$ and $m^*(A) = m(F)$ (I think).


\end{enumerate}
Given any Lebesgue \emph{measurable} set, $E\in\mathscr{M}$, we have the following regularity results 

\begin{enumerate}

\item $E$ is measurable if and only if there exists an open $U\supset E$ such that $m(U\setminus E)<\varepsilon$. This implies that the Lebesgue Measure is \emph{$\sigma$-finite}.

\item $E$ is measurable if and only if there exists a closed set $F\subset E$ such that $m(E\setminus F)<\varepsilon$. 

\item $m(E) = \sup_{F} \{m(F) \; | \; F \text{ closed, } F\subset E\}$.

\item $m(E) = \sup_{K} \{m(K) \; | \; K \text{ compact, }K\subset E\}$.

\end{enumerate}
\end{thm}
\begin{proof}
First, we proof the results for arbitrary sets.
\begin{enumerate}
\item The proof is very easy once we recall the definition of $m^*(A)$. But first, for an open set $U\supset A$, monotonicity implies $m(U)\leq m^*(A)$, which will hold taking the sup over $U$ as well. To prove the opposite direction, recall the definition of $m^*(A)$:
\[
    m^*(A) := 
    \inf \left\{ \left.\sum^\infty_{n=1}\ell(I_n) \; 
        \right\rvert\; \bigcup^\infty_{n=1} I_n \supset A\right\}  
\]
That means we can choose an open interval cover of $A$ such that
\[
    m\left(\bigcup^\infty_{n=1} I_n\right) \leq m^*(A) + \varepsilon
\]
Then, note that since the $I_n$ are all open, their countable union will be as well so that
\[
    U:=\bigcup^\infty_{n=1} I_n
\]
We now have an open set such that
\[
    m(U) \leq m^*(A) +\varepsilon
\]
But since $\varepsilon>0$ was arbitrary, the result is proved.

\item Recall Part (1) of this theorem. It will imply that there exists an open $U_n\supset A$ such that, for all $n$,
\[
    m(U_n) \leq m^*(A) + \frac{1}{n}
\]
Then choose our $G$ by setting
\[
    G = \bigcap^\infty_{n=1} U_n \supset A
\]
This will work because it's clear by monotonicity and $G\supset A$ that
\begin{align*}
    m^*(A) \leq m(G) &\leq m(U_n) 
    \leq m^*(A) + \frac{1}{n} 
\end{align*}
for all $n$. So let $n\rightarrow\infty$, and the result is achieved.
\end{enumerate}
Next, we prove the results for a measurable set $E\in\mathscr{M}$.
\begin{enumerate}
\item ($\Rightarrow$ Direction) First, assume $E$ is measurable, and use Part (1) of this theorem for arbitrary sets to choose an open set $U \supset E$ such that  
\begin{align*}
    m(U) = m(E) + \varepsilon
\end{align*}
Then, since $E\in\mathscr{M}$, we can use Caratheodory's Criterion:
\begin{align*}
    m(U\cap E) + m(U\cap E^c) &= m(U) \leq m(E) 
    +\varepsilon \\
    \Leftrightarrow  \quad
    m(E) + m(U\setminus E) &\leq m(E) +\varepsilon
\end{align*}
Now if $m(E)$ is finite, we can cancel to get 
\begin{align*}
    m(U\setminus E) &\leq \varepsilon
\end{align*}
If $m(E)$ is infinite, we can partition $\mathbb{R}$ and express $E_n$ as a countalbe union of disjoint sets:
\begin{align*}
    \mathbb{R} = \bigcup_{n\in\mathbb{Z}} [n,n+1)
    \qquad \qquad 
    E = \bigcup_{n\in\mathbb{Z}} E_n = 
    \bigcup_{n\in\mathbb{Z}} \left(E \cap [n,n+1)\right)
\end{align*}
Then, for all $n$, choose an open $U_n\supset E_n$ such that
\[
    m(U_n \setminus E_n) < \frac{\varepsilon}{2^{|n|}}
\]
which is possible because of what we just proved for finite $m(E)$. Then, we let $U = \bigcup_{n\in\mathbb{Z}} U_n \supset E$, which gives
\[
    U\setminus E \subset \bigcup^\infty_{n\in\mathbb{Z}}
    \left(U_n \setminus E_n \right)
    \quad \Rightarrow\quad
    m(U\setminus E) \leq \sum^\infty_{n\in\mathbb{Z}}
    \frac{\varepsilon}{2^{|n|}} = 3\varepsilon
\]
Since $\varepsilon$ was arbitrary, the result follows.

\end{enumerate}
\end{proof}

\begin{thm}
If $E$ is a measureable set and $m(E)<\infty$, then for all $\varepsilon>0$, there exists an open set $U$ and compact set $K$ such that
\[
    K\subset E\subset U
    \quad \text{and} \quad
    m(U\setminus K)<\varepsilon
\]
\end{thm}


\newpage
\subsection{The Lebesgue Measure and Integral on $[a,b]$ \label{sec.support2}}

We now look at what happens when we restrict ourselves to the compact interval $[a,b]$, which allows for direct comparison of the Lebesgue and Riemann integrals and, more generally, $\mathscr{L}([a,b])$ and $\mathscr{R}([a,b])$. But first, we have to define $\mathscr{L}([a,b])$.

Afterwards, we get to the big, big result that Riemann integrable functions are Lebesgue integrable (and the two integrals equal each other), which was the whole point of this section.

\begin{defn}
We can restrict our Lebesgue Measure Space on $\mathbb{R}$ to a measure space on $[a,b]$ by defining
\begin{align*}
    \mathscr{M}([a,b]) &= \{A\cap[a,b] \; | \;
        A\in\mathscr{M}(\mathbb{R})\} \\
    m \text{ on $[a,b]$} &:= 
        m|_{\mathscr{M}([a,b])}
\end{align*}
We will use $\mathscr{L}([a,b])$ as shorthand for the set of all Lebesgue integrable functions on the relative measure space restricted to $[a,b]$ as defined above.
\end{defn}

\begin{thm}
If $f\in\mathscr{R}([a,b])$, then
\begin{enumerate}
    \item $f$ is measurable.
    \item $f\in\mathscr{L}([a,b])$
    \item $\int_{[a,b]} f \; dm = \int_a^b f \; dx$
\end{enumerate}
\end{thm}
\begin{proof}
Let's consider each, one at a time.
\begin{enumerate}
    
\item To show $f$ is measurable, we want to show that for all open $V$, the preimage $f^{-1}(V)$ is measurable.  So define 
\[
    X := \{x\;|\; \text{$f$ continuous}\} 
\]
Now since $f$ is continuous on $X$, for any $x\in X \cap f^{-1}(V)$, there is an open set $U_x\ni x$ such that 
\[
    f(U_x) \subset V
\]
Then, can use this result to define
\[
    U := \bigcup_{x\in (X\cap f^{-1}(V))} U_x 
    \quad \Rightarrow \quad
    \begin{cases}
        \text{$U$ open} \\
        \text{$U\supset (X \cap f^{-1}(V))$} \\
        \text{$U\subset f^{-1}(V)$} 
    \end{cases}
\]
Next, given the definition of $U$, we can use a little set theory to say
\begin{align*}
    U^c &\subset (X\cap f^{-1}(V))^c 
    = X^c \cup f^{-1}(V)^c  \\
    \Rightarrow \quad
    f^{-1}(V) \cap U^c &\subset 
    \left(X^c \cup f^{-1}(V)^c\right)\cap f^{-1}(V)  \\
    \Leftrightarrow \quad
    f^{-1}(V) \setminus U &\subset
    X^c \cap f^{-1}(V)  \subset X^c 
\end{align*}
But by Lebesgue's Theorem, since $f$ is Riemann integrable, $X^c$, the set of discontinuity points of $f$, must be measure zero. As a result, $f^{-1}(V)\setminus U$ must also be in $\mathscr{M}$ since it is a complete measure space. Therefore, writing 
\[
    f^{-1}(V) = U \cup \left( f^{-1}(V)\setminus U\right)
\]
must be in $\mathscr{M}$ as the finite union of an open set and a measure zero set in $\mathscr{M}$. Therefore, $f$ is measurable.

\item Next, because $f$ is measurable and bounded (which follows from $f\in\mathscr{R}([a,b])$), we can conclude that $f\in\mathscr{L}([a,b])$. From Theorem \ref{lebprops}.

\item Finally, since $f$ is bounded, define $M:=\sup_{[a,b]}|f|$. Then
\[
    g := M+f \geq 0
\]
Now clearly, $g\in\mathscr{R}([a,b])$ as well. From there, recall the definition of the lower sum given some partition $P=\{x_0,x_1,\ldots,x_n\}$, and recognize that a lower sum is \emph{precisely} equivalent to the integral of a simple function $s\leq g$, where $s(x)=m_i$ when $x\in[x_{i-1},x_i)$. As a result,
\begin{align*}
    L(g,P) = \sum^n_{i=1} m_i\Delta x_i
    = \sum^n_{i=1} s_i \cdot m([x_{i-1},x_i))
    = \int_{[a,b]} s \; dm
    \leq \int_{[a,b]} g \; dm
\end{align*}
So we have bounded the lower sums by the Lebesgue integral of $g$. Now if we take the sup over lower sums, the result will equal the value of the Riemann integral of $f$, by the definition of Riemann integrability, giving us
\begin{equation}
    \label{lebriem}
    \int_a^b g \; dx \leq \int_{[a,b]} g \; dm
\end{equation}
Now, let's simplify and use the properties we know about for the two integrals:
\begin{align*}
    \int_a^b g \; dx &= 
    \int_a^b (f+M) \; dx = 
    \int_a^b f \; dx + M(b-a) \\
    \int_{[a,b]} g \; d\mu &= 
    \int_{[a,b]} (f+M) \; d\mu = 
    \int_{[a,b]} f \; d\mu + M(b-a) 
\end{align*}
Putting this together with Inequality \ref{lebriem}, we get
\begin{align*}
    \int_a^b f \; dx + M(b-a) &\leq
    \int_{[a,b]} f \; d\mu + M(b-a) \\
    \Rightarrow\quad
    \int_a^b f \; dx &\leq
    \int_{[a,b]} f \; d\mu
\end{align*}
But since this inequality holds for \emph{arbitrary} $f$, it will work for \emph{any} Riemann integrable function, including $-f$. So then we get
\begin{align*}
    \int_a^b -f \; dx &\leq
    \int_{[a,b]} -f \; d\mu \\
    \int_a^b f \; dx &\geq
    \int_{[a,b]} f \; d\mu 
\end{align*}
Together, the two directions of the inequality imply that the Riemann and Lebesgue integrals are equal.
\end{enumerate}
\end{proof}

\begin{thm} 
\label{thm.contapprox}
Let $f$ be a Lebesgue measurable function on $[a,b]$ that is finite almost everywhere.\footnote{This is automatically true if $f$ is real-valued, rather than extended real-valued, which this theorem will allow.} Then for all $\varepsilon>0$, there exists a continuous function $g$ such that 
\begin{equation}
    \label{contapprox}
    m\left(\{x \; : \; |f(x)-g(x)|>\varepsilon\}\right)
    <\varepsilon
\end{equation}
Equivalently, we could also say that, given any $\varepsilon_1>0$ and $\varepsilon_2>0$, there exists a continuous $g$ such that 
\[
    m\left(\{x \; : \; |f(x)-g(x)|>\varepsilon_1\}\right)
    <\varepsilon_2
\]
The two notions are equivalent.
\\
\\
In words, this says that we can approximate with continuous functions any arbitrary Lebesgue measurable function (that's finite almost everywhere) such that the set of points where the approximation is poor gets arbitrary small. 
\end{thm}
\begin{proof}
The proof will proceed in a series of approximations that reduce an arbitrary $f$ (that is finite a.e.) all the way to a continuous function $g$. The steps, given a starting function $f$, are 
\begin{enumerate}
    \item Reduce $f$ to a \emph{bounded} measurable function, $f_1$.
    \item Reduce $f_1$ to a \emph{simple} function, $f_2$.
    \item Reduce $f_2$ to a \emph{step} function, $f_3$.\footnote{You might say ``Wait a minute. Aren't step and simple functions pretty much the same thing?'' Well, step functions (with a finite range) are clearly simple functions. But $\Chi_{\mathbb{Q}}$ is a simple function on $\mathbb{R}$ that's definitely not a step function.}
    \item Reduce $f_3$ to a continuous function $g$.
\end{enumerate}
At each step, Property \ref{contapprox} will be maintained. So let's proceed, using these steps as a guide, fixing $\varepsilon>0$.

\begin{enumerate}

% Step 1
\item First, recall that $f$ is finite almost everywhere, so that the following set is measure zero: 
\begin{align*}
    \{x: |f| = \infty\} =  \bigcap_{M=1}^\infty 
    \{x: |f| > M\}
\end{align*}
That implies there exists an $M<\infty$, such that 
\[
    m(\{x: |f| > M\}) <\varepsilon
\]
So, we get our function $f_1$ by ``clipping'' $f$ and setting
\[
    f_1 := f\Chi_{\{|f|\leq M\}}
\]

\item Next, we discretize the range $[-M,M]$ up to resolution $\varepsilon$. By that, I mean break up the range into (roughly) $2M/\varepsilon$ intervals (say $N$ intervals from now on, just to make things simpler), stepping from $-M$ to $M$ by increments of $\varepsilon$ so that
\[
    [-M,M] = \bigcup_{j=1}^{N}
    [p_{j-1},\;p_{j}]
    \quad \text{where $p_j-p_{j-1}=\varepsilon$}
\]
Then define the sets
\[
    E_i := f^{-1}([p_{i-1},p_i])
\]
From there, we can define our simple function
\[
    f_2:=\sum^{N}_{i=1} p_i \Chi_{E_i}
\]
Clearly, the error in the approximation between $f$ and $f_1$ is, at most, $\varepsilon$ by this construction, which implies
\[
    \{x: |f_1-f_2|>\varepsilon\} = \emptyset
    \quad\Rightarrow\quad 
    m(\{x: |f_1-f_2|>\varepsilon\}) = m(\emptyset)=0
\]

\item Next, we approximate each $E_i$ by the union of finitely many intervals. Applying Theorem \ref{open.intervals} (see Appendix \ref{app.support2}) to each $E_i$, we can find finitely many intervals $\{I^{(i)}_1,\ldots,I^{(i)}_{J_i}\}$ such that  
\[
    m\left(\left(\bigcup^{J_i}_{j=1} I^{(i)}_j \right)
    \Delta E_i\right) < \frac{\varepsilon}{N}
    \qquad i = 1, \ldots,N
\]
So, set our new function $f_3$ to be 
\[
    f_3 := \sum^N_{i=1} p_i 
    \Chi_{F_i}
    \quad \text{where }
    F_i = \bigcup^{J_i}_{j=1} I_j^{(i)}
\]
This is now a step function. 

\item Finally, to go from $f_3$ to our continuous function $g$, ``connect the dots'', connecting the function linearly for the gap at a step point.

\end{enumerate}

\end{proof}

\begin{thm}
\label{thm.contchoose}
Suppose $f\in\mathscr{L}([a,b])$. Then for all $\varepsilon>0$, there exists a continuous function $g$ such that 
\[
    \int_{[a,b]} |f-g| \; dm < \varepsilon
\]
\end{thm}
\begin{proof}
To be Lebesgue integrable on $[a,b]$, $f$ must be finite almost everywhere. So fix $\varepsilon>0$ and, recalling Theorem \ref{tiny.domain}, let $M<\infty$ be such that 
\[
    \int\limits_{\{x:|f|>M\}} |f|\; dm<\varepsilon
    \quad\Leftrightarrow\quad
    \int_{[a,b]} |f|\Chi_{\{x:|f|>M\}}\; dm <\varepsilon
\]
Now, set $f_1=f\Chi_{\{|f|\leq M\}}$. Applying Theorem \ref{thm.contapprox}, we know there exists a continuous function $g$ such that $|g|\leq M$
\[
   m(\{|f_1-g| > \varepsilon/M\}) <\varepsilon/M 
\]
Hence, we can write, letting $S_1=\left\{x:|f_1-g|>\frac{\varepsilon}{M}\right\}$ and $S_2=\left\{x:|f_1-g|\leq\frac{\varepsilon}{M}\right\}$,
\begin{align*}
    \int_{[a,b]} |f_1-g|\; dm &\leq 
    \int_{S_1} f\; dm \;\;+\quad
    \int_{S_2} f\; dm \\
    &\leq 
    \int_{S_1} 2M\; dm +\quad 
    \int_{S_2} \frac{\varepsilon}{M}\; dm \\
    &\leq 2M\left(\frac{\varepsilon}{M}\right) + 
    \left(\frac{\varepsilon}{M}\right)(b-a) \\
    &\leq C\varepsilon
\end{align*}
for some finite constant $C$.

\end{proof}



\newpage
\subsection{FTC for Lebesgue\label{sec.lebftc}}

We now reconsider the Fundamental Theorem of Calculus in the context of the Lebesgue Integral. The resulting theorem will be slightly different than what we saw for the Riemann Integral. Some of the results in this chapter will be swept to \textbf{Appendix \ref{app.lebftc}} so as not to obscure the main results for the FTC.

\begin{defn} 
For a Lebesgue integrable function $f$, the \emph{average function} of $f$ over $h$ at $x$ is defined
\[
    (A_h f)(x) := \frac{F(x+h)-F(x-h)}{2h} = \frac{1}{2h} \int\limits_{[x-h,x+h]} f\; dm
    \qquad h>0
\]
\end{defn}

\begin{defn}
For a Lebesgue integrable function $f$, the Hardy-Littlewood \emph{maximal function} is defined as 
\[
    (Mf)(x) = \sup_{h>0} \left(A_h|f|\right)(x)
\]
It gives a sense of how well these local averages, $A_h|f|$, behave pointwise.
\end{defn}

\begin{thm}
\hypertarget{maxmeas.thm}{}
\label{maxmeas.thm.n}
For a Lebesgue integrable function $f$, the maximal function is measurable.
\end{thm}

\begin{thm}
\hypertarget{maxineq.thm}{}
\label{maxineq.thm.n}
\emph{(Maximaly Inequality)}
For a Lebesgue integrable function $f$, we now present the \emph{maximal inequality}:
\[
    m\left(\{x \; |\;(Mf)(x)>\alpha\}\right)
    < \frac{4}{\alpha} \int |f|\; dm
    \qquad \forall \alpha>0
\]
\end{thm}

\begin{lem}
\hypertarget{avefn.lem}{}
\label{avefn.lem.n}
For $f\in\mathscr{L}([a,b])$, $\lim_{h\rightarrow0^+}(A_hf)=f$ almost everywhere. Said another way,
\[
    \lim_{h\rightarrow0^+} \frac{1}{2h} \int\limits_{[x-h,x+h]} \left[f-f(x)\right]\; dm
    = 0
    \qquad \text{\emph{almost everywhere}}
\]
\end{lem}
\begin{rmk}
This is a slightly weaker statement that what we're interested in, the difference quotient: $\frac{F(x+h)-F(x)}{h}$. As an example, consider the function $f(x)=|x|$, where we would clearly have that $(A_hf)(x)=f(x)$; though, $f$ is not differentiable at 0, so we can't conclude $F'(x)=f(x)$.
\end{rmk}

\begin{lem}
\hypertarget{strongae.lem}{}
\label{strongae.lem.n}
Slightly stronger than Lemma \hyperlink{avefn.lem}{\ref*{avefn.lem.n}}, we also have that for a Lebesgue Integrable function $f$, 
\[
    \lim_{h\rightarrow0^+} \frac{1}{2h} \int\limits_{[x-h,x+h]} \bigl\lvert f-f(x)\bigr\rvert\; dm
    = 0
    \qquad \text{\emph{almost everywhere}}
\]
\end{lem}
\begin{rmk}
At first blush, this might look like a trivial restatement of Lemma \hyperlink{avefn.lem}{\ref*{avefn.lem.n}}, rather than a meaningful improvement beyond it. But note that this doesn't follow immediately. The closest we could get would be that $\frac{1}{2h}\int_{[x-h,x+h]} |f|\; dm  \rightarrow|f|$ almost everywhere or that for almost all $x$, $\frac{1}{2h}\int_{[x-h,x+h]} |f|-|f(x)|\; dm \rightarrow 0$. This is weaker.
\end{rmk}

\begin{thm}\emph{(Fundamental Theorem for Lebesgue)} Again, the theorem has two parts.
\begin{itemize} 
\item[\emph{I.}] If $f$ is absolutely continuous (See Appendix \ref{app.abscont}), then $f$ is differentiable almost everywhere, $f'\in\mathscr{L}([a,b])$, and 
\[
    f(b)-f(a) = \int_{[a,b]} f' \;dm
\]
Moreover and non-trivially, $f$ being absolutely continuous is \emph{sufficient} for the relationship above to hold.

\item[\emph{II.}] For $f\in\mathscr{L}([a,b])$, define 
\[
    F(x):=\int_{[a,b]} f\;dm
\]
Then $F$ is continuous and differentiable almost everywhere, and $F'(x)=f(x)$ almost everywhere. 
\end{itemize}
\end{thm}
\begin{proof}

\begin{enumerate} 
\item (Part I)
\item (Part II) The proof proceeds in three major steps. Each will be stated along with its implications for the next stage of the proof.
\begin{enumerate}
\item (AC $\Rightarrow$ BV) This means we can represent $f\in AC$ as the difference of two monotone functions, which will be useful in the next stage.
\end{enumerate}
\end{enumerate}
\end{proof}

\begin{ex} (Cantor's Devil Staircase) To see why we need an \emph{absolutely} continuous function\footnote{See Appendix \ref{app.abscont}.} in the FTC for Lebesgue, Cantor's Devil Staircase provides a function that is monotonic, continuous, differentiable almost everywhere, and surjective with $f([0,1])=[0,1]$. See Definition \ref{devstair} in appendix \ref{CantorSet} for details.
\end{ex}

\begin{thm}
Suppose $f\in\mathscr{L}([a,b])$. Then $F(x):=\int_{[a,x]} f\; dm$ is absolutely continuous (hence uniformly continuous) and of bounded variation.
\end{thm}
\begin{proof}
Using a result we saw for more general integrals, we know that for all $\varepsilon>0$, there exists a $\delta>0$ such that $m(A)<\delta$ an implies
\[
    \int_{A} f\; dm < \varepsilon
\]
So then stipulate that $\bigcup^N_{i=1} (a_i,b_i)$ has measure less than $\delta$, where $N$ can be any finite number.
\end{proof}


\newpage
\section{Banach, Hilbert, and $L^p$ Spaces}

We start with a discussion of \emph{normed vector spaces}, which take a vector space and add the notion of a \emph{norm}, a function that can be used to measure the length of vectors or distance between vectors.

After that, we move to \emph{inner product spaces}. which are vector spaces that have defined a special function: the inner product. But, from this inner product, we can always build to a norm, tying these two concepts together.

\subsection{Normed Vector Spaces and Banach Spaces}

\begin{defn} 
\label{defn.norm}
Let $X$ be a vector space over a field $\mathbb{R}$ or $\mathbb{C}$. Then a \emph{norm} is a function  $\lVert\cdot\rVert:X\rightarrow[0,\infty]$ 
\begin{enumerate}
\item $\lVert x\rVert=0$ if and only if $x=0$
\item $\lVert x + y\rVert\leq \lVert x\rVert+\lVert y\rVert$ for all $x,y\in X$
\item $\lVert cx\rVert= |c|\cdot\lVert x\rVert$ for all scalar $c$ in the field and $x\in X$.
\end{enumerate}
It will be possible to use the norm to define a metric on a space, as we will see in Definition \ref{defn.banach}.
\end{defn}

\begin{defn} 
\label{defn.banach}
A \emph{Banach Space} is a \emph{complete} normed vector space. Thus, Banach spaces are vector spaces that have a metric, $d(x,y)=\lVert x-y\rVert$, which can be used to compute vector lengths and distance bewteen vectors. On top of that, it is also complete so that Cauchy sequences always converge to a limit.
\end{defn}

\begin{thm}
A norm, $\lVert\cdot\rVert: x\mapsto \rVert x \lVert$ is a Lipschitz continuous function.
\end{thm}
\begin{proof}
\begin{align*}
    \bigl\lvert \lVert x \rvert - \lVert y\rVert \bigr\rvert
    \leq 
    \bigl\lvert \lVert x - y + y\rVert - \lVert y\rVert \bigr\rvert
    \leq
    \bigl\lvert \lVert x - y \rVert + \lVert y\rVert - \lVert y\rVert \bigr\rvert
    \leq
    \lVert x - y \rVert 
\end{align*}
\end{proof}

\begin{ex}
Here are a few classic norms where vectors $x\in X$ have $n$ elements:
\begin{enumerate}
\item $\lVert x\rVert_\infty = \max_{1\leq i\leq n} |x_i|$
\item $\lVert x\rVert_2 = \left(\sum_{i=1}^n |x_i|^2\right)^{\frac{1}{2}}$ 
\item $\lVert x\rVert_p = \left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}$ for $p\in[1,\infty]$, not necessarily an integer.
\end{enumerate}
\end{ex}

\begin{ex} Now for some norms in \emph{function} space, as opposed to norms for $\mathbb{R}^n$ or $\mathbb{C}^n$:
\begin{enumerate}
\item $X=C([a,b])$, the set of continuous functions on $[a,b]$, take 
\[
    \lVert f\rVert_\infty :=
    \sup_{x\in[a,b]} |f(x)|
\]
\item $X=C'([a,b])$, the set of once-differentiable continuous functions on $[a,b]$, use the previous definition to define
\[
    \lVert f\rVert := \lVert f\rVert_\infty + \lVert f'\rVert_\infty
\]
\end{enumerate}
\end{ex}

\begin{defn}
A \emph{semi-norm} is defined as above in Definition \ref{defn.norm}, except condition (1) is not there. That is, there can be non-zero vectors that the norm maps to zeros.
\end{defn}
\begin{ex}
For an example of a semi-norm, suppose that all vectors $x\in X$ have $n$ elements, $x=(x_1,\ldots,x_n)$. Then define the a semi-norm for $m<n$ as $\lVert x\rVert = \sum_{i=1}^m |x_i|$.
\end{ex}

\subsection{Inner Product Spaces}

\begin{defn}
An \emph{inner product space} is a vector space $X$ with an \emph{inner product}, a function $\langle\cdot,\cdot\rangle:X\times X\rightarrow\mathbb{R}$ (or $\mathbb{C}$). It has the following properities
\begin{enumerate}
\item $\langle x,x\rangle \geq 0$ for all $x\in X$. Moreover, $\langle x,x\rangle=0$ if and only if $x=0$, the zero vector.
\item $\langle x,y\rangle= \overline{\langle y,x\rangle}$.
\item $\langle \cdot,y\rangle$ is linear. That is 
\[
    \langle \alpha x + y,z\rangle = 
    \alpha \langle x ,z\rangle + \langle y,z\rangle
\]
Conjugate linearity by the previous statement.
\end{enumerate}
\end{defn}

\begin{defn}
A \emph{Hilbert Space} is a \emph{complete} inner product space. We shall see that all Hilbert spaces are Banach spaces since an inner product can be used to define a norm.
\end{defn}

\begin{defn}
We say that two vectors, $x$ and $y$, are \emph{orthogonal} if $\langle x,y\rangle =0$. We denote orthogonality by $x\perp y$.

We call a collection of vectors $\{x_i\}$ an \emph{orthogonal family} if $x_i\perp x_j$ for all $i\neq j$. We call the family \emph{orthonormal} if, in addition to being an orthogonal family, $\lVert x_i\rVert=1$ for all $i$. 
\end{defn}

\begin{thm}
\label{thm.cauchyscwarz}
\emph{(Cauchy-Schwartz Inequality)}
Given any inner product space, define $\lVert x\rVert := \sqrt{\langle x,x\rangle}$.\footnote{Later, we will show that this definition does, indeed, make $\lVert x\rVert$ a norm.  However, the proof requires the Cauchy-Scwarz inequality. So now, just treat $\lVert x\rVert$ as a function that has some special property we want to prove.} Then
\[
    \lvert \langle x,y\rangle\rvert \leq \lVert x\rVert\cdot\lVert y\rVert
\]
\end{thm}
\begin{proof}
Start by assuming that $|\alpha|=1$ and that $\alpha\in\mathbb{C}$. Then define
\[
    Q_\alpha(t) = \lVert x - t\alpha y\rVert^2
\]
By the definition of $\lVert \cdot\rVert$, it's clear that $Q_\alpha(t)\geq 0$ for all $t\in\mathbb{R}$. Then let's use the properties of the inner product to write
\begin{align}
    Q_\alpha(t) &= \langle x -t\alpha y,\; x -t\alpha y\rangle \notag \\
    &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2 
    - t\alpha \overline{\langle x,y\rangle}
    - t\bar{\alpha} {\langle x,y\rangle} \label{cauchyhold}
\end{align}
Without loss of generality, assume $\langle x,y\rangle\neq 0$, otherwise trivial. Now since $\alpha$ was only restricted to be such that $|\alpha|=1$, we can choose it to be whatever we want, and use the fact that the Equality \ref{cauchyhold} \emph{must} be true for all $|\alpha|=1$ and $t\in\mathbb{R}$, given the definition fo the inner product. So let
\begin{align*}
    \alpha = \frac{\langle x,y\rangle}{|\langle x,y\rangle|} 
    \quad\Rightarrow\quad
    Q_\alpha(t) &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2 
    - t\frac{\langle x,y\rangle\overline{\langle x,y\rangle}}{\langle x,y\rangle} 
    - t\frac{\overline{\langle x,y\rangle}{\langle x,y\rangle}}{\langle x,y\rangle}  \\
    &= \lVert x\rVert^2 + t^2 \lVert y\rVert^2 
    -2t |\langle x,y\rangle|
\end{align*}
\end{proof}

\begin{thm}
\label{norminduce}
Given an inner product space, we can define a norm by $\lVert x\rVert := \sqrt{\langle x,x\rangle}$.
\end{thm}
\begin{proof}
Given the definition, this is obvious for all but the triangle inequality, which relies on the Cauchy-Schwarz inequality, which will be proved in the following theorem.
\end{proof}

\begin{thm}
\label{pythag}
\emph{(Pythagorean Relation)} Given $x\perp y$, 
\[
    \lVert x + y\rVert^2 = \lVert x\rVert^2 + \lVert y \rVert^2
\]
More generally, given an orthonormal set $\{u_1,\ldots,u_N\}$:
\[
    \left\lVert \sum_{i=1}^N c_i u_i \right\rVert^2 
    = \sum_{i=1}^N |c_i|^2
\]
\end{thm}

\begin{prop}
Let $X$ be an inner product space. Then for all $x,y\in X$, we have the \emph{polarization identity}
\[
    \lVert x+y\rVert^2 + \lVert x-y\rVert^2
    = 4\langle x,y\rangle
\]
along with the \emph{parallelogram law}:
\[
    \lVert x+y\rVert^2 + \lVert x-y\rVert^2
    =2\left(\lVert x\rVert^2 +\lVert y\rVert^2\right)
\]
\end{prop}
\begin{rmk}
    This only works with inner product spaces. Altough the parallelogram law is written in terms of norms, it will not worked for \emph{all} normed vector spaces. It only holds for norms induced from an inner product as in Theorem
\ref{norminduce}.
\end{rmk}

\begin{thm}
\label{projorth}
Suppose $\{u_1,\ldots,u_N\}$ is an orthonormal set. Then given any $x\in X$, 
\[
    \sum_{n=1}^N \langle x,u_n\rangle u_n
    \perp
    x -\sum_{n=1}^N \langle x,u_n\rangle u_n
\]
This (I think) is like projecting $X$ onto the axes defined by the orthonormal set. So the residual will be perpendicular.
\end{thm}
\begin{proof}
Evaluate the inner product.
\end{proof}

\begin{thm}
Given an orthonormal set $\{u_1,\ldots,u_N\}$, 
\begin{align*}
    \lVert x\rVert^2 = 
    \sum_{i=1}^N \bigl\lvert\langle x,u_n\rangle\bigr\rvert^2 
    + \left\lVert x -  \sum_{i=1}^N\langle x,u_n\rangle u_n \right\rVert^2
\end{align*}
\end{thm}
\begin{proof}
In $\lVert x\rVert^2$,substitute in 
\[
    \lVert x\rVert^2 = 
    \left\lVert
    \sum_{n=1}^N \langle x,u_n\rangle u_n
    + \left( x 
    -\sum_{n=1}^N \langle x,u_n\rangle u_n\right)
    \right\rVert^2 
\]
Then use the Pythagorean Relation in Theorem \ref{pythag}, since these two objects in the sum are orthogonal by Theorem \ref{projorth}.
\end{proof}

\begin{cor}
\emph{(Bessel's Inequality)} 
Given an orthonormal set $\{u_1,\ldots,u_N\}$, 
\[
    \sum_{i=1}^N 
    \bigl\lvert\langle x,u_n\rangle\bigr\rvert^2 
    \leq \lVert x\rVert^2
\]
\end{cor}

\begin{rmk}
Cauchy Schwarz can also be proved via Bessel's Inequality if we take $N=1$ and $u_1=\frac{y}{\lVert y\rVert}$. Then 
\[
    \frac{\langle x,y\rangle^2}{\lVert y\rVert^2} \leq \lVert x\rVert^2
    \quad\Rightarrow\quad
    \langle x,y\rangle \leq \lVert x\rVert\cdot \lVert y\rVert
\]
\end{rmk}

\subsection{$L^p$ Spaces}


\begin{defn}
Given a measure space $X=L^p(\Omega,\mathscr{M},\mu)$, we define the following norm for functions on that space:
\begin{align*}
    \text{For }1\leq p < \infty \qquad
    \lVert f\rVert_p &:= \left(\int_\Omega 
    |f|^p \; d\mu\right)^{\frac{1}{p}}
    \in [0,\infty]\\
    \lVert f\rVert_\infty &:=
    \essup_{x\in\Omega} |f(x)|
    := \inf\{M\;:\; |f|\leq M \text{ $\mu$-a.e.}\}
\end{align*}
The essential supremum effectively ignores all measure zero infinities. 
\\
\\
Next, we then define $L^p$ space, for $p\in[1,\infty]$ as 
\begin{align*}
    L^p(\Omega,\mathscr{M},\mu)
    &:=\{f: \Omega \rightarrow \mathbb{C} \;:\;
    \lVert f\rVert_p < \infty\} 
\end{align*}
where two functions are equal in $L^p$ if they are equal almost everywhere on $X$.
\end{defn}

\begin{rmk}
Note, we will play a little fast and loose with notation in this section. Specifically, when you see something like $|f+g|\leq |f| + |g|$, we're really saying that , for all $x$, $|f(x)+g(x)|\leq |f(x)| + |g(x)|$. 

On the other, you might see the following expression, which looks similar: $|f|\leq \lVert f\rVert_p$. So note that $\lVert f\rVert_p$ is always a \emph{number}. So $|f|\leq\lVert f\rVert_p$ means that for all \emph{points} $x\in\Omega$, $f(x)$ is less than or equal to the \emph{number} $\lVert f\rVert_p$, an upper bound. 

Almost everywhere, the meaning will be immediately clear from the context, but for those measure zero instances where it's not, a note will be made. 
\end{rmk}

\begin{prop}
$L^p(\Omega,\mathscr{M},\mu)$ is a normed vector space.
\end{prop}
\begin{proof}
We check the properties that a normed vector space must have
\begin{enumerate}
    \item First, it's clear that $\lVert f\rVert_p=0$ if and only if $f=0$. (Recall that $f=0$ means that $f(x)=0$ for almost all $x\in\Omega$. It doesn't need to be the constant.)
    \item Next, it's obvious that $\lVert cf\rVert_p = c\lVert f\rVert_p$.
\item Finally, for the triangle inequality, suppose that $p=\infty$. Then since $\lVert \cdot\rVert_\infty$ is just the essential supremum, by definition
\begin{align*}
    |f|&\leq \lVert f\rVert_\infty
    \quad \text{$\mu$-a.e.}\\
    |g|&\leq \lVert g\rVert_\infty 
    \quad \text{$\mu$-a.e.}
\end{align*}
So then for almost all $x$, since $|f(x)+g(x)|\leq |f(x)|+|g(x)|$, we have
\begin{align*}
    |f(x)+g(x)|\leq \lVert f\rVert_\infty 
    + \lVert g\rVert_\infty
\end{align*}
Since this is true for all $x\in\Omega$, we can take the essential sup over the lefthand side to get
\begin{align*}
    \lVert f+g\rVert_p
    \leq \lVert f\rVert_\infty 
    + \lVert g\rVert_\infty
    \quad \text{$\mu$-a.e.}
\end{align*}
Next, suppose $p\in[1,\infty)$. Also, assume without loss of generality that $f\neq 0$ and $g\neq 0$ (where, again, equality is almost-everywhere equality). Then define
\begin{align*}
    f_0 := \frac{|f|}{\lVert f\rVert_p}
    \qquad
    g_0 := \frac{|g|}{\lVert g\rVert_p}
    \qquad 
    \quad\Rightarrow\quad
    \lVert f_0\rVert_p
    =\lVert g_0\rVert_p
    =1
\end{align*}
Then, if we look pointwise and let $C=\lVert f\rVert_p+\lVert g\rVert_p$ we have that
\begin{align}
    |f+g|^p&\leq \left(|f|+|g|\right)^p\notag\\
    &\leq C^p\left(f_0\frac{\lVert f\rVert_p}{C}
    + g_0\frac{\lVert g\rVert_p}{C}\right)^p
    \label{convstep}
\end{align}
Now let's consider the beast inside the parentheses, raised to the $p$th power. The term inside is mapped $t\mapsto t^p$, which is convex function for $p\in[1,\infty)$. Now we can use this convexity towards our purpose if we rewrite the whole term, a little more suggestively, using the definition of $C$:
\[
    \left(f_0\frac{\lVert f\rVert_p}{C}
    + g_0\frac{\lVert g\rVert_p}{C}\right)^p
    = 
    \left(\alpha f_0
    + (1-\alpha)g_0\right)^p
    \qquad \text{where}\quad
    \alpha=\frac{\lVert f\rVert_p}{\lVert f\rVert_p 
    +\lVert g\rVert_p }
\]
Finally, given a convex function written in this way, we know that 
\[
    \left(\alpha f_0
    + (1-\alpha)g_0\right)^p
    \leq 
    \left(\alpha f_0^p
    + (1-\alpha)g_0^p\right)
\]
Alright, almost there. Using this back in Inequality \ref{convstep}, we get an expression, which we'll then integrate:
\begin{align*}
    |f+g|^p  &\leq 
    C^p\left(f_0^p\cdot\frac{\lVert f\rVert_p}{C}
    + g_0^p\cdot\frac{\lVert g\rVert_p}{C}\right)\\
    \Rightarrow\quad
    \int_\Omega|f+g|^p \;d\mu &\leq \int_\Omega
    C^p\left(f_0^p\cdot\frac{\lVert f\rVert_p}{C}
    + g_0^p\cdot\frac{\lVert g\rVert_p}{C}\right)\;d\mu \\
    &\leq C^p
    \int_\Omega f_0^p\cdot\frac{\lVert f\rVert_p}{C}
    \;d\mu+ C^p\int_\Omega 
    g_0^p\cdot\frac{\lVert g\rVert_p}{C}\;d\mu
\end{align*}
\end{enumerate}
\end{proof}

\begin{thm}
Provided $p\in[1,\infty)$, $\lVert f+g\rVert_p=\lVert f\rVert_p+\lVert g\rVert_p$ if and only if $f=\lambda g$ for some $\lambda\in \mathbb{C}$.
\end{thm}

\begin{thm}
\emph{(Holder's Inequality)\footnote{Hodor.}}
For $p\in[1,\infty]$, define $p'$ such that
\[
    \frac{1}{p}
    +
    \frac{1}{p'}
    =1
\]
Then, if $f\in L^p$ and $g\in L^{p'}$, we have $fg\in L^1$ and 
\[ 
    \int_\Omega |fg| \; d\mu \leq \lVert f\rVert_p
    \lVert g\rVert_{p'}
\]
\end{thm}
\begin{rmk}
This a generalization of Cauchy-Schwarz, which is a special case where $p=p'=2$.
\end{rmk}
\begin{proof}
Suppose $p=1$. Then $p'=\infty$, and to have $g\in L^{p'}$, it must be that $g$ is bounded, i.e. there is an $M$ such that $|g|<M$ for all $x\in\Omega$. So
\begin{align*}
    \int_\Omega |fg| \; d\mu \leq 
    M\int_\Omega |f| \; d\mu \leq 
    \lVert g\rVert_\infty\int_\Omega |f| \; d\mu 
    = 
    \lVert g\rVert_\infty\lVert f\rVert_1
\end{align*}
The case where $p=\infty$ is the same.
\\
\\
Now suppose $p\in(1,\infty)$. Also, note a simple result that we will use later:
\begin{align}
    \text{Since $\log$ is concave }
    \Rightarrow \quad
    \log(\alpha x + (1-\alpha)y)&\geq
    \alpha \log x + (1-\alpha) \log y\notag\\
    \text{After exponentiating }
    \Rightarrow \quad
    \alpha x + (1-\alpha)y &\geq
    x^\alpha y^{1-\alpha}\label{logconc}
\end{align}
Now normalize
\[
    f_0 = \frac{|f|}{\lVert f\rVert_p}
    \quad
    g_0 = \frac{|g|}{\lVert g\rVert_{p'}}
    \quad \Rightarrow\quad
    \lVert f_0\rVert_p = \lVert g\rVert_{p'}=1
\]
and, so that we can use Inequality \ref{logconc}, define
\[
    x=|f_0|^p 
    \quad 
    y=|g_0|^{p'} 
    \quad
    \alpha=\frac{1}{p} 
    \quad
    1-\alpha=\frac{1}{p'}
\]
Plug into Result \ref{logconc}:
\[
    |f_0 \cdot g_0|=|f_0|\cdot|g_0|\leq 
    \frac{1}{p} |f_0|^p
    +\frac{1}{p'} |g_0|^{p'}
\]
We can now integrate the last expression to get 
\begin{align*}
    \int_\Omega
    |f_0\cdot g_0|\;d\mu&\leq 
    \frac{1}{p}
    \int_\Omega |f_0|^p \;d\mu
    +\frac{1}{p'}\int_\Omega |g_0|^{p'}\;d\mu \\
    \Leftrightarrow\quad
    \frac{1}{\lVert f\rVert_p \lVert g\rVert_{p'}}
    \int_\Omega
    |f g|\;d\mu
    &\leq 
    \frac{1}{p}\lVert f_0\rVert^p_p
    +\frac{1}{p'}\lVert g_0\rVert_{p'}^{p'} 
    = \frac{1}{p} + \frac{1}{p'}=1\\
    \Rightarrow\quad
    \int_\Omega
    |f g|\;d\mu
    &\leq 
    {\lVert f\rVert_p \lVert g\rVert_{p'}}
\end{align*}
\end{proof}

\begin{cor}
The last theorem gives equality when $|f|=\lambda |g|^{\frac{p'}{p}}$, hence 
\[
    \sup_{\lVert g\rVert_{p'}=1} 
    \int_\Omega fg \; d\mu = \lVert f\rVert_p
\]
\end{cor}

\begin{cor}
Suppose that $\mu(\Omega)<\infty$ and $p\geq q$. Then 
\[
    L^p(\Omega,\mathscr{M},\mu)
    \subset 
    L^q(\Omega,\mathscr{M},\mu)
\]
\end{cor}
\begin{rmk}
There are no inclusion relations between $L^p$ spaces when $\Omega=\mathbb{R}$. We obviously can't apply the last theorem since $\mu(\mathbb{R})$ is infinite, but to get some better inuition as to why there isn't inclusion, consider functions like $1/x^2$ and $1/\sqrt{x}$ which behave very differently in their tails and at $0$. 
\end{rmk}

\begin{rmk}
    For all $(\Omega,\mathscr{M},\mu)$, we have for all $p\in[1,\infty]$
\[
    L^1 \cap L^\infty \subset L^p \subset L^1+L^\infty
\]
where the largest set in the relation above is the set of all functions that are the sum $f+g$ where $f\in L^1$ and $g\in L^\infty$.
\end{rmk}


\subsection{Completeness of $L^p$ Spaces}

\begin{lem}
\label{lem.absonv}
Let $X$ be a normed vector space. Then $X$ is complete if and only if 
\[
    \sum^\infty_{n=1} \lVert x\rVert < \infty
    \quad\Rightarrow\quad
    \sum^\infty_{n=1}  x
\]
i.e. a summable series that is \emph{absolutely} convergent will also be convergent.
\end{lem}

\begin{thm}
(Riesz-Fisher) For $p\in[1,\infty]$, $L^p$ is complete, i.e. $L^p$ is a Banach Space.
\end{thm}
\begin{proof}
Let's take any sequence of functions $\{f_n\}\in L^p$ such that $M:=\sum^\infty_{n=1} \lVert f\rVert_p<\infty$. Then, by Lemma \ref{lem.absonv}, we can show $L^p$ to be complete if we can show that $\sum^\infty_{n=1} f_n <\infty$.  So define the following sums (pointwise)
\[
    S_N:=\sum^N_{n=1} f_n
    \qquad 
    T_N:=\sum^N_{n=1} |f_n|
\]
We deal first with $T_N$, since it uses absolute values, and so will always be increasing in $N$. But first, by Minkowski's inequality, it's clear that 
\[
    \lVert T_N \rVert_p =  
    \left\lVert \sum^\infty_{n=1} |f_n|
    \right\rVert_p \leq
    \sum^\infty_{n=1} \left\lVert f_n
    \right\rVert_p \leq
    M
\]
And since we have a bounded sum that is increasing in 
Now since $\sum^\infty_{n=1}\lVert f\rVert_p$ converges to $M$, it's clear that 
\end{proof}


\subsection{Duality in $L^p$ Spaces}

Inuition: Duals shift the focus from \emph{elements} of a normed vector space to \emph{functions} on that vector space. So suppose we have a norm defined for elements in $X$. Duals build off this using a norm for functions defined on $X$ (mapping into $\mathbb{R}$ or $\mathbb{C}$).

\begin{defn}
A \emph{linear functional} is a linear mapping from a vector space to the field over which it is defined.
\end{defn}

\begin{defn}
An \emph{algebraic dual} on a vector space $X$ is the set of all linear functionals $\ell: X\rightarrow\mathbb{R}$ (or $\mathbb{C}$). 
\end{defn}

\begin{defn}
A \emph{topological dual}, $X^*$ for vector space $X$, is the space of all bounded linear functions. We now define a norm for linear functionals so that we call $\ell: X\rightarrow \mathbb{R}$ (or $\mathbb{C}$) bounded if
\[
    \lVert \ell\rVert := \sup_{\lVert x\rVert=1} 
    |\ell(x)| 
    = \sup_{x\in X\setminus 0} 
    \frac{|l(x)|}{\lVert x\rVert} 
    = \sup_{x\in X\setminus 0} 
    \ell\left(
    \frac{|x|}{\lVert x\rVert} 
    \right)
    <\infty
\]
where each of the representations above are equally valid. As a result of this definition
\begin{equation}
    \label{funcineq}
    |\ell(x)| \leq \lVert \ell\rVert\cdot\lVert x\rVert
\end{equation}
\end{defn}

\begin{prop}
A linear functional $\ell$ is Lipschtz continuous.
\end{prop}
\begin{proof}
We use the fact, first, that $\ell$ is linear and, second, Inequality \ref{funcineq}
\[
    |\ell(x)-\ell(y)|\leq 
    |\ell(x-y)|\leq 
    \lVert \ell\rVert\cdot \lVert x-y\rVert
\] 
\end{proof}

\begin{prop}
A continuous linear functional is bounded.
\end{prop}
\begin{proof}
Suppose not. Then for all $n$, there is a vector $\lVert x_n\rVert=1$ such that 
\[
    |\ell(x_n)| \geq n 
\]
So then define the following
\[
    \tilde{x}_n = \frac{x_n}{\sqrt{n}}
\]
Then $\tilde{x}_n\rightarrow0$, but 
\end{proof}

\begin{thm}
$(X^*,\lVert \cdot\rVert)$ is a normed vector space, with 
\[
    \lVert \ell_1+\ell_2\rVert = \sup_{\lVert x\rVert=1}
    |\ell_1(x)+\ell_2(x)|\leq
    \lVert \ell_1\rVert+ \lVert \ell_2\rVert
\]
\end{thm}

\begin{prop}
$X^*$ is a Banach Space.
\end{prop}

\begin{thm}
For all $g\in L^{p'}$, where $p'$ is the Holder conjugate of $p$, we have $\ell_g\in(L^p)^*$, where 
\[
    \ell_g(f)=\int_X fg \; d\mu
\]
So $L^{p}$ is our base normed vector space consisting of functions $f\in L^p$. To evaluate a functional $\ell_g$ at the function $f$ in vector space $L^p$, just integrate $f$ against $g$.
\end{thm}
\begin{proof}
To show that $\ell_g$ is in the dual of $L^p$, we need to show that $\ell_g$ is a bounded linear functional for all $g\in L^{p'}$. So first, by Holder's inequality,
\[
    |\ell_g(f)| = 
    \left\lvert \int_X fg \; d\mu\right\rvert 
    \leq 
    \int_X \left\lvert fg \right\rvert \; d\mu
    \leq \lVert f\rVert_p \lVert g\rVert_{p'}
\]
Then, taking the sup over all $f$ such that $\lVert f\rVert_p=1$, we get
\[
    \lVert \ell_g\rVert :=
    \sup_{\lVert f\rVert_p=1}
    |\ell_g(f)| \leq \lVert g\rVert_{p'}
\]
Hence, $\ell_g\in (L^p)^*$. 
\end{proof}
\begin{rmk}
In words, this last theorem says that every $g\in L^{p'}$ gives rise to an element of the dual of $(L^p)^*$ with the norm (on the dual) equal to $\lVert g\rVert_{p'}$. In other words, $L^{p'}$ is ``embedded'' in $(L^p)^*$.
\end{rmk}

\begin{thm}
\emph{(Riesz Representation Theorem for $\mathbb{R}$)}
Suppose $p\in[1,\infty)$. Then for every $\ell\in(L^p(\mathbb{R}))^*$, there exists a unique function $g\in L^{p'}(\mathbb{R})$, where $p'$ is the Holder-conjugate of $p$ such that $\ell=\ell_g$, i.e.
\[
    \ell(f)=\int_\mathbb{R} fg \; d\mu
\]
In short $(L^p(\mathbb{R}))^* =\sim L^{p'}(\mathbb{R})$.
\end{thm}




\subsection{$L^1$ Space}
\begin{defn}
We define an equivalence relation between functions, denoted $f$ `$=$' $g$:
\[
    f \text{ `$=$' } g
    \quad \Leftrightarrow\quad 
    g(x) = f(x) \quad \text{a.e.}
\]
\end{defn}

\begin{defn} ($L^1$) We the norm on $L^1$ to be
\[
    ||f||:= \int_X |f| \; d\mu
\]
We then define $L_1$ as
\[
    L_1 := \mathscr{L}(X,\mathscr{M},\mu) / \sim
    = \{\text{Distinct equiv. classes of functions}\}
\]
This is a proper normed vector space because the following properties are satisfied:
\begin{enumerate}
    \item $||f|| = 0$ if and only if $f=0$, then zero element.
    \item $||f+g||\leq ||f|| + ||g||$.
    \item $||c f|| = |c| \cdot ||f||$.
\end{enumerate}
\end{defn}

\begin{defn} ($L^p$) We define $L^p$ with the norm
\[
    ||f||_p:= \left(\int_X |f|^p \; d\mu\right)^{1/p}
\]
We then define the space
\[
    L^p(X,\mu) := \{\text{$f$ measurable}\;:\;
    \text{$||f||_p < \infty$}\}
\]
\end{defn}




%%%% APPPENDIX %%%%%%%%%%%

\newpage
\appendix


\section{Additional Definitions}

\subsection{Modulus of Continuity}

\begin{defn} We define the modulus of continuity of the function $f$, $w_f(\delta)$ as 
\begin{equation}
    w_f(\delta) \equiv \sup_{|x-y|\leq\delta} |f(x)-f(y)|
\end{equation}
\end{defn}
It's a useful definition because of it's close relation to uniform continuity.
It's clear that if $f$ is uniformly continuous, then $w_f\left(||P||\right)\rightarrow0$ as as $||P||\rightarrow0$.

\subsection{Lipschitz Continuity} 

A function $f$ is Lipshitz at $x$ if, for some $C$ and $\delta>0$, we have that 
\begin{align*}
    |x-y|\leq \delta \quad \Rightarrow \quad 
        |f(x) - f(y)|\leq C|x-y|
\end{align*}
This is a \emph{stronger} definition of uniform continuity in that Lipschitz implies uniformly continuous, but uniformly continuous does not imply Lipschitz. 


\subsection{Absolute Continuity \label{app.abscont}}

\begin{defn}
We say that a function $f:[a,b]\rightarrow\mathbb{R}$ is \emph{absolutely continuous} (AC) if for all $\varepsilon>0$, there exists a $\delta>0$ such that for any finite, disjoint set of open interals $\{(a_i,b_i)\}_1^N$ 
\[
    \sum_{i=1}^N (b_i-a_i)<\delta
    \quad\Rightarrow\quad
    \sum_{i=1}^N |f(b_i)-f(a_i)|<\varepsilon
\]
This stronger \emph{uniform continuity}, which is a special case where you restrict $N=1$ above, such that AC implies uniform continuity.
\end{defn}
\begin{rmk}
Note that this is for \emph{any} finite $N$. So you can't use $N$ in your proofs since you won't know it; a proof that something is AC needs to be able to handle any $N$. 
\end{rmk}

\begin{ex}
Uniform continuity does not imply absolute continuity. For example, Cantor's Devil Staircase is a uniformly, but not absolutely continuous function.
\end{ex}

\begin{thm}
If $f\in AC$ and $f'=0$ almost everywhere, then $f$ is a constant function.
\end{thm}

\begin{defn}
We say that a function $f$ is \emph{strongly} continuous if for all $\varepsilon>0$, there is a $\delta>0$ such that for \emph{any} (not neccessarily disjoint) finite collection of open intervals, $\{(a_i,b_i)\}^N_1$, 
\[
    \sum_{i=1}^N (b_i-a_i) < \delta
    \quad\Rightarrow\quad
    \sum_{i=1}^N |f(b_i)-f(a_i)| <\varepsilon
\]
\end{defn}

\subsection{Classifying Functions}

We've seen many concepts regarding functions---absolute continuity, Lipschitz continuity, continuous almost everywhere, etc. This subsection puts all those results in one place. We start with the most elementary types of functions, then broaden.

\begin{thm}
Suppose that $f$ is a monontonic function. Then
\begin{enumerate}
    \item $f\in BV([a,b])$
    \item $f$ has at most countable many points of discontinuity (i.e. is continuous a.e.).
    \item $f$ is differentiable almost everywhere.
    \item If $\alpha$ is continuous, then $\alpha \in \mathscr{R}_f([a,b])$ and $f\in\mathscr{R}_\alpha([a,b])$.
    \item $f'\in\mathscr{L}([a,b])$ 
    \item $f$ is Borel measurable.
    \item $f$ can uniquely as $g+h$ where $g$ is absolutely continuous and $h'=0$ almost everywhere.
\end{enumerate}
\end{thm}

\begin{thm}
Suppose $f\in BV([a,b])$. Then
\begin{enumerate}
\item $f$ is continuous almost everywhere on $[a,b]$.
\item $f$ can be written as the difference of two monotonic functions: $f=u-v$.
\end{enumerate}
\end{thm}

\begin{thm} Suppose $f\in AC$. Then
\begin{enumerate}
\item $f\in AC\rightarrow f\in BV$
\item 
\end{enumerate}
\end{thm}

\begin{thm}
Suppose that $f$ is Lipschitz. Then
\begin{enumerate}
\item $f\in AC$
\item $f$ is \emph{strongly} AC. (And vice versa.)
\end{enumerate}
\end{thm}

\subsection{Cantor Set\label{CantorSet}}

\begin{defn}
To construct the Cantor Set, start with the closed interval $[0,1]$. Next, remove the open middle third, $I_1 = (1/3, 2/3)$. This leaves us with 
    \[ C_1 = [0, 1/3] \cup [2/3, 1] \]
Continue this process so that the Cantor set is the limit of this process
\begin{align*}
    C_0 &= [0,1] \\
    C_1 &= [0,1/3] \cup [2/3, 1] \\
    C_2 &= [0,1/9] \cup [2/9, 3/9]  \cup [6/9, 7/9] \cup [8/9, 1]
        \\
    &\;\;\vdots 
\end{align*}
Then we let the Cantor Set $C$ equal
    \[ C = \bigcap^\infty_{n=1} C_n \]
    The set is \textbf{closed} because it is an infinite intersection of closed sets.
\end{defn}

\begin{thm}
The Cantor set $C$ is \textbf{uncountable}. 
\end{thm}
\begin{proof}
The proof will make use of the fact that when we remove the inner third, the endpoints are ``sticky''---they stay in the set $C$ and never get removed.
\\
\\
To be more explicit and rigorous, suppose that $C$ is countable. Then you can write the set as $C = \{c_1, c_2, \ldots\}$. If this is the case, then choose the subinterval in $C_1$ to which $c_1$ does not belong, either $[0,1/3]$ or $[2/3,1]$. Call it $[a_1, b_1]$. Next, remove the middle third from $[a_1, b_1]$, and choose the subinterval to which $c_2$ does not belong. Call this interval $[a_2, b_2]$. 

Now it's clear that at each stage, the subinterval we choose (after removing the inner third) is a subset of the previous subinterval. And so we have nested closed sets with $[a_m, b_m] \subset [a_n, b_n]$ for $m>n$. So when we take the infinite intersection, we get a nonempty set:
\[ 
    \bigcap^\infty_{n=1} [a_n, b_n] \supset \{a\} \neq 
    \emptyset\qquad \text{where} \quad 
    \lim_{n\rightarrow\infty} a_n = a
\]
Now clearly, $a_n\in C$ for all $n$ since the endpoints are ``sticky'' in $C$.  On top of that, $a\neq c_n$ for all $n$, since we chose the subsintervals explicity not to contain $c_n$ for all $n$.  But this is a contradiction since $a$ is a limit point of $\{a_n\}\subset C$, and the closed set $C$ should contain its limit points. And so $C$ is uncountable.
\end{proof}

\begin{ex} 
\label{devstair}
(Devil's Staircase)
\end{ex}

\newpage
\section{Supporting Theorems and Results}

In this appendix, we detail some results that, while useful or even crucial in certain proofs, are more general results that are not specific to what they're used in.  They might also unnecessarily disrupt the flow of a particular section.

\subsection{Support for Subsection \ref{sec.support1}\label{app.support1}}
\begin{thm}
\label{littlewoods.1st}
\emph{(Littlewood's 1st Principle)}
\end{thm}

\subsection{Support for Subsection \ref{sec.support2}\label{app.support2}}
\begin{thm}
\label{open.intervals}
\end{thm}

\subsection{Support for Subsection \ref{sec.lebftc}\label{app.lebftc}}



\begin{proof}[\textbf{Proof of Theorem \hyperlink{maxmeas.thm}{\ref*{maxmeas.thm.n}}}]
We need to show the following set is in $\mathscr{M}$, for all $\alpha$:
\begin{equation}
    \label{maxmeas}
    \{x\;|\;(Mf)(x)>\alpha\} 
\end{equation}
But what does this set mean? It's the set of all points such that, for some $h>0$, $A_h|f|>\alpha$. But we can write that another way:
\[
    \{x\;|\;(Mf)(x)>\alpha\} = \bigcup_{h>0} 
    \{x \; |\; A_h|f|(x)>\alpha\}
\]
Now, fixing $h$, it's clear that $A_h|f|$ is continuous since $F(x)=\int_{[x,b]} f\; dm$ is continuous. As a result, the preimage of open sets, like $(\alpha,\infty]$, will be open so the union above is a union of open sets. Thus, the union is open as well implying the preimage in (\ref{maxmeas}) is in $\mathscr{M}$.
\end{proof}


\begin{proof}[\textbf{Proof of Theorem \hyperlink{maxineq.thm}{\ref*{maxineq.thm.n}}}]
We first prove a lemma that will help us. 
\begin{lem}
Let $\{I_\lambda\}$ be a collection of open intervals in $[a,b]$. Let $U=\cup I_\lambda$. Then there exists a finite set of indices $\lambda_1,\ldots,\lambda_N$ such that $\{I_{\lambda_i}\}^N_1$ are pairwise disjoint and 
\[
    \sum_{i=1}^N \ell(I_{\lambda_i}) \geq \frac{1}{4}m(U)
\]
\end{lem}
\begin{proof}
(Proof of Lemma) By the regularity conditions of the Lebesgue measure and the fact that $U$ is Lebesgue measurable, we know that we can approximate $U$ from the inside by a compact set. So let $K\subset U$ be a compact set such that
\[
    m(K)\geq \frac{3}{4} m(U)
\]
where we take $\delta=\frac{1}{4}m(U)$ as our approximation error. Then, by the compactness of $K$, there exists a finite set of indices $\alpha_i,\ldots,\alpha_M$ such that
\[
    \bigcup_{i=1}^M I_{\alpha_i} \supset K
\]
Now let $I_{\lambda_1}$ be an interval among $I_{\alpha_1},\ldots,I_{\alpha_M}$ with the largest length. Discard all $I_{\alpha_i}$ intersecting $I_{\lambda_1}$. Then take the next largest interval, and repeat the process until all the $I_{\alpha_i}$ are exhausted. In this way, we construct a set finite $I_{\lambda_1},\ldots,I_{\lambda_N}$, which will turn out to be our desired intervals. We just need to show that more explicitly.
\\
\\
So for the next step, let $3[I]$ denote an operation on interval $I$, whereby you keep the center fixed, but triple the length of the interval. Doing so to each of the $I_{\lambda_i}$ that we chose above, we get $\left\{3[I_{\lambda_i}]\right\}^N_1$. For any arbitrary $I_{\alpha_j}$ that intersected $I_{\lambda_i}$,  tripling the length will ensure that $3[I_{\lambda_i}]$ will cover all points in $I_{\alpha_j}$, implying
\[
    K\subset \bigcup_{i=1}^M I_{\alpha_i} \subset 
    \bigcup_{i=1}^N 3[I_{\lambda_i}]
\]
\[
    \Rightarrow\quad
    m(K)\leq 
    m\left(\bigcup_{i=1}^N 3[I_{\lambda_i}]\right) = 
    \sum_{i=1}^N \ell(3[I_{\lambda_i}]) = 
    3\sum_{i=1}^N \ell(I_{\lambda_i}) 
\]
\[
    \Rightarrow
    \sum_{i=1}^N\ell(I_{\lambda_i}) \geq
    \frac{1}{3} m(K) \geq 
    \frac{1}{3}\cdot\frac{3}{4}m(U) = 
    \frac{1}{4}m(U)
\]
\end{proof}
(Proof of Maximal Inequality) Now back to the main proof. Define the set 
\[
    E_\alpha := \{x\;|\; (Mf) >\alpha\} 
    = \left\{x\;\left\lvert\; \sup_{h>0} \int\limits_{[x-h,x+h]} |f|>\alpha\right.\right\}
\]
Now, if $x\in E_\alpha$, that means there exists an $h_x$ such that 
\begin{equation}
    \label{maxineq.ineq}
    \int_{I_x} |f| > \alpha \ell(I_x)
    \qquad \text{where $I_x:=(x-h_x,x+h_x)$}
\end{equation}
That means $\{I_x\}_{x\in E_\alpha}$ is an open cover of $E_\alpha$. 
\\
\\
Finally, by the Lemma we proved above, there exists set of points/indices $x_1,\ldots,x_N$ such that the $I_{x_i}$ are disjoint and
\[
    \sum_{i=1}^N \ell(I_{x_i}) \geq \frac{1}{4} m(E_\alpha)
\]
Next, we rearrange this last line and combine it with Inequality \ref{maxineq.ineq} to get
\[
    m(E_\alpha)\leq
    4\sum_{i=1}^N \ell(I_{x_i}) 
    \leq 
    4\sum_{i=1}^N \frac{1}{\alpha}\int_{I_{x_i}} |f|\; dm
    = \frac{4}{\alpha}\int_{\bigcup^N_{i=1} I_{x_i}} |f|\; dm
    \leq 
    \frac{4}{\alpha}\int_{E_\alpha} |f|\; dm
\]
And this was true for all $E_\alpha:=\{x\;:\;(Mf)>\alpha\}$.

\end{proof}


\begin{proof}[\textbf{Proof of Lemma \hyperlink{avefn.lem}{\ref*{avefn.lem.n}}}]
Start by invoking Theorem \ref{thm.contchoose} to find a continuous $g$ such that
\[
    \int_{[a,b]} |f-g|\; dm <\varepsilon 
\]
Since $g$ is continuous, $A_hg\rightarrow g$ uniformly as $h\rightarrow0$. So now let's write
\begin{align*}
    |(A_hf)-f|\leq|(A_h(f-g))|+|(A_hg)-g|+|g-f|
\end{align*}
We don't know that the limit of all elements in the inequality exist, so let's take the lim sup, only substituting in the limit if it exists
\begin{align*}
   \limsup_{h\rightarrow0^+}  
    |(A_hf)-f|&\leq
    \limsup_{h\rightarrow0^+}  
    \left[|(A_h(f-g))|+|(A_hg)-g|+|g-f|\right]\\
    &\leq 
    \limsup_{h\rightarrow0^+} 
    (A_h|f-g|)+0+|g-f|
\end{align*}
Then, note that we can bound $\limsup_{h\rightarrow0^+} |(A_hf)-g|\leq (M(f-g))$, where the upper bound is the Hardy-Littlewood Maximal function: 
\begin{align*}
   \limsup_{h\rightarrow0^+}
    |(A_hf)-f|&\leq
    (M(f-g))+|f-g|
\end{align*}
Next, we're going to translate this inequality into sets, which we'll then take the measure of. But first, note that this inequality tells us 
\begin{align*}
    \limsup_{h\rightarrow0^+}
    |(A_hf)-f|&>\alpha
    \quad\Rightarrow\quad
    (M(f-g))+|f-g|>\alpha
\end{align*}
Now we can translate this last statement directly into a statement about sets:
\begin{align*}
    \left\{x \;:\;\limsup_{h\rightarrow0^+} (A_h|f-g|) > \alpha\right\}
    &\subset 
    \left\{x \;:\;(M(f-g))+|f-g|>\alpha\right\}\\
    &\subset
    \left\{x \;:\;(M(f-g))+|f-g|>\alpha\right\}\\
\end{align*}

\end{proof}

\begin{proof}[\textbf{Proof of Lemma \hyperlink{strongae.lem}{\ref*{strongae.lem.n}}}]
Since $f\in\mathscr{L}([a,b])$ then clearly $|f-r|\in\mathscr{L}([a,b])$ as well. Then, by Lemma \hyperlink{avefn.lem}{\ref*{avefn.lem.n}}, 
\[
    \frac{1}{2h}\int\limits_{[x-h,x+h]} |f-r|\; dm \rightarrow |f(x)-r|
    \quad \text{almost everywhere}
\]
\end{proof}


\newpage
\section{Useful Tricks}

There are a number of useful ways to write different expressions, which are listed here:
\begin{itemize}
\item Often we know that addition, subtraction, and taking the absolute value of functions (which is a continuous operation) preserves certain properties. And so if we want to prove that the max or the min of two functions preserves certain properties, writing the max and min as follows often makes quick work of otherwise tedious proofs. These forms add or subtract the ``residual'' of one function relative to the other: 
\[
    \max \{f,g\} = \frac{f+g+|f-g|}{2}\qquad 
    \min\{f,g\} = \frac{f+g-|f-g|}{2}
\]

\item Building off the last point, we can write 
    \[ 
        |f|=\max\{f,-f\}
    \]
\end{itemize}

\newpage
\section{Quizzes}

\subsection{Quiz 1}

\subsubsection{Question 1}

Suppose that $f: [0,1]\rightarrow \mathbb{R}$ is bounded on $[0,1]$ and continuous on $(0,1)$. Without using Lebesgue's theorem, we want to show that $f\in\mathscr{R}([0,1])$.
\begin{proof}
Normally, we would use Theorem \ref{contthm}, but that requires continuity on $[0,1]$, and we only have it on $(0,1)$.
\\
\\
As a result, we use a standard analysis trick: bounding the parts that aren't well behaved, and using nice properties where the function \emph{is} well behaved.
\\
\\
And so to prove this, we want to show that there exists a $P$ such that $U(f,P)-L(f,P)<\varepsilon$ for any $\varepsilon>0$. Since $f$ is bounded, let $M = \sup_I |f(x)|$. Then, given $\varepsilon$, we set our partition so that $[x_0, x_1] = [0, \varepsilon/8M]$ and $[x_{n-1}, x_n] = [1-\varepsilon/8m, 1]$. This guarantees the contribution of this slice of the interval will be small.
\\
\\
Then, we know that the remaining portion from $[\varepsilon/8m, 1-\varepsilon/8m]$ will be continuous by assumption, so we can assert the nice property that there exists a partition for which the upper and lower sums over this interval will be $\varepsilon/2$ close.
\end{proof}

\subsubsection{Question 2}

Next, suppose that $f$ is improperly Riemann integrable on $[0,1]$. (Meaning that it blows up at some point, but it's okay). Define $F(x):=\int^x_0 f(t)\;dt$ and show that $F$ is uniformly continuous on $[0,1]$. Then, given an example where $F$ is not Lipschitz.

\begin{proof}
Proof by contradiction: Assume that $F$ is not uniformly continuos on $[0,1]$. Then there exists an $\varepsilon>0$ such that
\[
    |F(y) - F(x)| > \varepsilon \qquad
    \forall \; \delta >0 \quad \text{ s.t. } \quad 
    |x-y|< \delta
\]
But we can rewrite this as 
\[
    \left\lvert\int^y_x f(t)\;dt\right\rvert > \varepsilon \qquad
    \forall \; \delta >0 \quad \text{ s.t. } \quad 
    |x-y|< \delta
\]
Then the integral is bounded from below for all $\delta$, leading to problems.
\\
\\
As an example of a function $f$ where $F$ is not Lipschitz, take $f(x) = 1/\sqrt{x}$. It's improperly integrable, but blows up at zero, so that you can't say it's Lipschitz.
    
\end{proof}

\newpage
\section{Homework Checklist}

Because the TA is very\dots particular:
\begin{itemize}
    \item No contractions.
    \item Be careful about the word ``clearly.'' Use sparingly.
    \item Be explicit in notation. $M_i(\cdot)$ is not explicit.
    \item Check that equations are equations and not inequalities.
    \item Symmetry: $h(x) = \max\{f, 0\}$ isn't symmetric in arguments to the functions.
    \item Don't use the word ``formally.'' 
    \item Don't use $\forall$ or $\exists$ or s.t. or WLOG because reasons.
    \item Define variables and expressions \emph{always}. Say what $P$ is, even if it's clear that $P$ is a partition.
    \item Explain \emph{why} something is without loss of generality. Don't just say it.
    \item Don't use an em-dash.
\end{itemize}




%\cite{LabelInSourcesFile} 
%\citep{LabelInSourcesFile} Cites in parens
%\nocite{LabelInSourceFile} includes in refs w/o specific citation
%\bibliographystyle{apalike} 
%\bibliography{sources.bib} where sources.bib is file




\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
