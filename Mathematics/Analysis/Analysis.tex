\documentclass[12pt]{article}

\author{Matthew Cocci}
\title{\textbf{Analysis}}
\date{\today}

%% Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{fullpage}
\usepackage{setspace}
%\onehalfspacing
\usepackage{microtype}


%% Header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\pagestyle{fancy} 
%\lhead{}
%\rhead{}
%\chead{}
%\setlength{\headheight}{15.2pt} 
    %---Make the header bigger to avoid overlap

%\renewcommand{\headrulewidth}{0.3pt} 
    %---Width of the line

%\setlength{\headsep}{0.2in}    
    %---Distance from line to text
            

%% Mathematics Related %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm} %allows for labeling of theorems
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{note}{Note}


%% Font Choices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
%\usepackage{blindtext}


%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subfigure} 
    %---For plotting multiple figures at once
\graphicspath{ {Figures/} }
    %---Set a directory for where to look for figures


%% Hyperlinks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref} 
\hypersetup{	
    colorlinks,		
        %---This colors the links themselves, not boxes
    citecolor=black,	
        %---Everything here and below changes link colors
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%% Including Code %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{verbatim} 
    %---For including verbatim code from files, no colors

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\matlabcode}[1]{%
    \lstset{language=Matlab,%
        basicstyle=\footnotesize,%
        breaklines=true,%
        morekeywords={matlab2tikz},%
        keywordstyle=\color{blue},%
        morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},%
        identifierstyle=\color{black},%
        stringstyle=\color{mylilas},%
        commentstyle=\color{mygreen},%
        showstringspaces=false,%
            %---Without this there will be a symbol in 
            %---the places where there is a space
        numbers=left,%
        numberstyle={\tiny \color{black}},% 
            %---Size of the numbers
        numbersep=9pt,% 
            %---Defines how far the numbers are from the text
        emph=[1]{for,end,break,switch,case},emphstyle=[1]\color{red},%
            %---Some words to emphasise
    }%
    \lstinputlisting{#1}
}
    %---For including Matlab code from .m file with colors,
    %---line numbering, etc. 


%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{enumitem} 
    %---Has to do with enumeration	
\usepackage{appendix}
%\usepackage{natbib} 
    %---For bibliographies
\usepackage{pdfpages}
    %---For including whole pdf pages as a page in doc


%% User Defined %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%\newcommand{\nameofcmd}{Text to display}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\begin{document}

\maketitle

\tableofcontents %adds it here

\newpage
\section{The Riemann-Stieltjes Integral}

This definition of the integral was made rigorous in the 1800s by Riemann, Darboux, and Stieltjes.  It's an intuitive way to define the area under a curve, and it works well with numerical integration (approximations). \emph{However}, it is incomplete in the sense that there are functions of interest that we cannot integrate in a Riemann sense but can in a Lebesgue sense, which retains Riemann as a special case.

Throughout this section, unless otherwise noted, we'll largely stick to bounded functions that are univariate from a compact interval to $\mathbb{R}$: 

    \[ f: [a, b] \rightarrow \mathbb{R} \]

    We'll begin by discussing \emph{partitions} of that interval $[a,b]$ into smaller pieces, from which we'll construct sums that approximate the area under the curve.  This will lead us to a definition of the Riemann Integral.  Then, we'll generalize and allow the \emph{weight} we place on the sub-intervals (when summing over the entire interval) to vary, which will give us the Riemann-Stieltjes integral. From there, we discuss the relationships between the approximating sums and the integral.

\subsection{Partitions}

\begin{defn} A \emph{partition}, $P$, is an ordered tuple representing a finite sequence on the interval $[a,b]$,
    \[ a = x_0 < x_1 < \cdots < x_n = b
        \qquad \text{with} \quad \Delta x_i := x_i - x_{i-1}
    \]
\end{defn}

\begin{defn} The \emph{norm} of a partition $P$, sometimes called ``mesh $P$'' represents
    \[ || P || = \text{norm}(P) := \max_i |x_i - x_{i-1}| =
        \max_i |\Delta x_i| \] 
\end{defn}

\begin{defn} $Q$ is a \emph{refinement} of $P$ if $Q \supset P$ where $Q$ and $P$ are both partitions of $[a,b]$. $Q$ the intervals \emph{finer}.
\end{defn}

\begin{defn} For two partitions, $P_1$ and $P_2$, their \emph{common refinement} is $P_1 \cup P_2$.
\end{defn}

\begin{defn} A \emph{tagged partition} is a couplet $(P,T)$, where $P$ is some partition $\{x_0, \ldots, x_n\}$ and $T$ is a set of evaluation points, $\{t_1, \ldots, t_n\}$, for the function $f$  such that
    \[ x_{i-1} \leq t_i \leq x_i \]
\end{defn}

\begin{note} We will now generalize to allow weighting of the sub-intervals within the partition, defined for an \emph{increasing} function $\alpha: [a,b] \rightarrow \mathbb{R}$, where 
    \[ \Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}) > 0 \]
This is the main difference between the plain Riemann sum and integral, versus the Riemann-Stieltjes (RS) sum and integral.  The latter retains the former as a special case by taking $\alpha(x) = x$.  Therefore, the RS version is just a generalization of Riemann, weighting the contribution of the sub-intervals to the total sum/integral by the function $\alpha$, \emph{not} by the length of the sub-interval.
\end{note}

\subsection{Sum Definitions}

We now define the various sums approximating the Riemann and RS integrals.

\begin{defn} We define the upper and lower \emph{Darboux Sums}, respectively, as follows 
    \begin{align*}
        U(f,P) &:= \sum^n_{i=1} M_i(f)(x_i - x_{i-1}) 
            \quad\text{where} \quad 
            M_i(f) := \sup_{x \in [x_i, x_{i-1}]} f(x)\\
        L(f,P) &:= \sum^n_{i=1} m_i(f)(x_i - x_{i-1})
            \quad\text{where} \quad 
            m_i(f) := \inf_{x \in [x_i, x_{i-1}]} f(x) 
    \end{align*}
\end{defn}

\begin{defn} Given $f$ (bounded) and tagged partition $(P,T)$ we define the \emph{Riemann Sum} as 
    \begin{equation}
        S(f,P,T) := \sum^n_{i=1} f(t_i) (x_i - x_{i-1})
    \end{equation}
\end{defn}

\begin{defn} 
\label{RSD}
We define the upper and lower \emph{RS-Darboux Sums}, respectively, as follows 
    \begin{align*}
        U_\alpha(f,P) &:= \sum^n_{i=1} M_i(f) \Delta \alpha_i
            \quad\text{where} \quad 
            M_i(f) := \sup_{x \in [x_i, x_{i-1}]} f(x) \\
        L_\alpha(f,P) &:= \sum^n_{i=1} m_i(f)\Delta \alpha_i
            \quad\text{where} \quad 
            m_i(f) := \inf_{x \in [x_i, x_{i-1}]} f(x) 
    \end{align*}
\end{defn}

\begin{defn} 
\label{RSS}
Given $f$ (bounded) and tagged partition $(P,T)$ we define the \emph{Riemann-Stieltjes Sum} as 
    \begin{equation}
        S_\alpha(f,P,T) := \sum^n_{i=1} f(t_i) \Delta \alpha_i
    \end{equation}
\end{defn}

\subsection{Sum Relations}

\begin{rmk} Clearly, by Definitions \ref{RSD} and \ref{RSS}, for all $T$ associated with $P$
    \[ L_\alpha(f,P) \leq S_\alpha(f,P,T) \leq U_\alpha(f,P) \]
\end{rmk}

\begin{thm} 
\label{sumineq}
    If $Q \supset P$, i.e. if $Q$ refines $P$, then
    \[ L_\alpha(f,P) \leq L_\alpha(f,Q) \leq U_\alpha(f,Q) 
        \leq U_\alpha(f,P) \]
\end{thm}
\begin{proof} The proof proceeds by induction. Assume that $Q = P \cup \{x^*\}$, a single point. Then $x^*\in [x_{i-1}, x_i]$ for some interval, and it's easy show the relation from there.
\end{proof}

\begin{thm} 
\label{pineq}
    For all partitions $P_1, P_2$, 
    \[ L_\alpha(f,P_1) \leq U_\alpha(f,P_2) \]
\end{thm}
\begin{proof} Let $Q = P_1 \cup P_2$. Then by Theorem \ref{sumineq}, 
    \[ L_\alpha(f,P_1) \leq L_\alpha(f,Q) \leq U_\alpha(f,Q) 
        \leq U_\alpha(f,P_2) \]
\end{proof}


\subsection{Definition of $\mathscr{R}_\alpha([a,b])$}


\begin{defn} We define \emph{the upper and lower Riemann-Stieltjes integrals}, respectively, in terms of the RS-Darboux Sums
    \begin{align*} 
        \overline{\int^b_a} f d\alpha &:= \inf_P U_\alpha(f,P) \\
        \underline{\int^b_a} f d\alpha &:= \sup_P L_\alpha(f,P)
    \end{align*}
We can assert that the sup and inf exist because $f$ is bounded. From Theorem \ref{pineq}, it's clear that $\underline{\int} f d\alpha \leq \overline{\int} f d\alpha$.
\\
\\
\emph{Note}: This means the value of the upper (lower) integral might not be \emph{in} the set of upper (lower) sums, just like a real number will not be in a sequence of approaching rationals. 
\end{defn}

\begin{defn} We say  $f$ is \emph{Riemann-Stieltjes integrable} on $[a,b]$---i.e. $f \in \mathscr{R}_\alpha([a,b])$---if 
    \[ \overline{\int^b_a} f d\alpha  =
        \underline{\int^b_a} f d\alpha 
        := {\int^b_a} f d\alpha
        \]
\end{defn}

\begin{ex} A case where $f \notin \mathscr{R}_\alpha([a,b])$ is where 
    \[ f(x) = \chi_{\mathbb{Q}\cap [a,b]}
        =\begin{cases} 1 & $x$\text{ rational} \\
            0 & $x$\text{ irrational} \end{cases}\]
for $x\in[0,1]$. In this case, the upper integral is always 1, while the lower integral is always zero.
\end{ex}

\begin{thm}
\emph{(Riemann's Condition)}
\label{riemcond}
$f \in \mathscr{R}_\alpha([a,b])$ if and only if there exists a partition $P$ such that the upper and lower RS-Darboux sums can be made arbitrarily close given that $P$, i.e.
    \[  U_\alpha(f,P) - L_\alpha(f,P) \leq \varepsilon \]
\end{thm}
\begin{proof} First, the $\Leftarrow$ direction. Use Theorems \ref{sumineq} and \ref{pineq}. It's obvious. Next, for the $\Rightarrow$ direction. By the definition of the RS integral and the RS-Darboux sums, 
\begin{equation}
    \label{p1}
    U_\alpha(f,P_1) < \int^b_a f d\alpha + \varepsilon/2 \qquad
    L_\alpha(f,P_2) > \int^b_a f d\alpha - \varepsilon/2
\end{equation}
Taking the common refinement, and using Theorem \ref{sumineq}, we get that 
\begin{align*}
    U_\alpha(f,P_1 \cup P_2) - L_\alpha(f,P_1 \cup P_2) &\leq 
    U_\alpha(f,P_1) - L_\alpha(f,P_2) \\
    &= \left(\int^b_a f d\alpha + \varepsilon/2\right) - 
        \left(\int^b_a f d\alpha -\varepsilon/2  \right) \\
    \text{By Expression \ref{p1}}\qquad &\leq \varepsilon/2 + \varepsilon/2
\end{align*}
\end{proof}

\subsection{Properties of $\mathscr{R}_\alpha([a,b])$}

\begin{thm} 
    \label{contthm}    
The set of all continuous functions on $[a,b]$, denoted ${C}([a,b])$, is a subset of $\mathscr{R}([a,b])$.\footnote{Note that we need continuity over \emph{all} of $[a,b]$. If $f$ isn't continuous at the endpoints, we can't apply this proof directly. We instead need to split up $[a,b]$ into ``good'' and ``bad'' parts.}
\end{thm}
\begin{proof} 
By Theorem \ref{riemcond}, we want to show that, for all $\epsilon>0$, there exists a partition $P$ such that 
\begin{align*}
    U_\alpha(f,P) - L_\alpha(f,P) < \epsilon  \\
    \Leftrightarrow
    \sum^n_{i=1} (M_i(f) - m_i(f)) \Delta\alpha_i < \epsilon 
\end{align*}
Now since $f$ is continuous on a compact interval, $[a,b]$, $f$ is \emph{uniformly continuous} on $[a,b]$. That means, given our $\epsilon$ from above, 
    \[ \exists \; \delta >0 \quad \text{ s.t. } \quad
        |x_{i} - x_{i-1}| < \delta \quad \Rightarrow \quad
        |f(x_{i}) - f(x_{i-1})| < \frac{\epsilon}{\alpha(b)-\alpha(a)} \]
So we can choose $P$ such that that $||P|| < \delta$.  This means that 
\begin{align*}
    \sum^n_{i=1} [M_i(f) - m_i(f)] \Delta\alpha_i &\leq
    \sum^n_{i=1} \frac{\epsilon}{\alpha(b)-\alpha(a)} 
        \Delta\alpha_i  
    =\frac{\epsilon}{\alpha(b)-\alpha(a)}\sum^n_{i=1}  
        \Delta\alpha_i \\
    &\leq\frac{\epsilon}{\alpha(b)-\alpha(a)} \cdot [\alpha(b)-\alpha(a)] = \epsilon
\end{align*}
\end{proof}
Now for some useful properties of the set of Riemann-Stieltjes integrable functions. Consider $f,g \in \mathscr{R}_\alpha([a,b])$
and $c \in \mathbb{R}$.
\begin{itemize}
    \item \textbf{Linearity}: $f+g \in \mathscr{R}_\alpha([a,b])$
        and $cf \in \mathscr{R}_\alpha([a,b])$, with 
        \[ \int^b_a cf \; d\alpha = c \int^b_a f\; d\alpha  
            \qquad \text{and} \qquad 
            \int^b_a f+g\; d\alpha = \int^b_a f \;d\alpha + 
            \int^b_a g\; d\alpha 
        \]
    \item \textbf{Subsets}: If $[c,d]\subset[a,b]$, then $f \in \mathscr{R}_\alpha([c,d])$.
    \item \textbf{Splitting the Interval}: If $c \in [a,b]$, then 
        \[ \int^b_a f d\alpha = \int^c_a f d\alpha 
            + \int^b_c f d\alpha \]
    \item \textbf{Monotonicity}: If $f,g,h \in 
        \mathscr{R}_\alpha([a,b])$, $f\geq0$, and
        $g \leq h$ on $[a,b]$, then
            \[ \int^b_a f d\alpha \geq 0 \qquad
                \int^b_a g \; d\alpha \leq 
                \int^b_a h \; d\alpha  \]
    \item \textbf{Compositions}: Suppose that $f \in 
        \mathscr{R}_\alpha([a,b])$ and
        $g: \mathbb{R}\rightarrow\mathbb{R}$ is 
        continuous. Then $g \circ f\in\mathscr{R}_\alpha([a,b])$
        \begin{proof}
            Since $f$ is bounded, 
            let $m:=\inf_{[a,b]}f$ and $M:=\sup_{[a,b]}f$.
            Then since $g$ is continuous on the compact 
            interval $[m,M]$, $g$ is uniformly continuous.
            As a result, for all $\varepsilon>0$
            there then exists $\delta>0$ such that 
            \begin{equation}
                \label{cont}
                 |u- v| \leq 2K\delta \quad \Rightarrow
                    \quad |g(u) - g(v)| \leq 
                    \frac{\varepsilon}{2\left[\alpha(b)-\alpha(a)
                    \right]}
            \end{equation}
            where $K$ is some positive, finite number. But what
            is $K$ exactly? Well,
            $g$ also happens to be bounded because it is
            continuous on a compact interval, thus $|g(x)| \leq K$
            for some $K$ and all $x$. So we'll go with that.
            \\
            \\
            Next, since we know that $f \in 
            \mathscr{R}_\alpha([a,b])$, 
            there exists a partition $P$ such that
            \begin{equation}
                \label{forf}
                 U_\alpha(f,P)- L_\alpha(f,P) \leq \epsilon\cdot
                    \delta
            \end{equation}
            %And, considering the building blocks $M_i(f)$ and 
            %$m_i(f)$, by the uniform continuity of $g$, we
            %also have for any $i$, for all $\varepsilon$, there must
            %be a $\delta$ such that
            %\begin{equation}
            %    \label{compcont}
            %    M_i(f) - m_i(f) < 2K{\delta}
            %        \quad \Rightarrow \quad 
            %        M_i(g\circ f) - m_i(g\circ f) \leq 
            %        \frac{\varepsilon}{2[\alpha(b)-\alpha(a)]}
            %\end{equation}
            Now that everything's spelled out, 
            we're interested in showing Riemann 
            integrability for $g\circ f$, which requires us
            to show that the upper and lower RS-Darboux sums
            are arbitrarily close. We will do so by breaking
            the sums into two parts based on how
            the original function $f$ behaves on an interval:
            \begin{align*}
                A = \{ i \; | \; M_i(f) - m_i(f)\leq2K\delta\}\\
                B = \{ i \; | \; M_i(f) - m_i(f)>2K\delta\}
            \end{align*}
            Now let's work out the sums for $g\circ f$:
            \begin{align}
                U_\alpha(g\circ f,P) - L_\alpha(g\circ f,P)
                    &= \sum^n_{i=1} \left[
                    M_i(g\circ f) - m_i(g\circ f)\right]
                    \Delta\alpha_i \notag\\
                &= \sum_{i\in A} \quad + \quad
                    \sum_{i\in B} \notag\\
                \qquad
                &\leq \frac{\varepsilon}{2[\alpha(b)-\alpha(a)} 
                    \sum_{i\in A} \Delta\alpha_i+
                    K \sum_{i\in B} \Delta\alpha_i
                    \label{final}
            \end{align}
            where the result from the sum over $A$ comes from 
            Equation \ref{cont} and the result for the sum over 
            $B$ comes from $g$ being bounded.
            \\
            \\
            Now let's consider the sum over $B$ from the last
            line, and relate it back to Expression \ref{forf}.
            We know that by our definition of $B$:
            \begin{align*}
                {2K}{\delta} \sum_{i\in B} 
                \Delta\alpha_i &\leq    
                \sum_{i\in B} \left[M_i(f) - m_i(f) \right]
                \Delta\alpha_i \leq \varepsilon\cdot\delta \\
                &\Rightarrow \qquad \sum_{i\in B} \Delta\alpha_i
                \leq \frac{\varepsilon}{2K}
            \end{align*}
            Substituting this result back into Equation    
            \ref{final}, we see that we must have
            \begin{align*}
                U_\alpha(g\circ f,P) - L_\alpha(g\circ f,P)
                &\leq \frac{\varepsilon}{2[\alpha(b)-\alpha(a)]}    
                    [\alpha(b)-\alpha(a)]
                    + K\frac{\varepsilon}{2K} \\
                    &\leq \frac{\varepsilon}{2} + 
                        \frac{\varepsilon}{2} = \varepsilon
            \end{align*}
            Thus, we have that $g\circ 
            f\in\mathscr{R}_\alpha([a,b])$.
        \end{proof}
    \item \textbf{Multiplication}: If we have 
        $f,g\in\mathscr{R}_\alpha([a,b])$, then 
        $fg\in\mathscr{R}_\alpha([a,b])$.
        \begin{proof}
            We know that $f - g$ and $f+g \in
            \mathscr{R}_\alpha([a,b])$. We also know that 
            $h(x) = x^2$ is continuous, so then we can say that
            \[ fg = \frac{1}{4} \left[(f+g)^2 - (f-g)^2 \right]
                \in\mathscr{R}_\alpha([a,b])
                \]
        \end{proof}
    \item \textbf{Absolute Value Relations}: If we have
        $f \in \mathscr{R}_\alpha([a,b])$, then both
        \[ |f| \in \mathscr{R}_\alpha([a,b]) \qquad
            \left\lvert\int^b_a f d\alpha \right\rvert
            \leq \int^b_a |f| \; d\alpha \]
        \begin{proof}
            The first expression is clearly true because $g(x)=|x|$
            is a continuous function, so $g\circ f = |f|$ is
            RS integrable.
            \\
            \\
            Next, let $c$ be a single constant (either 1 or 
            $-1$) such that 
                \[ c\int^b_a f\; d\alpha = \left\lvert 
                \int^b_a f\;d\alpha \right\rvert \]
            Then we have that $cf \leq |f|$ and by the monotonicity
            property, we have that
            \begin{align*}
                \int^b_a |f| \; d\alpha &\geq 
                    \int^b_a cf \; d\alpha \\
                \Rightarrow \qquad 
                \int^b_a |f| \; d\alpha &\geq
                    \int^b_a cf \; d\alpha =
                    c \int^b_a f \; d\alpha = 
                    \left\lvert
                    \int^b_a f \; d\alpha \right\rvert 
            \end{align*}
        \end{proof}
        ($*$) This is an often-used result.
        
\end{itemize}

\newpage
\subsection{Mean Value Theorem}

The classical Mean-Value Theorem states that if $f$ is continuous on $[a,b]$ with $\alpha$ increasing, there's some $c\in[a,b]$ such that 
    \[ \int^b_a f\;d\alpha = f(c)[\alpha(b)-\alpha(a)] \]
We'll prove a more general result that retains this as a special case.
\begin{thm}
\label{mvt}
Suppose that $f,g\in\mathscr{R}_\alpha([a,b])$ and assume $g\geq0$. Then there exists a $\mu\in[m,M]$, i.e. in between the upper and lower bounds of $f$, such that 
        \[ \int^b_a fg\;d\alpha = \mu \int^b_a g\;d\alpha \]
Moreover, if $f$ is continuous, then there exists $c\in[a,b]$ such that $\mu = f(c)$.
\end{thm}
\begin{proof}
Since $g\geq0$, we can conclude that 
\begin{align*}
    m &\leq f \leq M \\
    mg &\leq fg \leq Mg \\
    \Rightarrow \qquad m\int^b_a g\;d\alpha &\leq 
        \int^b_a fg\;d\alpha \leq  M \int^b_a g\;d\alpha
\end{align*}
If we consider the case where $\int^b_a g\;d\alpha= 0$, then any old $\mu$ will work since we'll have that $\int^b_a fg\;d\alpha=0$. And if it's not 0, then we can set 
\[ \mu = \frac{\int^b_a fg\;d\alpha}{\int^b_a g\;d\alpha} \]
We know the numerator and denominator exist because both $f$ and $g$ were assumed integrable, so their product is as well.
\end{proof}
\begin{rmk}
If we take $g(x)=1$, then we get the plain vanilla MVT we opened up the subsection with.  And in the case where $f$ is continuous, we know from the intermediate value theorem that there exists a $c$ such that $\mu=f(c)$. 
\end{rmk}


\newpage
\subsection{Convergence of Riemann-Stieltjes Sums, $S_\alpha(f,P,T)$}

Basic calculus makes us all think that we approximate a Riemann-Sieltjes integral by taking finer and finer partitions, which we use to weight the value of the function at an increasing number of evaluation points along the interval $[a,b]$.  So it would be very nice and very convenient if Riemann-Stieltjes sums converged to the value of the integral whenever we construct just that sort of limiting approximation. 

\emph{However}, this will not always be the case.  That is, there will be some functions that are RS-integrable, which won't \emph{necessarily} converge if you take finer and finer partitions with more and more evaluations points. So the theorems in this section will illustrate when that will and won't apply. In certain cases, arbitrarily fine partitions might converge to different values, none of which must equal the value of the integral (if it even exists).

\begin{thm}
\label{weaker}
Let $f$ be a bounded function on $[a,b]$. Then $f\in\mathscr{R}_\alpha([a,b])$ if and only if there exists a number $I$ such that for every $\varepsilon>0$, there exists a corresponding partition $P$ where, for all $P^* \supset P$, we have 
    \[ |S_\alpha(f,P^*,T) - I | \leq \varepsilon \qquad \forall T\]
And if $f\in\mathscr{R}_\alpha([a,b])$, then $I = \int^b_a f\;d\alpha$.
\end{thm}
\begin{rmk}
We could have written a weaker theorem, leaving out the part about $P^*\supset P$, which is a special case of the above theorem.  But going forward, we'll want to consider the idea of taking finer and finer partitions. This will be particularly true when $\alpha$ might not be increasing.
\end{rmk}
\begin{proof}
First, we prove the $\Rightarrow$ direction. Since $f\in\mathscr{R}_\alpha([a,b])$, we know that there's a partition $P$ such that
    \[ U_\alpha(f,P) - L_\alpha(f,P)<\varepsilon \qquad 
    \forall \varepsilon \]
Considering $P^*\supset P$, we also have
\begin{align*}
    L_\alpha(f,P^*)&\leq S_\alpha(f,P^*,T) \leq U_\alpha(f,P^*)\\
    L_\alpha(f,P^*)&\leq \int^b_a f\;d\alpha \leq U_\alpha(f,P^*)
\end{align*}
Thus, we can conclude that 
\begin{align*}
    \left\lvert S_\alpha(f,P^*,T) - \int^b_a f\;d\alpha 
    \right\rvert \leq \varepsilon \qquad \forall T
\end{align*}
It's clear that $I=\int^b_a f\;d\alpha$,
\\
\\
Next, we want to show the $\Leftarrow$ direction. We begin by noting that given any $P$, there exists a $T_1$ and $T_2$ such that
\begin{align}
    S_\alpha(f,P,T_1) &\geq U_\alpha(f,P) - \varepsilon\notag\\
    S_\alpha(f,P,T_2) &\leq L_\alpha(f,P) + \varepsilon
    \label{proof1.17}
\end{align}
This works because given any partition, we can always choose a $T_1$ and $T_2$ to satisfy by taking $T_1$ ($T_2$) as the set of all $M_i$ ($m_i$). 
\\
\\
So now, let's take $P^*$ to be the partition such that the RS sum is $\varepsilon$ close to $I$ for all $T$. (Note that this $P^*$ is taken to exist in this direction of the proof.) Then we have from the inequalities in \ref{proof1.17} that
\begin{align*}
    U_\alpha(f,P) - L_\alpha(f,P) &\leq 
        \left[ S_\alpha(f,P,T_1) + \varepsilon\right] -
        \left[ S_\alpha(f,P,T_2) - \varepsilon\right] \\
    &\leq \left[ S_\alpha(f,P,T_1) - I + \varepsilon\right] -
        \left[ S_\alpha(f,P,T_2) - I - \varepsilon\right] \\
    &\leq \left\lvert S_\alpha(f,P,T_1) - I \right\rvert+       
        \left\lvert S_\alpha(f,P,T_2) - I \right\rvert
        + \varepsilon + \varepsilon \\
    \text{From Inequalities \ref{proof1.17}} \quad
        &\leq 4\varepsilon
\end{align*}
Since the Upper and Lower sums can be made arbitrarily close, we then know that $f\in\mathscr{R}_\alpha([a,b])$.
\end{proof}
\begin{rmk}
Before moving on, let's recap what the last theorem said in words---or at least what it didn't say.  It didn't say that if you keep taking smaller partitions, you will \emph{always} force your RS-Sum to converge to the value of an integral.  What it did say was twofold:
\begin{enumerate}
    \item First, if your function \emph{is} Riemann-Integrable, then and you can find \emph{some} partition (though it might not be true for all of them) where the RS-Sums given that partition and \emph{all finer} partitions get arbitrarily close to this integral (which exists).
    \item Next, suppose you know an $I$ that the RS-sums get arbitrarily close to.  More specifically, that means if you're handed any $\varepsilon>0$, you can find a partition that gets the RS-Sums arbitrarily close to that $I$, no matter the $T$ tagging the partition $P$. Now we're not saying that the sums \emph{must} converge to some $I$ when you take finer partitions. Instead, we're just saying it's possible, and if $I$ \emph{does} indeed exist, then $f$ is RS integrable.
\end{enumerate}
Now we can move on to consider that thorny case of ever-shrinking partitions.
\end{rmk}
\begin{defn}
\label{strongest}
For a bounded function $f$ on $[a,b]$, we say 
    \[ \lim_{||P||\rightarrow 0} S_\alpha(f,P,T) = I \]
if for every $\varepsilon>0$, there exists a $\delta>0$ such that $||P||\leq\delta$ implies that
    \[ |S_\alpha(f,P,T) - I | \leq \varepsilon \qquad \forall T \]
    Now here's the big difference between this and what we've seen earlier: the sums might converge, but I never said anything about the equivalence of this statement to RS-integrability, like we had in Theorem \ref{weaker}.  In fact, we can't.
\\
\\
There might be functions where this condition does \emph{not} hold (i.e. the limit does not exist), but still the function is integrable. (See the next example.) But we do have at least one direction intact, as shown in the next theorem.
\end{defn}
\begin{thm}
\label{oneway}
Suppose that $f$ is bounded on $[a,b]$. If $\lim_{||P||\rightarrow 0}$ exists, then $f\in\mathscr{R}_\alpha([a,b])$ and
    \[ \int^b_a f\;d\alpha = \lim_{||P||\rightarrow 0} 
    S_\alpha(f,P,T) \]
\end{thm}

\begin{ex}
Now we give an example of why the other direction in Theorem \ref{oneway} wouldn't work.  Consider
\[ f(x) = \begin{cases} 0 & x \in [0, 1/2) \\ 1 & x \in [1/2, 1] 
        \end{cases}  \qquad
    \alpha(x) = \begin{cases} 0 & x \in [0,1/2] \\ 1& x\in(1/2, 1] 
        \end{cases} \]
Depending on whether the point $1/2$ is included in the partition, we can construct ever-finer partitions that make the RS-Sums converge to 0 sometimes, and to 1 at other times.
\end{ex}
\begin{rmk}
    Thus we have that the strongest statement is in Definition \ref{strongest} and Theorem \ref{oneway}. If we can show the limit exists as $||P||\rightarrow 0$, then we can show that $f$ is integrable. If we can't use that, we should think a little harder and try to find a $P$ as in Theorem \ref{weaker} before concluding that $f$ isn't RS-integrable. However, if we require more from our function $f$, we can can be more conclusive, as in the next theorem.
\end{rmk}

\begin{thm}
Let $f\in\mathscr{R}_\alpha([a,b])$. If either $f$ or $\alpha$ is continuous on $[a,b]$, then 
\begin{equation}
    \label{limsum.toprove}
    \int^b_a f\;d\alpha = \lim_{||P||\rightarrow 0} 
    S_\alpha(f,P,T) 
\end{equation}
\end{thm}
\begin{rmk}
This previous theorem is important, because if we just consider \emph{Riemann} integrability, then $\alpha(x) = x$, which is continuous. Thus, Riemann-integrability and the existence of the limit of Riemann sums (as the partition intervals get smaller) are equivalent notions.
\end{rmk}
So our goal will be to show that, for all $\epsilon>0$,
\begin{equation}
    \label{contequiv.toshow}
    \lim_{||P||\rightarrow 0}
    \left\lvert \int^b_a f \; d\alpha - S_\alpha(f,P,T) \right\rvert
        \leq \varepsilon
\end{equation}
when $f$ or $\alpha$ is continuous.

\begin{proof}($f$ Continuous)
First, suppose that $f$ is continuous. We'll use the Mean Value Theorem (taking $g=1$) to rewrite the integral in Equation \ref{contequiv.toshow} in terms of the partition $P$ that's used for the sum. This allows us to assert that for some $\mu \in [m,M]$, we have that
    \[ \int^b_a f \; d\alpha = \mu \int^b_a d\alpha \]
But this will also hold if we break up our interval into subintervals defined by the points in any partition, $P$:
\begin{equation}
    \label{contequiv}
    \sum^n_{i=1} \int^{x_i}_{x_{i-1}} f \; d\alpha 
        = \sum^n_{i=1} \mu_i \int^{x_i}_{x_{i-1}} d\alpha 
        \qquad\qquad \mu_i \in [m_i, M_i] 
\end{equation}
Next, because $f$ is continuous, we can use the Intermediate Value Theorem to note that, for all $i$, there must be some $c_i \in [x_{i-1}, x_i]$ where $f(c_i) = \mu_i$. Substituting into Equation \ref{contequiv}, we get
\begin{align*}
    \sum^n_{i=1} \int^{x_i}_{x_{i-1}} f \; d\alpha 
        &= \sum^n_{i=1} f(c_i) \int^{x_i}_{x_{i-1}} d\alpha 
        \qquad\qquad c_i \in [x_{i-1}, x_{i}] \\
        &= \sum^n_{i=1} f(c_i) \Delta\alpha_i
\end{align*}
Then, summing over $i$, we can define 
\begin{align*}
    \int^{b}_{a} f \; d\alpha 
        &= \sum^n_{i=1} f(c_i) \Delta\alpha_i
        \equiv S_\alpha(f,P,C)
\end{align*}
So now, we can replace the integral with this sum in the statement that we want to prove, as given in Inequality \ref{contequiv.toshow}:
\begin{equation}
    \label{contequiv.toshow2}
    \lim_{||P||\rightarrow 0}
    \left\lvert  S_\alpha(f,P,C)- S_\alpha(f,P,T) \right\rvert
        \leq \varepsilon
\end{equation}
This is now our equivalent target statement to prove.
\\
\\
Next, because $f$ is continuous on a compact interval, $f$ is uniformly continuous on $[a,b]$.  Thus for any $\varepsilon$, there exists a $\delta$ such that $||P||\leq\delta$ implies that $|f(x) - f(y)| \leq \varepsilon$, where $x$ and $y$ are both in the same interval.  
\\
\\
So given $\epsilon>0$, let $\delta>0$ be such that 
    \[ |x-y|\leq \delta \Rightarrow \quad |f(x)-f(y)| \leq 
        \frac{\varepsilon}{\alpha(b)-\alpha(a)} \]
Now if we take our partition such that $||P||\leq\delta$, we can write our sums as
\begin{align*}
    \left\lvert  S_\alpha(f,P,C)- S_\alpha(f,P,T) \right\rvert
    &= \left\lvert  \sum^n_{i=1} \left[f(c_i) - f(t_i) \right]
    \Delta\alpha_i\right\rvert  \\
    &\leq \sum^n_{i=1} \left\lvert  f(c_i) - f(t_i) \right\rvert
    \Delta\alpha_i  \\
    &\leq \frac{\varepsilon}{\alpha(b)-\alpha(a)}
        \sum^n_{i=1} \Delta\alpha_i =  \varepsilon 
\end{align*}
\end{proof}
\begin{proof}($\alpha$ Continuous)
Suppose $\varepsilon$ is given. Since $f\in\mathscr{R}_\alpha([a,b])$, we know that there exists a $P^*$ (with $\#P^*$, or $n+1$, subintervals) such that 
\begin{equation}
    \label{bases}
    U_\alpha(f,P^*) < \int^b_a f\;d\alpha + \varepsilon
        \qquad L_\alpha(f,P^*) > \int^b_a f\;d\alpha - \varepsilon
\end{equation}
To construct the proof from here, we'll want eventually to compare any arbitrary RS-Sum $S_\alpha(f,P,T)$ to these specific upper and lower sums (using $P^*$) that we know something about---i.e. how close they are to the target integral. Then we'll take the limit for $P$ once we've established that relationship.
\\
\\
Also, assume that $f(x)>0$ over $[a,b]$. We'll resolve this simplifying assumption later on, and it will help in the meantime.
\\
\\
So since that interval is compact and $\alpha$ is continuous, we know that $\alpha$ is \emph{uniformly} continuous, implying that
    \[ \exists \delta > 0 \quad \text{s.t.} \qquad 
    w_\alpha(\delta) < \frac{\varepsilon}{\#P^*} \]
where $\#P^*$ is the number of sub-intervals in $P^*$. And so 
    \[ \forall P \qquad ||P||<\delta \quad \Rightarrow 
        \quad \alpha(x_i) - \alpha(x_{i-1}) < w_\alpha(\delta)
        <  \frac{\varepsilon}{\#P^*} \]
Now, considering any such set $P$, let's also define the following sets, which group the indices for the subintervals of any $P$ into two categories, based on how they interact with $P^*$:
\begin{align*}
    I_1 &:= \{i \; | \; [x_{i-1},x_i) \cap P^* = \emptyset \}\\
    I_2 &:= \{i \; | \; [x_{i-1},x_i) \cap P^* \neq \emptyset \}
\end{align*}
And so $I_2$ contains all the indices where an subinterval of $P$ contain a point of $P^*$. $I_1$ has all the indices where an subinterval of $P$ is contained entirely within a subinterval of $P^*$.
\\
\\
Having done all of groundwork from above, we can now write an expression for the upper sum relative to $P$:
\begin{align}
    U_\alpha(f,P) &= \sum^n_{i=1} M_i(f) \Delta\alpha_i \notag \\
        &= \sum_{I\in I_1} M_i(f) \Delta\alpha_i 
         + \sum_{I\in I_2} M_i(f) \Delta\alpha_i
         \label{twosums}
\end{align}
But clearly, $I_2$ can only contain, at most, $\#P$ points since there are only $\#P$ intervals of $P$, so we must have that $\#I_2 \leq \#P$. Thus, the sum over $I_2$ simplifies:
\begin{align}
        \sum_{I\in I_2} M_i(f) \Delta\alpha_i 
        &\leq \sum_{I\in I_2} M \frac{\varepsilon}{\#P} =    
            M\varepsilon
        \label{tocombine1}
\end{align}
where $M = \sup f(x)$ over the interval $[a,b]$.
\\
\\
To deal with the sum over $I_1$, we get the result that 
\begin{equation}
    \label{tocombine2}
    \sum_{I\in I_1} M_i(f) \Delta\alpha_i  \leq U_\alpha(f,P^*)
\end{equation}
This is true because in assuming $f(x)>0$ and restricting our sum to $I_1$, we throw out whole portions of the interval $[a,b]$ from the sum entirely. And on top of that, we threw out the very large intervals that span one or more of the points in the partition $P^*$.  So all that remains in our sum over $I_1$ are those tiny subintervals of $P$ that would fit entirely within a subinterval of $P^*$. And so we won't be able to make up for the parts we throw out by weighting parts of the interval $[a,b]$ with $M_i(f)$ from $P$ \emph{more than} we would in the case of $M_i(f)$ from $P^*$.
\\
\\
Combining the representation for $U_\alpha(f,P)$ with the two sum representation of Equation \ref{twosums} and the results in Inequalities \ref{tocombine1} and \ref{tocombine2}, we get that
\begin{equation}
    U_\alpha(f,P) \leq U_\alpha(f,P^*) + M\varepsilon
\end{equation}
And because we had an expression for $U_\alpha(f,P^*)$ in Equation \ref{bases}, we can substitute in and get that
\begin{equation}
    \label{ass1}
    U_\alpha(f,P) \leq \int^b_a f\;d\alpha  + (M+1)\varepsilon
\end{equation}
And by a similar argument, we also have that 
\begin{equation}
    \label{ass2}
    L_\alpha(f,P) \geq \int^b_a f\;d\alpha  - (M+1)\varepsilon
\end{equation}
Now, at this point we have to dispense with the assumption that $f(x)>0$.  We do that by choosing a $k$ such that $f(x) + k >0$, where now $f$ can be any arbitrary function such that $\int^b_a f\;d\alpha$ as we assume in the theorem.  Then we're in the world we described above in choose our $P$. 
\\
\\
And, since we're in that world, we get the inequality in the next line, which allows us to conclude a very useful result
\begin{align*}
    U(f,P) + k(\alpha(b)-\alpha(a))  
        = U(f+k, P) &\leq \int^b_a (f+k)\; d\alpha 
        + (M+1)\varepsilon \\
    &\leq \int^b_a f\; d\alpha  + k(\alpha(b)-\alpha(a))
        + (M+1)\varepsilon \\
    \Rightarrow U(f,P) &\leq \int^b_a f\; d\alpha
        + (M+1)\varepsilon 
\end{align*}
So we have our asses covered if $f(x)$ isn't always positive, and we can stick with the results in Inequalities \ref{ass1} and \ref{ass2}.
\\
\\
And now we're almost done.  All that's left is to recognize that we can sandwich, for all $P$ such that $||P||\leq\delta$
\begin{align*}
    \int^b_a f\;d\alpha - (M+1)\varepsilon \leq
    L_\alpha(f,P) \leq S_\alpha(f,P,T) &\leq U_\alpha(f,P) \leq
   \int^b_a f\;d\alpha + (M+1)\varepsilon  \\
        \Rightarrow \qquad \left\lvert S_\alpha(f,P,T) 
        - \int^b_a f\;d\alpha\right\rvert &\leq 2(M+1)\varepsilon \end{align*}
\end{proof}

\newpage
\subsection{Integration for Arbitrary Integrators, $\alpha$}

By ``arbitrary'', we mean ``$\alpha$ not necessarily increasing'', which contrasts our operating assumption up until this point. 

In order to make such a notion operational, we will need to dispense with our ``sandwiching'' approach, which uses $L_\alpha(f,P)\leq S_\alpha(f,P,T)\leq U_\alpha(f,P)$, as it no longer holds. In fact, we don't even get that if $P^*\supset P$, then $U_\alpha(f,P^*) \leq U_\alpha(f,P)$ (and the analogous case for the lower sum). With that, we give a new definition of integrability framed entirely in terms of RS-Sums:

\begin{defn}
Given any $f$ and $\alpha$ (not necessarily increasing) on $[a,b]$, we say that $f\in\mathscr{R}_\alpha([a,b])$ if and only if there exists an $I\in\mathbb{R}$ such that for all $\varepsilon>0$, there exists a $P$ where $P^*\supset P$ implies
    \[ \left\lvert S_\alpha(f,P^*,T) - I \right\rvert \leq
        \varepsilon \qquad \forall \; T \]
This $I$ is denoted by $\int^b_a f\;d\alpha$. And despite this new definition, we still retain of the usual nice linearity properties discussed above in the increasing $\alpha$ case.
\end{defn}
The next subsection goes onto a very important topic when we consider arbitrary integrators---so important that I gave its own subsection, although it actually should fall under this current one.

\subsection{Integration by Parts}

\begin{thm}
\label{thm.intbyparts}
\emph{(Integration by Parts)} 
Let $f$ and $\alpha$ (not necessarily increasing) be bounded on $[a,b]$. If $f\in\mathscr{R}_\alpha([a,b])$, then $\alpha\in\mathscr{R}_f([a,b])$ and 
\begin{equation}
    \label{intbyparts}
    \int^b_a f\;d\alpha = f(b) \alpha(b) - f(a)\alpha(a) - 
    \int^b_a \alpha \; df 
\end{equation}
This is a slightly more general case that the classic calculus version, which is a special case where $\alpha(x)=x$.
\end{thm}
\begin{proof}
To start, we take $P^*$ as the partition such that 
\begin{equation}
    \label{ibpassump}
    \left\lvert S_\alpha(f,P^*,T) - \int^b_a f\;d\alpha 
    \right\rvert \leq \varepsilon 
\end{equation}
which we know exists, since the integrability of $f$ is assumed. 
\\
\\
Now we want to show that for all $\varepsilon>0$, there exists a $P$ such that $P^*\supset P$ implies that 
    \[ \left\lvert S_f(\alpha,P^*,T) -  f(b) \alpha(b) - 
        f(a)\alpha(a) - \int^b_a f \; d\alpha \right\rvert \leq 
        \varepsilon \qquad \forall \; T\]
        So we rewrite the righthand side of Equation \ref{intbyparts}, recasting them as telescoping sums, also substituting in $S_f(\alpha,P^*,T)$ for the integral:\footnote{You might object and say ``Hey, you can't do that.  You don't know that there's an equivalent RS-Sum for the integral $\int^b_a \alpha\;df$---particularly if you use that same $P^*$ we've been using all along.'' But I actually don't care whether this sum I'm substituting in is actually equivalent (yet). I'm just taking the righthand side of Equation \ref{intbyparts} as inspiration, and using the $P^*$ we've been using to write an RS-Sum. We'll see at the very end that it's a \emph{result} that this RS-Sum approximates the integral, not an assumption.}
\begin{align}
    f(b) \alpha(b) - f(a)\alpha(a) - S_f(\alpha,P^*,T)
        &= \sum^n_{i=1} f(x_i) \alpha(x_i) - f(x_{i-1})
        \alpha(x_{i-1}) \notag\\
    &\quad - \sum^n_{i=1} \alpha(t_i) \left[f(x_i) - f(x_{i-1})
        \right] \notag\\
    &= \sum^n_{i=1} f(x_i) \left[\alpha(x_i) - \alpha(t_i) \right]
        \notag\\
    &\qquad +\sum^n_{i=1} f(x_{i-1}) \left[\alpha(t_i) 
    - \alpha(x_{i-1})\right]\label{righthandtwosums}
\end{align}
Now the righthand side kind of looks like a RS-Sum, so let's define a partition and set of tag points so it is. Define define a new partition, call it $P^{**}$, which will be the union of the old $P^*$ and the points $t_i$. So $P^{**} = P^*\cup T$, with multiplicities. Next, define a new set of tagging points,
    \[ T^{**} = \{ x_0, x_1, x_1, x_2, x_2,\ldots, x_n \} \]
Now we can collapse the righthand side in Equation \ref{righthandtwosums} into a proper RS-Sum:
\begin{align*}
    f(b) \alpha(b) - f(a)\alpha(a) - S_f(\alpha,P,T)
    &= S_\alpha(f, P^{**}, T^{**})
\end{align*}
Since $P^{**}\supset P^* \supset P$, it's clear that 
\begin{align*}
    \left\lvert S_\alpha(f,P^{**},T^{**}) - \int^b_a f\;d\alpha
    \right\rvert \leq &\varepsilon  \\
    \Leftrightarrow \quad 
        \left\lvert f(b) \alpha(b) - f(a)\alpha(a) 
        - S_f(\alpha,P^*,T) - 
        \int^b_a f \; d\alpha \right\rvert &\leq 
        \varepsilon \qquad \forall \; T
\end{align*}
which is exactly what we wanted to prove, since we can take $P^*$ as our partition. 
\end{proof}

\begin{cor}
The big result from integration by parts is that 
$f\in\mathscr{R}_\alpha([a,b])$ if and only if $\alpha\in\mathscr{R}_f([a,b])$
\end{cor}

\begin{cor}
If $f\in\mathscr{R}_{\alpha_1}([a,b]) \cap \mathscr{R}_{\alpha_2}([a,b])$, then $f\in\mathscr{R}_{\alpha_1 + \alpha_2}([a,b])$ and 
    \[ \int^b_a f\;d(\alpha_1 + \alpha_2) = 
    \int^b_a f\;d\alpha_1 + \int^b_a f\;d\alpha_2 \]
\end{cor}

\newpage
\subsection{Functions of Bounded Variation}

In the previous subsection, ``Integration by Parts,'' we assumed that $f$ was integrable with respect to an arbitrary integrator $\alpha$. This relaxed our earlier assumptions in the sense that $\alpha$ didn't have to be increasing anymore.  \emph{However}, we just went ahead and assumed $f\in\mathscr{R}_\alpha([a,b])$, which doesn't seem very general.

So now, we'll relax our assumptions even further and consider what characteristics and conditions for the functions $f$ and $\alpha$ truly permit integrability---we won't just assume.  This returns us to earlier subsections where we were concerned about \emph{whether} $f$ would be integrable given $\alpha$.  To answer that question, we'll have to define the concept of \emph{bounded variation}.

\begin{defn}
Let $f$ be a function on $[a,b]$. Define the \emph{total variation} as
\[ V_a^b(f) := \sup_P \sum^n_{i=1} \left\lvert f(x_i) - f(x_{i-1})
    \right\rvert \]
We say that $f \in BV([a,b])$, i.e. ``$f$ is of bounded variation'', if $V^b_a(f)<\infty$, i.e. the total variation is finite.
\end{defn}

\begin{thm}
If $f$ is monotonically increasing on $[a,b]$, then $f\in BV([a,b])$.
\end{thm}

\begin{thm}
If $f$ is continuous and differentiable on $[a,b]$ and $f'$ is bounded, then $f\in BV([a,b])$.
\end{thm}
\begin{proof}
Given any partition $P$, from the Mean Value Theorem, we have that for all subintervals, 
    \[ \exists\; c_i \in [x_{i-1}, x_i] \quad \text{s.t.}
        \quad f(x_i) - f(x_{i-1}) = c_i \left[
        x_i - x_{i-1}\right]    \]
Since $f'$ is bounded, we know that $M=\sup_{x\in[a,b]} |f'(x)|$ is finite. Thus we can write, for any partition $P$,
\begin{align*}
    \sum^n_{i=1} \left\lvert f(x_i) - f(x_{i-1})
        \right\rvert &= \sum^n_{i=1} \left\lvert f(c_i) 
        \left[x_i - x_{i-1}\right] \right\rvert \\
    &\leq \sum^n_{i=1} M \left\lvert \Delta x_i 
        \right\rvert = M(b-a) < \infty 
\end{align*}
Thus, since the righthand side is independent of the partition $P$, when we take the sup over $P$'s, we still have that $V_a^b(f)$ is finite.
\end{proof}

\begin{ex}
You might ask why we threw in the condition that $f'$ is bounded in the previous theorem.  It turns out continuous and differentiable doesn't necessarily mean that the derivative is bounded, so that was a necessarily addition.  Here's an example:
    \[ f(x) =
        \begin{cases} 
            x^2 \sin(1/x^2) & x \neq 0\\
            0               & x = 0
        \end{cases}  \]
It's continuous (take the limit of the top while $x\rightarrow 0$ to see this), and it's differentiable:
    \[ f'(x) = 2x \sin(1/x^2) - \frac{2}{x} \cos(1/x^2) \]
Clearly, that derivative isn't bounded as $x\rightarrow 0$.
\end{ex}

\begin{thm}
$f$ continuous does not imply that $f\in BV$; $f \in BV$ does not imply $f$ continuous.
\end{thm}
\begin{ex}
As you might also suspect, continuous functions aren't necessarily of bounded variation. An example given in the book defines the following function on $[0,1]$:
    \[ f(x) = 
        \begin{cases}
        \frac{1}{n} & \text{$x = \frac{1}{n}$, where $n$ is an even integer}\\
        0 & \text{$x = \frac{1}{n}$, where $n$ is an odd integer} \\
        0 & \text{$x = 0$ or $x = 1$}
        \end{cases}
    \]
and linear in between.  Another similar example take
    \[ f(x) = 
        \begin{cases}
            x \sin(1/x) & x\neq 0 \\
            0 & x=0 
        \end{cases} \]
Both look something like the harmonic series that sums $1/n$, which we know diverges.
\end{ex}
\begin{ex}
As an example of a function of bounded variation that's not continuous, consider the function $f=\chi_{[1/2,1]}$ on $[0,1]$. Clearly there's a jump, but $V_a^b(f) = 1 < \infty$.
\end{ex}

\begin{thm}
$f\in BV([a,b])$ implies that $f$ is bounded on $[a,b]$, as 
    \[ \sup_{x\in[a,b]} |f(x) | \leq |f(a)| + V_a^b(f) \]
\end{thm}
\begin{proof}
Suppose that the statement above were not true---that for some $x^* \in [a,b]$, we have
\begin{align*}
    |f(x^*)| &> |f(a)| + V_a^b(f) \\
    \Rightarrow \qquad 
        |f(x^*)| - |f(a)|  &> V_a^b(f) 
\end{align*}
But we can keep using identities about the absolute value to show that the above statement implies
\begin{align*}
    V_a^b(f) < |f(x^*)| - |f(a)| \leq \left\lvert \;
        |f(x^*)| - |f(a)|\;\right\rvert
        \leq |f(x^*) - f(a)|
\end{align*}
And if this were the case, we clearly chose $V_a^b(f)$ incorrectly because we could define a partition by $\{a, x^*, b\}$ to get a bigger value for the total variation than $V_a^b(f)$.
\end{proof}

\begin{thm}
For $f, g\in BV([a,b])$, we have that
    \[ V_a^b(f+g) \leq V_a^b(f) + V_a^b(g)
    \qquad V_a^b(cf) = |c|V_a^b(f) \]
This implies that $f+g$ and $cf$ are both of bounded variation as well.
\end{thm}
\begin{rmk}
Given this definition and the triangle inequality, we see that Bounded Variation looks like a linear space. In fact, it's \emph{semi-norm}, meaning that it satisfies all but the 0 condition, since a non-zero (constant) function will have 0 total variation. 

But we can fix that, getting a full norm by adding the sup-norm ($||f||_\infty = \sup_{[a,b]} f$) to total variation. Thus if we define our norm as
    \[ ||f||_\infty + V_a^b(f) \]
we see that we've satisfied all of the conditions for a norm on the function space:
\begin{enumerate}
    \item $||f||_\infty + V_a^b(f) = 0$ if and only if $f=0$.
    \item $||cf||_\infty + V_a^b(cf) = c||f||_\infty + |c|V_a^b(f)
        = c\left[||f||_\infty + V_a^b(f)\right]$.
    \item $||f+g||_\infty + V_a^b(f+g) \leq 
        \left[||f||_\infty + V_a^b(f)\right] +
        \left[||g||_\infty + V_a^b(g)\right]$.
\end{enumerate}
\end{rmk}

\begin{thm}
If $f,g\in BV([a,b])$, then $f\cdot g \in BV([a,b])$.
\end{thm}

\begin{thm}
If $f\in BV([a,b])$ and $c \in (a,b)$, then 
    \[ V_a^b(f) = V_a^c(f) + V_c^b(f) \]
\end{thm}
\begin{cor}
As a result of the last theorem, we have that, as a function of $x$, $V_a^x(f)$ is monotonically increasing.
\end{cor}
\begin{proof}
This is because if $y>x$, then we have 
    \[ V_a^y(f) = V_a^x(f) + V_x^y(f) \]
So clearly, $V_a^y(f) >V_a^x(f)$.
\end{proof}

\begin{note}
This next result is an important theorem that establishes the link between functions of bounded variation and increasing functions, allowing us to apply some old results where we had to deal with strictly increasing integrators.
\end{note}

\begin{thm}
\label{monot}
$f\in BV([a,b])$ if and only if $f = u-v$, where $u$ and $v$ are monotonically increasing functions on $[a,b]$. 
\end{thm}
\begin{proof}
First, take the $\Leftarrow$ direction. This is pretty trivial since 
    \[ V_a^b(f) = V_a^b(u-v) \leq V_a^b(u) + V_a^b(v) \]
And since $u$ and $v$ are monotonically increasing, the terms $V_a^b(u)$ and $V_a^b(v)$ are finite.
\\
\\
Now for the $\Rightarrow$ direction. Set $u(x) = V_a^x(f)$, and let $v(x)=V_a^x(f) - f(x)$. Clearly, $f = u-v$; we just have to prove that $v$ is monotonically increasing, since the $u$ case is fairly obvious.
\\
\\
Supposing that $y>x$, we want to show that $v(y) > v(x)$. We can do so by starting with an obvious statement that 
    \[ f(y) - f(x) \leq V_x^y(f) \]
Then we rewrite and rearrange terms to show
\begin{align*}
    f(y) - f(x) &\leq V_x^y(f) = V_a^y(f) - V_a^x(f)  \\
    \Leftrightarrow\quad V_a^x(f) - f(x) &\leq V_a^y(f) - f(y)  \\
    \Leftrightarrow\quad v(x) &\leq v(y)  
\end{align*}
\end{proof}
Now that we have the whole framework for functions of bounded variation in place, we can return to integration where either $f$ or $\alpha$ is of bounded variation (or both). 

\subsection{Integration with Respect to Functions of Bounded Variation}

\begin{thm}
If $f$ is continuous on $[a,b]$ and $\alpha\in BV([a,b])$, then $f\in\mathscr{R}_\alpha([a,b])$. Also, since we can swap the integrand and integrator, if $f\in BV([a,b])$ and $\alpha$ continuous, then $f\in\mathscr{R}_\alpha([a,b])$
\end{thm}
\begin{proof}
Suppose that $f$ is continuous and $\alpha\in BV([a,b])$. Then by Theorem \ref{monot}, we have that $\alpha = u -v$, both of which are increasing. Thus $f\in\mathscr{R}_u([a,b]) \;\cap \;\mathscr{R}_v([a,b])$, which entails $u, v \in \mathscr{R}_f([a,b])$ by Theorem \ref{thm.intbyparts}.  This implies that $u-v\in\mathscr{R}_f([a,b])$, which implies $f\in\mathscr{R}_{u-v}([a,b]) = \mathscr{R}_\alpha([a,b])$.
\end{proof}

\begin{thm}
We get a number of familiar results from regular integration when $\alpha\in BV([a,b])$, $f,g\in\mathscr{R}_\alpha([a,b])$, $h$ continuous, and $c\in(a,b)$:
\begin{itemize}
    \item $\int^c_a f\;d\alpha + \int^b_c f\;d\alpha = \int^b_a f\;d\alpha$.
    \item $h\circ f\in\mathscr{R}_\alpha([a,b])$.
    \item $fg\in\mathscr{R}_\alpha([a,b])$.
    \item $|f|\in\mathscr{R}_\alpha([a,b])$
\end{itemize}
\end{thm}

\begin{thm}
\label{ftcbasis}
Suppose $\alpha$ is continuous and differentiable on $[a,b]$ with $\alpha', f\in\mathscr{R}([a,b])$. Then $f\in\mathscr{R}_\alpha([a,b])$ and
    \[ \int^b_a f\;d\alpha = \int^b_a f \alpha'\;dx\]
Since $\alpha$ is continuous and $\alpha'$ is bounded on $[a,b]$, $\alpha \in BV([a,b])$. 
\end{thm}
\begin{proof}
Now we know by the Mean Value Theorem that, for all subintervals,
    \[ 
        \exists \; \xi_i \in [x_{i-1}, x_i] \quad \text{s.t.} 
        \qquad \Delta\alpha_i = \alpha'(\xi_i) \Delta x_i 
    \]
Now consider the function $f\cdot\alpha'$. Since both $f,\alpha' \in\mathscr{R}([a,b])$ by assumption, we know that their product is Riemann integrable.  That means there's some partition $P$ such that $P^*\supset P$ implies that 
    \[ \left\lvert S(f\alpha',P,T) - \int^b_a f\alpha'\;dx 
    \right\rvert \leq \varepsilon/2 \]
Using partition $P$ and the MVT result above, we can write the sum for the other integral:
\begin{align*}
        S_\alpha(f,P,T) &= \sum^n_{i=1} f(t_i) \Delta\alpha_i 
        = \sum^n_{i=1} f(t_i)\alpha'(\xi_i) \Delta x_i
\end{align*}
Now, we want to show the following for any arbitrary $\varepsilon>0$:
\begin{align}
    \label{toexpand}
    \left\lvert  S_\alpha(f,P,T) - \int^b_a f\alpha'\;dx 
        \right\rvert &\leq \varepsilon
\end{align}
If we can prove this, then we know that the limit of the sum exists (so the corresponding integral exists) and that it is equal to the integral of $f\alpha'$. So let's expand out
But we can expand out Inequality \ref{toexpand}:
\begin{align*}
    \left\lvert S_\alpha(f,P,T) - \int^b_a f\alpha'\;dx 
        \right\rvert &\leq 
        \left\lvert S_\alpha(f,P,T) - S(f\alpha',P,T)
        \right\rvert \\
    &\qquad + \left\lvert S(f\alpha',P,T)
        - \int^b_a f\alpha'\;dx \right\rvert 
\end{align*}
Now the second term will be $\varepsilon/2$ small by our choice of $P$ above. As for the first term:
\begin{align*}
    \left\lvert S_\alpha(f,P,T) - S(f\alpha',P,T)
        \right\rvert &\leq 
        \left\lvert \sum^n_{i=1} f(t_i)\left[\alpha'(\xi_i) - 
        \alpha'(t_i)\right] \Delta x_i \right\rvert\\
    &\leq M \cdot
        \sum^n_{i=1} \left\lvert \alpha'(\xi_i) - 
        \alpha'(t_i)\right\rvert \Delta x_i 
\end{align*}
where $M =\sup_{x\in[a,b]}\left\lvert f(x)\right\rvert$. Finally, note that for each $i$,
\[ 
    |\alpha'(\xi_i) - \alpha'(t_i)| \leq 
    \sup_{[x_{i-1}, x]} \alpha(x) - 
    \inf_{[x_{i-1}, x]} \alpha(x) 
\]
like we might have if we consider the upper and lower sums. As a result, because $\alpha'\in\mathscr{R}_\alpha([a,b])$, we can choose a $Q$ such that 
\[ 
    \sum^n_{i=1} \left\lvert \alpha'(\xi_i) - 
    \alpha'(t_i)\right\rvert \Delta x_i 
    \leq U_\alpha(f,Q) - L_\alpha(f,Q) \leq \frac{\varepsilon}{2M}
\]
Taking our new partition to be $P \cup Q$, which is a superset of $P$ (where $P$ is from above), we see that our desired result is achieved.
\end{proof}

\newpage
\subsection{Fundamental Theorem of Calculus}

\begin{thm} \emph{(Part I)}
\label{ftc1}
If $f$ is differentiable and $f'\in\mathscr{R}([a,b])$, then 
    \[ \int^b_a f'(x)\;dx = f(b) - f(a) \]
\end{thm}
\begin{proof}
Careful with the notation here, but take the $f$ in Theorem \ref{ftcbasis} to be the constant function $f(x) =1$. Then, take the $\alpha(x)$ in Theorem \ref{ftcbasis} to be the $f$ in this theorem's statement. Then we get that
\[ \int^b_a f'(x) \;dx = \int^b_a 1 \;df = f(b) - f(a) \]
\end{proof}

\begin{thm} \emph{(Part II)}
\label{ftc2}
Suppose $f\in\mathscr{R}([a,b])$, and take 
    \[ F(x) = \int^x_a f(t)\;dt \]
Then $F$ is Lipschitz continuous on $[a,b]$, and $F$ is differentiable at those points where $f$ is continuous with $F'(x) = f(x)$ at those points.
\end{thm}
\begin{proof}
First, we deal with the continuity. Since $f$ is bounded, we know that 
    \[ 
        f(x) \leq C = \sup_{x\in[a,b]} f(x)
    \]
Thus, we can say that
\begin{align*}
    |F(y) - F(x)| &= \left\lvert \int^y_x f(t)\;dt \right\rvert
        \leq \int^y_x |f(t)|\;dt \\
    &\leq \int^y_x C\;dt = C|y-x| 
\end{align*}
Thus, $F$ is Lipschitz continuous.
\\
\\
Next, for $x\in[a,b]$ and $h\neq 0$:
\[ 
    \frac{F(x+h)-F(x)}{h} = \frac{1}{h}\int^{x+h}_x f(t)\;dt
\]
Applying the Mean Value Theorem for integrals, we know that there is some $\mu \in [m, M]$, where $m = \inf f$ and $M = \sup f$ over the interval $[x,x+h]$, such that 
    \[ \mu = \frac{1}{h}\int^{x+h}_x f(t)\;dt \]
Now, if $f$ is continuous at $x$, then for all $\varepsilon>0$, there exists a $\delta>0$ such that 
\begin{align*}
    |h| \leq \delta \quad \Rightarrow \quad |f(x) - f(y)|
        &\leq\varepsilon \\
    \Rightarrow \qquad |\mu - f(x)|&\leq\varepsilon \\
    \Rightarrow \quad \left\lvert \frac{F(x+h)-F(x)}{h}- f(x)
        \right\rvert
        &\leq\varepsilon 
\end{align*}
So $f$ is differentiable at $x$ and $F'(x)=f(x)$.
\end{proof}

\begin{thm}\emph{(Change of Variables)}
Suppose that we have $\varphi: [a,b] \rightarrow [\varphi(a), \varphi(b)]$, a differentiable, monotonically increasing function with $\varphi'\in\mathscr{R}([a,b])$. Also suppose that $f\in\mathscr{R}([\varphi(a),\varphi(b)])$. Then $(f\circ \varphi)\cdot\varphi' \in \mathscr{R}([a,b])$ and
\[
    \int^b_a f\left(\varphi(x)\right) \varphi'(x)\;dx
    = \int^{\varphi(b)}_{\varphi(a)} f(u)\;du
\]
\end{thm}
\begin{proof}
   to do 
\end{proof}


\newpage
\subsection{Improper Integrals}

\begin{defn}
(Cauchy Principal Value) Sometimes we want to consider integrating a function over the real line, we take 
\[
    \lim_{b\rightarrow\infty}
    \left(\lim_{a\rightarrow-\infty} \int^b_a f\;d\alpha
    \right) \neq 
    \left(\lim_{a\rightarrow\infty} \int^a_{-a} f\;d\alpha
    \right)
\]
where the second term is the \emph{Cauchy Principle Value}. It may sometimes exist, while the integral on the left does not. 
\end{defn}

\begin{ex}
For the simplest example, take $f(x)=x$. If evaluated in the usual sense, we get $\infty-\infty$, which makes no sense. However, if we evaluate as on the righthand side above, we get a Cauchy Principle Value of $0$.
\end{ex}

\begin{ex}
Now we have an example of a function that is continuous, non-negative everywhere, and $f\not\rightarrow 0$, but $f\in\mathscr{R}([a,\infty))$ nonetheless.
To construct such a function, set up
\[
    f(x) =
    \begin{cases}
        1 & x\in\mathbb{Z} \\
        / & [x - 1/x^2, x] \\
        \text{\textbackslash} & [x, x + 1/x^2] \\
        0 & \text{otherwise}
    \end{cases}
\]
where $/$ and \textbackslash denote increasingly linear and decreasingly linear, respectively. An example is below
\begin{figure}[h!]
   \centering
   %\includegraphics[scale=0.45]{TriangleFunction.pdf}
\end{figure}
\end{ex}

\begin{ex}
Suppose that in addition to the conditions we had above, we also want $f$ to be unbounded. To deal with that, make the triangles higher, but shrink the base faster.
\end{ex}


\newpage
\section{Sequences of Functions}

In this section, we will consider sequences of real-valued functions on the interval $[a,b]$, defined as $\{f_n\}_{n=1}^\infty$. This sequence might approach some function $f$, and will will consider how properties of functions in the sequence translate into properties for the function $f$.

\subsection{Types of Convergence}

\begin{defn} (Pointwise Convergence) A sequence $\{f_n\}_{n=1}^\infty$ converges \emph{pointwise} to $f$ if, for all $x\in[a,b]$,
\[ 
    \lim_{n\rightarrow\infty} f_n(x) = f(x) 
\]
in which case we write $f_n\rightarrow f$, p.w.
\end{defn}

\begin{defn} (Uniform Convergence) A sequence $\{f_n\}_{n=1}^\infty$ converges \emph{uniformly} to $f$ if .ns 
\[ 
    \sup_{x\in[a,b]} |f_n(x) - f(x)| \rightarrow0 
\]
in which case we write $f_n\rightarrow f$ uniformly. This uses the $\sup$-norm metric to define convergence, where the metric is defined 
\[
    d_\infty(f,g)=||f-g||_\infty := \sup_{x\in[a,b]} |f(x)-g(x)|
\]
for the functions $f$ and $g$.
\end{defn}

\begin{thm}
Suppose $f_n\rightarrow f$ uniformly. If each function $f_n$ is continuous on $[a,b]$, then $f$ is continuous on $[a,b]$.
\end{thm}
The last theorem requires \emph{uniform} convergence, not pointwise. Here's an example which demonstrates why.
\begin{ex}
Suppose that we have $f_n(x)=x^n$ on $[0,1]$. Then $f_n\rightarrow f$ pointwise, where
    \[ f(x) = \begin{cases} 0 & x \in [0,1) \\ 1 & x=1 
\end{cases} \]
Each $f_n$ is continuous, but $f$ clearly is not.
\end{ex}

\subsection{Integrability}

\begin{thm}
Suppose that $\{f_n\}$ is a sequence of functions, with each $f_n\in\mathscr{R}_\alpha([a,b])$. If $f_n\rightarrow f$ uniformly,
then 
\[ 
    \lim_{n\rightarrow\infty} \int^b_a f_n\;d\alpha
    = \int^b_a \left(\lim_{n\rightarrow\infty} f_n \right)\;
    d\alpha = \int^b_a f\;d\alpha
\]
\end{thm}

Again, we need uniform convergence, and the next example shows why pointwise convergence is insufficient.

\begin{ex}
    Enumerate the rationals on $[0,1]$: $\mathbb{Q}\cap[0,1] = \{q_1, q_2, \ldots\}$. Then set the sequence of functions equal to a sequence of characteristic functions:
\[ 
    f_n = \chi_{\{q_1, \ldots, q_n\}} \in \mathscr{R}([a,b])
\]
But we know that the sequence $\{f_n\}$ converges to $f=\chi_{\mathbb{Q}\cap [0,1]}\not\in\mathscr{R}([a,b])$.
\end{ex}

\begin{thm}
Let $\alpha\in BV([a,b])$ and $\{f_n\}_{n=1}^\infty\subset \mathscr{R}_\alpha([a,b])$. Also assume that $f_n\rightarrow f$ uniformly on $[a,b]$. Then $f\in\mathscr{R}_\alpha([a,b])$ and 
\[ 
    \lim_{n\rightarrow\infty} \int^b_a f_n\;d\alpha
    = \int^b_a \left(\lim_{n\rightarrow\infty} f_n \right)\;
    d\alpha = \int^b_a f\;d\alpha
\]

\end{thm}


\newpage
\section{Measure Zero and Lebesgue's Theorem}

This section tackles Lebesgue's Theorem, which provides a necessary and sufficient condition for $f\in\mathscr{R}_\alpha([a,b])$, using the concept of measure zero, which will develop in the next subsection.

\subsection{Measure Zero}

\begin{defn}
We say that $X\subset\mathbb{R}$ is \emph{measure zero} (or \emph{negligible}) if, for all $\varepsilon>0$, there exists a set of open intervals $\{I_n\}_{n=1}^\infty$ such that 
\[ 
    \bigcup^\infty_{n=1} I_n \supset X \qquad
    \text{and} \qquad \sum^n_{n=1} |I_n|\leq \varepsilon
\]
\end{defn}

\begin{defn} 
We say that the statement, proposition, or condition $P(x)$ for $x$ on an interval $I$ holds \emph{almost everywhere} (a.e.) if 
\[
    \{ x \in I \; | \; P(x) \text{ is false or fails} \} 
    \;\; \text{is measure zero}
\]
\end{defn}

\begin{prop}
\label{countunion}
Finite sets are of measure zero, and a countable union of measure zero sets is itself measure zero.
\end{prop}
\begin{proof}
The finite case is trivial, so let's consider the second part of the proposition. Suppose that we have a sequence $\{X_n\}_{n=1}^\infty$ with $X_n$ measure zero for all $n$. Then Let $\{I_{n,k}\}_{k=1}^\infty$ be a set of open intervals such that 
    \[
        \bigcup^\infty_{k=1} I_{n,k} \supset X_n 
        \qquad \text{and} \qquad
        \sum^n_{k=1} |I_{n,k}| < \frac{\varepsilon}{2^n}  
    \]
Then we know that 
    \[
        \bigcup^\infty_{n=1}\bigcup^\infty_{k=1} 
        I_{n,k} \supset \bigcup^\infty_{n=1}  X_n 
    \]
and that the double union on the left is countable as the countable union of countable sets. In addition, we know that 
    \[
        \sum^\infty_{n=1}\sum^\infty_{k=1} 
        |I_{n,k}| < \sum^\infty_{n=1} \frac{\varepsilon}{2^n} 
        = \varepsilon
    \]
\end{proof}
\begin{cor}
Any countable set is measure zero, in which case $\mathbb{Q}$ is measure zero as well.
\end{cor}


\begin{ex}
The Cantor Set\footnote{See appendix for construction.} is an uncountable set that is measure zero. We know its uncountable from the proof in the appendix; now we want to show measure zero.
\\
\\
Recall that we constructed $C$ as 
\[
    C = \bigcap^\infty_{n=1} C_n \quad\Rightarrow\quad
    C \subset C_n \quad \forall\;n
\]
So if we can find an index $n^*$ such that we can cover $C_{n^*}$ by arbitrarily small open intervals, we can cover $C$ too.  To find that particular $n^*$, note that each $C_n$ is the union of $2^n$ subintervals, each of length $1/3^n$. 
\\
\\
So given $\varepsilon>0$, we choose $n^*$ so that $\left(\frac{2}{3}\right)^{n^*} < \varepsilon/2$. Then, we note that we can cover each subinterval of $C_{n^*}$ (which we'll index by $k$) by an open interval of length 
\[ 
    |I_k| < \frac{1}{3^{n^*}} + \frac{\varepsilon}{2^{{n^*}+1}} 
\] 
This is the length of each subinterval (which we mentioned above), plus a tiny amount.
\\
\\
Then, when it's time to add up the size of our open cover of $C_{n^*}$ over the $2^{n^*}$ component subintervals, we find
\begin{align*}
    \sum^{2^{n^*}}_{k=1} |I_k| &\leq \sum^{2^{n^*}}_{k=1}
    \left(\frac{1}{3^{n^*}} + \frac{\varepsilon}{2^{{n^*}+1}}\right)
    = 2^{n^*} \cdot 
    \left(\frac{1}{3^{n^*}} + \frac{\varepsilon}{2^{{n^*}+1}}\right)
    \\
    &\leq \left(\frac{2}{3}\right)^{n^*} + 
        \frac{2^{n^*}\varepsilon}{2^{{n^*}+1}}
    = \left(\frac{2}{3}\right)^{n^*} + 
        \frac{\varepsilon}{2}
\end{align*}
And by our choice of $n^*$, we know that $\left(\frac{2}{3}\right)^{n^*} < \varepsilon/2$, which implies
\begin{align*}
    \sum^{2^{n^*}}_{k=1} |I_k| &\leq 
    \frac{\varepsilon}{2} +
    \frac{\varepsilon}{2}
    = \varepsilon
\end{align*}
And so $C$ is measure zero.
\end{ex}

\begin{thm}
Any closed interval $[a,b]$ is not measure zero.
\end{thm}
\begin{proof}
    Suppose that we have some closed interval $I = [a,b]$ that is measure zero. Then given any $\varepsilon>0$, there must be an open cover 
\[  
    \{I_n\}_{n=1}^\infty \supset I 
    \quad \text{s.t.} \quad
    \sum^\infty_{i=1} |I_n|<\varepsilon
\]
Now since $I$ is compact, there must by a finite subcover $\{I_{n_i}\}^k_{i=1} \supset I$ with $\sum^k_{i=1} |J_{n_i}| <\varepsilon$.
\\
\\
But this leads to a contradiction because, via induction, we can show that the finite sum of the lengths of the intervals must be greater than $b-a$.
\end{proof}

\begin{note}
    Continuous a.e. is \emph{not} the same thing as being equal almost everywhere to a continuous function. For example, the function $f=\chi_{\mathbb{Q} \cap [a,b]}$ equals the function $g(x)=0$ almost everywhere. \emph{However}, $f$ is discontinuous everywhere. 
\end{note}

\subsection{Lebesgue's Theorem}

\begin{defn} 
For a bounded function $f:[a,b]\rightarrow\mathbb{R}$ and nonempty $I\subset[a,b]$, we define the \emph{oscillation of $f$ on $I$}, denoted $\Omega_f(I)$, as
\[
    \Omega_f(I) := \sup_I f - \inf_I f 
    = \sup_{x,y\in I} |f(x)-f(y)|
\]
We define the oscillation at a particular point $x\in I$ (for $I$ open) as 
\[
\Omega_f(x) = \inf_{I\ni \; x} \Omega_f(I)
\]
And just to be explicit, the infimum is taken over the set of \emph{open} intervals containing $x$.
\end{defn}

\begin{prop}
\label{osccts}
$f$ is continuous at $x$ if and only if $\Omega_f(x)=0$.
\end{prop}

\begin{prop}
\label{rewriteuml}
Tying the concept back to integration, we can write 
\[
    U(f,P)-L(f,P) 
    = \sum^n_{k=1} \Omega_f(I_k) \; |I_k|
\]
where $I_k = [x_{k-1}, x_k]$.
\end{prop}

\begin{lem}
    \label{leblem}
Let $f$ be a bounded function on a closed interval, $J$. If $\Omega_f(x) < \varepsilon$ for every $x$ in $J$, then there exists a partition $P$ of $J$ such that 
\[
    U(f,P)- L(f,P) < \varepsilon |J|
\]
\end{lem}

\begin{proof}
For each $x\in J$, then there's an open interval $I_x \ni x$ such that $\Omega_f(I_x)<\varepsilon$ by assumption and the definition of $\Omega_f(x)$. Thus we can say 
\[
    \bigcup_{x\in J} I_x \supset J
\]
But since $J$ is closed (and thus compact), there's a finite subcover $\{I_1, \ldots, I_n\}$. Let $P=\{x_0, x_1, \ldots, x_n\}$ be the set of endpoints of the intervals $\{I_1\cap J, \ldots, I_n\cap J\}$. Then $\Omega_f([x_{i-1},x_i])<\varepsilon$ for all $i$, and by Proposition \ref{rewriteuml}, we can write
 
\[
    U(f,P)- L(f,P) = \sum^n_{i=1} \Omega_f([x_{i-1}, x_i]) \Delta 
    x_i < \varepsilon |J|
\]
\end{proof}

\begin{lem}
\label{leblem2}
Suppose that $f$ is bounded on $[a,b]$ and that $\varepsilon>0$. Then the set $A = \{x \in [a,b] \; | \; \Omega_f(x) < \varepsilon \}$ is open in $[a,b]$.
\end{lem}

\begin{proof}
To see this, note that $x\in A$ implies there's an open set $I\ni x$ such that $\Omega_f(I)<\varepsilon$, by the definition of $\Omega_f(x)$ and $A$.  Now since $I$ is open and $x\in I$, $x$ is interior to $I$. That means there exists a $\delta>0$ such that $(x-\delta, x+\delta)\subset I$. 

Given this $\delta$, we then have that $|y-x|<\delta$ implies $\Omega_f(y) \leq \Omega_f(I) < \varepsilon$ because $y\in I$ (clearly), and $\Omega_f(y)$ is the minimum over all intervals containing $y$. 

Thus, $y \in A$, and $A$ is open because you can construct an open ball, $B_\eta(x)$ that is entirely in $A$, as we just did.

\end{proof}

\begin{thm}
\emph{\textbf{(Lebesgue's Theorem)}}
Let $f$ be bounded. Then $f\in\mathscr{R}([a,b])$ if and only if $f$ is continuous ``almost everywhere''--i.e. if the set of points at which $f$ is discontinuous is measure zero or ``negligible.''
\end{thm}

\begin{proof}
Let $X=\{x \; | \; \text{$f$ discts at $x$} \}$. Now we don't know whether $X$ is countable or uncountable, but regardless, we can write it as a union of countable sets.  Here's how.

Recall from Proposition \ref{osccts} that if $f$ is discontinuous at $x$, then the oscillation $\Omega_f(x) > 0$. So an equivalent way to write our set $X$ is 
\begin{align*}
    X &= \{x \; | \; \text{$f$ discts at $x$} \} \\
        &= \bigcup_{\delta>0} \{x \; | \; \Omega_f(x)\geq \delta \} 
\end{align*}
($*$) Now the next step is a big deal, because the union above is an \emph{uncountable} union over the uncountable set of $\delta>0$. But with the following trick, we're going to write this uncountable union as a \emph{countable} union instead: 
\begin{align*}
    X &= \bigcup_{\delta>0} \{x \; | \; \Omega_f(x)\geq \delta \} \\
    &= \bigcup^\infty_{n=1} \{x \; | \; \Omega_f(x)\geq 1/n \} =: \bigcup^\infty_{n=1}  X_n
\end{align*}
This works because $X$ is a union of points whose oscillation $\Omega_f(x)$ is non-zero. And by the Archimedian property, if we take $n\rightarrow \infty$ so that $1/n$ is sufficiently small, we'll capture \emph{all} of those points $\Omega_f(x)>0$ eventually, since we can get $0<1/n<\delta$ for all $\delta$. 

In fact, as $n\rightarrow\infty$, the size of each successive set $X_n$ grows to include more and more points. On top of that, it's also clear that $X_n\supset X_m$ for $n>m$. So given all of these facts, we then know that the union of those sets will equal $X$.

So now the task is to show that $X_n$ is measure zero for all $\delta$. If we can do this, then in which case the countable union of measure zero sets will also be measure zero by Proposition \ref{countunion}.
\\
\\
\textbf{($\Rightarrow$ Direction)} So consider $X_m$ (which we want to show is measure zero), and suppose that $\varepsilon>0$ is given. Then since $f\in\mathscr{R}([a,b])$, we know that there exists a partition $P$ such that
\begin{equation}
    \label{lebeg1}
    U(f,P) - L(f,P) = 
    \sum^n_{i=1} \Omega_f(I_i)\; |I_i| < \frac{\varepsilon}{2m} 
\end{equation}
where we rewrote the sum according to Proposition \ref{rewriteuml}, and $m$ corresponds that of $X_m$.
\\
\\
Now since $P$ is finite, it suffices to show that $X_m\setminus P$ is measure zero. And so we can say
\begin{equation}
    \label{lebeg2}
    \forall x \in X_m \setminus P \qquad
    \exists I_{j} \ni x \quad \text{s.t.} \quad
    \Omega_f(I_j) \geq \Omega_f(x) \geq \frac{1}{m}
\end{equation}
In words, this says that if $x$ is in the set $X_m\setminus P$, then it's in some open interval $I_j = (x_{j-1}, x_j)$. (Open since we removed the endpoints; these are our open sets covering $X_m\setminus P$, which we'll show are measure zero.) And that interval will have an oscillation greater than the ``minimum'' oscillation over all intervals containing $x$. And that minimum happens to be greater than or equal to $1/m$, since we're working with $X_m$, which is the set of all points $x$ where $\Omega_f(x)\geq 1/m$.
\\
\\
And so let $I_{j}$ for $j=1,\ldots,k$ denote those intervals containing some $x\in X_m\setminus P$. Then combining Inequality \ref{lebeg1} with the information we have about the oscillation from the inequalities in \ref{lebeg2}, we see that
\begin{align*}
    \frac{1}{m} \sum^k_{j=1} |I_j| \leq 
    \sum^k_{j=1} \Omega_f(x)\; |I_j| &\leq 
    \sum^n_{k=1} \Omega_f(I_i)\; |I_i| < \frac{\varepsilon}{2m} \\
    \Rightarrow \quad 
    \sum^k_{j=1} |I_j| < \frac{\varepsilon}{2}
\end{align*}
And so taking the open sets $I_j = (x_{j-1}, x_j)$, where the endpoints correspond to the partition $P$ we got from the integrability of $f$, we can say that these intervals cover $X_m\setminus P$, and we can make them as small as necessary. 

Adding in a finite number of sufficiently small open sets to account for any points we removed that were in $P$ (which we can easily do), we see that we can cover $X_m$, implying it's measure zero for all $m$.
\\
\\
\textbf{($\Leftarrow$ Direction)} Next, assume that $f$ is continuous almost everywhere. We'll use the properties of $\Omega_f$ to find a good partition. Also, we'll again let $X$ be the set of points at which $f$ is discontinuous, and the sets $X_m$ will be as above as well.

So let $\varepsilon>0$ (which dictates how close we'll have to make $U$ and $L$) be given. Without loss of generality, assume $f$ is not constant, which implies $\Omega_f([a,b]) > 0$.\footnote{Otherwise, $f$ is trivially integrable.} Also choose $m$ such that $\frac{b-a}{m}<\frac{\varepsilon}{2}$. Since $X_m$ is measure zero, we know that there exists a set of open intervals $\{I_n\}_1^\infty$ such that
\[ 
    \bigcup^\infty_{n=1} I_n \supset X_m \qquad  \text{and} \qquad
    \sum^\infty_{n=1} |I_n| < \frac{ \varepsilon}{2\Omega_f([a,b])}
\]
We know that $\Omega_f([a,b])<\infty$ because $f$ is bounded.

Next, we'll use the fact that $X_m$ is closed for all $m$ with respect to the relative topology of $[a,b]$. This is because the following set, by Lemma \ref{leblem2}, is open:
\[ 
    X_m^c = \{ x \in [a,b] \; | \; \Omega_f(x) < 1/m \}
\]
Since $X_m$ is closed and $[a,b]$ is compact, we know that $X_m$ is compact (as a closed subset of a compact space). This means there exists a finite subcover
\begin{equation}
    \label{badpartcover}
    \bigcup^k_{j=1} I_{n_j} \supset X_m
\end{equation}
Now, express the interval not covered by those sets in \ref{badpartcover} as 
\[ 
    [a,b] \setminus \left( \bigcup^k_{j=1} I_{n_j}\right)
    = \bigcup^p_{i=1} J_i 
\]
It's clear that the $J_i$ will be closed and disjoint. We just need to control the oscillation on these intervals, since the ``bad'' parts are already covered by the tiny $I_{n_j}$.

Now by our definitions of everything up to this point, $x\in J_i$ implies that $x\not\in X$, which implies $x\in X^c_m$, which implies $\Omega_f(x) < 1/m$. So we can apply Lemma \ref{leblem} to say that there exists a partition $P_i$ for each $J_i$ such that 
\[ 
    U(f,P_i) - U(f,P_i) \leq \frac{1}{m}|P_i|
\]
Taking $P = \bigcup_{i=1}^p P_i \cup \{a,b\}$, we see that $P$ is a partition of $[a,b]$. Thus
\begin{align*}
    U(f,P) - U(f,P) &= \sum^p_{i=1} \left[
        U(f,P_i) - U(f,P_i)\right] + \sum^k_{j=1}
        \Omega_f(I_{n_j}) |I_{n_j}| \\
    &\leq \frac{1}{m} \sum^p_{i=1} |J_i| 
        + \Omega_f([a,b]) \sum^k_{j=1} |I_{n_j}| \\
    \text{By choice of $m$ at beginning} \quad 
        &\leq \frac{b-a}{m} + \Omega_f([a,b]) 
        \frac{\varepsilon}{2\Omega_f([a,b])} \\
    &\leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} =
        \varepsilon
\end{align*}
\end{proof}

\begin{rmk}
The main method used in the last proof represents fairly standard approach to solving integration problems (and analysis problems, even more generally).  Namely, you separate the function into ``good'' and ``bad'' parts. (In the previous theorem, those parts were determined by the oscillations of $f$ on $[a,b]$.) Then, you make the contribution from the ``bad'' parts very small.
\end{rmk}

\begin{cor}
If $f\in\mathscr{R}([a,b])$ and $\int^b_a |f| = 0$, then $f=0$ almost everywhere.
\end{cor}

\begin{proof}
$f$ is continuous almost everywhere (by Lebesgue's Theorem); therefore, $|f|$ is continuous almost everywhere. 

If $f$ were continuous at a point $x$ for which $f(x)\neq 0$, then there exists an interval $I\ni x$ such that $|I|>0$ and $|f|>0$, which would imply $\int^b_a |f| > 0$. Hence we see that that the set of points $\{x \; | \; f(x)\neq 0\} \subset \{x \; | \; f \text{ discont.}\}$, which is of measure zero.
\end{proof}

\begin{cor}
If $f\in\mathscr{R}([a,b])$, then $F(x):=\int^x_0 f(t) \; dt$ implies that $F$ is differentiable almost everywhere and $F'(x)=f(x)$ almost everywhere.
\end{cor}

\newpage
\section{General Theory of Measure Integration}

Riemann integrals are good for continuous or piecewise continuous functions.  But for more irregular and general functions, Riemann integration breaks down.

For example, we've seen in the section about sequences of functions that we can't always exchange the limit and the integral sign. In particular, there are sequences $\{f_n\}\rightarrow f$ such that each $f_n \in \mathscr{R}_\alpha([a,b])$, but $f \not\in \mathscr{R}_\alpha([a,b])$.

This stems from a deficiency in Riemann integrals arising from the relatively rigid structure of intervals---the sets over which we integrate. So this chapter will be about replacing intervals with more general ``measurable sets'', which contain intervals as a special case.

So we'll want a generalized notion of ``measure'' for subsets $A\subset\mathbb{R}$, which we'll denote $m(A)$. The function $m(\cdot)$ will be how to weight subsets of the real line.

\subsection{Desired Structure and Limitations}

If we're constructing a general theory from scratch, there's a number of desirable features that we would like:
\begin{enumerate}
    \item If $A$ is in an interval, $m(A) = |A|$, the length of the interval.
    \item Translation invariance: $m(A+x) = m(A)$.
    \item Countable additivity: If $A_1, \ldots, A_n$ disjoint, then $m(\cup A_n) = \sum m(A_n)$. 
    \item If $A_1 \subset A_2$, $m(A_1) < m(A_2)$.
\end{enumerate}

\begin{rmk}
This wishlist is incompatible if our number system consists exclusively of rationals. This is because $m(\{p\}) = 0$, for all single rationals $p\in\mathbb{Q}$. Then $m(\mathbb{Q})=\sum m(\{p_n\}) = 0$. So the measure of all sets is zero, giving us no useful theory.

We also can't make it work if we define $m(\{p\})=c>0$, because then $m(\mathbb{Q})=\infty$. This normally wouldn't be a problem, but then $m([0,1]\cap \mathbb{Q}) = \infty \neq 1$. 
\end{rmk}

\begin{rmk}
    It also turns out that you can't satisfy the wishlist if you try to define a measure for the power set,$\mathscr{P}(\mathbb{R})$. We'll need to consider a proper subset of $\mathscr{P}(\mathbb{R})$.
\end{rmk}

\subsection{Extended Real Number System}

\begin{defn}
We denote the extended reals by $\bar{\mathbb{R}}$, and it is constructed as 
\[ 
    \bar{\mathbb{R}} = {\mathbb{R}} \cup \{-\infty, \infty\}
\]
Open sets are defined as the open sets of $\mathbb{R}$ plus any sets of the form $[-\infty, a)$ or $(a,\infty]$ for $a\in\mathbb{R}$, along with unions of these basic building blocks too.
\end{defn}

\begin{rmk}
For the remainder of this section, we will construct our theory for the \emph{extended} reals to allow for the greatest possible generality.
\end{rmk}

\subsection{$\sigma$-Algebras and Measurable Spaces}

\begin{defn} 
\label{algebra}
(Algebra of Sets)
Let $X$ be any set, and let $\mathscr{A}$ be a collection of subsets of $X$ (so $\mathscr{A}\subset\mathscr{P}(\mathbb{R})$). We say that $\mathscr{A}$ is an \emph{algebra} if 
\begin{enumerate}
    \item $A\in\mathscr{A}\Rightarrow A^c\in\mathscr{A}$
    \item $A,B\in\mathscr{A} \Rightarrow A\cup B\in\mathscr{A}$
\end{enumerate}
Implications of our definition:
\begin{itemize}
    \item Since $A\cap B = (A^c \cup B^c)^c$, we then know that $A,B\in\mathscr{A}\Rightarrow A\cap B\in \mathscr{A}$.
    \item Since $A, A^c \in \mathscr{A}$ and so are intersections, then $\emptyset = A\cap A^c \in \mathscr{A}$.
    \item The trivial set---the simplest algebra---is $\{\emptyset, X\}$.
\end{itemize}
\end{defn}

\begin{defn} 
($\sigma$-algebra)
An algebra $\mathscr{A}$ (satisfying the conditions in Definition \ref{algebra}) is a \emph{$\sigma$-algebra} if it's closed under countable unions,  i.e. whenever $\{A_n\}^\infty_{n=1}$ is a sequence in $\mathscr{A}$, then $\bigcup^\infty_{n=1} A_n \in \mathscr{A}$. As a consequence, in a $\sigma$-algebra, $\bigcap^\infty_{n=1} A_n \in \mathscr{A}$ as well.
\end{defn}

\begin{prop}
If $\{\mathscr{M}_\alpha\}_{\alpha\in I}$ is a collection of $\sigma$-algebras, then $\bigcap_{\alpha\in I} \mathscr{M}_\alpha$ is one too.
\end{prop}

\begin{ex}
The easiest non-trivial $\sigma$-algebra is the power set $\mathscr{P}(X)$.
\end{ex}

\begin{ex}
The \emph{Borel $\sigma$-algebra} is defined as the smallest $\sigma$ algebra that contains all open sets (and hence, all closed sets too). It can be written as the intersection of all $\sigma$-algebras containing all open sets of $X$, which we know to be nonempty, since the power set $\mathscr{P}(X)$ is one such set.
\end{ex}

\begin{ex}
There exist algebras which are not $\sigma$-algebras. Take $X=\mathbb{Z}$. Then take $\mathscr{A} = \{A\subset \mathbb{Z} \; |\; \text{either $A$ or $A^c$ finite}\}$. 
\end{ex}

\begin{defn} 
    Let $X$ be a set and $\mathscr{M}$ be a $\sigma$-algebra on $X$. Then the couplet $(X, \mathscr{M})$ is called a \emph{measurable space}. It is ``measur\emph{able}'' in the sense that we can assign a \emph{measure} to it, a concept we'll discuss in this next subsection.
\end{defn}



\subsection{Non-Negative Measures and Measure Spaces}

\begin{defn} 
A \emph{measure} on a measurable space $(X,\mathscr{M})$ is a set function $\mu: \mathscr{M} \rightarrow [0,\infty]$.
\end{defn}

\begin{defn} 
A \emph{non-negative measure} is a measure that satisfies 
\begin{enumerate}
    \item $\mu(A)<\infty$ for at least one $A\in\mathscr{M}$ 
    \item \emph{Countable additivity} for all sequences $\{A_n\}_{n=1}^\infty$ in $\mu$. That means 
        \[
            \mu\left(\bigcup^\infty_{n=1}A_n\right) 
            = \sum^n_{n=1} \mu(A_n)
        \]
        where the $A_i$ are pairwise disjoint.
\end{enumerate}
\end{defn}

\begin{defn} 
If we add a non-negative measure to a measure space, then the triplet $(X,\mathscr{M},\mu)$ is called a \emph{measure space}. 
\end{defn}

\subsection{Properties of Measure Spaces}

Measure spaces have a number of properties that follow from the framework and conditions imposed upon them, which we saw in the definitions of $\sigma$-algebras and positive measures. Here, we enumerate those properties and provide justifications for them:
\begin{enumerate}

\item $\mu(\emptyset)=0$. To see this, write for any non-empty set $A=A\cup\emptyset\cup\emptyset\cdots$. Then use countable additivity.

\item Finite additivity, which follows from countable additivity and an argument like (1).

\item If $A,B\in \mathscr{M}$, then $A\setminus B \in \mathscr{M}$. To see this, write $A\setminus B = A \cap B^c$.

\item If $A \subseteq B$, then $\mu(A) \leq \mu(B)$. 
\begin{proof}
This follows because $B = A \cup (B\setminus A)$, which is a union of disjoint sets. Then use finite additivity to say $\mu(B) = \mu(A) + \mu(B\setminus A) \geq \mu(A)$.
\end{proof}

\item Given a nesting of measurable sets $A_1\subset A_2\subset A_3 \subset \cdots$
\[
    \mu\left(\bigcup^\infty_{n=1} A_n\right) = 
    \lim_{n\rightarrow\infty} \mu(A_n)
\]
\begin{proof}
To exchange sums and unions, we need to use countable additivity. But we can only do so if the sequences are pairwise disjoint, so construct the following pairwise disjoint sequence of sets 
\begin{align*}
    B_1 = A_1 \qquad B_2 = A_2 \setminus A_1
    \qquad B_3 = A_3 \setminus A_2 \qquad \cdots
\end{align*}
Then note that we can equivalently work with the sequence $\{B_n\}_1^\infty$ instead of $\{A_n\}_1^\infty$:
\[ 
    A_N = \bigcup^N_{n=1} A_n =
    \bigcup^N_{n=1} B_n 
    \qquad \Rightarrow \qquad
    \bigcup^\infty_{n=1} A_n =
    \bigcup^\infty_{n=1} B_n 
\]
So then we can exploit countable and finite additivity for disjoint sets:
\begin{align*}
    \mu\left(\bigcup^\infty_{n=1} A_n \right)
    = \mu\left(\bigcup^\infty_{n=1} B_n \right)
    &= \sum^\infty_{n=1} \mu(B_n)
        = \lim_{N\rightarrow\infty}\sum^N_{n=1} \mu(B_n) 
    = \lim_{N\rightarrow\infty}
        \mu\left(\bigcup^N_{n=1} B_n\right) \\
    &= \lim_{N\rightarrow\infty}
        \mu\left(\bigcup^N_{n=1} A_n\right) 
    = \lim_{N\rightarrow\infty}
        \mu\left(A_n\right)  
\end{align*}
\end{proof}


\item \emph{Countable Sub-Additivity}: For \emph{any} (non necessarily pairwise disjoint or nested) sequence of sets $\{A_n\}_1^\infty$, we have
    \[ 
        \mu\left(\bigcup^\infty_{n=1} A_n\right)
        \leq \sum^n_{i=1} \mu(A_n)
    \]
\begin{proof}
Similar to the proof of the last property, construct the sets
\[
    B_1 = A_1 \qquad B_2 = A_2 \setminus A_1 
    \qquad B_3 = A_3 \setminus (B_1 \cup B_2)
\]
Clearly, we have $B_i \cap B_j$ for all $i\neq j$. On top of that $B_n \subset A_n$ for all $n$. Thus
\[
    \mu\left(\bigcup^\infty_{n=1} A_n\right)
    = \mu\left(\bigcup^\infty_{n=1} B_n\right)
    = \sum^n_{i=1}\mu(B_n) 
    \leq \sum^n_{i=1} \mu(A_n)
\]
\end{proof}


\item Let $A_1 \supset A_2 \supset A_3 \supset \cdots$. If there exists a $k$ such that $\mu(A_k)<\infty$, then 
    \[
        \mu\left(\bigcap^\infty_{n=1} A_n\right)
        = \lim_{n\rightarrow\infty} \mu(A_n)
    \]
\begin{proof}
Without loss of generality, assume that $k=1$. Then construct the following sets
\[
    B_1 = A_1 \setminus A_2 \qquad
    B_2 = A_2 \setminus A_3 \qquad
    \cdots\qquad
    B_n = A_n \setminus A_{n+1} 
\]
Then, for all $n$, we can express $A_1$ as a disjoint union
\begin{align}
    A_1 &= B_1 \cup B_2 \cup \cdots \cup B_n
    \cup A_{n+1} \notag\\
    \Rightarrow \qquad 
    \mu(A_1) &= \sum^n_{\ell=1} \mu(B_\ell) + 
    \mu(A_{n+1}) \label{prop.6.1}
\end{align}
Next, we can also say that if $x\not\in \bigcup^\infty_{n=1} B_n$, then we must have $x\in A_n$ for all $n$. This implies that we can express $A_1$ as the following disjoint union:
\begin{align}
    A_1 &= \left(\bigcup^\infty_{n=1} B_n \right)\cup 
        \left(\bigcap^\infty_{n=1} A_n\right) \notag \\
    \Rightarrow \qquad 
        \mu(A_1) &= 
        \sum^\infty_{n=1} \mu(B_n) +
        \mu\left(\bigcap^\infty_{n=1} A_n\right) 
        \label{prop.6.2}
\end{align}
And since we know that $\lim_{n\rightarrow \infty} \sum^n_{\ell=1} \mu(B_\ell) = \sum^\infty_{\ell=1} \mu(B_\ell)$, we can deduce from Equations \ref{prop.6.1} and \ref{prop.6.2} (and from the fact that $\mu(A_{n+1}) \leq \mu(A_n) <\infty$) that 
\[ \mu(A_{n+1})\rightarrow 
    \mu\left(\bigcap^\infty_{n=1} A_n\right)
\]
\end{proof}
\end{enumerate}


\begin{ex}
Take $X=\mathbb{Z}$ and $\mathscr{M}=\mathscr{P}(\mathbb{Z})$. Then if we take a measure $\mu(A) = \#A = \text{card}\left( A \right)$, simple reasoning shows that $(X,\mathscr{M},\mu)$ is a measure space.
\end{ex}

\begin{ex}
    We stipulated in property (7) for our measure space that there must be some $k$ such that $\mu(A_k)<\infty$. Suppose that didn't hold, and we can construct a counterexample. 
    
    Consider $X = \mathbb{N}$ and $\mathscr{M} =\mathscr{P}(\mathbb{N})$. Next let $A_n = \{n, n+1, n+2, \ldots\}$. In this case, $\mu(A_n) = \infty$ for all $n$, but
    \[ 
        \mu\left(\bigcap^\infty_{n=1} A_n\right) = 
        \mu(\emptyset) = 0
    \]
\end{ex}

\subsection{Measurable Functions}


Given a function $f:X\rightarrow\mathbb{R}$, an equivalent representation of the function $f$ is by the disjoint collection $\{E_a\}_{a\in\mathbb{R}}$ where $E_a = f^{-1}\left(\{a\}\right)$. In this way, we partition the space $X$ based on what $f$ maps different subsets of $X$ to.  

This is exactly equivalent to defining an output $y=f(x)\in\mathbb{R}$ for every $x\in X$.  It's just a different way of looking at the function $f$. Moreover, it's a way that we'll use to our benefit for integration, by thinking about a $\sigma$-algebra on $X$. In this way, some of the sets $E_a$ will be measurable (i.e. they will be in $\mathscr{M}$) while others will not.

\begin{defn}
Let $(X,\mathscr{M})$ be a measurable space and let $f:X\rightarrow\bar{\mathbb{R}}$ be a function mapping to the extended reals. We say that $f$ is a \emph{measurable function} if $f^{-1}(U)$ is measurable (i.e.  $f^{-1}(U)\in\mathscr{M}$) whenever $U\subset\bar{\mathbb{R}}$ is open.\footnote{And, hence, closed as well} 
\end{defn}

\begin{rmk}
The notion of a measurable function is \emph{heavily} dependent upon our choice of $\mathscr{M}$.  Specifically, measurable functions are always \emph{relative to a given $\sigma$-algebra}, as we'll see in the next example.
\end{rmk}

\begin{ex}
Let $X$ be any set. If we take $\mathscr{M} = \mathscr{P}(X)$, the power set of $X$, then \emph{any} function $f$ is measurable, since $f^{-1}(U)$ will be in $\mathscr{P}(X)$ for any open $U\subset \bar{\mathbb{R}}$.
\end{ex}

\begin{prop}
Given a measurable space $(X,\mathscr{M})$ and a measurable function $f$, the set
\[
    S = \{B \subset \bar{\mathbb{R}} \; | \; f^{-1}(B) 
    \text{ measurable} \}
\]
is a $\sigma$-algebra on $\bar{\mathbb{R}}$ that contains all open sets of $\bar{\mathbb{R}}$. So we can make a $\sigma$-algebra for the range set of $f$ if we have a $\sigma$-algebra defined on the domain.
\end{prop}

\begin{rmk}
This implies something about the Borel Set for $\bar{\mathbb{R}}$, denoted $\mathscr{B}_{\bar{\mathbb{R}}}$. Namely, that $f^{-1}(B)\in \mathscr{M}$ for all open sets $B\in\mathscr{B}_{\bar{\mathbb{R}}}$.
\end{rmk}

\begin{thm}
\label{measequiv}
$f$ is measurable if and only if $f^{-1}\left((a,\infty]\right)\in\mathscr{M}$ for all $a\in\mathbb{R}$. 
\end{thm}

\begin{thm}
If $X=\mathbb{R}$ and $\mathscr{M}=\mathscr{B}_\mathbb{R}$ with $f:\mathbb{R}\rightarrow\mathbb{R}$ continuous, then $f$ is measurable.
\end{thm}

\begin{lem}
Let $f:X\rightarrow\mathbb{R}$ be measurable and $\varphi: \mathbb{R}\rightarrow\mathbb{R}$ be continuous. Then $\varphi\circ f$ is also measurable.
\end{lem}
\begin{proof}
For an open set $U\subset\mathbb{R}$, 
\begin{align*}
    (\varphi\circ f)^{-1}(U) &= 
        f^{-1}\left(\varphi^{-1}(U)\right)
    = f^{-1}(V) \qquad \text{where }V = \varphi^{-1}(U)
\end{align*}
We know that $V$ is open by the continuity of $\varphi$, and since $f$ is measurable, $f^{-1}(V)\in\mathscr{M}$.
\end{proof}

\begin{cor}
If $f$ is measurable, then so is $c f$ for $c\in\mathbb{R}$, $|f|$, and $f^2$.
\end{cor}

\begin{thm}
Let $\{f_n\}^\infty_1$ be a sequence of measurable functions with $f_n: X\rightarrow\bar{\mathbb{R}}$. Then the following function is measurable as well:
    \[ f(x) := \sup_n f_n(x) \]
\end{thm}
\begin{proof}
For all $a\in\mathbb{R}$, note that 
\begin{equation}
    \label{measseq}
    \{x \; | \; f(x) > a \} = \bigcup_n \; \{x \; | \; f_n(x)>a\}
\end{equation}
To see this, note that if $f(x)>a$, then there's some $n$ for which $f_n(x)>a$, given that same $x$. So the lefthand side is a subset of the righthand side. On the other hand, if $f_m(x)>a$ for some $m$, then the sup over the $f_n$ will be at least as large as $f_m(x)$. Hence the right is a subset of the left, proving equality.

Next, note that we can rewrite Equality \ref{measseq} above as 
\[ 
    f^{-1}\left((a,\infty]\right) = 
    \{x \; | \; f(x) > a \} = \bigcup_n \; \{x \; | \; f_n(x)>a\}
    = \bigcup_n \; f_n^{-1}\left((a,\infty]\right) 
\]
Since $f_n^{-1}\left((a,\infty]\right)$ will be in $\mathscr{M}$ for all $n$ by the measurability of each $f_n$, we can say that the union of all these sets in $\mathscr{M}$ will also be in $\mathscr{M}$ by the definition of $\mathscr{M}$ being a $\sigma$-algebra. Thus, $f^{-1}\left( (a, \infty]\right) \in\mathscr{M}$, implying that $f$ is measurable.
\end{proof}

\begin{cor}
If $f_n$ is measurable, then so is 
\begin{itemize}
    \item $f(x) := \inf_n f_n(x)$
    \item $f(x) := \lim \sup f_n(x) = \lim_{k\rightarrow\infty} \left(\sup_{n\geq k} f_n\right)$.
    \item $f(x) := \lim \inf f_n(x) = \lim_{k\rightarrow\infty} \left(\inf_{n\geq k} f_n\right)$.
\end{itemize}
\end{cor}

\begin{thm}
\label{convmeas}
If a sequence of measurable functions $\{f_n\}\rightarrow f$ pointwise, then $f$ is measurable.\footnote{This shows that measurable functions are potentially more powerful, because continuity isn't preserved so nicely. That requires more stringent uniform convergence.}
\end{thm}

\begin{thm}
    \label{summeas}
If $f, g$ are measurable, then $f+g$ is also measurable.
\end{thm}
\begin{proof}
To prove, we will make use of Theorem \ref{measequiv}. That means we have to show that $(f+g)^{-1}\left( (a, \infty]\right)$ is in $\mathscr{M}$ for all $a\in\mathbb{R}$. So let's start with another way to represent that set
\[
    (f+g)^{-1}\left( (a, \infty]\right)
    =\{x \; | \; f(x) + g(x) > a\} 
\]
But we can rewrite the expression as follows, rearranging terms a bit as we go:
\begin{align*}
    \{x \; | \; f(x) + g(x) > a\} &= 
        \{x \; | \; f(x) > a - g(x) \}  \\
    &= \bigcup_{r\in \mathbb{R}} 
        \left( \{ x \; | \: f(x) > r\}
        \cap \{ x \; | \; r > a - g(x) \}
        \right) \\
    &= \bigcup_{r\in \mathbb{R}} 
        \left( \{ x \; | \: f(x) > r\}
        \cap \{ x \; | \; g(x) > a - r \}
        \right) 
\end{align*}
Now we'll rewrite the intersection above, naming the sets in the intersection. On top of that, we'll also take the union over $\mathbb{Q}$ instead of $\mathbb{R}$. See the end of the proof for why we can do that, but in the meantime.
\begin{align*}
    \{x \; | \; f(x) + g(x) > a\} 
    &= \bigcup_{r\in \mathbb{Q}} 
        A_r \cap B_r  \\\\
    \text{where } \quad 
        A_r = \{ x \; | \: f(x) > r\} &\qquad
        B_r = \{ x \; | \; g(x) > a - r \}
\end{align*}
Now since $f$ is measurable and $g$ is measurable, we know that $A_r = f^{-1}\left( (r, \infty]\right)$ and $B_r = g^{-1}\left( (a-r, \infty])\right)$ will both be in $\mathscr{M}$. And thus, their intersection will be, and a countable union will be as well, implying that $f^{-1}\left( (a,\infty]\right)$, our starting set, is in $\mathscr{M}$. And so $f+g$ is measurable.

Oh, and that part about taking the union over $\mathbb{Q}$ instead of $\mathbb{R}$? We can do that for this reason. Suppose there's some $r \in \mathbb{R} \setminus \mathbb{Q}$ such that $f(x)>r$ and $r>a-g(x)$. Also suppose, without loss of generality, that $r>0$. 

Well then, since the rationals are dense in the reals, we know that there's a rational, call it $q$ between $r$ and $a-g(x)$ for all $f\in\mathbb{R}$. That means $f(x)>r>q$. It also means that $q>a-g(x)$. Thus, $r$ and $q$ accomplish the same task: we'll have \emph{both} $f(x)>n$ \emph{and} $n>a-g(x)$ whether $n=r$ or $n=q$. So we'll hit all the points we need to if we just take the union we saw above over $\mathbb{Q}$ instead of $\mathbb{R}$.
\end{proof}

\begin{cor}
This last proof shows that measurable functions for a \emph{vector space}, since they are closed under addition and scalar multiples.
\end{cor}

\begin{cor}
If $f$ and $g$ are measurable functions, then so is $fg$.
\end{cor}
\begin{proof}
Recall that we can write the product
\[ 
    fg = \frac{1}{2}\left[(f+g)^2 - f^2 - g^2\right]
\]
Then apply the theorems we had about sums and continuous compositions.
\end{proof}

\begin{cor}
If $\{f_n\}$ is a sequence of measurable unctions and $\sum^n_{i=1} f_n$ converges, then $\sum^n_{i=1} f_n$ is also measurable. 
\end{cor}
\begin{proof}
We know that $S_N = \sum^N_{i=1} f_n$ is also measurable by Theorem \ref{summeas}. Then, since $S_N\rightarrow F$, we know that $F$ is measurable too by Theorem \ref{convmeas}.
\end{proof}

\subsection{Simple Functions}

Before we get into the specifics of simple functions, note that we approximated the Riemann integral by (effectively) a step function over the intervals of $[a,b]$. In fact, we can cast an Riemann sum in the notation we'll see for simple functions:
    \[ S(f,P,T) = \sum^N_{i=1} c_i \chi_{I_i} \]
We'll do something exactly similar in this section.

\begin{defn}
Let $(X,\mathscr{M})$ be given. Also let $A=\{A_1, \ldots, A_N\}$ be a disjoint collection in $\mathscr{M}$. Then any function that can be written 
\[ 
    f = \sum^N_{i=1} c_i \cdot \chi_{A_i} \quad
    \text{ where } A_i = f^{-1}(c_i)
\]
is called a \emph{simple function}. Thus, $A$ is a \emph{measurable partition} of $X$. Said another way, simple functions have a finite range, $\{c_1, \ldots, c_N\}$.
\end{defn}

\begin{rmk} Simple functions form a vector space.
\end{rmk}

\begin{thm}
\label{lebapprox}
Let $f: X\rightarrow[0,\infty]$ be measurable. Then there exists a sequence $\{s_n\}^\infty_1$ of simple and monotonic functions 
\[
    0 \leq s_1 \leq s_2 \leq \cdots \leq f 
    \qquad \forall x\in X
\]
such that $s_n\rightarrow f$ (pointwise, or ``everywhere'') from below.
\end{thm}
\begin{proof}
We prove by successive approximation. Let's denote how we'll partition our intervals and then construct our approximation functions:
\begin{align*}
    [0,1) \cup [1,\infty] = [0,\infty] \quad&\quad  
        s_1 := 0\cdot \chi_{f^{-1}([0,1) )}
        + 1 \cdot \chi_{f^{-1}([1,\infty] )} \\
    [0,1/2) \cup [1/2,1] \cup [3/2, 2)
        \cup [2, \infty] = [0,\infty] \quad&\quad  
        s_2 := 0\cdot \chi_{f^{-1}([0,1/2) )}
        + \frac{1}{2} \chi_{f^{-1}([1/2,1] )} \\
    &\qquad
        + \cdots
        + 2 \cdot \chi_{f^{-1}([2,\infty] )} 
\end{align*}
So what are we doing here? Well, in the case of $s_1$, for all those points $x\in X$ that $f$ maps to $[0,1)$, we weight them by $0$, the minimum possible value $f(x)$ could take for $x\in f^{-1}([0,1))$. Similarly, for all points $x\in X$ that $f$ maps to $[1,\infty]$, we weight them by $1$, the minimum possible value \emph{they} could take.  

And so we see the basic logic here. Each $s_n$ splits the range of $f$, which is $[0,\infty]$ into two parts. The first is a finely approximated part, running from $[0,n]$ where we have many subdivisions of $[0,n]$. Second, there's a badly approximated part, where we just take the whole $[n,\infty]$. We pull these subdivisions of $[0,\infty]$ back into $X$, allowing us to partition the space $X$. Then, for each point in the pre-image $f^{-1}([a,b))$, we assume that it maps to the \emph{smallest possible} value it could, $a$.

And so we work out the process we saw above, first extending the finely approximated part of $f$'s range by one integer span with each $n$, but then halving the size of those fine subdivisions. We also continue to approximate $f$ with the minimum value, giving us
\[
    s_n = \left(\sum^{n\cdot 2^{n-1}}_{k=0}\frac{k}{2^{n-1}}
    \chi_{f^{-1}\left( \left[\frac{k}{2^{n-1}}, \;
        \frac{k+1}{2^{n-1}} \right)\right)}\right)
        + n \chi_{f^{-1}\left([n,\infty]\right)}
\]
Now it's clear that $s_n$ approaches $f$ from above, since with each $s_n$, we're taking the \emph{smallest possible} approximation of $f$. So we can never have $s_n>f$ for any $f$. It's also clear that $s_n\leq s_m$ for $m>n$ for the same reasons that a finer partition can only increase the value of a Lower Darboux sum.

Finally, we just wish to show that we, in fact, do have $s_n(x)\rightarrow f(x)$ for all $x\in X$. Let's take it case by case.
\begin{enumerate}
\item Suppose that $f(x)=\infty$. Since $x\in f^{-1}([n,\infty])$ for all $n$, we know that 
\[
    \lim_{n\rightarrow\infty} s_n(x) = \lim_{n\rightarrow\infty}
    n\chi_{f^{-1}([n,\infty])} = \lim_{n\rightarrow\infty}
    n = \infty = f(x)
\]
\item Next, suppose that $f(x) = 0$. Then $x\in f^{-1}([0,1/2^{n-1}))$ for all $n$, so 
\[
    \lim_{n\rightarrow\infty} s_n(x) = \lim_{n\rightarrow\infty}
    \frac{0}{2^{n-1}}\chi_{f^{-1}([0,1/2^{n-1}])} = 
    \lim_{n\rightarrow\infty} 0 = 0 = f(x)
\]
\item Finally, suppose $f(x)\in(0,\infty)$. Then, $N>f(x)$ for some $N$. That implies 
\[ 
    f(x) \in \left[\frac{k}{2^{N-1}}, \frac{k+1}{2^{N-1}}\right)
        \qquad
    \text{for some $k\in\{0,1\cdots,N\cdot 2^{N-1}\}$} 
\]
This, in turn, implies that
\[
    f(x) - S_N \leq \frac{1}{2^{N-1}}
    \quad \Rightarrow\quad
    \lim_{n\rightarrow\infty}  S_N  =
    f(x)
\]
\end{enumerate}

\end{proof}

\subsection{Integration of Non-Negative Measurable Functions}

\begin{defn}
Consider the measure space $(X,\mathscr{M},\mu)$, and let $f\geq0$ be a measurable function. Then define
\[
    \int_X f \; d\mu := \sup_{0\leq s\leq f} \int_X s \; d\mu
    \qquad \text{where}\qquad
    \int_X s \; d\mu := \sum^n_{i=1} c_i \; \mu(A_i)
\]
where the sup is taken over $s$, which denotes a simple function
\end{defn}

\begin{ex}
Suppose $f$ is, itself, a simple, measurable function. Then we just need to compute the following
\[
    \int_X f \; d\mu := \sum^n_{i=1} c_i \; \mu(A_i)
\]
I mention this to stress that the definition of the integral we gave above does, indeed, make sense. We see, in the case of a simple function, that we're just weighting the values that $f$ takes on by the measure over each set, $A_i$, in the partition of our domain. This is similar to what we did for the Riemann integral, only now, our $A_i$ partitioning the domain can be more arbitrary measurable sets, rather than more restrictive intervals.
\end{ex}

\begin{ex}
    Next, suppose that we have the measure space $(\mathbb{N},\mathscr{P}(\mathbb{N}), \mu)$, where $\mu$ is the counting measure. Then any function $f: \mathbb{N}\rightarrow[0,\infty]$  will be measurable, and functions are simply sequences, where we use $f=\{f(n)\}_1^\infty$ to denote any arbitrary function on $[0,\infty]$.\footnote{Recall that a sequence is a function from the natural numbers to some set ($[0,\infty]\subset\bar{\mathbb{R}}$, in this case).} I claim that the value of the integral can be written 
\begin{equation}
    \int_X f \; d\mu = \sum^\infty_{n=1}f(n) 
    \label{ex4.8.3.1}
\end{equation}
\begin{proof}
To prove this, we now consider a few cases for the integral of $f$:
\begin{enumerate}
    \item {\sl $f$ simple}: Start by unpacking the definition of the integral, and recall the previous example
    \begin{equation}
        \label{ex4.8.3.2}
        \int_X f \; d\mu := \sum^n_{i=1} c_i \; \mu(A_i)
    \end{equation}
    Now we consider whether Equation \ref{ex4.8.3.2} does indeed equal Equation \ref{ex4.8.3.1} given all the possible values \ref{ex4.8.3.2} could take on:
    \begin{enumerate}
        \item Suppose that Equation \ref{ex4.8.3.2} equals $\infty$. Then either there's a $c_i=\infty$ (in which case Equation \ref{ex4.8.3.1} would clearly be infinite as well) or there exists a $0< c_i<\infty$ such that $f = c_i$ at infinitely many points. Then $f(n)=c_i$ for infinitely many $n$, which implies that Equation \ref{ex4.8.3.1} equals $\infty$ as well.
        \item Suppose that Equation \ref{ex4.8.3.2} is less than $\infty$. Then by the same reasoning as we had before, $c_i<\infty$ for all $i$, and there must be only finitely many non-zero terms. In that case, we must also have an $N$ such that $f(n)=0$ for all $n>N$, in which case Equation \ref{ex4.8.3.1} equals Equation \ref{ex4.8.3.2}.
    \end{enumerate}

\item {\sl $f$ is any non-negative function on $\mathbb{N}$}: First, assume that $f$ is finite valued.\footnote{In the case where it is \emph{not} finite valued, the statement is trivially true.}  Then, start by letting $s_N = f\cdot\chi_{[1,N]}$. Because we're restricting the non-zero values of the function $s_N$ to a finite domain, it will have a finite range too: $\{f(n)\}$ for $n=\{1,\ldots,N\}$, and 0 for $n>N$. 
    
So clearly, $s_N$ is a simple function such that $0\leq s_N \leq f$ for all $N$, since we're just considering non-negative functions. That means
\begin{equation}
    \label{4.8.3tosub}
    \int_\mathbb{N} s_N \;d\mu \leq
    \int_\mathbb{N} f \;d\mu 
\end{equation}
since the latter is the sup over all simple functions with $0\leq s\leq f$. But we can simplify the integral of $s_N$, using the fact that Equation \ref{ex4.8.3.1} is true for simple functions like $s_N$ (which we proved in Case 1 of this exercise):
    \[
        \int_\mathbb{N} s_N \;d\mu = \sum^\infty_{n=1}s_N(n) 
        = \sum^N_{n=1} f(n)
    \]
Substituting this expression for the integral of $s_N$ into Equation \ref{4.8.3tosub}, and taking the limit, we get that 
\begin{equation}
    \label{toreverse}
    \sum^\infty_{n=1} f(n) \leq \int_\mathbb{N} f\; d\mu 
\end{equation}
Note that the righthand side is unaffected by the limit, since it doesn't depend upon $N$ at all.

Next, we need to show that we can reverse the inequality in Equation \ref{toreverse} to show equality as in Equation \ref{ex4.8.3.1}. To do so, suppose that $\sum^\infty_{n=1} f(n)<\infty$, otherwise there is nothing to prove. Then consider a simple function $s$ such that $0\leq s \leq f$. Then it's clear that 
\[
    \sum^\infty_{n=1} s(n)<\infty
\]
It's also clear that, since $s$ is simple, we can write it as
\[
    \int_\mathbb{N} s \; d\mu = \sum^\infty_{n=1} s(n)
    \leq \sum^\infty_{n=1} f(n)
\]
given the work we did above for the case where $f$ is simple. Finally, note that since $s$ was an arbitrary simple function, since the integral of $f$ involves taking the sup over all simple functions, and since the sum $\sum^\infty_{n=1}f(n)$ does not depend upon $s$, we can assert that
\[
    \int_\mathbb{N} f \; d\mu \leq \sum^\infty_{n=1} f(n)
\]
which, together with Equation \ref{toreverse} establishes Equation \ref{ex4.8.3.1} for arbitrary non-negative functions on $(\mathbb{N}, \mathscr{P}(\mathbb{N}), \mu)$.
\end{enumerate}
\end{proof}
\end{ex}


\subsection{Integration of Arbitrary Measurable Functions}

\begin{defn}
For any measurable function $f$ in $(X,\mathscr{M},\mu)$, define
\[
    f_+(x) := \max\{f(x), 0\}
    \qquad
    f_-(x) := -\min\{f(x), 0\}
\]
Because both $f$ and $0$ are measurable, the functions that equal the pointwise max and min will \emph{also} be measurable. Thus $f_+$ and $f_-$ are measurable.

Also note, by our definition, 
\begin{itemize}
    \item $f = (f_+) - (f_-)$ and $|f| = (f_+) + (f_-)$
    \item $f_+$ and $f_-$ will be non-negative \emph{everywhere}.  
\end{itemize}
These items will provide a link to the results from the previous subsection.
\end{defn}

\begin{defn}
Let $f$ be any \emph{arbitrary} measurable function (i.e. non neccessarily non-negative) on the measure space $(X,\mathscr{M},\mu)$. Then define 
\[
    \int_X f\; d\mu := \int_X f_+ \; d\mu - \int_X f_- \; d\mu
\]
provided that at least one is finite---otherwise the integral is undefined.

Moreover, we say that $f$ is integrable if \emph{both} $\int_X f_+ \; d\mu$ and $\int_X f_- \; d\mu$ are \textbf{finite}, in which case we say $f\in\mathscr{L}(X,\mathscr{M},\mu)$, the set of all integrable functions $f:X\rightarrow\mathbb{R}$.
\end{defn}


\subsection{($*$) Lebesgue Intuition}

What we did in the proof of Theorem \ref{lebapprox} is \emph{the key} to the Lebesgue Integral.  And it all has to do with how we partitioned $X$.

Namely, we partitioned $X$ in almost \emph{the complete opposite way} compared to what we did for Riemann Integrals. Instead of imposing that our partitions of $X$ will be intervals, we partitioned $X$ by what the points $x\in X$ \emph{get mapped to} by $f$. The sets of this partition could be crazy---things way more complicated that an interval $[x_{i-1}, x_i]$. This is because the $x\in f^{-1}([a,b))$ don't even have to be near each other. And that's key.

Instead of starting with a partition of $X$ into intervals and studying how a function $f$ behaves on those intervals, we look at the range of values $f$ can take, then pull it back to $X$.

\paragraph{Metaphor}
A wise man, Sam Kapon, in quoting Lebesgue himself, described the situation in the following way. Your coffee is rung up for \$1.74. Riemann starts taking successive coins out of his pocket one by one until he has enough to pay. He does so blindly, without any reference to their value (their $f(x)$) and to the fact that one coin could represent different amounts.

On the other hand, Mr. Lebesgue first takes all of the coins out of his pocket and surveys their value (their $f(x)$) before putting down a series of coins that makes sense \emph{given} their value. He pays in a way that makes a distinction between quarters, dimes, and nickels rather than lumping them all together simply as coins. 


%%%% APPPENDIX %%%%%%%%%%%

\newpage
\appendix
\section{Additional Definitions}

\subsection{Modulus of Continuity}

\begin{defn} We define the modulus of continuity of the function $f$, $w_f(\delta)$ as 
\begin{equation}
    w_f(\delta) \equiv \sup_{|x-y|\leq\delta} |f(x)-f(y)|
\end{equation}
\end{defn}
It's a useful definition because of it's close relation to uniform continuity.
It's clear that if $f$ is uniformly continuous, then $w_f\left(||P||\right)\rightarrow0$ as as $||P||\rightarrow0$.

\subsection{Lipschitz Condition}

A function $f$ is Lipshitz at $x$ if, for some $C$ and $\delta>0$, we have that 
\begin{align*}
    |x-y|\leq \delta \quad \Rightarrow \quad 
        |f(x) - f(y)|\leq C|x-y|
\end{align*}
This is a \emph{stronger} definition of uniform continuity in that Lipschitz implies uniformly continuous, but uniformly continuous does not imply Lipschitz.

\subsection{Cantor Set}

To construct the Cantor Set, start with the closed interval $[0,1]$. Next, remove the open middle third, $I_1 = (1/3, 2/3)$. This leaves us with 
    \[ C_1 = [0, 1/3] \cup [2/3, 1] \]
Continue this process so that the Cantor set is the limit of this process
\begin{align*}
    C_0 &= [0,1] \\
    C_1 &= [0,1/3] \cup [2/3, 1] \\
    C_2 &= [0,1/9] \cup [2/9, 3/9]  \cup [6/9, 7/9] \cup [8/9, 1]
        \\
    &\;\;\vdots 
\end{align*}
Then we let the Cantor Set $C$ equal
    \[ C = \bigcap^\infty_{n=1} C_n \]
    The set is \textbf{closed} because it is an infinite intersection of closed sets.
\\
\\
It is also \textbf{uncountable}. The proof will make use of the fact that when we remove the inner third, the endpoints are ``sticky''---they stay in the set $C$ and never get removed.
\begin{proof}
To see this, suppose that $C$ is countable. Then you can write the set as $C = \{c_1, c_2, \ldots\}$. If this is the case, then choose the subinterval in $C_1$ to which $c_1$ does not belong, either $[0,1/3]$ or $[2/3,1]$. Call it $[a_1, b_1]$. Next, remove the middle third from $[a_1, b_1]$, and choose the subinterval to which $c_2$ does not belong. Call this interval $[a_2, b_2]$. 

Now it's clear that at each stage, the subinterval we choose (after removing the inner third) is a subset of the previous subinterval. And so we have nested closed sets with $[a_m, b_m] \subset [a_n, b_n]$ for $m>n$. So when we take the infinite intersection, we get a nonempty set:
\[ 
    \bigcap^\infty_{n=1} [a_n, b_n] \supset \{a\} \neq 
    \emptyset\qquad \text{where} \quad 
    \lim_{n\rightarrow\infty} a_n = a
\]
Now clearly, $a_n\in C$ for all $n$ since the endpoints are ``sticky'' in $C$.  On top of that, $a\neq c_n$ for all $n$, since we chose the subsintervals explicity not to contain $c_n$ for all $n$.  But this is a contradiction since $a$ is a limit point of $\{a_n\}\subset C$, and the closed set $C$ should contain its limit points. And so $C$ is uncountable.
\end{proof}


\newpage
\section{Useful Tricks}

There are a number of useful ways to write different expressions, which are listed here:
\begin{itemize}
\item Often we know that addition, subtraction, and taking the absolute value of functions (which is a continuous operation) preserves certain properties. And so if we want to prove that the max or the min of two functions preserves certain properties, writing the max and min as follows often makes quick work of otherwise tedious proofs. These forms add or subtract the ``residual'' of one function relative to the other: 
\[
    \max \{f,g\} = \frac{f+g+|f-g|}{2}\qquad 
    \min\{f,g\} = \frac{f+g-|f-g|}{2}
\]

\item Building off the last point, we can write 
    \[ 
        |f|=\max\{f,-f\}
    \]
\end{itemize}

\newpage
\section{Quizzes}

\subsection{Quiz 1}

\subsubsection{Question 1}

Suppose that $f: [0,1]\rightarrow \mathbb{R}$ is bounded on $[0,1]$ and continuous on $(0,1)$. Without using Lebesgue's theorem, we want to show that $f\in\mathscr{R}([0,1])$.
\begin{proof}
Normally, we would use Theorem \ref{contthm}, but that requires continuity on $[0,1]$, and we only have it on $(0,1)$.
\\
\\
As a result, we use a standard analysis trick: bounding the parts that aren't well behaved, and using nice properties where the function \emph{is} well behaved.
\\
\\
And so to prove this, we want to show that there exists a $P$ such that $U(f,P)-L(f,P)<\varepsilon$ for any $\varepsilon>0$. Since $f$ is bounded, let $M = \sup_I |f(x)|$. Then, given $\varepsilon$, we set our partition so that $[x_0, x_1] = [0, \varepsilon/8M]$ and $[x_{n-1}, x_n] = [1-\varepsilon/8m, 1]$. This guarantees the contribution of this slice of the interval will be small.
\\
\\
Then, we know that the remaining portion from $[\varepsilon/8m, 1-\varepsilon/8m]$ will be continuous by assumption, so we can assert the nice property that there exists a partition for which the upper and lower sums over this interval will be $\varepsilon/2$ close.
\end{proof}

\subsubsection{Question 2}

Next, suppose that $f$ is improperly Riemann integrable on $[0,1]$. (Meaning that it blows up at some point, but it's okay). Define $F(x):=\int^x_0 f(t)\;dt$ and show that $F$ is uniformly continuous on $[0,1]$. Then, given an example where $F$ is not Lipschitz.

\begin{proof}
Proof by contradiction: Assume that $F$ is not uniformly continuos on $[0,1]$. Then there exists an $\varepsilon>0$ such that
\[
    |F(y) - F(x)| > \varepsilon \qquad
    \forall \; \delta >0 \quad \text{ s.t. } \quad 
    |x-y|< \delta
\]
But we can rewrite this as 
\[
    \left\lvert\int^y_x f(t)\;dt\right\rvert > \varepsilon \qquad
    \forall \; \delta >0 \quad \text{ s.t. } \quad 
    |x-y|< \delta
\]
Then the integral is bounded from below for all $\delta$, leading to problems.
\\
\\
As an example of a function $f$ where $F$ is not Lipschitz, take $f(x) = 1/\sqrt{x}$. It's improperly integrable, but blows up at zero, so that you can't say it's Lipschitz.
    
\end{proof}

\newpage
\section{Homework Checklist}

Because the TA is very\dots particular:
\begin{itemize}
    \item No contractions.
    \item Be careful about the word ``clearly.'' Use sparingly.
    \item Be explicit in notation. $M_i(\cdot)$ is not explicit.
    \item Check that equations are equations and not inequalities.
    \item Symmetry: $h(x) = \max\{f, 0\}$ isn't symmetric in arguments to the functions.
    \item Don't use the word ``formally.'' 
    \item Don't use $\forall$ or $\exists$ or s.t. or WLOG because reasons.
    \item Define variables and expressions \emph{always}. Say what $P$ is, even if it's clear that $P$ is a partition.
    \item Explain \emph{why} something is without loss of generality. Don't just say it.
    \item Don't use an em-dash.
\end{itemize}




%\cite{LabelInSourcesFile} 
%\citep{LabelInSourcesFile} Cites in parens
%\nocite{LabelInSourceFile} includes in refs w/o specific citation
%\bibliographystyle{apalike} 
%\bibliography{sources.bib} where sources.bib is file




\end{document}



%%%% INCLUDING FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%

   % H indicates here 
   %\begin{figure}[h!]
   %   \centering
   %   \includegraphics[scale=1]{file.pdf}
   %\end{figure}

%   \begin{figure}[h!]
%      \centering
%      \mbox{
%	 \subfigure{
%	    \includegraphics[scale=1]{file1.pdf}
%	 }\quad
%	 \subfigure{
%	    \includegraphics[scale=1]{file2.pdf} 
%	 }
%      }
%   \end{figure}
 

%%%%% Including Code %%%%%%%%%%%%%%%%%%%%%5
% \verbatiminput{file.ext}    % Includes verbatim text from the file
% \texttt{text}	  % includes text in courier, or code-like, font
